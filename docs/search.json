[{"path":"index.html","id":"section","chapter":"","heading":"","text":"Autor:Miguel Ángel Tarancón Morán.Catedrático de Economía Aplicada. Universidad de Castilla - La Mancha.","code":""},{"path":"prefacio.html","id":"prefacio","chapter":"Prefacio","heading":"Prefacio","text":"Este libro recoge diversas prácticas que se han ido desarrollando lo largo de multitud de cursos en varias asignaturas de grado y máster relacionadas con el análisis de datos, especialmente de tipo económico, en la Facultad de Derecho y Ciencias Sociales de Ciudad Real, tomando como motor de análisis el lenguaje R, y el IDE (Interfaz) R-Studio.Podría extenderme en por qué elegir R en lugar de otros lenguajes o software. En algún momento del capítulo introductorio hablo de la cuestión brevemente. Aquí, diré que R cuenta con una serie de ventajas: es un recurso de uso libre y gratuito (igual que R-Studio), orientado especialmente para el análisis estadístico (frente otros lenguajes realmente potentes, como Python), y cuenta con una comunidad muy activa que genera continuamente una gran variedad de algoritmos y código para el análisis estadístico de datos.Como desventaja, hay que reconocer que, al principio, la curva de aprendizaje puede tener una pendiente relativamente elevada. Pero, una vez familiarizados con el funcionamiento de R y su entorno, las posibilidades son casi infitas.Para hacer más llevadera esta etapa inicial de aprendizaje, o al menos más divertida, los ejemplos prácticos que se ofrecen se han extraído del proyecto R-Stars. Este proyecto consiste en la generación, mediante herramientas de inteligencia artificial, de una base de datos con múltiples variables, especialmente de tipo económico, que engloban 300 empresas ficticias. Pese ser todo una gran simulación, los datos mantienen las normas de coherencia económico-contables básicas, y cierta dosis de realismo (dentro de lo que cabe).Este sector es el del transporte interestelar de mercancías.Poco poco, este sector imaginario ha ido rodeándose de un contexto que, espero, sea divertido. El nombre de las empresas, planetas, etc. son un constante homenaje al cine de ciencia ficción.Por tanto, ruego que disculpéis por anticipado cierta orientación “friki” de los datos utilizados para aprender las diferentes técnicas. Pero bueno, esta afición por el cine fantástico tiene la culpa. Disfrutemos de ello, con imaginación y cierta (muchas) dosis de humor.Adicionalmente, hay secciones en algunos capítulos en que se os da acceso la realización de algunas de prácticas propuestas haciendo uso de varios paquetes creados ad-hoc (sistema MAT) para aquellos de vosotros que, cambio de una menor personalización y flexibilidad en el código, queráis realizar algunas acciones habituales y análisis de un modo más sencillo, con menos código (funciones creadas expresamente).Las secciones en las que se organizan los diferentes capítulos pueden ser de diversos tipos. Cada uno de ellos se identifica por medio de un pequeño icono:Tipo de Secciones.También, por supuesto, pido disculpas por los errores que inevitablemente encontraréis en el libro, que poco poco irán puliéndose (esperemos) tras constantes revisiones y mejoras. En este sentido, os agradeceré infinitamente que cualquier error que encontréis, ortográfico, de redacción, o de contenido o programación, lo comuniquéis.Para finalizar, quiero dar las gracias tantas y tantas personas y compañeros/que hacen posible la construcción y mejora constante del libro.Y gracias vosotros también, por usar (si lo hacéis) esta peculiar guía de R enfocada al análisis de datos económicos mediante herramientas estadísticas.Y ahora, vamos comenzar este viaje galáctico hasta adentrarnos en el corazón del propio lenguaje R.","code":""},{"path":"introducción..html","id":"introducción.","chapter":"1 Introducción.","heading":"1 Introducción.","text":"","code":""},{"path":"introducción..html","id":"llámalo-estadística.","chapter":"1 Introducción.","heading":"1.1  Llámalo Estadística.","text":"Todo el mundo habla de las estadísticas. Constantemente se les hace referencia en los medios de comunicación: todo está medido y estructurado por estos entes que convierten la realidad en una amalgama de números. Y más aún en el campo del comportamiento humano, es decir, en lo que conocemos como Ciencias Sociales. diario nos llegan las estadísticas sobre la intención de voto cuando hay unas elecciones, del crecimiento de la economía en términos del PIB, del comportamiento de los precios medido mediante el concepto de inflación…El secreto de la relevancia que les damos las estadísticas subyace que, de partida, suponen una forma sintética y objetiva de representar la realidad que nos rodea, de manera que podamos abarcar el conocimiento de tal realidad de un modo más o menos plausible. Y esta representación de la realidad es priori objetiva porque las estadísticas se elaboran siguiendo unas metodologías que se apoyan en un lenguaje universal: las matemáticas.Sí. El lenguaje matemático es un lenguaje que pueden entender todas las personas, tengan la procedencia que tengan, y sean de la condición que sean. Si necesitas comunicarte con casi cualquier persona del mundo, habla en inglés. Si necesitas comunicarte con cualquier persona del mundo, hazlo mediante las matemáticas, aunque sean matemáticas más o menos elementales.Por ello, las estadísticas se expresan en lenguaje matemático.Pero lo que comúnmente entendemos como estadísticas son más que unos resultados, unos outputs de la Estadística. La Estadística en realidad es algo mucho más complejo. Es una ciencia. Las estadísticas son construidas usando el método estadístico.Pero la Estadística se utiliza para muchas más cosas que para crear y publicar estadísticas.","code":""},{"path":"introducción..html","id":"concepto-de-estadística.","chapter":"1 Introducción.","heading":"1.1.1  Concepto de Estadística.","text":"El término “Estadística” proviene de la palabra latina status, “el Estado”, y fue acuñado por Achenwall mediados del siglo XVIII con el significado de “recogida, procesamiento y utilización de datos por parte del Estado”.Sin embargo, tal y como se entiende hoy en día, es decir, en el sentido de Ciencia Estadística, surgió como resultado de la integración de dos disciplinas: la Aritmética Política, en ese sentido de la cuantificación del Estado; y del Cálculo de Probabilidades, que nace en el siglo XVII como Teoría Matemática de los juegos de azar y que podríamos asociar al sentido de Estadística Matemática.Ciñéndonos pues este último sentido, lo largo de la Historia se han dado múltiples definiciones de Estadística. Fisher propone una definición quizá demasiado generalista al decir que la Ciencia Estadística es esencialmente una rama de las matemáticas aplicada los datos observados. Una reflexión que puede ayudar delimitar la definición de la Ciencia Estadística es la que realiza (Peña 1983) cuando realiza la siguiente reflexión:“La Estadística como disciplina científica ocupa un lugar muy singular en el conjunto de las ciencias. La Física, la Medicina o la Sociología tienen un área sustantiva de conocimiento y cuando utilizan modelos matemáticos, los subordinan al objeto principal de hacer avanzar el conocimiento en su parcela de estudio de la realidad. El objetivo de la Matemática, en contraposición, es ampliar la concepción y generalidad de sus propias herramientas analíticas, con absoluta independencia de la posible relación entre los entes matemáticos abstractos y los fenómenos reales. La Estadística participa de esos dos objetivos, aunque con rasgos muy peculiares. Su campo de estudio son los fenómenos aleatorios que están presentes, en mayor o menor medida, en toda actividad humana de adquisición de conocimiento empírico.”En este mismo sentido, (Martín-Pliego 2004) apunta:“La Estadística, por tanto, se configura como la tecnología del método científico que proporciona instrumentos para la toma de decisiones cuando éstas se adoptan en ambiente de incertidumbre, siempre que esa incertidumbre pueda ser medida en términos de probabilidad. Por ello, la Estadística se preocupa de los métodos de recogida y descripción de datos, así como de generar técnicas para el análisis de esta información.”En definitiva, la Estadística reúne tanto la concepción derivada de la Aritmética Política, entendida como recopilación sistemática de datos cara la descripción de la realidad (“hacer” estadísticas); como la concepción probabilística, entendida como la modelización de dicha realidad cuando está inscrita en un ambiente de incertidumbre, con el objeto de acotar dicha incertidumbre y servir de ayuda en la toma decisiones (representar matemáticamente el comportamiento de fenómenos sujetos incertidumbre, cuando contamos con datos que caracterizan esos fenómenos).","code":""},{"path":"introducción..html","id":"el-método-estadístico.","chapter":"1 Introducción.","heading":"1.1.2  El método estadístico.","text":"En cuanto al método seguido por la Ciencia Estadística, prima el razonamiento inductivo: las hipótesis que se plantean en la investigación implican propiedades observables en un conjunto de casos, cuyo análisis lleva formular hipótesis más generales, aplicables ya un conjunto mayor de casos. El método estadístico consiste, en definitiva, en sistematizar y organizar este procedimiento de aprendizaje que parte de lo particular para llegar lo general.En la aplicación del método estadístico podemos diferenciar una serie de etapas básicas que se exponen continuación:) Planteamiento del problema. Consiste en definir el objeto de la investigación, (¿qué quiero obtener? ¿dónde quiero llegar?), para lo cual debemos precisar la población de referencia y determinar las características que debemos observar y cómo serán recogidas. El resultado de esta fase es un sistema de características de interés observadas en un subconjunto de la población representativo de esta, al que llamamos muestra. Estas características se llamarán variables si están en escala métrica; o atributos, variables cualitativas o factores si están en escala -métrica (nominal u ordinal). Las variables toman valores para cada elemento o caso de la muestra. Los atributos adoptan una categoría o nivel para cada uno de los casos que integran la muestra. Según los objetivos planteados en la investigación, el tamaño de la muestra, tipo de características, etc., se podrá hacer una primera selección de los posibles tipos de técnicas y modelos estadíticos aplicar.b) Recogida y preparación de la información muestral. Los datos, que son los valores (en caso de trabajar con variables) o categorías o niveles (en el caso de trabajar con atributos o factores) que adoptan los distintos casos que constituyen la muestra en relación con las características de interés de la población; han de ser obtenidos de las fuentes disponibles. Estas fuentes pueden ser primarias, cuando somos los propios investigadores los que generamos los datos (través de la observación o la realización de encuestas), o secundarias, cuando estos datos ya han sido generados y/o recopilados por otros investigadores o instituciones. En cualquier caso, la muestra debe ser lo suficientemente amplia como para extraer conclusiones válidas para toda la población, y los datos deben ser de calidad, pues son la materia prima con la que trabajamos. Para ello, un requisito importante es que las fuentes de datos sean fiables.c) Depuración de los datos. Antes de utilizar los datos muestrales conviene aplicar un análisis descriptivo que permitirá detectar posibles inconsistencias en los datos identificando los valores anómalos, posibles errores, etc. En esta fase es clave tanto identificar las carencias de datos existentes (datos faltantes o missing data), como identificar aquellos elementos de la muestra que representan bien la población, puesto que presentan comportamientos extraños en alguna o algunas de las variables o atributos en estudio (casos atípicos u outliers).d) Aplicación de técnicas o modelos estadísticos para obtener resultados generalizables al conjunto de la población. Una vez se tienen claros los objetivos de la investigación y las características de la información muestral de la que se dispone (datos), y se han depurado convenientemente los datos, será el momento de plantear qué técnica o modelo estadístico aplicar. Aquí podemos distinguir, su vez, distintas subetapas.Por un lado, la aplicación correcta de ciertas técnicas o modelos de naturaleza inferencial, requiere del cumplimiento por parte de los datos de ciertos patrones de comportamiento (por ejemplo, el cumplimiento por parte de las variables de un comportamiento acorde con una Ley Normal). Así, deberán aplicarse una serie de pruebas para comprobar hasta qué punto los datos de partida cumplen con estos patrones.Por un lado, la aplicación correcta de ciertas técnicas o modelos de naturaleza inferencial, requiere del cumplimiento por parte de los datos de ciertos patrones de comportamiento (por ejemplo, el cumplimiento por parte de las variables de un comportamiento acorde con una Ley Normal). Así, deberán aplicarse una serie de pruebas para comprobar hasta qué punto los datos de partida cumplen con estos patrones.Tras superar el punto anterior, podrá aplicarse la técnica o modelo los datos para obtener los resultados que contribuyan cubrir los objetivos de la investigación (usualmente, esta etapa se corresponde con la de estimación del modelo estadístico aplicado).Tras superar el punto anterior, podrá aplicarse la técnica o modelo los datos para obtener los resultados que contribuyan cubrir los objetivos de la investigación (usualmente, esta etapa se corresponde con la de estimación del modelo estadístico aplicado).Por último, los resultados deben ser sometidos una subetapa de validación y contraste, en la que se valora hasta qué punto los resultados representan el comportamiento real de los casos estudiados (estudio de la bondad del modelo), y el grado de aptitud técnica del modelo, en el sentido de si el modelo estimado cumple con los requisitos que garantizan la calidad de los resultados (por ejemplo, si se cumplen ciertas hipótesis básicas que garanticen que los coeficientes estimados del modelo gozan de las mejores propiedades estadísticas, como insesgadez, eficiencia y consistencia).Por último, los resultados deben ser sometidos una subetapa de validación y contraste, en la que se valora hasta qué punto los resultados representan el comportamiento real de los casos estudiados (estudio de la bondad del modelo), y el grado de aptitud técnica del modelo, en el sentido de si el modelo estimado cumple con los requisitos que garantizan la calidad de los resultados (por ejemplo, si se cumplen ciertas hipótesis básicas que garanticen que los coeficientes estimados del modelo gozan de las mejores propiedades estadísticas, como insesgadez, eficiencia y consistencia).En esta etapa, además, se intentará simplificar el modelo, es decir, conseguir un modelo tan sencillo como sea posible, sin más parámetros de los necesariosy, que represente la realidad sin mucha pérdida de calidad con respecto otro modelo más complejo, o sea, ciñéndose al principio de parsimonia de la modelización.En esta etapa, además, se intentará simplificar el modelo, es decir, conseguir un modelo tan sencillo como sea posible, sin más parámetros de los necesariosy, que represente la realidad sin mucha pérdida de calidad con respecto otro modelo más complejo, o sea, ciñéndose al principio de parsimonia de la modelización.e) Crítica y diagnosis del modelo. Si una vez culminada la fase anterior se considera que el modelo es válido y técnicamente correcto, podrá ser adoptado para ayudar la toma de decisiones, mediante análisis estructural, realización de previsiones o planteamiento de simulaciones. En caso contrario, si el modelo se considera válido y/o correcto, deberemos reformular dicho modelo repitiendo las etapas anteriores hasta obtener un modelo que represente la realidad en estudio más adecuado.En definitiva, el método estadístico sigue el método científico en cuanto que tiene unas etapas bien delimitadas en las que se trata el conocimiento priori (teoría) para obtener un conocimiento posteriori, lo que pasa engrosar el cuerpo de la Ciencia.Es relevante destacar cómo, su vez, el método científico, al ser aplicado al resto de ciencias, y la propia Ciencia Estadística, recurre al método estadístico en su ejecución. Así, por ejemplo, en la etapa de recogida de evidencias observables (datos), fin de verificar las consecuencias o hipótesis que se desprenden de una teoría previa, la Estadística interviene tanto partir de la Teoría de Muestras como del Diseño de Experimentos para garantizar la validez y coherencia de los datos. En una fase posterior del método científico, se pasaría verificar la nueva teoría que se desprende de las hipótesis articuladas partir de la teoría preexistente. Nuevamente aquí interviene la Estadística como herramienta auxiliar, mediante la modelización inferencial. Además, en todo el proceso, que abarca tanto la observación de la realidad como la generalización de los resultados como modo de confirmar una nueva teoría, aparece la incertidumbre en mediciones y resultados, por lo que el papel de la Estadística como procedimiento para la medición de dicha incertidumbre es indispensable.De lo dicho se desprende una característica que hace de la Estadística una ciencia singular: su carácter de ciencia instrumental que auxilia al resto de ciencias en el desarrollo de sus cuerpos de conocimiento. De ahí que la Estadística es aplicada en la totalidad de las ciencias, bien sean naturales, jurídicas o sociales, y en todos los campos del saber, desde las áreas más técnicas hasta en las propias humanidades. Es decir, la Estadística es una herramienta fundamental en todo el proceso de adquisición de conocimientos través de datos empíricos y, desde este punto de vista, podemos referirnos la afirmación de (Mood 1963):“La Estadística es la tecnología del método científico”.Esta extensión de la Ciencia Estadística como ciencia auxiliar de otras ciencias, junto con su crecimiento y madurez metodológica, ha permitido el nacimiento de áreas con un cuerpo de conocimiento específico que pueden ser consideradas, su vez, como entidades con la categoría de ciencia, como pueden ser la Psicometría, la Estadística Económica y la Econometría[1]. Así, continuación, nos centraremos en la Estadística Económica, rama que ha ocupado un papel primordial en el desarrollo de la propia Ciencia Estadística desde el principio de sus orígenes.[1] En nuestra opinión, existe una delimitación clara entre Estadística Económica y Econometría, siendo la diferencia en todo caso un matiz dependiente de las técnicas y el enfoque empleado al enfrentarse un determinado estudio. Quizá ambas disciplinas pudieran englobarse en otra disciplina más general que podría ser llamada ‘Economía Cuantitativa’. Véase en relación con este respecto (Hernández-Alonso 2000).","code":""},{"path":"introducción..html","id":"economía-y-estadística.","chapter":"1 Introducción.","heading":"1.1.3  Economía y Estadística.","text":"La aplicación del método estadístico la Economía puede entenderse como el proceso de representación de los sistemas económicos, constituidos por los distintos agentes que operan en las economías, y las relaciones que los ligan. La Economía suele especificar dichas relaciones dándoles forma de teorías económicas. obstante, las teorías económicas con frecuencia son demasiado imprecisas la hora de plantear modelos económicos verificables. Como Paul Samuelson apunta ((Samuelson 2006)):“Solo en una muy pequeña parte de las obras de Economía teóricas o aplicadas se ha tratado la derivación de los teoremas significativos operacionalmente. En parte, por lo menos, tal situación se debe los malos preconceptos metodológicos, según los cuales, las leyes económicas deducidas de los supuestos priori poseen rigor y validez, independientemente de cualquier conducta humana real… De hecho, las obras de economía rebosan de malas generalizaciones.”La aplicación de los instrumentos estadísticos, y en concreto del Método Estadístico, permite dotar la Teoría Económica del grado de concreción necesario para verificar en los sistemas reales el cumplimiento y la validez de dichas teorías. Este proceso de representación de sistemas reales puede llegar tal grado de especificación que se puedan cuantificar las consecuencias en los cambios provocados en los elementos y relaciones del sistema ((Intriligator 1996), capítulo II). Sin embargo, por muy alto que sea el nivel de especificación del modelo que representa la realidad económica, este deberá llevar implícito cierta carga de abstracción de la realidad la que representa, para poder ser abarcable. La realidad económica, el sistema económico, supera necesariamente en complejidad cualquier modelo propuesto por la Teoría Económica, ya que el sistema económico depende, en última instancia, de fenómenos inmersos en cierto grado de incertidumbre; lo que es atribuible, su vez, su vinculación con el comportamiento humano. De este hecho se deduce la necesidad de incluir en la modelización de la realidad económica elementos estocásticos, lo que origina una visión determinista, sino probabilista de la realidad económica.Como señala (Martín-Pliego 2004), parte del conjunto de técnicas estadísticas aplicadas la investigación económica es común otras ciencias, mientras que otra parte es específica de este tipo de investigación, fruto de una evolución de la aplicación de la disciplina en el tratamiento de los temas económicos. Entre estas metodologías específicas se encuentran el estudio de las series temporales económicas, de la distribución de la renta, la construcción y análisis de números índices, la modelización regional, el análisis input-output e intersectorial, las técnicas demográficas e incluso, en nuestra opinión, la propia Econometría.","code":""},{"path":"introducción..html","id":"qué-es-r-y-cómo-nos-ayuda-a-analizar-datos-desde-el-punto-de-vista-estadístico","chapter":"1 Introducción.","heading":"1.2  ¿Qué es R y cómo nos ayuda a analizar datos desde el punto de vista estadístico?","text":"En los apartados anteriores hemos partido del concepto de Estadística como ciencia instrumental hasta llegar la Estadística Económica, como aquel cuerpo de la Ciencia Económica que se sirve de las herramientas que ofrece la Estadística para profundizar en el conocimiento de la realidad económica.Pero claro, lo interesante de esto es llevarlo la práctica. Se necesita un soporte de hardware y software para poder aplicar las técnicas estadísticas los datos económicos, con el objetivo de crear conocimiento partir de dichos datos. Este conocimeinto se traducirá en una reducción de la incertidumbre que inevitablemente viene aparejada los fenómenos económicos, lo que redundará en una mejor toma de decisiones.En los últimos tiempos se ha producido una evolución de hardware sin precedentes, lo que ha dado soporte al desarrollo de un potente software dedicado al análisis de datos (todo tipo de datos, solamente económicos). Este software permite cualquier investigador aplicar las últimas técnicas de análisis estadístico cualquier masa de datos, lo que ha supuesto una verdadera revolución. su vez, esta realidad se ha retroalimentado, de modo que se ha producido un constante avance en el desarrollo de técnicas y tecnologías de análisis de datos cada vez más complejas. Así, podemos hablar de técnicas de aprendizaje automático o machine learning (supervisado, -supervisado o reforzado) o, más recientemente, de modelos de análisis basados en la inteligencia artificial.En este caldo de cultivo, en el que se dispone de grandes masas de datos, de hardware capaz de procesarlas, y de técnicas capaces de extraer información de las mismas, se ha desarrollado un software cada vez más potente que une todos estos elementos para modelizar la realidad. Este software se concreta en aplicaciones y plataformas diversas: SPSS® Stata, SAS®… Y también lenguajes de programación orientados al análisis estadístico y matemático, como pueden ser Python, Matlab, Julia o… R.Sí. R es solo una aplicación al uso. Es todo un lenguaje de programación, orientado principalmente la analítica de datos, sobre todo desde una perspectiva estadística. R es un proyecto de GNU, por lo que los usuarios son libres de modificarlo y extenderlo. R se distribuye como software libre bajo la licencia GNU y es multiplataforma, lo que ha facilitado su difusión y la existencia de una comunidad muy activa de ususarios y desarrolladores.","code":""},{"path":"introducción..html","id":"instalación-de-r-y-r-studio.","chapter":"1 Introducción.","heading":"1.3  Instalación de R y R-Studio.","text":"Como ya se ha mencionado, R es un software o lenguaje de uso y difusión gratuitos, bajo licencia GNU. El modo de instalar R es sencillo: basta con ir la web CRAN (Comprehensive R Archive Network) y descargar la última versión disponible en el sistema operativo del que se sea usuario (en este manual, Microsoft® Windows®). Se ejecutará el archivo descargado, y se completará la instalación.Una limitación de R es la interfaz o IDE (entorno de desarrollo integrado) que incorpora. Es decir, el “software” con el que se interactúa con el lenguaje R. Esta IDE es muy poco amigable. Para superar esta limitación, existen IDEs alternativas, entre las que destaca RStudio, desarrollada por Posit® Software. Esta IDE es gratuita. De nuevo, simplemente tendremos que ir la web deRStudio y descargar e instalar la versión gratuita.","code":""},{"path":"introducción..html","id":"r-y-rstudio.-comienzo-proyectos.","chapter":"1 Introducción.","heading":"1.4  R y RStudio. Comienzo: Proyectos.","text":"Tras instalar R y su IDE RStudio, podremos comenzar trabajar. Para ello, abriremos RStudio pulsando en el icono correspondiente. Aparecerá la siguiente ventana:La parte izquierda de la ventana es la consola. La consola es la sección de RStudio donde podemos manejar R mediante la introducción de código. Por ejemplo, podemos escribir 2+2 después del cursor (signo “>”), y pulsar Enter. La propia consola nos devolverá el valor 4:De todos modos, la forma más eficiente de trabajar es mediante “proyectos” y “scripts”.Un proyecto básicamente viene asociado la carpeta donde R trabajará, buscando los datos que sean sus “inputs”, y, en su caso, enviando sus resultados u “outputs”. Dicho de otro modo, es una carpeta más de nuestro sistema de carpetas o directorios; pero la que dotamos de una característica especial: ser un proyecto de R. Si abrimos desde RStudio el proyecto, estaremos diciendo R que, por defecto, preferentemente busque todos los archivos e inputs (datos, etc.) que necesite en esa carpeta de proyecto; y que, en su caso, guarde en tal carpeta los outputs que genere.Para crear un nuevo proyecto, seguiremos la instrucción File → New Project, luego se nos preguntará si se crea el proyecto en una nueva carpeta o en una ya existente. Vamos crearlo, por ejemplo, en el disco extraíble D, carpeta R, subcarpeta “explora”, que ya está creada. Nos saldrá una ventana para buscar la carpeta y, cuando la encontremos, pulsaremos Open y Create Project. Ya tendremos creado nuestro proyecto. Si nos vamos al explorador de Windows®, y buscamos la carpeta “explora”, encontraremos que en tal carpeta aparece un archivo de nombre “explora”, con un icono de un cubo con una “R”. Ese archivo lo que está haciendo es actuar como un “faro” que le dice R que, cuando trabajemos en el proyecto “explora”, todos los archivos de datos necesarios estarán en esa carpeta (también llamada “explora”, porque el proyecto adopta el nombre de la carpeta donde lo localizamos). Y que, si nuestro trabajo aporta algún fichero de “output”, también se depositará en esa carpeta del proyecto.En futuras sesiones, si queremos trabajar en el mismo proyecto, en lugar de seguir la ruta File → New Project, tendremos que hacer File → Open Project.","code":"\n2+2## [1] 4"},{"path":"introducción..html","id":"scripts.","chapter":"1 Introducción.","heading":"1.5  Scripts.","text":"En cuanto los scripts, son programas o rutinas donde varias instrucciones se ejecutan secuencialmente. Para crear un script, se seguirá la ruta File → New File → R Script. Y si el script lo guardamos, ¿dónde lo hará? Pues en la carpeta “explora”, que es la del proyecto en el que estamos trabajando.Informáticamente, un script es simplemente un archivo de texto plano. Se puede modificar con cualquier editor de texto. Afortunadamente, para estar entrando y saliendo de R-Studio, esta interfaz incorpora un editor de scripts, lo cual es muy cómodo.Vemos cómo ahora, la izquierda de RStudio, ha aparecido, en la parte superior, una nueva ventana, pasando la consola ocupar la parte inferior. Es la ventana del “editor”:Igual que con los proyectos, podemos crear desde RStudio un script nuevo, o abrir uno preexistente; y modificarlo, ejecutarlo, o volverlo guardar.Vamos comenzar escribir nuestro script. Si queremos hacer un comentario que ejecute ninguna instrucción, éste irá precedido del símbolo almohadilla o hashtag “#”. Luego, vamos ordenar R que haga la operación de suma: 2+2. Escribimos, por tanto, en el editor:Si pulsamos Control + Mayúsculas + ENTER o al desplegable de Source → Source Echo, se ejecutará el script (para ejecutar solo la línea donde está el cursor, pulsaremos Control + ENTER o el botón de Run; y para ejecutar varias líneas, hemos de sombrearlas y pulsar Control + ENTER o el botón de Run). En la consola aparecerá:Podemos guardar el script con File → Save … ¿Dónde se guardará por defecto? Pues en la carpeta “explora”, que es la de nuestro proyecto. Una vez nuestro script ya tiene nombre, podemos ir guardándolo de vez en cuando pulsando simplemente en el botón del “disquete” del editor. Vamos llamarlo, por ejemplo, “explorando”. Si vamos, en el explorador de Windows®, nuestra carpeta de proyecto, veremos que hay un archivo de texto llamado “explorando” con extensión “.R” (explorando.R). Este script lo podremos ejecutar cuantas veces queramos sin tener que escribir nada, o reescribirlo si vemos que funciona o que necesitamos hacer modificaciones. Esa es la ventaja de trabajar con scripts.Para recuperar un script en una nueva sesión de trabajo simplemente tenemos que seguir las instrucciones File → Open File… y seleccionarlo.","code":"\n#Ejemplo de Script\n2+2  #este script hace una simple suma.## [1] 4"},{"path":"introducción..html","id":"funciones.","chapter":"1 Introducción.","heading":"1.6  Funciones.","text":"R trabaja con datos y funciones, principalmente. Pero, ¿qué es una función?Una función es un conjunto o sistema de instrucciones que convierten unos datos de entrada o inputs en otros datos de salida, resultados, u outputs. Una función puede ser muy sencilla o ser verdaderamente compleja. Por otro lado, todas las funciones están integradas en “paquetes”; sino que el usuario puede crear sus propias funciones (por ejemplo, escribiéndolas en un script) y ejecutarlas.Las partes básicas de una función son:Entradas, inputs o argumentos: son las diversas informaciones necesarias para realizar el procedimiento de la función. Los argumentos pueden ser introducidos por el usuario, o pueden venir dados por defecto, lo que quiere decir que, si el usuario dota de valor un argumento, este tomará automáticamente un valor prestablecido.Entradas, inputs o argumentos: son las diversas informaciones necesarias para realizar el procedimiento de la función. Los argumentos pueden ser introducidos por el usuario, o pueden venir dados por defecto, lo que quiere decir que, si el usuario dota de valor un argumento, este tomará automáticamente un valor prestablecido.Cuerpo: está formado por un conjunto de instrucciones que transforman los inputs o entradas en los outputs o salidas. Si el cuerpo de la función está formado por varias instrucciones, éstas deben escribirse entre llaves { }.Cuerpo: está formado por un conjunto de instrucciones que transforman los inputs o entradas en los outputs o salidas. Si el cuerpo de la función está formado por varias instrucciones, éstas deben escribirse entre llaves { }.Salidas: son los resultados u output de la función. Si una función ofrece como salida varios tipos de objetos, estos objetos suelen ser almacenados en una estructura de almacenaje de lista.Salidas: son los resultados u output de la función. Si una función ofrece como salida varios tipos de objetos, estos objetos suelen ser almacenados en una estructura de almacenaje de lista.Como ejemplo, vamos integrar en nuestro script una función, llamada “suma”. Esta función requerirá de dos entradas o argumentos (dos números cualesquiera), y ofrecerá, como resultado, salida u output; la suma de tales entradas. El código es:Ahora, una vez ejecutado el código anterior; si queremos sumar, por ejemplo, los números 12 y 16, solo tendremos que teclear en la consola, o escribir en el script y hacer run, la línea:","code":"\nsuma <- function(x, y) {\n  resultado <- x + y\n  return(resultado)\n}\nsuma(x=12, y=16)## [1] 28"},{"path":"introducción..html","id":"paquetes-packages.","chapter":"1 Introducción.","heading":"1.7  Paquetes (packages).","text":"R es un lenguaje de programación en torno al cual se ha desarrollado una cantidad casi inimaginable de recursos: funciones, bases de datos, utilidades… Tal es la cantidad de recursos, que sería operativo abrir R (directamente, o través de una IDE, como RStudio) y tener inmediatamente todos esos recursos activos y preparados para ser utilizados. Además, R debería ser actualizado de un modo casi constante.Por todo ello, todos los recursos disponibles están organizados mediante “paquetes” (“packages” en inglés). Un paquete es una colección de funciones y/o un conjunto de datos desarrollados por la comunidad de R. Estos incrementan el potencial de R ampliando sus capacidades básicas, o añadiendo otras nuevas.De hecho, cuando abrimos R, algunos de estos paquetes, que se han instalado junto al propio lenguaje, se activan. Pero solo algunos. Un ejemplo es el paquete {base} o el paquete {stats} (R Core Team 2025).La mayor parte de los paquetes disponibles forman parte, por “defecto”, en la misma instalación de R. Se encuentran en diversos servidores llamados repositorios. El más importante, es CRAN, que es el “repositorio oficial” y que alberga más de 10.000 paquetes. Pero existen otros repositorios, destacar, por ejemplo, GitHub.Para instalar un paquete en nuestra máquina que esté albergado en CRAN, un modo sencillo es, dentro de R-Studio, pulsar en la ventana inferior / izquierda sobre la pestaña “Packages”, y sobre el botón “Install”. Emergerá entonces una ventana donde hay un campo para escribir el nombre del paquete (al comenzar escribirlo, el propio R-Studio te sugerirá los paquetes disponibles). Esto equivale usar (bien directamente en la consola, o bien como línea de código insertada en un script) la instrucción install.packages(), con el nombre del paquete entre comillas (si son varios, pues irán separados por comas.Una vez se tiene instalado el paquete, ya habrá que volver instalarlo para utilizarlo; sino activarlo. De hecho, todos los paquetes que se encuentran por defecto en la propia instalación de R, deben ser activados para poder usar sus funcionalidades y/o datos. Para hacerlo, se debe utilizar la instrucción library(), y el nombre del paquete dentro del paréntesis.Del nombre de esta instrucción surge la confusión común de tomar como sinónimos las palabras “paquete” y “librería” en el entorno de R. Si nos referimos estas colecciones de funcionalidades y/o datos; lo correcto es “paquete”, ya que “librería” tiene más que ver con la organización informática de un software.","code":""},{"path":"introducción..html","id":"help-sistema-de-ayuda.","chapter":"1 Introducción.","heading":"1.8  Help! (sistema de ayuda).","text":"veces podemos albergar dudas sobre la correcta utilización de las funcionalidades y herramientas que nos proporciona un paquete. Hay varias fuentes de ayuda para intentar encontrar respuesta las cuestiones que se nos plantean.Una opción, para obtener información general sobre un paquete, es utilizar la función help(), con el argumento “package”. Por ejemplo:Observaremos como en la ventana inferior / izquierda de R-Studio nos saldrá la información correspondiente. De hecho, en tal ventana existe una pestaña “Help” para obtener la ayuda sin teclear código.Además, cada función puede ser consultada individualmente mediante help(\"nombre de la función\") o help(function, package = \"package\") si el paquete ha sido cargado. Estas instrucciones nos mostrarán la descripción de la función y sus argumentos acompañados de ejemplos de utilización. Por ejemplo:La instrucción anterior nos aporta la documentación sobre la función rm() del paquete {base} de R (nota: este paquete se activa por defecto al abrir R o R-Studio; por lo que el segundo argumento, con el nombre del paquete que contiene la instrucción es necesario).Otra opción para mostrar información de ayuda es la exploración de las “viñetas” (vignettes). Las viñetas son documentos que muestran de un modo más detallado las funcionalidades de un paquete. La información de las viñetas de un paquete están disponibles en el archivo “documentation”. Puede obtenerse una lista de las viñetas de nuestros paquetes instalados con la función browseVignettes(). Si solo queremos consultar las viñetas de un paquete concreto pasaremos como argumento la función el nombre del mismo: browseVignettes(package = \"packagename\"). En ambos casos, una ventana del navegador se abrirá para que podamos fácilmente explorar el documento.Si optamos por permanecer en la consola, la instrucción vignette() nos mostrará una lista de viñetas, vignette(package = \"packagename\") las viñetas incluidas en el paquete, y una vez identificada la viñeta de interés podremos consultarla mediante vignette(\"vignettename\").","code":"\nhelp(package=\"base\")\nhelp(\"rm\", package=\"base\")"},{"path":"introducción..html","id":"origen-y-evolución-del-transporte-interestelar-de-mercancías-una-historia-del-sector-2210.","chapter":"1 Introducción.","heading":"1.9  Origen y Evolución del Transporte Interestelar de Mercancías: Una Historia del Sector (2210).","text":"El transporte interestelar de mercancías nació oficialmente mediados del siglo XXI, en un contexto marcado por la exploración espacial comercial y la necesidad de recursos más allá de la Tierra. El primer hito significativo fue la explotación minera de asteroides cercanos, liderada por corporaciones como SpaceX y Blue Origin, que sentaron las bases de un incipiente comercio más allá del Sistema Solar. Sin embargo, fue en 2084 cuando el sector dio un salto cuántico: el descubrimiento del plasma warp permitió velocidades cercanas la de la luz, abriendo rutas hacia sistemas estelares vecinos y revolucionando la logística galáctica.","code":""},{"path":"introducción..html","id":"la-expansión-del-sector.","chapter":"1 Introducción.","heading":"1.9.1  La Expansión del Sector.","text":"principios del siglo XXII, la colonización de planetas como Pandora (Vía Láctea) y Tatooine (Andrómeda) impulsó la creación de bases interestelares. Este desarrollo solo conectó galaxias distantes, sino que también fomentó el surgimiento de empresas especializadas en transporte de mercancías, como Arrakis Freight y Vader & Company. Estas compañías diseñaron naves capaces de transportar cargamentos masivos, como minerales raros de Arrakis o componentes biotecnológicos de Fhloston Paradise, distancias inimaginables.En paralelo, el Tratado de Libre Comercio Interestelar (TLCI) de 2135 formalizó las reglas para el intercambio entre galaxias. Este acuerdo facilitó el crecimiento exponencial del sector al garantizar rutas seguras entre la Vía Láctea, Andrómeda y más allá, permitiendo empresas como Nova Haulers y ExoCargo Alliance expandir sus operaciones.","code":""},{"path":"introducción..html","id":"cifras-globales-del-sector-en-2210.","chapter":"1 Introducción.","heading":"1.9.2  Cifras globales del sector en 2210.","text":"Cifras Globales del Sector.Estos datos muestran un sector altamente rentable y con una fuerte estabilidad financiera, con márgenes elevados y una solvencia promedio sólida. La distancia media de las rutas operadas confirma la expansión del comercio interestelar través de vastas regiones del universo conocido.","code":""},{"path":"introducción..html","id":"empresas-líderes-en-el-transporte-interestelar-de-mercancías-en-2210.","chapter":"1 Introducción.","heading":"1.9.3 Empresas Líderes en el Transporte Interestelar de Mercancías en 2210.","text":"En el competitivo sector del transporte interestelar de mercancías, destacan varias empresas que, por su solidez financiera, innovación tecnológica y alcance operativo, dominan el mercado. continuación, analizamos algunas de las más relevantes basándonos en los datos más recientes de la industria.1. Arrakis Freight (Gran Nube de Magallanes).Arrakis Freight es una de las empresas más rentables y con mayor solidez del sector. Su alta solvencia le permite operar con una gran estabilidad financiera, minimizando riesgos. Además, su margen del 51.42% la convierte en una compañía altamente eficiente en costos operativos. Gracias su origen en Arrakis, una fuente clave de especias y minerales raros, ha consolidado su posición como un actor clave en la Gran Nube de Magallanes.2. Hyperdrive Express (Gran Nube de Magallanes).Hyperdrive Express se distingue por su altísimo margen de rentabilidad, reflejando su eficiencia operativa y control de costos. Con sede en Miranda, su especialización en el transporte de bienes tecnológicos la ha convertido en una empresa de referencia dentro de la Gran Nube de Magallanes. Su fuerte inversión en innovación y digitalización la mantiene en la vanguardia del sector.3. Excalibur Freight Systems (Gran Nube de Magallanes).Excalibur Freight Systems es una de las empresas más rentables de toda la industria. Con un margen del 77.25%, logra obtener grandes beneficios por cada unidad transportada. Su índice de diversificación es alto, lo que le permite operar en múltiples sectores, desde el comercio de biotecnología hasta el transporte de bienes de lujo. Su alto beneficio medio por año luz es una muestra de su extrema eficiencia operativa.4. Kaiju Haulage Co. (Galaxia de Andrómeda).Desde su base en Mustafar, Kaiju Haulage Co. se ha consolidado como una de las mayores transportistas en la Galaxia de Andrómeda. Su enfoque en rutas de larga distancia y comercio de materiales pesados la hace indispensable en las cadenas de suministro intergalácticas. Su solvencia extremadamente alta la posiciona como una de las empresas más estables financieramente en el sector.","code":""},{"path":"introducción..html","id":"análisis-dafo-del-sector.","chapter":"1 Introducción.","heading":"1.9.4 Análisis DAFO del sector.","text":"Fortalezas del Sector.Tecnología avanzada: El uso de reactores warp y escudos de energía ha reducido significativamente los tiempos y riesgos de transporte.Tecnología avanzada: El uso de reactores warp y escudos de energía ha reducido significativamente los tiempos y riesgos de transporte.Diversidad de mercados: La expansión múltiples galaxias ha diversificado los bienes transportados, desde minerales raros hasta biotecnología avanzada.Diversidad de mercados: La expansión múltiples galaxias ha diversificado los bienes transportados, desde minerales raros hasta biotecnología avanzada.Colaboración intergaláctica: Acuerdos como el TLCI aseguran rutas seguras y relaciones comerciales estables.Colaboración intergaláctica: Acuerdos como el TLCI aseguran rutas seguras y relaciones comerciales estables.Debilidades del Sector.Dependencia tecnológica: Las interrupciones en la producción de plasma warp podrían paralizar el comercio interestelar.Dependencia tecnológica: Las interrupciones en la producción de plasma warp podrían paralizar el comercio interestelar.Costos operativos elevados: El mantenimiento de naves y estaciones en galaxias remotas sigue siendo prohibitivo para nuevas empresas.Costos operativos elevados: El mantenimiento de naves y estaciones en galaxias remotas sigue siendo prohibitivo para nuevas empresas.Oportunidades.Nuevas rutas comerciales: La exploración de la Galaxia de Cetus y la Nube Circungaláctica abre oportunidades para expandir las operaciones.Nuevas rutas comerciales: La exploración de la Galaxia de Cetus y la Nube Circungaláctica abre oportunidades para expandir las operaciones.Innovación en biotecnología: Los avances en bioingeniería permiten transportar organismos vivos de forma segura, ampliando mercados.Innovación en biotecnología: Los avances en bioingeniería permiten transportar organismos vivos de forma segura, ampliando mercados.Riesgos.Piratería espacial: Sistemas como Klendathu y Miller son zonas rojas donde el contrabando y el saqueo afectan el comercio legítimo.Piratería espacial: Sistemas como Klendathu y Miller son zonas rojas donde el contrabando y el saqueo afectan el comercio legítimo.Inestabilidad política: Conflictos en planetas estratégicos como Mustafar y Nueva Caprica ponen en riesgo las rutas clave.Inestabilidad política: Conflictos en planetas estratégicos como Mustafar y Nueva Caprica ponen en riesgo las rutas clave.Cambio climático galáctico: Fenómenos como tormentas de radiación en el Cinturón de Magallanes afectan la navegación segura.Cambio climático galáctico: Fenómenos como tormentas de radiación en el Cinturón de Magallanes afectan la navegación segura.Grandes Retos Pendientes.Estabilidad energética: Encontrar fuentes de energía alternativas al plasma warp para reducir costos y mitigar riesgos.Estabilidad energética: Encontrar fuentes de energía alternativas al plasma warp para reducir costos y mitigar riesgos.Regulación interestelar: Crear una autoridad galáctica que estandarice leyes de comercio, impuestos y responsabilidad.Regulación interestelar: Crear una autoridad galáctica que estandarice leyes de comercio, impuestos y responsabilidad.Sostenibilidad ambiental: Minimizar el impacto de la actividad industrial en planetas habitados y proteger ecosistemas únicos como los de Naboo y Fhloston Paradise.Sostenibilidad ambiental: Minimizar el impacto de la actividad industrial en planetas habitados y proteger ecosistemas únicos como los de Naboo y Fhloston Paradise.El sector del transporte interestelar de mercancías, pesar de sus desafíos, sigue siendo el motor económico de la civilización galáctica en 2210. Su capacidad para conectar mundos y galaxias garantiza su relevancia en los siglos venideros, siempre que pueda adaptarse un universo en constante cambio.","code":""},{"path":"almacenando-y-manipulando-datos..html","id":"almacenando-y-manipulando-datos.","chapter":"2 Almacenando y manipulando datos.","heading":"2 Almacenando y manipulando datos.","text":"","code":""},{"path":"almacenando-y-manipulando-datos..html","id":"objetos.-datos.","chapter":"2 Almacenando y manipulando datos.","heading":"2.1  Objetos. Datos.","text":"Como vimos en el capítulo 1, tras ejecutar un sencillo script (o al escribir instrucciones directamente desde la consola), R es interactivo: responde las entradas que recibe. Las entradas o expresiones pueden ser, básicamente:Expresiones aritméticas.Expresiones aritméticas.Expresiones lógicas.Expresiones lógicas.Llamadas funciones.Llamadas funciones.Asignaciones.Asignaciones.Las expresiones realizan acciones sobre objetos de R. Los objetos en R son entes que tienen ciertas características, metadatos, llamados atributos. todos los objetos tienen los mismos atributos y, ni tan siquiera, todos los objetos tienen atributos que los caractericen.Los objetos más importantes en R son ciertas estructuras o contenedores diseñados para almacenar elementos:Vectores.Vectores.Matrices.Matrices.Listas.Listas.Data frames.Data frames.Factores.Factores.Los elementos almacenados en los objetos se dividen en clases. Entre las diferentes clases, destacan las clases referidas datos, que pueden ser de diferentes modos: logical (verdadero/falso), numeric (números) o character (cadena de texto). El modo numeric puede ser, la vez, de tipo integer (número entero) o double (número real). En el caso de logical y carácter, modo y tipo coinciden.Vamos profundizar un poco en algunas de estos contenedores de datos. Vamos suponer que trabajamos en el proyecto que creamos en el capítulo anterior (proyecto “explora”), y que vamos editar el script que también creamos en tal capítulo (script “explorando.R”, que se encontrará ubicado en la carpeta del proyecto “explora”).","code":""},{"path":"almacenando-y-manipulando-datos..html","id":"vectores.","chapter":"2 Almacenando y manipulando datos.","heading":"2.1.1  Vectores.","text":"Los vectores, son conjuntos de elementos de la misma clase. Vamos definir por ejemplo el vector x = (1,3,5,8). Para ello, vamos escribir en nuestro script:Ejecutamos la línea (situando el cursor en algún lugar de ella, dentro del script; y pulsando la vez las teclas Control + Enter o pinchando con el ratón en el botón Run del editor). Ya tenemos nuestro primer objeto de tipo vector en memoria. Por cierto, lo que hemos hecho es una asignación, que se escribe con una flecha creada mediante los signos “<” y “-”. Hemos asignado un vector llamado “x” los elementos 1, 3, 5 y 8.Para ver el vector simplemente escribimos en la consola (o en el script) el nombre del vector, “x”. El resultado será:Además, si miramos en la ventana superior-derecha de R-Studio, veremos que en el Global Environment se muestra nuestro vector y que, además, se nos informa de que tiene modo numérico. El Global Environment nos informa de los objetos que R tiene en memoria:Si queremos obtener un vector de números consecutivos del 2 al 6, basta con ejecutar en la “consola” (o escribir y ejecutar en el script):Al escribir el nombre del vector “y” en la “consola” obtendremos:Si queremos saber la longitud de un vector, usaremos la función length(). Por ejemplo, length(y) nos devolverá el valor 5. Escribamos en el script y ejecutemos:Un vector puede incluir, además de números, caracteres o grupos de caracteres alfanuméricos; siempre entrecomillados (lo fundamental es que sean elementos de la misma clase). Por ejemplo, el vector “genero” (¡pongamos tildes o podemos tener problemas!). Así, si ejecutamos estas dos líneas de código:Se habrá creado el vector “genero”:Podemos obtener la clase de los elementos almacenados en nuestro vector con la función class():Si falta un dato en un vector, habrá que escribir “NA” (available). Por ejemplo, si falta el tercer dato de este vector “z”, este vector se escribirá como:Para seleccionar un elemento concreto de un vector, indicaremos entre corchetes la posición en la que se encuentra. Por ejemplo, refiriéndonos al vector “x”, para obtener el valor de su tercer elemento, haremos:Si queremos que se nos muestren los elementos del vector x del 2º al 4º:Por último, si queremos sacar en pantalla los elementos 1º y 4º, tendremos que incluir una “c” seguida de un paréntesis que recoja el orden de los elementos que queremos seleccionar:","code":"\nx <- c(1,3,5,8)## [1] 1 3 5 8\ny <- c(2:6)\ny## [1] 2 3 4 5 6\nlength(y)## [1] 5\ngenero<-c(\"Mujer\",\"Hombre\")\ngenero## [1] \"Mujer\"  \"Hombre\"\nclass(genero)## [1] \"character\"\nZ <- c(1,2,NA,2,8)\nx[3]## [1] 5\nx[2:4]## [1] 3 5 8\nx[c(1,4)]## [1] 1 8"},{"path":"almacenando-y-manipulando-datos..html","id":"matrices.","chapter":"2 Almacenando y manipulando datos.","heading":"2.1.2  Matrices.","text":"Las matrices, internamente en R, son vectores; pero con dos atributos adicionales: número de filas y número de columnas. Se definen mediante la función matrix(). Por ejemplo, para definir la matriz “”:\\[\n\\begin{pmatrix}\n    1 & 4 & 7 \\\\\n    2 & 5 & 8 \\\\\n    3 & 6 & 9\n\\end{pmatrix}\n\\] Tendremos que escribir:El número de filas de la matriz (y por tanto, el número de columnas) se fija con el argumento nrow = . También podríamos fijar el número de columnas, con ncol = .Como vemos, por defecto, R va “cortando” el vector por columnas (si lo preferimos, lo puede hacer también por filas, añadiendo la función matrix() el argumento row = true; pero, en nuestro ejemplo, obtendríamos la matriz traspuesta la que queremos almacenar).Las dimensiones (número de filas y de columnas) de la matriz pueden obtenerse mediante la función dim():3 filas y 3 columnas.Si queremos seleccionar elementos concretos de una matriz, lo haremos utilizando corchetes para indicar filas y columnas. Hemos de tener en cuenta que, trabajando con matrices, siempre tenemos \\[rango de filas, rango de columnas\\] Si se deja en blanco el espacio entre el corchete inicial y la coma, esto querrá decir que consideramos todas las filas. Y si insertamos nada entre la coma y el corchete de cierre, esto significará que consideramos todas las columnas. continuación tenemos varios ejemplos de código, con el resultado obtenido en la consola:Tanto para vectores como para matrices, funcionan las operaciones suma y diferencia sin más complicaciones. En el caso del producto, sin embargo, hay que tener en cuenta que, por ejemplo, *devuelve la multiplicación elemento elemento, es decir:Devuelve la multiplicación elemento elemento (en este caso, el cuadrado de cada número, al multiplicar la matriz por sí misma):Para hacer el verdadero producto matricial deberá introducirse:","code":"\na <- matrix(c(1,2,3,4,5,6,7,8,9),nrow=3)\na##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\ndim(a)## [1] 3 3\na[2,3]## [1] 8\na[1:2,2:3] ##      [,1] [,2]\n## [1,]    4    7\n## [2,]    5    8\na[,c(1,3)]##      [,1] [,2]\n## [1,]    1    7\n## [2,]    2    8\n## [3,]    3    9\na[c(1,3),]##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    3    6    9\na*a##      [,1] [,2] [,3]\n## [1,]    1   16   49\n## [2,]    4   25   64\n## [3,]    9   36   81\na%*%a##      [,1] [,2] [,3]\n## [1,]   30   66  102\n## [2,]   36   81  126\n## [3,]   42   96  150"},{"path":"almacenando-y-manipulando-datos..html","id":"data-frames.","chapter":"2 Almacenando y manipulando datos.","heading":"2.1.3  Data frames.","text":"Un data frame es un objeto que almacena datos organizados mediante la clase data.frame. Esta organización consiste en que, por filas, se disponen los diferentes casos o sujetos; mientras que por columnas se posicionan las variables. Así:Es similar una matriz en el sentido de que tiene dos dimensiones. Podemos acceder sus elementos con corchetes, tenemos nombres de filas y columnas, y podemos operar con ellas.Es similar una matriz en el sentido de que tiene dos dimensiones. Podemos acceder sus elementos con corchetes, tenemos nombres de filas y columnas, y podemos operar con ellas.Cada columna tiene un nombre, de manera que podemos acceder una columna concreta con el símbolo $. Todas las columnas (variables) son vectores con la misma longitud.Cada columna tiene un nombre, de manera que podemos acceder una columna concreta con el símbolo $. Todas las columnas (variables) son vectores con la misma longitud.Cada columna puede ser un vector numérico, factor, de tipo carácter o lógico.Cada columna puede ser un vector numérico, factor, de tipo carácter o lógico.Por ejemplo, vamos crear el data frame “datos”, con tres variables: “peso”, “altura”, y “color de ojos”, llamadas “Peso”, “Altura” y “Cl.ojos”, respectivamente; para 3 individuos o casos. Una opción es crear primero las tres variables como vectores, y luego crear el data frame mediante la función dataframe():Si ahora ejecutamos una línea con el nombre de nuestro data frame, lo obtendremos como resultado en la consola:Para obtener los nombres de las variables (es decir, el nombre de cada columna) teclearemos la función:Obteniéndose:Para obtener solo los datos de la columna (variable) color de ojos teclearemos datos$Cl.ojos:Y para obtener los datos de peso: datos$Peso:Para saber el número de filas y de columnas de una hoja de datos utilizaremos las funciones nrow() y ncol():Para seleccionar elementos de un data frame, se pueden seguir las mismas reglas que para la selección de elementos de una matriz (con el número de cada fila, que es cada individuo; y el número de cada columna, que es cada variable. Para elegir una variable, obstante, ya hemos visto que es posible usar su nombre; aunque precedido del nombre del data frame y el signo $. Por ejemplo, si ejecutamos:Obtenemos el mismo resultado.","code":"\nPeso<-c(68,75,88)\nAltura<-c(1.6,1.8,1.9)\nCl.ojos<-c(\"azules\",\"marrones\",\"marrones\")\ndatos<-data.frame(Peso,Altura,Cl.ojos)\ndatos##   Peso Altura  Cl.ojos\n## 1   68    1.6   azules\n## 2   75    1.8 marrones\n## 3   88    1.9 marrones\nnames(datos)## [1] \"Peso\"    \"Altura\"  \"Cl.ojos\"\ndatos$Cl.ojos## [1] \"azules\"   \"marrones\" \"marrones\"\ndatos$Peso## [1] 68 75 88\nnrow(datos)## [1] 3\nncol(datos)## [1] 3\ndatos[,2]## [1] 1.6 1.8 1.9\ndatos$Altura## [1] 1.6 1.8 1.9"},{"path":"almacenando-y-manipulando-datos..html","id":"factores.","chapter":"2 Almacenando y manipulando datos.","heading":"2.1.4  Factores.","text":"En R, un factor es un tipo especial de objeto que se usa para representar datos categóricos. Es decir, datos que son números continuos sino que pertenecen un conjunto limitado de categorías (también llamadas niveles). Por ejemplo, anteriormente creamos el vector “Cl.ojos”. Es un vector tipo carácter; pero, para R, aún es factor.Para convertirlo en un objeto de la clase “factor”, podemos ejecutar:En el Global Environment vemos que se ha creado el objaeto “Cl.ojosF”, de tipo factor. Si representamos ambos objetos:Vemos como “Cl.ojosF” es un vector de elementos tipo carácter; sino algo más: es una variable cuyos elementos pueden tomar dos valores categóricos (que pueden estar en el vector una vez, o estar repetidos, o estar), categorías o niveles, que son “azules” y “marrones”.¿Por qué es necesario, veces, transformar los datos “categóricos” en “factores”?Ahorro de memoria: internamente, en lugar de guardar “marrones” muchas veces, guarda un número que representa la categoría (ej. 1 = marrones, 2 = azules).Ahorro de memoria: internamente, en lugar de guardar “marrones” muchas veces, guarda un número que representa la categoría (ej. 1 = marrones, 2 = azules).Orden y niveles: puedes definir si las categorías del factor están en escala ordinal. Por ejemplo, supongamos que tenemos las siguientes categorías de calificaciones: “Suspenso”, “Aprobado”, “Notable”, “Sobresaliente”. Si las guardamos como factor en escala ordinal, R puede saber que Suspenso < Aprobado < Notable < Sobresaliente, lo que te permitirá hacer comparaciones, ordenar, etc.Orden y niveles: puedes definir si las categorías del factor están en escala ordinal. Por ejemplo, supongamos que tenemos las siguientes categorías de calificaciones: “Suspenso”, “Aprobado”, “Notable”, “Sobresaliente”. Si las guardamos como factor en escala ordinal, R puede saber que Suspenso < Aprobado < Notable < Sobresaliente, lo que te permitirá hacer comparaciones, ordenar, etc.Modelos estadísticos: muchos métodos de R tratan automáticamente los factores como variables cualitativas y hacen análisis adecuados (tablas de contingencia, regresiones categóricas, etc.).En síntesis, Un factor en R es como una etiqueta inteligente para categorías, que permite trabajar con datos cualitativos de forma más organizada y útil para análisis.","code":"\nCl.ojosF <- factor(Cl.ojos)\nCl.ojos## [1] \"azules\"   \"marrones\" \"marrones\"\nCl.ojosF## [1] azules   marrones marrones\n## Levels: azules marrones\nnotas <- factor(\n  c(\"Aprobado\", \"Notable\", \"Sobresaliente\", \"Suspenso\"),\n  levels = c(\"Suspenso\", \"Aprobado\", \"Notable\", \"Sobresaliente\"),\n  ordered = TRUE\n)\nnotas## [1] Aprobado      Notable       Sobresaliente Suspenso     \n## Levels: Suspenso < Aprobado < Notable < Sobresaliente"},{"path":"almacenando-y-manipulando-datos..html","id":"listas.","chapter":"2 Almacenando y manipulando datos.","heading":"2.1.5  Listas.","text":"Por último, una lista es un tipo de objeto que puede contener cualquier cosa dentro:númerosnúmerostextostextosvectoresvectoresmatricesmatricesdata framesdata framesotras listasotras listasEs decir, es como una caja organizadora con muchos compartimentos, y en cada compartimento puedes guardar un objeto distinto, tengan la naturaleza que tengan. Por ejemplo, vamos crear la lista “mi_lista”:Esta lista está almacenada en el Global Environment. Si queremos visualizar solo el tercer elemento podemos hacerlo invocando su posición (con corchetes dobles), o bien su nombre:Las listas son las estructuras de almacenamiento más flexibles de R, y son muy utilizadas para organizar y empaquetar resultados complejos.","code":"\nmi_lista <- list(\n  nombre = \"María\",\n  edad = 21,\n  notas = c(8.5, 9.0, 7.2),\n  aprobado = TRUE\n)\n\nmi_lista## $nombre\n## [1] \"María\"\n## \n## $edad\n## [1] 21\n## \n## $notas\n## [1] 8.5 9.0 7.2\n## \n## $aprobado\n## [1] TRUE\nmi_lista$notas## [1] 8.5 9.0 7.2\nmi_lista[[3]]## [1] 8.5 9.0 7.2"},{"path":"almacenando-y-manipulando-datos..html","id":"importando-datos.","chapter":"2 Almacenando y manipulando datos.","heading":"2.2  Importando datos.","text":"Lo más frecuente es que tecleemos los datos, como hemos hecho hasta ahora; sino que los importemos R desde algún contenedor externo (archivo de texto, hoja de cálculo, base de datos…). Nosotros vamos importar nuestros datos desde Microsoft® Excel®. Vamos cerrar el script que hemos estado construyendo en los apartados anteriores (para conservarlo hay que guardarlo antes), aunque vamos seguir trabajando en el mismo proyecto (que habíamos llamado “explora”). Iremos la carpeta del proyecto y guardaremos en ella los dos archivos de esta práctica (obtén el enlace los archivos en la sección final del capítulo):Un archivo de Microsoft® Excel® llamado “interestelar_100.xlsx”Un archivo de Microsoft® Excel® llamado “interestelar_100.xlsx”Un script con las instrucciones que vamos mostrar continuación, y que se llama “explora_rstars.R”Un script con las instrucciones que vamos mostrar continuación, y que se llama “explora_rstars.R”Si abrimos el archivo de Microsoft® Excel® “interestelar_100.xlsx”, comprobaremos que se compone de tres hojas. La primera muestra un aviso sobre el uso exclusivo que se debe dar los datos incorporados; la segunda recoge la descripción de las variables consideradas; y la tercera (hoja “Datos”) guarda los datos que debemos importar desde R-Studio. Estos datos se corresponden con diferentes variables económico-financieras y de diverso índole de una muestra de empresas que se dedican al transporte de mercancías interestelar.Vamos abrir nuestro script “explora_rstars.R” con File → Open File… (o haciendo click en el archivo correspondiente en la ventana inferior derecha de RStudio, pestaña “Files”). Este script contiene el programa que vamos ir ejecutando en la práctica.La primera línea / instrucción en los scripts suele ser:La instrucción tiene como objeto limpiar el Global Environment (memoria) de objetos de anteriores sesiones de trabajo.Luego, pueden cargarse los paquetes que harán falta para ejecutar el código, si bien se puden cargar en cualquier parte del script (aunque siempre antes de ejecutar una línea que requiera de algún elemento incluido en uno de estos paquetes).Obviamente, para cargar o activar un paquete, previamente debe de haber sido instalado en la máquina donde estamos trabajando. Anteriormente hemos visto como instalar un paquete del repositorio CRAN. De nuevo, si hemos instalado antes los paquetes readxl y gtExtras, tendremos que instalarlos, o nos dará error.Para instalar, por ejemplo,el paquete readxl, (que contiene el código necesario para importar datos de un archivo de Microsoft® Excel®), iremos la ventana inferior-derecha y pulsaremos la pestaña Packages, pulsaremos en Install, y emergerá una ventana donde dejaremos el “repositorio” que viene por defecto. En el campo “Packages”, escribiremos el nombre del “paquete” que contiene la librería que nos hace falta (normalmente coincide con el nombre de la propia librería, en nuestro caso readxl. Una vez descargado el “paquete”, podremos ejecutar el código anterior sin problemas. Si hemos de instalar el paquete gtExtras, procederemos de idéntico modo.Para importar los datos localizados en el archivo de Microsoft® Excel® “interestelar_100.xlsx” el código que podemos usar es:La función encargada de importar los datos es read_excel(), del paquete readxl.Una cuestión importante tener en cuenta es que, si hay valores perdidos que, en la hoja de cálculo vienen indicados en las celdas mediante algún tipo de anotación, como por ejemplo, “n.d.” (disponible); deberá incluirse el argumento na = para informar de las anotaciones en las celdas que deben traducirse como valores faltantes. Si las celdas sin dato aparecieran en la hoja de cálculo siempre en blanco, este argumento haría falta.Volviendo nuestro ejemplo, podemos observar cómo en el Environment ya aparece un objeto. Este objeto es una estructura de datos tipo data frame, se llama “interestelar_100” y contiene 104 filas (una por empresa) y 29 columnas, una por cada una de las variables almacenadas en el archivo de Microsoft® Excel®. De estas variables, cinco son de tipo cualitativo, formadas por cadenas de caracteres.Puede explorarse el contenido del data frame y los principales estadísticos con la función summary():Veremos cómo aparecen 29 variables con algunos estadísticos básicos.R ha considerado la primera columna como una variable de tipo cualitativo (atributo). En realidad es una variable, sino el nombre de los individuos o casos. Para evitar que R tome los nombres de los casos como una variable, podemos redefinir nuestro data frame diciéndole que considere esa primera columna como los nombres de los individuos o filas:En la línea anterior hemos asignado al data frame “interestelar_100” los propios datos de “interestelar_100”; pero indicando que la primera columna de datos es una variable; sino el nombre de los casos. Si hacemos ahora el summary():Vemos que ya aparece “NOMBRE” como variable, y en el Environment ya aparece el data frame “interestelar_100” con 100 observaciones (casos), pero con 28 variables (una menos).Un modo visualmente más elegante de explorar el contenido del data frame es la utilización de la función gt_plt_summary() del paquete gtExtras:Este código asigna al nombre “datos_df_graph” (o al que queramos) un gráfico/tabla con las variables que contiene el data frame (en este caso, “datos”, añadiendo características y medidas básicas de las diferentes variables (según sea su tipología). Podremos ver este gráfico/tabla evocando su nombre:Y el resultado será:Antes de seguir con la manipulación de nuestros datos, es preciso decir que existen otros muchos formatos de datos que pueden ser importados. Por ejemplo, con el paquete readr se pueden importar datos de archivos de texto de tipo tabular (por ejemplo, archivos *.csv). Con el paquete haven se pueden capturar los datos almacenados en archivos de SPSS® (.sav), Stata® (.dta), SAS® (.sas7bdat), etc. Finamente, se pueden capturar datos almacenados en páginas web (archivos en formato JSON o XML, o en tablas HTML)) o en bases de datos gestionadas mediante diversos sistemas (SQLite, MySQL, MariaDB, PostgreSQL, Oracle®).","code":"\n## Importando datos de las empresas R-Stars\n\n# Limpiando el Global Environment\nrm(list = ls())\n# Cargando paquetes\nlibrary(readxl)\nlibrary(gtExtras)\n# DATOS\n\ninterestelar_100 <- read_excel(\"interestelar_100.xlsx\", sheet = \"Datos\", na = c(\"n.d.\"))\nsummary (interestelar_100)##     NOMBRE              SEDE             GALAXIA              ACTIVO       \n##  Length:104         Length:104         Length:104         Min.   :  17.55  \n##  Class :character   Class :character   Class :character   1st Qu.:  51.16  \n##  Mode  :character   Mode  :character   Mode  :character   Median : 159.08  \n##                                                           Mean   : 259.98  \n##                                                           3rd Qu.: 231.86  \n##                                                           Max.   :2966.52  \n##                                                           NA's   :1        \n## \n##      FPIOS              ING              MARGEN           RES         \n##  Min.   :   8.67   Min.   :  19.43   Min.   :17.42   Min.   :  11.02  \n##  1st Qu.:  22.46   1st Qu.:  44.33   1st Qu.:48.96   1st Qu.:  26.55  \n##  Median :  59.81   Median : 103.91   Median :56.72   Median :  54.61  \n##  Mean   : 115.64   Mean   : 238.23   Mean   :60.35   Mean   : 128.22  \n##  3rd Qu.: 109.67   3rd Qu.: 254.88   3rd Qu.:73.44   3rd Qu.: 167.98  \n##  Max.   :1441.44   Max.   :2313.99   Max.   :95.00   Max.   :1189.86  \n##                                      NA's   :1                        \n## \n##    SOLVENCIA         ACTCOR           PASCOR           LIQUIDEZ     \n##  Min.   :123.1   Min.   :  6.81   Min.   :   6.66   Min.   :0.2253  \n##  1st Qu.:160.4   1st Qu.: 17.80   1st Qu.:  20.49   1st Qu.:0.4962  \n##  Median :174.7   Median : 23.57   Median :  59.92   Median :0.6984  \n##  Mean   :179.5   Mean   : 64.64   Mean   : 107.89   Mean   :0.6614  \n##  3rd Qu.:194.0   3rd Qu.: 58.27   3rd Qu.: 101.88   3rd Qu.:0.8272  \n##  Max.   :325.3   Max.   :889.96   Max.   :1143.81   Max.   :1.2580  \n## \n##     APALANCA          RENECO          RENFIN           EMPLEA      \n##  Min.   : 44.38   Min.   :28.04   Min.   : 70.18   Min.   :  7.00  \n##  1st Qu.:106.20   1st Qu.:45.02   1st Qu.:101.69   1st Qu.: 22.00  \n##  Median :133.73   Median :50.43   Median :119.24   Median : 30.00  \n##  Mean   :145.60   Mean   :52.38   Mean   :121.54   Mean   : 35.78  \n##  3rd Qu.:164.55   3rd Qu.:59.41   3rd Qu.:139.48   3rd Qu.: 42.00  \n##  Max.   :433.55   Max.   :93.85   Max.   :262.25   Max.   :100.00  \n##  NA's   :1        NA's   :1                        NA's   :1       \n## \n##      FJUR               FLOTA           COSTOP            CAPEX         \n##  Length:104         Min.   : 3.00   Min.   :   0.98   Min.   :   1.038  \n##  Class :character   1st Qu.:15.00   1st Qu.:  14.78   1st Qu.:   5.474  \n##  Mode  :character   Median :24.50   Median :  29.82   Median :  11.155  \n##                     Mean   :27.35   Mean   : 110.04   Mean   :  39.395  \n##                     3rd Qu.:40.00   3rd Qu.: 115.32   3rd Qu.:  34.084  \n##                     Max.   :50.00   Max.   :1124.13   Max.   :1087.196  \n##                                     NA's   :1                           \n## \n##       IMD               EFLO               RUTAS             IDIG        \n##  Min.   :  0.4779   Length:104         Min.   :  42.0   Min.   : 0.6007  \n##  1st Qu.:  1.4395   Class :character   1st Qu.: 203.2   1st Qu.:10.7957  \n##  Median :  4.9474   Mode  :character   Median : 327.5   Median :16.7092  \n##  Mean   : 16.3313                      Mean   : 397.7   Mean   :18.1645  \n##  3rd Qu.: 19.7304                      3rd Qu.: 411.2   3rd Qu.:24.8249  \n##  Max.   :202.4779                      Max.   :2915.0   Max.   :60.8325  \n## \n##     IDIVERSE           IFIDE          GMEDRUT              DIST          \n##  Min.   : 0.7737   Min.   :26.44   Min.   :0.005197   Min.   :   0.1513  \n##  1st Qu.:13.0142   1st Qu.:37.32   1st Qu.:0.095229   1st Qu.:  18.3251  \n##  Median :22.3920   Median :39.18   Median :0.198253   Median :  57.7865  \n##  Mean   :23.8114   Mean   :39.39   Mean   :0.334388   Mean   : 164.9319  \n##  3rd Qu.:34.6776   3rd Qu.:40.40   3rd Qu.:0.520769   3rd Qu.: 197.3137  \n##  Max.   :71.3507   Max.   :73.30   Max.   :1.550435   Max.   :2151.6418  \n##  NA's   :1         NA's   :1       NA's   :1          NA's   :2          \n## \n##       BMAL          \n##  Min.   :    31.45  \n##  1st Qu.:   356.56  \n##  Median :  1285.59  \n##  Mean   :  8292.13  \n##  3rd Qu.:  3206.33  \n##  Max.   :262555.82  \n##  NA's   :2\ninterestelar_100 <- data.frame(interestelar_100, row.names = 1)\nsummary (interestelar_100)##      SEDE             GALAXIA              ACTIVO            FPIOS        \n##  Length:104         Length:104         Min.   :  17.55   Min.   :   8.67  \n##  Class :character   Class :character   1st Qu.:  51.16   1st Qu.:  22.46  \n##  Mode  :character   Mode  :character   Median : 159.08   Median :  59.81  \n##                                        Mean   : 259.98   Mean   : 115.64  \n##                                        3rd Qu.: 231.86   3rd Qu.: 109.67  \n##                                        Max.   :2966.52   Max.   :1441.44  \n##                                        NA's   :1                          \n## \n##       ING              MARGEN           RES            SOLVENCIA    \n##  Min.   :  19.43   Min.   :17.42   Min.   :  11.02   Min.   :123.1  \n##  1st Qu.:  44.33   1st Qu.:48.96   1st Qu.:  26.55   1st Qu.:160.4  \n##  Median : 103.91   Median :56.72   Median :  54.61   Median :174.7  \n##  Mean   : 238.23   Mean   :60.35   Mean   : 128.22   Mean   :179.5  \n##  3rd Qu.: 254.88   3rd Qu.:73.44   3rd Qu.: 167.98   3rd Qu.:194.0  \n##  Max.   :2313.99   Max.   :95.00   Max.   :1189.86   Max.   :325.3  \n##                    NA's   :1                                        \n## \n##      ACTCOR           PASCOR           LIQUIDEZ         APALANCA     \n##  Min.   :  6.81   Min.   :   6.66   Min.   :0.2253   Min.   : 44.38  \n##  1st Qu.: 17.80   1st Qu.:  20.49   1st Qu.:0.4962   1st Qu.:106.20  \n##  Median : 23.57   Median :  59.92   Median :0.6984   Median :133.73  \n##  Mean   : 64.64   Mean   : 107.89   Mean   :0.6614   Mean   :145.60  \n##  3rd Qu.: 58.27   3rd Qu.: 101.88   3rd Qu.:0.8272   3rd Qu.:164.55  \n##  Max.   :889.96   Max.   :1143.81   Max.   :1.2580   Max.   :433.55  \n##                                                      NA's   :1       \n## \n##      RENECO          RENFIN           EMPLEA           FJUR          \n##  Min.   :28.04   Min.   : 70.18   Min.   :  7.00   Length:104        \n##  1st Qu.:45.02   1st Qu.:101.69   1st Qu.: 22.00   Class :character  \n##  Median :50.43   Median :119.24   Median : 30.00   Mode  :character  \n##  Mean   :52.38   Mean   :121.54   Mean   : 35.78                     \n##  3rd Qu.:59.41   3rd Qu.:139.48   3rd Qu.: 42.00                     \n##  Max.   :93.85   Max.   :262.25   Max.   :100.00                     \n##  NA's   :1                        NA's   :1                          \n## \n##      FLOTA           COSTOP            CAPEX               IMD          \n##  Min.   : 3.00   Min.   :   0.98   Min.   :   1.038   Min.   :  0.4779  \n##  1st Qu.:15.00   1st Qu.:  14.78   1st Qu.:   5.474   1st Qu.:  1.4395  \n##  Median :24.50   Median :  29.82   Median :  11.155   Median :  4.9474  \n##  Mean   :27.35   Mean   : 110.04   Mean   :  39.395   Mean   : 16.3313  \n##  3rd Qu.:40.00   3rd Qu.: 115.32   3rd Qu.:  34.084   3rd Qu.: 19.7304  \n##  Max.   :50.00   Max.   :1124.13   Max.   :1087.196   Max.   :202.4779  \n##                  NA's   :1                                              \n## \n##      EFLO               RUTAS             IDIG            IDIVERSE      \n##  Length:104         Min.   :  42.0   Min.   : 0.6007   Min.   : 0.7737  \n##  Class :character   1st Qu.: 203.2   1st Qu.:10.7957   1st Qu.:13.0142  \n##  Mode  :character   Median : 327.5   Median :16.7092   Median :22.3920  \n##                     Mean   : 397.7   Mean   :18.1645   Mean   :23.8114  \n##                     3rd Qu.: 411.2   3rd Qu.:24.8249   3rd Qu.:34.6776  \n##                     Max.   :2915.0   Max.   :60.8325   Max.   :71.3507  \n##                                                        NA's   :1        \n## \n##      IFIDE          GMEDRUT              DIST                BMAL          \n##  Min.   :26.44   Min.   :0.005197   Min.   :   0.1513   Min.   :    31.45  \n##  1st Qu.:37.32   1st Qu.:0.095229   1st Qu.:  18.3251   1st Qu.:   356.56  \n##  Median :39.18   Median :0.198253   Median :  57.7865   Median :  1285.59  \n##  Mean   :39.39   Mean   :0.334388   Mean   : 164.9319   Mean   :  8292.13  \n##  3rd Qu.:40.40   3rd Qu.:0.520769   3rd Qu.: 197.3137   3rd Qu.:  3206.33  \n##  Max.   :73.30   Max.   :1.550435   Max.   :2151.6418   Max.   :262555.82  \n##  NA's   :1       NA's   :1          NA's   :2           NA's   :2\n# visualizando el data frame de modo elegante con {gtExtras}\ndatos_df_graph <- gt_plt_summary(interestelar_100)\ndatos_df_graph"},{"path":"almacenando-y-manipulando-datos..html","id":"dplyr.","chapter":"2 Almacenando y manipulando datos.","heading":"2.3  {dplyr}.","text":"","code":""},{"path":"almacenando-y-manipulando-datos..html","id":"el-tidyverse.-cargando-dplyr.","chapter":"2 Almacenando y manipulando datos.","heading":"2.3.1  El Tidyverse. Cargando {dplyr}.","text":"El Tidyverse es un conjunto de paquetes / librerías con una filosofía común, como es el uso de ciertas estructuras gramaticales, que facilitan muchas de las tareas y análisis que podrían hacerse con el lenguaje R estándar. Una buena obra para profundizar en el Tidyverse es Wickham Grolemund (2017).Uno de esos paquetes es dplyr, que proporciona una gramática más sencilla que la del lenguaje R convencional para manipular los objetos de estructuras de datos conocidos como data frames.Los data frames, como ya sabemos, son estructuras en las que se almacenan datos de modo que, por columnas, se disponen las variables del análisis; y por filas los casos que conforman la muestra / población.Vamos suponer que trabajamos dentro del proyecto que hemos creado previamente, de nombre “explora” (ver capítulo 1), y que seguimos con el script “explora_rstars.R”, co el que importamos los datos del archivo de Microsoft® Excel® “interestelar_100.xlsx”, que contení información sobre 104 empresas de transporte de mercancías escala interestelar.Suponemos que hemos ejecutado la primera parte del script (importación de datos con corrección de la columna de nombres de las filas, al data frame interestelar_100, que está almacenado en el Global Environment.Vamos continuar desde este punto.continuación, cargaremos el paquete dplyr. Si nunca antes se ha utilizado este paquete, cuando lo intentemos activar con la función library() nos dará un error o nos dirá que previamente hay que importarlo. En ese caso, iremos la ventana inferior-derecha y pulsaremos la pestaña “Packages”, pulsaremos en Install, y emergerá una ventana donde dejaremos el “repositorio” que viene por defecto y, en el campo Packages, escribiremos el nombre del “paquete” (en nuestro caso dplyr). Una vez descargado el “paquete”, podremos ejecutar el código sin problemas:Para entender mejor la sintaxis que siguen las funciones o instrucciones las que da acceso dplyr, hay que tener en cuenta lo siguiente:El primer argumento que tiene una función de dplyr es el data frame con el que se va trabajar.El primer argumento que tiene una función de dplyr es el data frame con el que se va trabajar.Los otros argumentos describen qué hay que hacer con el data frame especificado en el primer argumento. Es posible referirse las columnas (variables) del data frame con su nombre, sin utilizar el operador $.Los otros argumentos describen qué hay que hacer con el data frame especificado en el primer argumento. Es posible referirse las columnas (variables) del data frame con su nombre, sin utilizar el operador $.El valor de retorno es un nuevo data frame.El valor de retorno es un nuevo data frame.En los siguientes subapartados practicaremos con algunas de las principales funciones que aporta dplyr.","code":"\n## dplyr\n# Cargando dplyr\nlibrary (dplyr)"},{"path":"almacenando-y-manipulando-datos..html","id":"seleccionando-columnas-de-un-data-frame.","chapter":"2 Almacenando y manipulando datos.","heading":"2.3.2  Seleccionando columnas de un data frame.","text":"La función clave de dplyr para seleccionar una o varias columnas (variables) de un data frame es la función select().Así, vamos imaginar por ejemplo que queremos eliminar de nuestro data frame la variable (de tipo “carácter”) FJUR (forma jurídica de la empresa). Podremos ejecutar la asignación:Podemos verificar que, en el Environment, el data frame ha pasado tener una variable menos (27), ya que hemos eliminado la variable FJUR. Es decir, con el guión “-” se pueden eliminar directamente variables de un data frame.Ahora, suponemos que queremos visualizar las variables del data frame “interestelar_100”: ACTIVO, FPIOS, LIQUIDEZ. Para ello, ejecutaremos el código:(Nota: solo mostramos aquí los 10 primeros casos del data frame).Como hemos asignado el resultado de la función ningún “nombre”, R simplemente muestra el resultado en pantalla; pero guarda ningún objeto en el Environment. Si asignamos un select() un “nombre”, se creará un data frame con ese nombre, y las variables seleccionadas:Podemos comprobar cómo en el Global Environment hay otro objeto data frame llamado “interestelar_100A”, con 4 variables (y los mismos 104 casos).Otra posibilidad que tenemos es hacer una copia de un data frame rápidamente con el argumento everything(). Por ejemplo:Se ha creado el date frame “interestelar_100_replica” que es una copia exacta de “interestelar_100”.","code":"\n# Seleccionando variables\ninterestelar_100 <-select(interestelar_100, -FJUR)\nsummary (interestelar_100)##      SEDE             GALAXIA              ACTIVO            FPIOS        \n##  Length:104         Length:104         Min.   :  17.55   Min.   :   8.67  \n##  Class :character   Class :character   1st Qu.:  51.16   1st Qu.:  22.46  \n##  Mode  :character   Mode  :character   Median : 159.08   Median :  59.81  \n##                                        Mean   : 259.98   Mean   : 115.64  \n##                                        3rd Qu.: 231.86   3rd Qu.: 109.67  \n##                                        Max.   :2966.52   Max.   :1441.44  \n##                                        NA's   :1                          \n## \n##       ING              MARGEN           RES            SOLVENCIA    \n##  Min.   :  19.43   Min.   :17.42   Min.   :  11.02   Min.   :123.1  \n##  1st Qu.:  44.33   1st Qu.:48.96   1st Qu.:  26.55   1st Qu.:160.4  \n##  Median : 103.91   Median :56.72   Median :  54.61   Median :174.7  \n##  Mean   : 238.23   Mean   :60.35   Mean   : 128.22   Mean   :179.5  \n##  3rd Qu.: 254.88   3rd Qu.:73.44   3rd Qu.: 167.98   3rd Qu.:194.0  \n##  Max.   :2313.99   Max.   :95.00   Max.   :1189.86   Max.   :325.3  \n##                    NA's   :1                                        \n## \n##      ACTCOR           PASCOR           LIQUIDEZ         APALANCA     \n##  Min.   :  6.81   Min.   :   6.66   Min.   :0.2253   Min.   : 44.38  \n##  1st Qu.: 17.80   1st Qu.:  20.49   1st Qu.:0.4962   1st Qu.:106.20  \n##  Median : 23.57   Median :  59.92   Median :0.6984   Median :133.73  \n##  Mean   : 64.64   Mean   : 107.89   Mean   :0.6614   Mean   :145.60  \n##  3rd Qu.: 58.27   3rd Qu.: 101.88   3rd Qu.:0.8272   3rd Qu.:164.55  \n##  Max.   :889.96   Max.   :1143.81   Max.   :1.2580   Max.   :433.55  \n##                                                      NA's   :1       \n## \n##      RENECO          RENFIN           EMPLEA           FLOTA      \n##  Min.   :28.04   Min.   : 70.18   Min.   :  7.00   Min.   : 3.00  \n##  1st Qu.:45.02   1st Qu.:101.69   1st Qu.: 22.00   1st Qu.:15.00  \n##  Median :50.43   Median :119.24   Median : 30.00   Median :24.50  \n##  Mean   :52.38   Mean   :121.54   Mean   : 35.78   Mean   :27.35  \n##  3rd Qu.:59.41   3rd Qu.:139.48   3rd Qu.: 42.00   3rd Qu.:40.00  \n##  Max.   :93.85   Max.   :262.25   Max.   :100.00   Max.   :50.00  \n##  NA's   :1                        NA's   :1                       \n## \n##      COSTOP            CAPEX               IMD               EFLO          \n##  Min.   :   0.98   Min.   :   1.038   Min.   :  0.4779   Length:104        \n##  1st Qu.:  14.78   1st Qu.:   5.474   1st Qu.:  1.4395   Class :character  \n##  Median :  29.82   Median :  11.155   Median :  4.9474   Mode  :character  \n##  Mean   : 110.04   Mean   :  39.395   Mean   : 16.3313                     \n##  3rd Qu.: 115.32   3rd Qu.:  34.084   3rd Qu.: 19.7304                     \n##  Max.   :1124.13   Max.   :1087.196   Max.   :202.4779                     \n##  NA's   :1                                                                 \n## \n##      RUTAS             IDIG            IDIVERSE           IFIDE      \n##  Min.   :  42.0   Min.   : 0.6007   Min.   : 0.7737   Min.   :26.44  \n##  1st Qu.: 203.2   1st Qu.:10.7957   1st Qu.:13.0142   1st Qu.:37.32  \n##  Median : 327.5   Median :16.7092   Median :22.3920   Median :39.18  \n##  Mean   : 397.7   Mean   :18.1645   Mean   :23.8114   Mean   :39.39  \n##  3rd Qu.: 411.2   3rd Qu.:24.8249   3rd Qu.:34.6776   3rd Qu.:40.40  \n##  Max.   :2915.0   Max.   :60.8325   Max.   :71.3507   Max.   :73.30  \n##                                     NA's   :1         NA's   :1      \n## \n##     GMEDRUT              DIST                BMAL          \n##  Min.   :0.005197   Min.   :   0.1513   Min.   :    31.45  \n##  1st Qu.:0.095229   1st Qu.:  18.3251   1st Qu.:   356.56  \n##  Median :0.198253   Median :  57.7865   Median :  1285.59  \n##  Mean   :0.334388   Mean   : 164.9319   Mean   :  8292.13  \n##  3rd Qu.:0.520769   3rd Qu.: 197.3137   3rd Qu.:  3206.33  \n##  Max.   :1.550435   Max.   :2151.6418   Max.   :262555.82  \n##  NA's   :1          NA's   :2           NA's   :2\nselect(interestelar_100, ACTIVO, FPIOS, LIQUIDEZ)##                              ACTIVO    FPIOS  LIQUIDEZ\n## hyperion star haulage        211.14  88.9400 0.5592011\n## jovian logistics             220.56  78.9300 0.5358355\n## sandworm freight              17.55   8.6700 0.9659574\n## ripley interstellar freight   37.07  19.6300 0.8586790\n## chakotay cargo systems      2158.69 971.4105 0.5963287\n## dune dynamics                515.86 178.6400 0.4919417\n## sith transport corp.         515.98 212.9600 0.6386439\n## seven of nine logistics      477.80 215.0100 1.2580305\n## moya cargo systems           520.29 169.4900 0.5264522\n## travis star haulage          466.63 244.9200 0.4893415\ninterestelar_100A <-select(interestelar_100, ACTIVO, FPIOS, LIQUIDEZ)\nsummary (interestelar_100A)##      ACTIVO            FPIOS            LIQUIDEZ     \n##  Min.   :  17.55   Min.   :   8.67   Min.   :0.2253  \n##  1st Qu.:  51.16   1st Qu.:  22.46   1st Qu.:0.4962  \n##  Median : 159.08   Median :  59.81   Median :0.6984  \n##  Mean   : 259.98   Mean   : 115.64   Mean   :0.6614  \n##  3rd Qu.: 231.86   3rd Qu.: 109.67   3rd Qu.:0.8272  \n##  Max.   :2966.52   Max.   :1441.44   Max.   :1.2580  \n##  NA's   :1\ninterestelar_100_replica <-select(interestelar_100, everything())"},{"path":"almacenando-y-manipulando-datos..html","id":"seleccionando-casos-de-un-data-frame.","chapter":"2 Almacenando y manipulando datos.","heading":"2.3.3  Seleccionando casos de un data frame.","text":"Además de seleccionar variables, con dplyr también se pueden seleccionar casos que cumplan ciertas condiciones. La función para realizar este cometido es filter(). Por ejemplo, si queremos seleccionar las empresas con un resultado (variable RES) mayor o igual 500 miles de PAVOs y presentar en pantalla el valor de RES y RENECO, la instrucción será:Se pueden incluir varias condiciones en un mismo filtro. Por ejemplo, vamos construir un nuevo data frame llamado “interestelar_100B” con las empresas que posean un resultado mayor o igual 500 miles de PAVOs y una rentabilidad económica (variable RENECO) inferior al 40%, y que contenga las variables RES, RENECO y ACTIVO:En el Global Environment aparecerá el data frame “interestelar_100B” con solo un caso: la empresa que cumple con ambas condiciones, introducidas mediante el operador lógico relacional “&”, que es el equivalente la conjunción “y” o, dicho de otro modo, la intersección. Otro operador lógico relacional muy utilizado es la barra vertical “|”, que es el equivalente la conjunción “o”, es decir, la unión.Los filtros más usuales son >, <, >=, <=, == (igual, ojo, con dos símbolos de igualdad seguidos) y != (igual).","code":"\n# Seleccionando casos\nselect(filter(interestelar_100, RES >= 500), RES, RENECO)##                            RES   RENECO\n## chakotay cargo systems  800.81 37.09704\n## hyperdrive express     1130.14 45.20668\n## home one cargo          609.58 53.21612\n## arrakis freight        1189.86 40.10962\ninterestelar_100B <-select(filter(interestelar_100,\n                                  RES >= 500 & RENECO < 40),\n                                  RES, RENECO, ACTIVO)\ninterestelar_100B##                           RES   RENECO  ACTIVO\n## chakotay cargo systems 800.81 37.09704 2158.69"},{"path":"almacenando-y-manipulando-datos..html","id":"ordenando-casos-de-un-data-frame.","chapter":"2 Almacenando y manipulando datos.","heading":"2.3.4  Ordenando casos de un data frame.","text":"Además de seleccionar determinados casos u observaciones (filas) de un data frame, con las funciones de dplyr también se pueden ordenar estos casos partir de los valores de ciertas variables (columnas). La función utilizar es arrange(). Esta función, por defecto, ordena los casos de modo ascendente. Por ejemplo, creremos el data frame “interestelar_100C” con variables RENECO, EFLO y ACTIVO, con los casos ordenados de modo ascendente según el valor de RENECO:(Nota: solo mostramos aquí los 10 primeros casos del data frame).En cambio, para ordenar de modo descendente, hay que utilizar el argumento desc():(Nota: solo mostramos aquí los 10 primeros casos del data frame).En el supuesto de que, en relación con una variable, hubiera varios casos con el mismo valor, podría añadirse otro criterio de ordenación con otra variable, que afectaría tales casos para deshacer el “empate”. Por ejemplo,vamos ordenar las empresas según la variable EFLO (categorización de la antigüedad promedio de la flota, con categorías: ANTIGUA, MADURA, RENOVADA). Al ser una variable categórica, los casos se ordenarán por orden alfabético según la categoría la que pertenenecen. Para ordenar los casos pertenecientes una misma categoría de EFLO, utilizaremos, de nuevo RENECO (en orden de valor descendente):(Nota: solo mostramos aquí los 10 primeros casos del data frame. Una vez concluidos los casos con categoría de EFLO “ANTIGUA”, comenzarían aparecer los de categoría “MADURA”, desde el caso con posición 52, exactamente).","code":"\n# Ordenando casos\ninterestelar_100C <- select(interestelar_100, RENECO, EFLO, ACTIVO)\ninterestelar_100C <- arrange(interestelar_100C, RENECO)\ninterestelar_100C##                            RENECO     EFLO  ACTIVO\n## mustafar haulage         28.04152   MADURA  183.05\n## mos eisley haulage       31.05354   MADURA  186.42\n## forest moon freight      32.46888   MADURA  222.49\n## kelvin universe movers   33.67418  ANTIGUA   43.03\n## io star transport        34.26908   MADURA  498.00\n## event horizon haulage    35.31742   MADURA  204.46\n## chakotay cargo systems   37.09704 RENOVADA 2158.69\n## tannhäuser freight       37.33466   MADURA  190.52\n## mon calamari freight co. 37.73973  ANTIGUA   29.20\n## darth vader logistics    38.20225  ANTIGUA   29.37\ninterestelar_100D <- arrange(interestelar_100C, desc(RENECO))\ninterestelar_100D##                         RENECO     EFLO ACTIVO\n## jovian logistics      93.84748   MADURA 220.56\n## sandworm freight      89.40171  ANTIGUA  17.55\n## hyperion star haulage 79.68646   MADURA 211.14\n## vulcan star freight   74.21537   MADURA 226.22\n## shuttlepod movers     72.50491   MADURA 249.39\n## sdf-1 freightworks    68.66035  ANTIGUA  36.95\n## betazoid transport    68.44444  ANTIGUA  56.25\n## dagobah freightlines  66.83549 RENOVADA 511.39\n## super star logistics  66.25660 RENOVADA 490.17\n## moya cargo systems    66.05547 RENOVADA 520.29\ninterestelar_100E <- arrange(interestelar_100C, EFLO, desc(RENECO))\ninterestelar_100E##                               RENECO    EFLO ACTIVO\n## sandworm freight            89.40171 ANTIGUA  17.55\n## sdf-1 freightworks          68.66035 ANTIGUA  36.95\n## betazoid transport          68.44444 ANTIGUA  56.25\n## solar flare logistics       65.27527 ANTIGUA  44.32\n## jedha star movers           64.60270 ANTIGUA  57.01\n## venator transport systems   62.71060 ANTIGUA  53.42\n## orion intergalactic freight 62.26345 ANTIGUA  53.90\n## eva star transport          62.24329 ANTIGUA  50.64\n## atreides logistics          61.99449 ANTIGUA  43.52\n## uhura cargo systems         60.28957 ANTIGUA  61.47"},{"path":"almacenando-y-manipulando-datos..html","id":"cambiando-el-nombre-de-las-variables-de-un-data-frame.","chapter":"2 Almacenando y manipulando datos.","heading":"2.3.5  Cambiando el nombre de las variables de un data frame.","text":"dplyr cuenta con una función que cambia fácilmente el nombre de una variable o columna de un data frame: la función rename(). Por ejemplo, si queremos cambiar el nombre de la variable SOLVENCIA por SOLVE, simplemente ejecutaremos:Podemos comprobar en el Global Environment, despegando el data frame “interestelar_100”, cómo ya aparece la variable SOLVENCIA; pero sí SOLVE en su lugar (obviamente, con los mismos datos). Es necesario tener en cuenta que en el lado izquierdo de la igualdad hay que poner el nuevo nombre, y en la derecha el antiguo. Además, en el mismo rename() se pueden cambiar los nombres de varias variables, separando las igualdades correspondientes con comas.","code":"\n# Renombrando variables\ninterestelar_100 <- rename(interestelar_100, SOLVE = SOLVENCIA)"},{"path":"almacenando-y-manipulando-datos..html","id":"añadiendo-variables-como-transformación-de-otras-variables-en-un-data-frame.","chapter":"2 Almacenando y manipulando datos.","heading":"2.3.6  Añadiendo variables como transformación de otras variables en un data frame.","text":"El paquete dplyr también permite añadir un data frame variables que son el resultado de someter otras variables diversas transformaciones. La función para realizar este cometido es mutate().Así, por ejemplo, imaginemos que necesitamos calcular una variable como el cociente entre los resultados obtenidos y el activo. esta nueva variable la denominaremos RATIO. El código será:En la transformación de variables mediante la función mutate(), se pueden utilizar funciones integradas en otros paquetes de R. Por ejemplo, si queremos calcular la variable ACTIVOS_ACUM como la variable que recoge los activos acumulados de las empresas, comenzando por la empresa con menor activo, podríamos utilizar la función cumsum() del paquete {base}, y hacer:Podemos verificar cómo se ha integrado en el data frame la variable ACTIVOS_ACUM:(Nota: solo mostramos aquí los 10 primeros casos del data frame)Un último ejemplo de adición de una variable que es transformación de otras. En este caso, crearemos la variable DIM (dimensión), que es categórica (los datos son conjuntos de caracteres). Esta variable tomará valor “ALTA” para las empresas con un valor de la variable ACTIVO mayor o igual que 216 miles de PAVOs, “MEDIA” para las empresas con un ACTIVO menor que 216 miles de PAVOs y mayor o igual que 54 miles de PAVOs, y “REDUCIDA” para las que tengan un valor en la variable ACTIVO menor que 54 miles de PAVOs. Para calcular automáticamente esta nueva variable categórica, utilizaremos la función de {base} llamada cut(). De este modo, haremos:Podemos advertir cómo la función cut(), que incluimos dentro de nuestra función de dplyr mutate(), tiene, su vez, varios argumentos: la variable numérica de referencia (ACTIVO); el argumento breaks =, en el que decimos los intervalos en que quedarán divididos los casos (uno, de menos infinito 54, otro de 54 216, y otro de 216 más infinito), y labels =, que es el valor que tomará la variable creada (DIM) según el intervalo en el que se sitúe cada caso de la muestra:(Nota: solo mostramos aquí los 10 primeros casos del data frame)Cabe destacar que podíamos haber escrito el código para crear la variable DIM de un modo más elegante y cómodo, utilizando el operador “pipe” %>%. Este operador permite concatenar una serie de instrucciones:Podríamos interpretar la línea de código así: asigna al data frame “interestelar_100” sus propios datos, después (%>%) crea la variable DIM con la función cut() y añádela “interestelar_100”. En el segundo caso, es más sencillo aún: toma “interestelar_100” y saca en pantalla los valores de las variables ACTIVO y DIM.","code":"\n# Añadiendo variables como transformacion de otras variables\ninterestelar_100 <- mutate (interestelar_100, RATIO = RES / ACTIVO)\nsummary(interestelar_100)##      SEDE             GALAXIA              ACTIVO            FPIOS        \n##  Length:104         Length:104         Min.   :  17.55   Min.   :   8.67  \n##  Class :character   Class :character   1st Qu.:  51.16   1st Qu.:  22.46  \n##  Mode  :character   Mode  :character   Median : 159.08   Median :  59.81  \n##                                        Mean   : 259.98   Mean   : 115.64  \n##                                        3rd Qu.: 231.86   3rd Qu.: 109.67  \n##                                        Max.   :2966.52   Max.   :1441.44  \n##                                        NA's   :1                          \n## \n##       ING              MARGEN           RES              SOLVE      \n##  Min.   :  19.43   Min.   :17.42   Min.   :  11.02   Min.   :123.1  \n##  1st Qu.:  44.33   1st Qu.:48.96   1st Qu.:  26.55   1st Qu.:160.4  \n##  Median : 103.91   Median :56.72   Median :  54.61   Median :174.7  \n##  Mean   : 238.23   Mean   :60.35   Mean   : 128.22   Mean   :179.5  \n##  3rd Qu.: 254.88   3rd Qu.:73.44   3rd Qu.: 167.98   3rd Qu.:194.0  \n##  Max.   :2313.99   Max.   :95.00   Max.   :1189.86   Max.   :325.3  \n##                    NA's   :1                                        \n## \n##      ACTCOR           PASCOR           LIQUIDEZ         APALANCA     \n##  Min.   :  6.81   Min.   :   6.66   Min.   :0.2253   Min.   : 44.38  \n##  1st Qu.: 17.80   1st Qu.:  20.49   1st Qu.:0.4962   1st Qu.:106.20  \n##  Median : 23.57   Median :  59.92   Median :0.6984   Median :133.73  \n##  Mean   : 64.64   Mean   : 107.89   Mean   :0.6614   Mean   :145.60  \n##  3rd Qu.: 58.27   3rd Qu.: 101.88   3rd Qu.:0.8272   3rd Qu.:164.55  \n##  Max.   :889.96   Max.   :1143.81   Max.   :1.2580   Max.   :433.55  \n##                                                      NA's   :1       \n## \n##      RENECO          RENFIN           EMPLEA           FLOTA      \n##  Min.   :28.04   Min.   : 70.18   Min.   :  7.00   Min.   : 3.00  \n##  1st Qu.:45.02   1st Qu.:101.69   1st Qu.: 22.00   1st Qu.:15.00  \n##  Median :50.43   Median :119.24   Median : 30.00   Median :24.50  \n##  Mean   :52.38   Mean   :121.54   Mean   : 35.78   Mean   :27.35  \n##  3rd Qu.:59.41   3rd Qu.:139.48   3rd Qu.: 42.00   3rd Qu.:40.00  \n##  Max.   :93.85   Max.   :262.25   Max.   :100.00   Max.   :50.00  \n##  NA's   :1                        NA's   :1                       \n## \n##      COSTOP            CAPEX               IMD               EFLO          \n##  Min.   :   0.98   Min.   :   1.038   Min.   :  0.4779   Length:104        \n##  1st Qu.:  14.78   1st Qu.:   5.474   1st Qu.:  1.4395   Class :character  \n##  Median :  29.82   Median :  11.155   Median :  4.9474   Mode  :character  \n##  Mean   : 110.04   Mean   :  39.395   Mean   : 16.3313                     \n##  3rd Qu.: 115.32   3rd Qu.:  34.084   3rd Qu.: 19.7304                     \n##  Max.   :1124.13   Max.   :1087.196   Max.   :202.4779                     \n##  NA's   :1                                                                 \n## \n##      RUTAS             IDIG            IDIVERSE           IFIDE      \n##  Min.   :  42.0   Min.   : 0.6007   Min.   : 0.7737   Min.   :26.44  \n##  1st Qu.: 203.2   1st Qu.:10.7957   1st Qu.:13.0142   1st Qu.:37.32  \n##  Median : 327.5   Median :16.7092   Median :22.3920   Median :39.18  \n##  Mean   : 397.7   Mean   :18.1645   Mean   :23.8114   Mean   :39.39  \n##  3rd Qu.: 411.2   3rd Qu.:24.8249   3rd Qu.:34.6776   3rd Qu.:40.40  \n##  Max.   :2915.0   Max.   :60.8325   Max.   :71.3507   Max.   :73.30  \n##                                     NA's   :1         NA's   :1      \n## \n##     GMEDRUT              DIST                BMAL               RATIO       \n##  Min.   :0.005197   Min.   :   0.1513   Min.   :    31.45   Min.   :0.2804  \n##  1st Qu.:0.095229   1st Qu.:  18.3251   1st Qu.:   356.56   1st Qu.:0.4479  \n##  Median :0.198253   Median :  57.7865   Median :  1285.59   Median :0.5036  \n##  Mean   :0.334388   Mean   : 164.9319   Mean   :  8292.13   Mean   :0.5230  \n##  3rd Qu.:0.520769   3rd Qu.: 197.3137   3rd Qu.:  3206.33   3rd Qu.:0.5941  \n##  Max.   :1.550435   Max.   :2151.6418   Max.   :262555.82   Max.   :0.9385  \n##  NA's   :1          NA's   :2           NA's   :2           NA's   :1\ninterestelar_100 <- arrange(interestelar_100, ACTIVO)\ninterestelar_100 <- mutate (interestelar_100, ACTIVOS_ACUM = cumsum(ACTIVO))\nselect(interestelar_100, ACTIVO, ACTIVOS_ACUM)##                             ACTIVO ACTIVOS_ACUM\n## sandworm freight             17.55        17.55\n## mon calamari freight co.     29.20        46.75\n## darth vader logistics        29.37        76.12\n## sdf-1 freightworks           36.95       113.07\n## ripley interstellar freight  37.07       150.14\n## gungan haulage co.           40.06       190.20\n## x-wing deliveries            40.35       230.55\n## kelvin universe movers       43.03       273.58\n## quantum freightlines         43.26       316.84\n## atreides logistics           43.52       360.36\ninterestelar_100 <- mutate(interestelar_100,\n                           DIM = cut(ACTIVO,\n                                 breaks = c(-Inf, 54, 216, Inf),\n                                 labels = c(\"REDUCIDA\", \"MEDIA\", \"ALTA\")))\nselect(interestelar_100, ACTIVO, DIM)##                             ACTIVO      DIM\n## sandworm freight             17.55 REDUCIDA\n## mon calamari freight co.     29.20 REDUCIDA\n## darth vader logistics        29.37 REDUCIDA\n## sdf-1 freightworks           36.95 REDUCIDA\n## ripley interstellar freight  37.07 REDUCIDA\n## gungan haulage co.           40.06 REDUCIDA\n## x-wing deliveries            40.35 REDUCIDA\n## kelvin universe movers       43.03 REDUCIDA\n## quantum freightlines         43.26 REDUCIDA\n## atreides logistics           43.52 REDUCIDA\ninterestelar_100 <- interestelar_100 %>%\n                              mutate(DIM = cut(ACTIVO,\n                                breaks = c(-Inf, 54, 216, Inf),\n                                labels = c(\"REDUCIDA\", \"MEDIA\", \"ALTA\")))\ninterestelar_100 %>% select(ACTIVO, DIM)"},{"path":"almacenando-y-manipulando-datos..html","id":"extrayendo-y-sintetizando-información-de-las-variables-de-un-data-frame.","chapter":"2 Almacenando y manipulando datos.","heading":"2.3.7  Extrayendo y sintetizando información de las variables de un data frame.","text":"Otra posibilidad que permite dplyr es extraer y sintetizar mediante medidas la información de las variables contenidas en un data frame. Para ello, nos ayudaremos de la función summarise(). Como ejemplo, calculemos la rentabilidad financiera media de las 20 empresas:veces, es de gran utilidad combinar summarise() con group_by(), que extrae la información dividiendo el conjunto de casos por grupos definidos por una de las variables. Para ilustrarlo, vamos utilizar la variable recién creada DIM para hacer tres grupos de empresas, tras lo cual calcularemos la media de las rentabilidades para cada grupo:Hemos utilizado el operador pipe %>% para concatenar diferentes instrucciones de dplyr: primero agrupar casos, y luego calcular las medias de cada grupo. Es decir, en este caso se podría “traducir” la línea de código como: “Toma el data frame”interestelar_100”, divide sus casos en grupos según el valor de la variable DIM, y para cada grupo calcula la media de la variable RENFIN”. Es de destacar que hay elementos que tienen como valor categórico en DIM “NA” (es decir, hay dato). Esto se debe que, cuando se creo DIM, había una empresa sin valor de ACTIVO (exactamente, “Vega Transport”).","code":"\n#Extrayendo información de las variables de un data frame\nsummarise(interestelar_100, RENFIN_media = mean(RENFIN)) ##   RENFIN_media\n## 1     121.5434\ninterestelar_100 %>%\n  group_by(DIM) %>%\n  summarise(RENFIN_media = mean(RENFIN))## # A tibble: 4 × 2\n##   DIM      RENFIN_media\n##   <fct>           <dbl>\n## 1 REDUCIDA         125.\n## 2 MEDIA            114.\n## 3 ALTA             126.\n## 4 <NA>             115."},{"path":"almacenando-y-manipulando-datos..html","id":"exportando-datos.","chapter":"2 Almacenando y manipulando datos.","heading":"2.4  Exportando datos.","text":"Antes de concluir el capítulo, vamos tratar brevemente el aspecto de la exportación de datos.R cuenta con un formato propio de datos, que se traduce en archivos de extensión “RData”, y que puede incluir cualquier objeto de R. Vamos exportar el data frame “interestelar_100” como el archivo de datos de R “interestelar_100.RData”. Posteriormente, borraremos el data frame del Environment y recuperaremos los datos cargando ese archivo “interestelar_100.RData”.Para exportar el data frame “interestelar_100” al archivo de formato R, “interestelar_100.RData”, utilizaremos la función save():Puede comprobarse cómo se ha generado el archivo correspondiente en la carpeta de proyecto. Para comprobar que la exportación es correcta, vamos borrar del Global Environment el data frame “interestelar_100” con la función rm() (remove). Después, cargaremos el archivo “interestelar_100.RData” con la función load(). Como resultado, podremos comprobar que tenemos un nuevo data frame “interestelar_100” que es exactamente igual al que teníamos al principio:Por supuesto, hay más formatos en los que se pueden exportar datos desde R. Por ejemplo, un archivo de Microsoft® Excel®. Un modo de hacerlo es haciendo uso de la función write_xlsx() del paquete {writexl}. Para que en la hoja de cálculo resultante se incluyan los nombres de las filas (empresas eólicas), hemos tenido previamente que crear un vector con el nombre de estas (vector “NOMBRE”), mediante la función row.names(), y unir ese vector al data frame “interestelar_100”, modo de primera columna, creando un nuevo finalmente un data frame llamado “interestelar_100n”, para lo que se ha utilizado la función cbind(), que permite pegar columnas de datos que tengan un mismo número de filas.Como resultado de todo el código, se ha obtenido el archivo de Microsoft® Excel® “interestelar_100_new.xlsx”:","code":"\n## Exportación de datos\n\n# Exportando data frame a formato R (.RData)\nsave(interestelar_100, file = \"interestelar_100.RData\")\n# Borrando el data frame interestelar_100\nrm(interestelar_100)\n\n# Importando el archivo .RData con los mismos datos\nload(\"interestelar_100.RData\")\nsummary (interestelar_100)##      SEDE             GALAXIA              ACTIVO            FPIOS        \n##  Length:104         Length:104         Min.   :  17.55   Min.   :   8.67  \n##  Class :character   Class :character   1st Qu.:  51.16   1st Qu.:  22.46  \n##  Mode  :character   Mode  :character   Median : 159.08   Median :  59.81  \n##                                        Mean   : 259.98   Mean   : 115.64  \n##                                        3rd Qu.: 231.86   3rd Qu.: 109.67  \n##                                        Max.   :2966.52   Max.   :1441.44  \n##                                        NA's   :1                          \n## \n##       ING              MARGEN           RES              SOLVE      \n##  Min.   :  19.43   Min.   :17.42   Min.   :  11.02   Min.   :123.1  \n##  1st Qu.:  44.33   1st Qu.:48.96   1st Qu.:  26.55   1st Qu.:160.4  \n##  Median : 103.91   Median :56.72   Median :  54.61   Median :174.7  \n##  Mean   : 238.23   Mean   :60.35   Mean   : 128.22   Mean   :179.5  \n##  3rd Qu.: 254.88   3rd Qu.:73.44   3rd Qu.: 167.98   3rd Qu.:194.0  \n##  Max.   :2313.99   Max.   :95.00   Max.   :1189.86   Max.   :325.3  \n##                    NA's   :1                                        \n## \n##      ACTCOR           PASCOR           LIQUIDEZ         APALANCA     \n##  Min.   :  6.81   Min.   :   6.66   Min.   :0.2253   Min.   : 44.38  \n##  1st Qu.: 17.80   1st Qu.:  20.49   1st Qu.:0.4962   1st Qu.:106.20  \n##  Median : 23.57   Median :  59.92   Median :0.6984   Median :133.73  \n##  Mean   : 64.64   Mean   : 107.89   Mean   :0.6614   Mean   :145.60  \n##  3rd Qu.: 58.27   3rd Qu.: 101.88   3rd Qu.:0.8272   3rd Qu.:164.55  \n##  Max.   :889.96   Max.   :1143.81   Max.   :1.2580   Max.   :433.55  \n##                                                      NA's   :1       \n## \n##      RENECO          RENFIN           EMPLEA           FLOTA      \n##  Min.   :28.04   Min.   : 70.18   Min.   :  7.00   Min.   : 3.00  \n##  1st Qu.:45.02   1st Qu.:101.69   1st Qu.: 22.00   1st Qu.:15.00  \n##  Median :50.43   Median :119.24   Median : 30.00   Median :24.50  \n##  Mean   :52.38   Mean   :121.54   Mean   : 35.78   Mean   :27.35  \n##  3rd Qu.:59.41   3rd Qu.:139.48   3rd Qu.: 42.00   3rd Qu.:40.00  \n##  Max.   :93.85   Max.   :262.25   Max.   :100.00   Max.   :50.00  \n##  NA's   :1                        NA's   :1                       \n## \n##      COSTOP            CAPEX               IMD               EFLO          \n##  Min.   :   0.98   Min.   :   1.038   Min.   :  0.4779   Length:104        \n##  1st Qu.:  14.78   1st Qu.:   5.474   1st Qu.:  1.4395   Class :character  \n##  Median :  29.82   Median :  11.155   Median :  4.9474   Mode  :character  \n##  Mean   : 110.04   Mean   :  39.395   Mean   : 16.3313                     \n##  3rd Qu.: 115.32   3rd Qu.:  34.084   3rd Qu.: 19.7304                     \n##  Max.   :1124.13   Max.   :1087.196   Max.   :202.4779                     \n##  NA's   :1                                                                 \n## \n##      RUTAS             IDIG            IDIVERSE           IFIDE      \n##  Min.   :  42.0   Min.   : 0.6007   Min.   : 0.7737   Min.   :26.44  \n##  1st Qu.: 203.2   1st Qu.:10.7957   1st Qu.:13.0142   1st Qu.:37.32  \n##  Median : 327.5   Median :16.7092   Median :22.3920   Median :39.18  \n##  Mean   : 397.7   Mean   :18.1645   Mean   :23.8114   Mean   :39.39  \n##  3rd Qu.: 411.2   3rd Qu.:24.8249   3rd Qu.:34.6776   3rd Qu.:40.40  \n##  Max.   :2915.0   Max.   :60.8325   Max.   :71.3507   Max.   :73.30  \n##                                     NA's   :1         NA's   :1      \n## \n##     GMEDRUT              DIST                BMAL               RATIO       \n##  Min.   :0.005197   Min.   :   0.1513   Min.   :    31.45   Min.   :0.2804  \n##  1st Qu.:0.095229   1st Qu.:  18.3251   1st Qu.:   356.56   1st Qu.:0.4479  \n##  Median :0.198253   Median :  57.7865   Median :  1285.59   Median :0.5036  \n##  Mean   :0.334388   Mean   : 164.9319   Mean   :  8292.13   Mean   :0.5230  \n##  3rd Qu.:0.520769   3rd Qu.: 197.3137   3rd Qu.:  3206.33   3rd Qu.:0.5941  \n##  Max.   :1.550435   Max.   :2151.6418   Max.   :262555.82   Max.   :0.9385  \n##  NA's   :1          NA's   :2           NA's   :2           NA's   :1       \n## \n##   ACTIVOS_ACUM            DIM    \n##  Min.   :   17.55   REDUCIDA:35  \n##  1st Qu.: 1143.06   MEDIA   :33  \n##  Median : 2696.22   ALTA    :35  \n##  Mean   : 5454.11   NA's    : 1  \n##  3rd Qu.: 7861.85                \n##  Max.   :26778.41                \n##  NA's   :1\n# Exportando el data frame interestelar_100 a Microsoft (R) Excel (R)\nlibrary(writexl)\nNOMBRE <- row.names(interestelar_100)\ninterestelar_100n <- cbind(NOMBRE, interestelar_100)\nwrite_xlsx(interestelar_100n, path = \"interestelar_100_new.xlsx\")\n\n#Fin del script :)"},{"path":"almacenando-y-manipulando-datos..html","id":"las-variables-de-la-base-de-datos-del-proyecto-r-stars.","chapter":"2 Almacenando y manipulando datos.","heading":"2.5  Las variables de la base de datos del proyecto R-Stars.","text":"La base de datos completa del proyecto R-Stars contiene datos económicos, financieros y de diversa índole de 300 empresas de trasnporte interestelar de mercancías, recogidos en unas 30 variables.El sector del transporte interestelar se extiende lo largo y ancho de 5 galaxias. Son 30 los planetas donde se localizan las sedes de las 300 empresas.Las galaxias donde se desarrolla este tipo de actividad son:Andrómeda (M31), Vía Láctea y Galaxia del Triángulo (M33): Son galaxias espirales, caracterizadas por un disco con brazos espirales, un bulbo central y un halo estelar. Presentan componentes como discos delgados y gruesos, bulbos y halos, con poblaciones estelares de diferentes edades y metalicidades.Andrómeda (M31), Vía Láctea y Galaxia del Triángulo (M33): Son galaxias espirales, caracterizadas por un disco con brazos espirales, un bulbo central y un halo estelar. Presentan componentes como discos delgados y gruesos, bulbos y halos, con poblaciones estelares de diferentes edades y metalicidades.Gran Nube de Magallanes (LMC) y Pequeña Nube de Magallanes (SMC): Son galaxias irregulares, con estructuras menos definidas, ricas en gas y con intensa formación estelar. Su morfología es más caótica y carecen de un bulbo central prominente.Gran Nube de Magallanes (LMC) y Pequeña Nube de Magallanes (SMC): Son galaxias irregulares, con estructuras menos definidas, ricas en gas y con intensa formación estelar. Su morfología es más caótica y carecen de un bulbo central prominente.En cuanto las variables que componen la base de dato, son las siguientes (importante: estos datos se refieren la base de datos madre, de 300 casos. se refiere la muestra de 104 observaciones utilizada para desarrollar los ejemplos de este capítulo):ACTCOR. Activo corriente. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 6.810, ‘q1’: 17.450, ‘median’: 24.005, ‘mean’: 59.787, ‘q3’: 55.635, ‘max’: 889.956}.\nNúmero estimado de outliers: 36.ACTIVO. Valor total de los recursos de la empresa (miles de PAVOs). Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 17.550, ‘q1’: 50.530, ‘median’: 156.315, ‘mean’: 241.407, ‘q3’: 227.017, ‘max’: 2966.520}.\nNúmero estimado de outliers: 45.APALANCA. Nivel de apalancamiento (Pasivo total / Fondos propios). Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 44.380, ‘q1’: 100.593, ‘median’: 121.860, ‘mean’: 132.667, ‘q3’: 154.550, ‘max’: 433.550}.\nNúmero estimado de outliers: 13.BMAL. Beneficio medio por año luz. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 24.135, ‘q1’: 466.114, ‘median’: 1491.416, ‘mean’: 6084.857, ‘q3’: 4368.461, ‘max’: 262555.815}.\nNúmero estimado de outliers: 31.CAPEX. Gastos de capital. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 1.038, ‘q1’: 6.156, ‘median’: 11.600, ‘mean’: 33.748, ‘q3’: 34.329, ‘max’: 1087.196}.\nNúmero estimado de outliers: 29.COSTOP. Costes operativos. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 0.980, ‘q1’: 15.305, ‘median’: 31.195, ‘mean’: 97.803, ‘q3’: 105.143, ‘max’: 1422.300}.\nNúmero estimado de outliers: 28.DIST. Distancia operada globalmente. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 0.151, ‘q1’: 12.606, ‘median’: 44.229, ‘mean’: 133.686, ‘q3’: 165.292, ‘max’: 2151.642}.\nNúmero estimado de outliers: 26.EFLO. Edad media de la flota (categorías). Tipo de datos: carácter/categórico. NAs: 0.\nDistribución/aproximación: {‘ANTIGUA’: 148, ‘MADURA’: 90, ‘RENOVADA’: 62}.EFLO_FACTOR. Factor derivado de la edad media de la flota. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 0.800, ‘q1’: 0.800, ‘median’: 1.000, ‘mean’: 0.943, ‘q3’: 1.000, ‘max’: 1.200}.\nNúmero estimado de outliers: 0.EMPLEA. Número de empleados. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 7.000, ‘q1’: 22.750, ‘median’: 30.000, ‘mean’: 36.287, ‘q3’: 44.000, ‘max’: 106.000}.\nNúmero estimado de outliers: 16.FLOTA. Número de cargueros espaciales. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 3.000, ‘q1’: 15.000, ‘median’: 24.000, ‘mean’: 26.143, ‘q3’: 38.000, ‘max’: 50.000}.\nNúmero estimado de outliers: 0.FJUR. Forma jurídica de la empresa. Tipo de datos: carácter/categórico. NAs: 0.FPIOS. Fondos propios de la empresa. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 8.670, ‘q1’: 22.530, ‘median’: 57.430, ‘mean’: 111.042, ‘q3’: 108.408, ‘max’: 1453.450}.\nNúmero estimado de outliers: 23.. Inspiración temática de la empresa. Tipo de datos: carácter/categórico. NAs: 0.\nDistribución/aproximación: {‘Star Wars’: 84, ‘2001: Una Odisea en el Espacio’: 84, ‘Dune’: 52, ‘Star Trek’: 47, ‘Interstellar’: 33}.GALAXIA. Galaxia principal en la que opera la empresa. Tipo de datos: carácter/categórico. NAs: 0.GALAXY_FACTOR. Factor de distancia intergaláctica. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 1.000, ‘q1’: 1.000, ‘median’: 2.500, ‘mean’: 2.812, ‘q3’: 4.500, ‘max’: 5.000}.\nNúmero estimado de outliers: 0.GMEDRUT. Distancia media de las rutas (años luz). Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 0.005, ‘q1’: 0.112, ‘median’: 0.224, ‘mean’: 0.319, ‘q3’: 0.448, ‘max’: 1.550}.\nNúmero estimado de outliers: 10.IDIG. Índice de digitalización. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 0.338, ‘q1’: 8.498, ‘median’: 15.672, ‘mean’: 17.234, ‘q3’: 23.848, ‘max’: 60.833}.\nNúmero estimado de outliers: 4.IDIVERSE. Índice de diversificación. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 0.000, ‘q1’: 11.655, ‘median’: 21.165, ‘mean’: 22.210, ‘q3’: 32.390, ‘max’: 78.298}.\nNúmero estimado de outliers: 2.IFIDE. Índice de fidelización. Tipo de datos: numérico (real o entero). NAs: 0. Observaciones corruptas: 0.\nDistribución/aproximación: {‘min’: 5.261, ‘q1’: 36.928, ‘median’: 39.268, ‘mean’: 39.002, ‘q3’: 40.432, ‘max’: 79.154}.\nNúmero estimado de outliers: 33.IMD. Gasto en +D. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 0.478, ‘q1’: 1.409, ‘median’: 4.700, ‘mean’: 15.203, ‘q3’: 18.240, ‘max’: 202.478}.\nNúmero estimado de outliers: 40.ING. Ingresos generados por la actividad principal de la empresa. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 19.400, ‘q1’: 42.005, ‘median’: 103.470, ‘mean’: 218.386, ‘q3’: 240.973, ‘max’: 2471.540}.\nNúmero estimado de outliers: 23.LIQUIDEZ. Cociente entre ACTCOR y PASCOR. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 0.225, ‘q1’: 0.526, ‘median’: 0.706, ‘mean’: 0.702, ‘q3’: 0.836, ‘max’: 3.118}.\nNúmero estimado de outliers: 7.MARGEN. Margen operativo como porcentaje sobre los ingresos. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 12.432, ‘q1’: 48.095, ‘median’: 57.449, ‘mean’: 60.160, ‘q3’: 73.610, ‘max’: 96.489}.\nNúmero estimado de outliers: 0.NOMBRE. Nombre de la empresa. Tipo de datos: carácter/categórico. NAs: 0.PASCOR. Pasivo corriente. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 6.660, ‘q1’: 20.222, ‘median’: 59.224, ‘mean’: 97.773, ‘q3’: 100.558, ‘max’: 1143.810}.\nNúmero estimado de outliers: 25.RENECO. Rentabilidad económica. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 24.082, ‘q1’: 44.110, ‘median’: 50.323, ‘mean’: 51.055, ‘q3’: 57.745, ‘max’: 93.847}.\nNúmero estimado de outliers: 4.RENFIN. Rentabilidad financiera. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 51.388, ‘q1’: 95.620, ‘median’: 113.048, ‘mean’: 116.138, ‘q3’: 131.138, ‘max’: 262.245}.\nNúmero estimado de outliers: 6.RES. Resultado del ejercicio (beneficio neto). Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 9.620, ‘q1’: 25.315, ‘median’: 59.220, ‘mean’: 120.583, ‘q3’: 128.178, ‘max’: 1561.930}.\nNúmero estimado de outliers: 28.RUTAS. Número de rutas atendidas. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 36.000, ‘q1’: 176.000, ‘median’: 310.000, ‘mean’: 365.580, ‘q3’: 436.500, ‘max’: 3138.000}.\nNúmero estimado de outliers: 12.SEDE. Planeta principal donde se encuentra la base de operaciones de la empresa. Tipo de datos: carácter/categórico. NAs: 0.SOLVENCIA. Capacidad de la empresa para cubrir sus obligaciones financieras largo plazo. Tipo de datos: numérico (real o entero). NAs: 0.\nDistribución/aproximación: {‘min’: 123.070, ‘q1’: 164.703, ‘median’: 182.060, ‘mean’: 185.718, ‘q3’: 199.415, ‘max’: 325.320}.\nNúmero estimado de outliers: 10.","code":""},{"path":"almacenando-y-manipulando-datos..html","id":"materiales-para-realizar-las-prácticas-del-capítulo.","chapter":"2 Almacenando y manipulando datos.","heading":"2.6  Materiales para realizar las prácticas del capítulo.","text":"En esta sección se muestran los links de acceso los diferentes materiales (scripts, datos…) necesarios para llevar cabo los contenidos prácticos del capítulo.Datos (en formato Microsoft® Excel®:interestelar_100.xlsx (obtener aquí)Scripts:explora_rstars.R (obtener aquí)","code":""},{"path":"gráficos..html","id":"gráficos.","chapter":"3 Gráficos.","heading":"3 Gráficos.","text":"","code":""},{"path":"gráficos..html","id":"el-valor-de-las-imágenes.","chapter":"3 Gráficos.","heading":"3.1  El valor de las imágenes.","text":"Una imagen vale más que mil palabras.En el análisis de datos, las representaciones gráficas son un adorno, sino el lenguaje común entre datos, analistas y decisores.Un buen gráfico puede condensar miles de observaciones en patrones perceptibles de un vistazo, revelando tendencias, rupturas, relaciones y valores atípicos que las tablas suelen ocultar.En la fase exploratoria del análisis de datos, orienta hipótesis y elecciones de modelos; en la validación comprueba supuestos y diagnostica errores; y en la comunicación transforma resultados complejos en evidencia comprensible y accionable.Su fuerza descansa en cómo aprovechan la percepción humana —posición, longitud, forma, color— para reducir la carga cognitiva y evitar interpretaciones equívocas. Por ello, dominar el diseño con intención, elegir el tipo de gráfico adecuado cada pregunta y aplicar principios de claridad, comparabilidad y honestidad es tan crucial como cualquier técnica estadística.Este capítulo recorre ese camino: del boceto exploratorio la visualización final que informa, persuade y, sobre todo, ayuda decidir mejor.","code":""},{"path":"gráficos..html","id":"tidyverse-para-gráficos-ggplot2.","chapter":"3 Gráficos.","heading":"3.2  Tidyverse para gráficos: ggplot2.","text":"R, en su instalación básica (paquete {base}), cuenta con funciones destinadas crear gráficos y, de este modo, visualizar nuestros datos fin de generar información y extraer conclusiones de un modo sencillo.obstante, estas funciones, veces, se quedan “cortas”, o requieren de un complejo y/o extenso código. Esta es la razón por la que en el Tidyverse se incluyó un paquete específico destinado la construcción de gráficos de un modo flexible y amigable. Recordemos que el Tidyverse es un conjunto de paquetes con una filosofía común, como es el uso de ciertas estructuras gramaticales, que facilitan muchas de las tareas y análisis que podrían hacerse con el lenguaje R estándar.Este paquete destinado la producción de gráficos es ggplot2, que proporciona unas herramientas muy flexibles para visualizar conjuntos de datos. continuación, se expondrán los fundamentos de la sintaxis de ggplot2 y se indicará cómo construir algunos de los gráficos más habituales.Para ilustrar la creación de gráficos, vamos suponer que trabajamos dentro de un proyecto que hemos creado previamente, de nombre “explora”. Dentro de la carpeta del proyecto guardaremos el script llamado “ggplot2_rstars.R” y el archivo de Microsoft® Excel® llamado “interestelar_100.xlsx”. Para decargar los ficheros, ve al final de este capítulo y pincha en los enlaces.Si abrimos “interestelar_100.xlsx”, comprobaremos que se compone de tres hojas. La primera muestra un aviso sobre el uso exclusivo que se debe dar los datos incorporados; la segunda recoge la descripción de las variables consideradas; y la tercera (hoja “Datos”) guarda los datos que debemos importar desde R-Studio. Estos datos se corresponden con diferentes variables económico-financieras y de diversa índole de una muestra de empresas que se dedican al transporte de mercancías interestelar.Vamos abrir nuestro script “ggplot2_rstars.R” con File → Open File… (o haciendo click en el archivo correspondiente en la ventana inferior derecha de RStudio, pestaña “Files”). Este script contiene el programa que vamos ir ejecutando en la práctica.Tras abrir el script “explora_ggplot2.R” en el editor de R-Studio, observaremos que la primera línea / instrucción es:La instrucción tiene como objeto limpiar el Global Environment (memoria) de objetos de anteriores sesiones de trabajo.Luego, si queremos despreocuparnos de la carga de los paquetes que utilizaremos en el script, podemos activarlos ahora:Recuerda que, si alguno de los paquetes está instalado en la máquina, dará error. Entonces, habrá que instalarlo y volver ejecutar el bloque de código anterior.Para importar los datos que hay en la hoja “Datos” del archivo de Microsoft® Excel® “interestelar_100.xlsx”, ejecutaremos el código:Podemos observar cómo en el Environment ya aparece un objetodata frame* denominado “interestelar_100”, que contiene 104 filas (una por empresa) y 28 variables. Podemos listar las variables de modo visual con la función gt_plt_summary() del paquete gtExtras:","code":"\n## Generando gráficos con {ggplot2}\n\n# Limpiando el Global Environment\nrm(list = ls())\n# Cargando paquetes\nlibrary(readxl)\nlibrary (ggplot2)\nlibrary(gtExtras)\nlibrary (ggExtra)\nlibrary(ggrepel)\n# Importando datos desde Excel\ninterestelar_100 <- read_excel(\"interestelar_100.xlsx\",\n                               sheet = \"Datos\",\n                               na = c(\"n.d.\"))\ninterestelar_100 <- data.frame(interestelar_100, row.names = 1)\n# visualizando el data frame de modo elegante con {gtExtras}\ndatos_df_graph <- gt_plt_summary(interestelar_100)\ndatos_df_graph"},{"path":"gráficos..html","id":"gráficos-de-una-variable-histogramas-gráficos-de-densidad-gráficos-de-caja-o-boxplots.","chapter":"3 Gráficos.","heading":"3.3  Gráficos de una variable: histogramas, gráficos de densidad, gráficos de caja o boxplots.","text":"La primera instrucción para crear un gráfico con el paquete ggplot2 es ggplot(). continuación, entre paréntesis, se deberán aportar una serie de argumentos o informaciones. Estas informaciones irán definiendo el gráfico en mayor o menor detalle.En realidad, lo que se hace es definir el conjunto de datos representar (que suelen estar contenidos en un data frame, o en varios), y partir de ellos se van añadiendo capas gráficas o geoms, que son caracterizadas con ciertos atributos estéticos (aesthetics, o aes).","code":""},{"path":"gráficos..html","id":"histograma.","chapter":"3 Gráficos.","heading":"3.3.1  Histograma.","text":"Uno de los gráficos indispensables para tener una idea de la distribución de frecuencias que siguen los casos (en nuestro ejemplo, las empresas eólicas) con relación una variable métrica es el histograma. Vamos construir un histograma para la variable LIQUIDEZ, que recoge la ratio de liquidez, calculada como cociente entre el activo corriente y el pasivo corriente, y que mide la capacidad de la empresa para hacer frente sus deudas corto plazo con activos corto plazo. Para algunos sectores se acepta que un valor adecuado de la ratio oscila entre 1,5 y 2 (menos de 1,5 puede indicar tensiones de caja, mientras que más de 2 puede ser un síntoma de capital ocioso).El código será:Como acabamos de decir, en primer lugar viene el comando ggplot(), seguido de unos paréntesis que recogen ciertas informaciones:“data =”, seguido de la fuente que almacena los datos graficar (en nuestro caso, el data frame “interestelar_100”).“data =”, seguido de la fuente que almacena los datos graficar (en nuestro caso, el data frame “interestelar_100”).“map =”, o “mapping =”, que define los aspectos del gráfico que dependen del valor de alguna o algunas variables (es decir, se fijan roles o “misiones” diferentes variables para realizar el gráfico). Siempre que alguna característica del gráfico sea “fija”, sino que dependa de los valores que toma una variable, tal variable deberá ir indicada dentro de un elemento estético (aes). En el código de ejemplo, el elemento aes sirve para indicar que las coordenadas del eje x que toman los casos representar, dependen de los valores de la variable LIQUIDEZ.“map =”, o “mapping =”, que define los aspectos del gráfico que dependen del valor de alguna o algunas variables (es decir, se fijan roles o “misiones” diferentes variables para realizar el gráfico). Siempre que alguna característica del gráfico sea “fija”, sino que dependa de los valores que toma una variable, tal variable deberá ir indicada dentro de un elemento estético (aes). En el código de ejemplo, el elemento aes sirve para indicar que las coordenadas del eje x que toman los casos representar, dependen de los valores de la variable LIQUIDEZ.Para indicar que las siguientes líneas continúan con el código del gráfico, se añade al final de esta línea el símbolo “+”.En la segunda línea, se establece el tipo de gráfico que se va realizar, mediante la inclusión de un elemento geom. Para decir que lo que queremos construir es un histograma, el elemento geom será geom_histogram().El resultado del código anterior es el siguiente gráfico:Nota: se puede prescindir en la línea de la función ggplot() del argumento map = y pasar los elementos estéticos aes = las diversas capas o geoms que se usen. Por ejemplo, este código da el mismo resultado:Por supuesto, ggplot2 permite personalizar y refinar la apariencia del gráfico. Uno de los aspectos que nos puede interesar modificar es el número de intervalos en los que queda dividido el rango de valores que puede tomar la variable (“grosor” de las barras), o bins. Por defecto, el número es 30. Para reducir este número de barras 20, por ejemplo, añadiremos en la línea del geom el argumento bins =:veces es difícil determinar el número adecuado de intervalos. Puede obtarse, por ejemplo, por el método de Sturges, que sugiere que este número sea calculado como el máximo entre 1 y el número k redondeado hacia arriba:\\[\nk = \\left\\lceil \\log_{2}(n) + 1 \\right\\rceil\n\\]donde n es el tamaño muestral.Así, utilizando la función nclass.Sturges() del paquete {base}:Nota: la función .finite() devuelve un vector lógico indicando qué elementos son numéricos finitos:TRUE → números reales/enteros (y complejos) finitos.TRUE → números reales/enteros (y complejos) finitos.FALSE → Inf, -Inf, NaN y NA.FALSE → Inf, -Inf, NaN y NA.Sturges propone tan solo 8 intervalos de valores.continuación, vamos modificar el color de las barras. Para el borde de estas, se utiliza el argumento colour =; y, para el relleno, fill =. Además, vamos mejorar la presentación del gráfico añadiéndole un título y un subtítulo, y unas etiquetas en los ejes. Hay que prestar atención los signos “+” incluidos para que R entienda que el código de la siguiente línea pertenece al mismo gráfico que estamos diseñando:Puede ser que nos interese diferenciar los casos según grupos preestablecidos. Por ejemplo, entre las variables de nuestro data frame “interestelar_100”, existe una variable categórica denominada EFLO, que clasifica las empresas, según la antigüedad media de su flota, en “ANTIGUA”, “MADURA” o “RENOVADA”. Lo que vamos hacer es crear, en el mismo gráfico, un histograma del ratio de liquidez, pero para cada categoría de DIMENSION. Para ello, habrá que incluir esta variable categórica en las características estéticas del geom, en concreto mediante el argumento “fill =”. Es necesario hacerlo dentro del elemento aes, ya que el resultado (color de grupo de empresas) depende del valor que toma la variable EFLO para cada caso o empresa:Como puede verificarse, se superponen los tres histogramas, con tres colores diferentes, dependiendo de la dimensión considerada. Además, aparece, al lado derecho del gráfico, una leyenda que detalla qué color se asocia cada uno de los grupos de empresas. La función scale_fill_brewer() nos permite personalizar la paleta de colores utilizar (para ver las paletas disponibles, podemos consultar esta sección de (Wickham 2021).","code":"\n# Histograma\nggplot(data = interestelar_100, map = aes(x = LIQUIDEZ)) +\n       geom_histogram()\n# Histograma\nggplot(data = interestelar_100) +\n       geom_histogram( aes(x = LIQUIDEZ))\nggplot(data = interestelar_100, map = aes(x = LIQUIDEZ)) +\n       geom_histogram(bins = 20)\n# Nº de bins según Sturges (con NA/Inf fuera)\nnbins <- nclass.Sturges(interestelar_100$LIQUIDEZ[is.finite(interestelar_100$LIQUIDEZ)])\nggplot(data = interestelar_100, map = aes(x = LIQUIDEZ)) +\n       geom_histogram(bins = nbins)\nggplot(data = interestelar_100, map = aes(x = LIQUIDEZ)) +\n  geom_histogram(bins = nbins, colour = \"red\", fill = \"orange\") +\n  ggtitle(\"RATIO DE LIQUIDEZ\", subtitle = \"Transporte Interestelar\")+\n  xlab(\"Liquidez (ratio)\") +\n  ylab(\"Frecuencias\")\nggplot(data = interestelar_100, map = aes(x = LIQUIDEZ)) +\n  geom_histogram(bins = nbins, colour = \"red\", aes(fill = EFLO)) +\n  scale_fill_brewer(palette = \"Oranges\") +\n  ggtitle(\"RATIO DE LIQUIDEZ\", subtitle = \"Transporte Interestelar\")+\n  xlab(\"Liquidez (ratio)\") +\n  ylab(\"Frecuencias\")"},{"path":"gráficos..html","id":"gráfico-de-densidad.","chapter":"3 Gráficos.","heading":"3.3.2  Gráfico de densidad.","text":"Un gráfico parecido al histograma es el de densidad. Un gráfico de densidad estima la función de densidad de probabilidad empírica de la variable representada. En realidad, podemos considerarlo como un histograma “suavizado”. Probemos ejecutar este código:En el código se observa la utilización del tipo de gráfico geom_density(). Además, desaparece el número de intervalos o bins, y se puede dotar la función de densidad estimada de un color en su borde (colour=), y de un color de relleno (fill=).Como en el caso del histograma, se puede crear una función de densidad estimada para cada grupo de empresas (según la variable EFLO), incluyendo la característica colour= y fill= dentro del correspondiente aes():En efecto, el argumento fill= ha pasado integrarse, en el geom, dentro de un elemento aes, ya que el color de relleno va variar dependiendo del grupo de pertenencia de la empresa (variable EFLO). Por otro lado, también se ha añadido el argumento alpha=. Esta información consiste en un número de 0 1 que gradúa el grado de transparencia / opacidad de los rellenos de las figuras (en este caso las funciones de densidad estimadas) incluidas en los gráficos.","code":"\nggplot(data = interestelar_100, map = aes(x = LIQUIDEZ)) +\n  geom_density(colour = \"red\", fill = \"orange\") +\n  ggtitle(\"RATIO DE LIQUIDEZ\", subtitle = \"Transporte Interestelar\")+\n  xlab(\"Liquidez (ratio)\") +\n  ylab(\"Densidad\")\nggplot(data = interestelar_100, map = aes(x = LIQUIDEZ)) +\n  geom_density(aes(fill = EFLO), colour = \"red\", alpha = 0.70, ) +\n  scale_fill_brewer(palette = \"Oranges\") +\n  ggtitle(\"RATIO DE LIQUIDEZ\", subtitle = \"Transporte Interestelar\") +\n  xlab(\"Liquidez (ratio)\") +\n  ylab(\"Densidad\")"},{"path":"gráficos..html","id":"gráfico-de-caja-o-box-plot.","chapter":"3 Gráficos.","heading":"3.3.3  Gráfico de caja o Box-Plot.","text":"Un tipo muy interesante de gráfico es el de “caja” (box-plot), que informa de la dispersión de una variable. Fijémonos en el siguiente código:Puede observarse cómo en el “mapeo” se fija la variable que va determinar las coordenadas del eje “y”. Como es una variable, hay que incluirla en el “mapeo” mediante una característica aes. El geom o tipo de gráfico es geom_boxplot(), y en este caso le hemos añadido ninguna característica específica. Las últimas líneas configuran los títulos del gráfico y del eje “y”. El resultado de ejecutar el código es el siguiente gráfico:Nota: Es necesario añadir en el eje “x” una variable “vacía”, y en el nombre de dicho eje también un campo vacío, para evitar que en dicho eje salga una escala y el título “x”.El gráfico se caracteriza por una “caja” (rectángulo) central. Esta caja está limitada por el primer y tercer cuartil, luego recoge el 50% de los casos con una ratio de liquidez superior al 25% de los casos con menor ratio, y por debajo del 25% de los casos con la ratio más alta. Así, la altura de la caja es la diferencia entre los cuartiles tercero y primero, que es lo que se denomina “rango intercuartílico” (IQR por las siglas en inglés). La caja, su vez, está dividida en dos zonas por una línea horizontal, que es la mediana de la distribución: la ratio de liquidez que divide la muestra en dos grupos con el mismo número de elementos, uno con los casos de mayor ratio de liquidez, y otro con los casos de menor ratio.Por encima y por debajo de la caja se disponen dos segmentos (llamados “bigotes”). Estos “bigotes” recogen los casos con valores en la variable inferiores al primer cuartil (comenzando por la base de la caja, hacia abajo), o superiores al tercer cuartil (comenzando por el techo de la caja, hacia arriba); y que están menos de 1.5 veces la altura de la caja. Los casos con valores de la ratio de liquidez inferiores al primer cuartil (por abajo) y superiores al tercero (por arriba), que están alejados de la caja en más de 1.5 veces la altura de esta, se indican con puntos, y se corresponden con los casos conocidos como casos atípicos o outliers. La identificación de los outliers es una fase muy importante la hora de aplicar algunas técnicas estadísticas.En esta práctica, comprobamos cómo, en el caso de la ratio de liquidez (LIQUIDEZ), existen outliers, es decir, casos que presentan ratios anormalmente elevadas o bajas.En cambio, vamos representar ahora la variable RES (resultado del ejercicio o beneficio neto):En esta ocasión apreciamos cómo existen 4 empresas outliers, es decir, compañías con resultados atípicamente elevados (superiores al tercer cuartil más 1,5 veces el rango intercuartílico del resultado del beneficio neto. Dos de las empresas superan, como beneficio neto o resultado del ejercicio, el millón de PAVOs.ggplot2 permite integrar en el gráfico medidas estadísticas y otros cálculos. Por ejemplo, en el box-plot se representa el valor de la mediana; pero el de la media. Si queremos incluir el valor de la media (u otro estadístico), podemos calcularlo e integrarlo con la función stat_summary(), algo parecido al summarise() de dplyr. El argumento fun = \"mean\" indica que la medida calcular y representar es la media aritmética, el argumento geom = \"point\" el tipo de gráfico para representar esa medida (un punto). También hay otros argumentos opcionales:Se aprecia cómo el valor de la rentabilidad económica media se ha insertado como un punto azul oscuro grueso dentro del gráfico de caja (tiene un valor algo superior la mediana).Vamos refinar el box-plot anterior. Por ejemplo, quizá nos pueda interesar crear un box-plot para cada grupo de empresas, según el tamaño del grupo empresarial de pertenencia (atributo DIMENSION). Esto lo conseguiremos con el código:Para construir una caja por categoría de la variable EFLO, se ha incluido, en el “mapeo” de la primera línea, el eje x con la variable tal variable. Como, además, queremos que cada caja sea de un color diferente, hemos hecho que los colores de estas dependan de la variable EFLO; añadiendo en el aes() del geom_boxplot respecto la característica fill= (que se refiere al color de relleno de las cajas). También se ha incluido una línea con el scale_fill_brewer() para que los colores de las cajas consistan en diferentes tonalidades de naranjas.En el ejemplo, el primer bloque de stat_summary() consigue puntear, para cada grupo de empresas, la media de RES en dicho grupo, en color azul oscuro. Para comparar mejor estas medias, se ha procedido unir los puntos con unos segmentos o líneas de color azul oscuro, lo que se consigue con el segundo bloque de stat_summary(). La última línea de ese bloque, map = (aes(group = TRUE)), obliga que las líneas vayan de una media otra de los grupos (de punto azul oscuro punto azul oscuro), es decir, al añadir aes(group = TRUE) (equivalente aes(group = 1)), se le pide ggplot() que ignore el agrupamiento por EFLO y conecte todos los promedios con una sola línea.Como última extensión, se ha considerado que, veces, es conveniente tener en cuenta la posición de cada caso individual dentro del gráfico. Una opción es utilizar una capa o bloque geom_jitter(). Con este geom se dispondrán, para cada grupo, los valores individuales de la variable RES; y para que estos, en su caso, se solapen, se situarán un poco más la izquierda o la derecha, de modo aleatorio. Como los outliers son ya casos individuales, para que se dupliquen con los provenientes del “jitter”, se indicará en el geom_boxplot() que, en ese bloque gráfico, se señalen los outliers. Esto se conseguirá con el argumento outlier.shape = NA. El código, en definitiva, será:Como puede observarse, el geom_jitter() proporciona, en cada caja, la nube de casos (empresas) individuales, en cuanto la rentabilidad económica (incluidos los outliers). Las características de estos puntos (amplitud del desplazamiento lateral “aleatorio”, tamaño, color, opacidad) se controlan con diversos argumentos (width=, size=, col=, alpha=).","code":"\nggplot(data = interestelar_100, map = (aes(x = \"\", y = LIQUIDEZ))) +\n  geom_boxplot(fill= \"orange\") +\n  ggtitle(\"RATIO DE LIQUIDEZ\", subtitle = \"Transporte Interestelar\") +\n  xlab(\"\") +\n  ylab(\"Ratio de liquidez\")\nggplot(data = interestelar_100, map = (aes(x = \"\", y = RES))) +\n  geom_boxplot(fill = \"orange\") +\n  stat_summary(fun = \"mean\",\n               geom = \"point\",\n               size = 3,\n               col = \"darkblue\") +\n  ggtitle(\"RESULTADO DEL EJERCICIO\", subtitle = \"Transporte Interestelar\") +\n  xlab(\"\") +\n  ylab(\"Resultado (miles de PAVOs)\")\nggplot(data = interestelar_100, map = (aes(x = EFLO, y = RES))) +\n  geom_boxplot(aes(fill = EFLO)) +\n  stat_summary(fun = \"mean\",\n               geom = \"point\",\n               size = 3,\n               col = \"darkblue\") +\n  stat_summary(fun = \"mean\",\n               geom = \"line\",\n               col = \"darkblue\",\n               map = (aes(group = TRUE))) +\n  scale_fill_brewer(palette = \"Oranges\") +\n  ggtitle(\"RESULTADO DEL EJERCICIO\", subtitle = \"Transporte Interestelar\") +\n  xlab(\"\") +\n  ylab(\"Resultado (miles de PAVOs)\")\nggplot(data = interestelar_100, map = (aes(x = EFLO, y = RES))) +\n  geom_boxplot(aes(fill = EFLO), outlier.shape = NA) +\n  stat_summary(fun = \"mean\",\n               geom = \"point\",\n               size = 3,\n               col = \"darkblue\") +\n  stat_summary(fun = \"mean\",\n               geom = \"line\",\n               col = \"darkblue\",\n               map = (aes(group = TRUE))) +\n    geom_jitter(width = 0.1,\n              size = 1,\n              col = \"darkred\",\n              alpha = 0.40) +\n  scale_fill_brewer(palette = \"Oranges\") +\n  ggtitle(\"RESULTADO DEL EJERCICIO\", subtitle = \"Transporte Interestelar\") +\n  xlab(\"\") +\n  ylab(\"Resultado (miles de PAVOs)\")"},{"path":"gráficos..html","id":"gráficos-de-dos-variables.","chapter":"3 Gráficos.","heading":"3.4  Gráficos de dos variables.","text":"","code":""},{"path":"gráficos..html","id":"gráfico-de-dispersión-o-scatterplot.","chapter":"3 Gráficos.","heading":"3.4.1  Gráfico de dispersión o scatterplot.","text":"Pasamos ahora comentar un tipo de gráfico muy común cuando trabajamos con dos variables métricas: los gráficos de dispersión (o scatterplots). En este tipo de gráficos, cada variable ocupa un eje (x o y), y los puntos internos al gráfico representan los diversos casos u observaciones.Como ejemplo, vamos crear un gráfico de dispersión que represente las empresas de transporte de mercancías interestelar en función de su ratio de liquidez (LIQUIDEZ) y del resultado del ejercicio (RES). El código es el siguiente:El resultado es el siguiente gráfico:Vamos refinar el gráfico algo más. En primer lugar, puede ser interesante distinguir entre los tipos de empresas, según la antigüedad de sus flotas ( variable EFLO). Para ello, podemos poner el color de los puntos en función de esa variable:En los dos gráficos anteriores pueden observarse puntos (casos) candidatos ser outliers particularmente para el caso de la variable RES (beneficio neto o resultado del ejercicio), como ya se pudo advertir al construir los boxplots.Por otro lado, podría ser interesante complementar el gráfico con información sobre las dos variables por separado, es decir, con información sobre las distribuciones marginales. Existe un paquete complementario ggplot2, llamado ggExtra, que puede ayudar fácilmente este cometido. Para ello, hemos de activar dicho paquete con library() (si se ha hecho previamente, en este script sí se hizo en la parte inicial). El segundo paso consistirá en asignar nuestro scatterplot, diseñado con la función ggplot(), un objeto con el nombre que queramos, por ejemplo, “scatter_plus”. Luego, ese objeto, que contiene nuestro gráfico, entrará como argumento en la función de ggExtra llamada ggMarginal(), como se muestra en el siguiente código:Con el código anterior, apreciamos cómo se añaden los histogramas de cada variable, RENECO y RENFIN, en los márgenes del gráfico:Conviene apuntar que el argumento position = \"identity\" hace que las barras del histograma estén perfectamente alineadas con los datos del gráfico de dispersión, sin ningún tipo de desplazamiento.Adicionalmente, los diámetros de los puntos de los diversos casos podrían contener también información, haciéndolos proporcionales una tercera variable. Por ejemplo, podrían ser proporcionales al nivel de solvencia (variable SOLVENCIA). Para ello, ejecutaríamos el código:En el código anterior, puede comprobarse que la característica size = pasa incluirse en el aes, debido que el diámetro de cada punto ya va ser un parámetro fijo, sino que va depender de la magnitud de la variable SOLVENCIA.Finalmente, podría ser útil, en algunos gráficos, añadir una etiqueta (label) cada punto, para identificar el caso concreto al que representa. Si bien en esta práctica, el elevado número de casos y el extenso nombre de las empresas hacen poco claro el uso de estas etiquetas, vamos añadirlas por motivos pedagógicos. Para ello, se añadirá un bloque geom llamado geom_text(), con una información label = que se hace depender de valores que cambian (en concreto, el nombre de los casos, es decir, de las filas del data frame), por lo que tendrá que integrarse en una característica aes:Las etiquetas de los casos pueden refinarse algo más mediante la función geom_label_repel(), disponible al cargar el paquete ggrepel:La ventaja de este gráfico, como se puede apreciar, es que se omiten las etiquetas superpuestas, si bien existe el riesgo de que se omitan una gran cantidad de estas. Los principales argumentos que utiliza la función geom_label_repel() para controlar la separación entre etiquetas, son:box.padding – “Aire” alrededor de cada etiqueta.box.padding – “Aire” alrededor de cada etiqueta.point.padding – “Aire” extra alrededor del punto ancla.point.padding – “Aire” extra alrededor del punto ancla.direction – \"\", \"x\" o \"y\" para empujar sólo en ese eje.direction – \"\", \"x\" o \"y\" para empujar sólo en ese eje.force – Intensidad de repulsión entre etiquetas.force – Intensidad de repulsión entre etiquetas.force_pull – Fuerza que tira de la etiqueta hacia su punto (evita que se vaya demasiado lejos).force_pull – Fuerza que tira de la etiqueta hacia su punto (evita que se vaya demasiado lejos).max.overlaps – Nº máximo de solapes tolerados antes de descartar etiquetas (usa Inf para descartar).max.overlaps – Nº máximo de solapes tolerados antes de descartar etiquetas (usa Inf para descartar).max.time / max.iter – Límite de tiempo o iteraciones del algoritmo (según versión del paquete).max.time / max.iter – Límite de tiempo o iteraciones del algoritmo (según versión del paquete).seed – Semilla para reproducibilidad de la colocación.seed – Semilla para reproducibilidad de la colocación.","code":"\nggplot(data = interestelar_100, map = (aes(x = LIQUIDEZ, y = RES))) +\n  geom_point(color = \"red\", size = 2, alpha = 0.7) +\n  ggtitle(\"LIQUIDEZ vs RESULTADO DEL EJERCICIO\",\n          subtitle = \"Transporte Interestelar\") +\n  xlab(\"Ratio de Liquidez\") +\n  ylab(\"Resultado del ejercicio (miles PAVOs)\")\nggplot(data = interestelar_100, map = (aes(x = LIQUIDEZ, y = RES))) +\n  geom_point(aes(col = EFLO), size = 2, alpha = 0.7) +\n  ggtitle(\"LIQUIDEZ vs RESULTADO DEL EJERCICIO\",\n          subtitle = \"Transporte Interestelar\") +\n  xlab(\"Ratio de Liquidez\") +\n  ylab(\"Resultado del ejercicio (miles PAVOs)\")\nscatter_plus <- ggplot(data = interestelar_100,\n                       map = (aes(x = LIQUIDEZ, y = RES))) +\n  geom_point(aes(col = EFLO), size = 2, alpha = 0.7) +\n  ggtitle(\"LIQUIDEZ vs RESULTADO DEL EJERCICIO\",\n          subtitle = \"Transporte Interestelar\") +\n  xlab(\"Ratio de Liquidez\") +\n  ylab(\"Resultado del ejercicio (miles PAVOs)\")\n\nggMarginal(scatter_plus, type = \"histogram\", groupColour = T,\n           groupFill = T, position = \"identity\", alpha = 0.5)\nscatter_plus <- ggplot(data = interestelar_100,\n                       map = (aes(x = LIQUIDEZ, y = RES))) +\n  geom_point(aes(col = EFLO, size = SOLVENCIA), alpha = 0.7) +\n  ggtitle(\"LIQUIDEZ vs RESULTADO DEL EJERCICIO\",\n          subtitle = \"Transporte Interestelar\") +\n  xlab(\"Ratio de Liquidez\") +\n  ylab(\"Resultado del ejercicio (miles PAVOs)\")\n\nggMarginal(scatter_plus, type = \"histogram\", groupColour = T,\n           groupFill = T, position = \"identity\", alpha = 0.5)\nscatter_plus <- ggplot(data = interestelar_100,\n                       map = (aes(x = LIQUIDEZ, y = RES))) +\n  geom_point(aes(col = EFLO, size = SOLVENCIA), alpha = 0.7) +\n  geom_text(aes(label=row.names(interestelar_100)),\n            size=2, color=\"black\", alpha = 0.7) +\n  ggtitle(\"LIQUIDEZ vs RESULTADO DEL EJERCICIO\",\n          subtitle = \"Transporte Interestelar\") +\n  xlab(\"Ratio de Liquidez\") +\n  ylab(\"Resultado del ejercicio (miles PAVOs)\")\n\nggMarginal(scatter_plus, type = \"histogram\", groupColour = T,\n           groupFill = T, position = \"identity\", alpha = 0.5)\nscatter_plus <- ggplot(data = interestelar_100,\n                       map = (aes(x = LIQUIDEZ, y = RES))) +\n  geom_point(aes(col = EFLO, size = SOLVENCIA), alpha = 0.7) +\n  geom_label_repel(aes(label = row.names(interestelar_100)),\n                   size = 2,\n                   color = \"black\",\n                   alpha = 0.5,\n                   max.overlaps = 20,   \n                   box.padding = 0.1,   \n                   point.padding = 0.05,\n                   force = 3,\n                   max.time = 2,\n                   seed = 123) +\n  ggtitle(\"LIQUIDEZ vs RESULTADO DEL EJERCICIO\",\n          subtitle = \"Transporte Interestelar\") +\n  xlab(\"Ratio de Liquidez\") +\n  ylab(\"Resultado del ejercicio (miles PAVOs)\")\n\nggMarginal(scatter_plus, type = \"histogram\", groupColour = T,\n           groupFill = T, position = \"identity\", alpha = 0.5)"},{"path":"gráficos..html","id":"materiales-para-realizar-las-prácticas-del-capítulo.-1","chapter":"3 Gráficos.","heading":"3.5  Materiales para realizar las prácticas del capítulo.","text":"En esta sección se muestran los links de acceso los diferentes materiales (scripts, datos…) necesarios para llevar cabo los contenidos prácticos del capítulo.Datos (en formato Microsoft® Excel®:interestelar_100.xlsx (obtener aquí)Scripts:explora_rstars.R (obtener aquí)","code":""},{"path":"estadística-descriptiva..html","id":"estadística-descriptiva.","chapter":"4 Estadística descriptiva.","heading":"4 Estadística descriptiva.","text":"La Estadística Descriptiva es la parte de la Ciencia Estadística que se ocupa de la recopilación de datos, su depuración, y la caracterización, mediante tales datos, de un conjunto de casos o individuos.Los datos se organizan en variables y/o atributos.Las variables son características de los casos o individuos en estudio que se plasman en valores que están expresados en escala métrica. Los atributos son características de los casos o individuos en estudio que se concretan en diversas categorías (si el atributo tiene escala nominal) o niveles (si el atributo tiene escala ordinal). Los atributos se denominan también variables categóricas, cualitativas o factores.Centrándonos en las variables (características que afectan un grupo de casos o individuos, y que se concretan en valores que poseen una escala métrica), podemos plantearnos el estudio de una única variable sin tener en cuenta la existencia de otras variables que caracterizan al mismo grupo de casos o individuos. En tal caso estaremos planteando un análisis estadístico univariante. Si nuestro análisis se centra en cómo dos variables caracterizan al mismo conjunto de individuos o casos, y la posible relación entre ambas, estaremos planteando un análisis bivariante. Generalizando, si estudiamos cómo un grupo de variables caracterizan de modo conjunto un mismo grupo de casos o individuos, estaremos planteando un análisis estadístico multivariante.","code":""},{"path":"estadística-descriptiva..html","id":"análisis-univariante.","chapter":"4 Estadística descriptiva.","heading":"4.1  Análisis univariante.","text":"En el análisis estadístico univariante, estudiamos cómo una única característica (nos centraremos en una variable, aunque también puede tratarse de un atributo) afecta un grupo de casos, individuos o elementos. Por ejemplo, la variable podría ser el salario percibido por un grupo de individuos que podría ser el conjunto de trabajadores en nómina en una empresa. Otro ejemplo podría ser el de la (variable) rentabilidad económica obtenida por un grupo de empresas pertenecientes un determinado sector económico.El conjunto de pares formado por cada valor que puede tomar la variable en estudio (o categoría o nivel, en el caso de un atributo) y el número de casos que toman tal valor se denomina distribución de frecuencias de la variable.¿Cómo podemos estudiar el modo en que afecta una variable, de modo global, un grupo de casos? Mediante el cálculo de una serie de medidas. Las medidas son instrumentos matemáticos que extraen y sintetizan la información contenida en una distribución de frecuencias.Hay diferentes tipos de medidas, principalmente las de posición, dispersión y forma.Antes de profundizar en las principales medidas, su significado y su obtención; mostraremos el modo de presentar en R, mediante la creación de tablas, los datos referentes un grupo de individuos y las variables o atributos que los caracterizan, y las distribuciones de frecuencias univariantes.Se va utilizar, modo de ilustración, los datos correspondientes los salarios percibidos por una de las empresas de transporte de mercancías interestelar del proyecto R-Stars.","code":""},{"path":"estadística-descriptiva..html","id":"shuttlepod-movers.","chapter":"4 Estadística descriptiva.","heading":"4.2 Shuttlepod Movers.","text":"Origen y Contexto: Shuttlepod Movers nació en el planeta Arrakis, en plena Gran Nube de Magallanes, un enclave estratégico del comercio galáctico gracias su proximidad fuentes de minerales raros y especias. La empresa se fundó como una Sociedad Anónima Galáctica (SAG) con el propósito de atender rutas medianas y largas, apoyándose en la tradición logística de la zona y en la creciente demanda de transporte interestelar durante el siglo XXII. Actualmente, la mayor parte de las acciones de la compañía pertenece varios empresarios del propio Arrakis. La actual CEO es Tasha Tachybaptus, que afronta el reto de la renovación de la flota sin debilitar más la liquidez de la empresa.Características Operativas:\nActivos: 249,39 mil PAVOs\nFlota: 47 cargueros espaciales\nEdad media de la flota: Madura\nRutas operadas: 594\nDistancia global anual: 340,93 millones de años luz\nDistancia media por ruta: 0,30 años luz\nCaracterísticas Operativas:Activos: 249,39 mil PAVOsFlota: 47 cargueros espacialesEdad media de la flota: MaduraRutas operadas: 594Distancia global anual: 340,93 millones de años luzDistancia media por ruta: 0,30 años luzSituación Financiera:\nIngresos: 241,73 mil PAVOs\nBeneficio neto (RES): 180,82 mil PAVOs\nMargen operativo: 74,8%\nRentabilidad económica (RENECO): 72,5%\nRentabilidad financiera (RENFIN): 167,5%\nSolvencia: 176,3%\nApalancamiento: 131\nLiquidez: 0,56\nSituación Financiera:Ingresos: 241,73 mil PAVOsBeneficio neto (RES): 180,82 mil PAVOsMargen operativo: 74,8%Rentabilidad económica (RENECO): 72,5%Rentabilidad financiera (RENFIN): 167,5%Solvencia: 176,3%Apalancamiento: 131Liquidez: 0,56Atención: Aunque la empresa presenta márgenes y rentabilidades muy elevados, la liquidez reducida supone un riesgo corto plazo.Capital Humano:\nPlantilla de 49 trabajadores, organizados en áreas como almacén, gestión interna, mantenimiento y operaciones de vuelo. El tamaño de la plantilla es coherente con la escala de la compañía y el mantenimiento de 47 naves operativas.Capital Humano:Plantilla de 49 trabajadores, organizados en áreas como almacén, gestión interna, mantenimiento y operaciones de vuelo. El tamaño de la plantilla es coherente con la escala de la compañía y el mantenimiento de 47 naves operativas.Innovación y Estrategia:\nInversión en +D: 18,5 mil PAVOs\nÍndice de digitalización (IDIG): 28,1\nÍndice de diversificación (IDIVERSE): 40,7\nÍndice de fidelización (IFIDE): 43,1\nInnovación y Estrategia:Inversión en +D: 18,5 mil PAVOsÍndice de digitalización (IDIG): 28,1Índice de diversificación (IDIVERSE): 40,7Índice de fidelización (IFIDE): 43,1La empresa presenta una alta diversificación de mercados y una fidelización sólida de clientes, apoyándose en un grado notable de digitalización para su tamaño.Diagnóstico General:\nFortalezas:\nMárgenes de beneficio extraordinariamente altos.\nExcelente rentabilidad financiera y económica.\nDiversificación y fidelización destacadas.\n\nDebilidades:\nLiquidez reducida (riesgo en el corto plazo).\nFlota madura que requerirá renovación en próximos ciclos.\n\nDiagnóstico General:Fortalezas:\nMárgenes de beneficio extraordinariamente altos.\nExcelente rentabilidad financiera y económica.\nDiversificación y fidelización destacadas.\nMárgenes de beneficio extraordinariamente altos.Excelente rentabilidad financiera y económica.Diversificación y fidelización destacadas.Debilidades:\nLiquidez reducida (riesgo en el corto plazo).\nFlota madura que requerirá renovación en próximos ciclos.\nLiquidez reducida (riesgo en el corto plazo).Flota madura que requerirá renovación en próximos ciclos.","code":""},{"path":"estadística-descriptiva..html","id":"representando-datos-y-distribuciones-de-frecuencias-en-tablas-con-r.","chapter":"4 Estadística descriptiva.","heading":"4.3  Representando datos y distribuciones de frecuencias en tablas con R.","text":"Para aprender representar los datos referentes las variables y atributos que caracterizan un grupo de casos o individuos, y las distribuciones de frecuencias univariantes, vamos suponer que trabajamos dentro de un proyecto que hemos creado previamente, de nombre “explora”. Dentro de la carpeta del proyecto guardaremos el script llamado “describe_rstars.R” y el archivo de Microsoft® Excel® llamado “trabajadores.xlsx”. Al final del capítulo se encuentran los enlaces correspondientes estos archivos.El archivo de Excel® recoge algunos datos correspondientes la plantilla que compone el personal de la compañía de transporte de mercancías interestelar Shuttlepod Movers (hoja “Datos”), tales como el salario percibido (variable SALARIO, expresada en cientos de PAVOs), el nivel de estudios (atributo NESTUDIOS) y departamento al que se pertenece (atributo DEP).Las primeras líneas del script se refieren, como ya hemos visto en otras secciones del libro, la limpieza de la memoria o environment, eliminando objetos que se hayan podido crear con anterioridad, carga de los paquetes necesarios, e importación de los datos:Los datos se han almacenado en el data frame “datos”, y hemos presentado las variables contenidas en el mismo con la función gt_plt_summary() del paquete gtExtras Sabemos que, simplemente escribiendo el nombre del data frame, aparecerán en la consola los datos almacenados en él. obstante, esta presentación es muy elegante para presentar los datos. Vamos presentarlos de un modo más amigable, mediante la confección de una “tabla”.Un paquete de R muy popular para generar tablas de datos es knitr. Este paquete contiene la función kable(), que permite generar tablas en varios formatos y con diversas características que pueden ser personalizadas (como el título de la tabla). Si queremos personalizar más aún la apariencia de nuestras tablas, podemos usar las facilidades del paquete kableExtra, que complementa las posibilidades que ofrece la función kable() de knitr.Para hacer una tabla con nuestros casos y variables, es decir, para escribir nuestro data frame “datos” de un modo más elegante, activaremos los paquetes anteriores (si lo hemos hecho ya), y generaremos nuestra tabla con los datos contenidos en el data frame “datos”. El código para generar la tabla, y el resultado, es el siguiente:Primero llamamos al data frame partir de cuyos datos vamos generar la tabla, “datos”. Con el operador pipe %>%, ligamos los datos del data frame al diseño la tabla realizado con la función kable() de knitr. kable() tiene diversos argumentos, entre los que destacan:caption =: Este argumento informa del título de la tabla.caption =: Este argumento informa del título de la tabla.col.names =: Este argumento, opcional, fija el nombre para las columnas de la tabla, si queremos que aparezcan los nombres “por defecto”, que son los nombres de cada columna en el propio data frame.col.names =: Este argumento, opcional, fija el nombre para las columnas de la tabla, si queremos que aparezcan los nombres “por defecto”, que son los nombres de cada columna en el propio data frame.Luego, con el operador pipe %>% informamos de que vamos completar o personalizar el diseño de esta tabla con otras funciones complementarias del paquete kableExtra. En primer lugar, utilizamos la función kable_styling(), que aporta algunas características adicionales la tabla, según sus argumentos:full_width = : este argumento ha de tener un valor lógico, y se refiere si deseamos que la tabla ocupe todo el ancho del documento (TRUE) o solo lo necesario (FALSE).full_width = : este argumento ha de tener un valor lógico, y se refiere si deseamos que la tabla ocupe todo el ancho del documento (TRUE) o solo lo necesario (FALSE).bootstrap_options = : este argumento es de tipo alfanumérico, y sirve para fijar ciertas características estéticas complementarias. “striped” se refiere que las filas aparezcan sombreadas de modo alternativo, “bordered” se refiere que cada fila quede delimitada por unas finas líneas en la parte superior y en la inferior, “condensed” significa que la tabla tendrá un aspecto más compacto.bootstrap_options = : este argumento es de tipo alfanumérico, y sirve para fijar ciertas características estéticas complementarias. “striped” se refiere que las filas aparezcan sombreadas de modo alternativo, “bordered” se refiere que cada fila quede delimitada por unas finas líneas en la parte superior y en la inferior, “condensed” significa que la tabla tendrá un aspecto más compacto.position = : este argumento se utiliza para situar la tabla centrada, la izquierda del párrafo, o la derecha.position = : este argumento se utiliza para situar la tabla centrada, la izquierda del párrafo, o la derecha.font_size = : este argumento numérico se refiere al tamaño de los caracteres, lo cuál es importante la hora de que una tabla “quepa” en un documento de deteminada anchura.font_size = : este argumento numérico se refiere al tamaño de los caracteres, lo cuál es importante la hora de que una tabla “quepa” en un documento de deteminada anchura.Por último, hacemos uso dos veces de la función row_spec() del paquete kableExtra. Esta función sirve para personalizar algo más las filas concretas de la tabla que consideremos. El encabezado se identifica como la fila “0”. En el ejemplo, se ha utilizado esta función dos veces: una para el encabezado (el primer argumento de la función nos informa de las filas las que se refiere, en esta ocasión la fila 0), y otra para el resto de filas (desde la fila 1 hasta la que contiene al último caso individuo, la fila con posición nrow(datos)). Los otros argumentos definen si se quiere que los caracteres aparezcan en negrita (bold = ) y cómo deben estar alineados los elementos, dentro de las columnas ( align = ).El resultado es:\nTable 4.1: Table 4.2: Trabajadores de Shuttlepod Movers\nveces, puede ocurrir que solo nos interese estudiar una variable (columna del data frame). Además, es posible que el conjunto de casos sea muy numeroso, y que, adicionalmente, algunos de los valores de la variable que queremos estudiar estén repetidos para varios casos. Cuando esto ocurre, una opción interesante es, en lugar de representar en una tabla todos nuestros datos, representar la distribución de frecuencias de la variable (o atributo) que nos interesa. Es lo que vamos hacer continuación, tomando SALARIO como variable analizar.Lo primero tener en cuenta es que, en las distribuciones de frecuencias, los valores de la variable suelen disponerse de menor mayor. Para ello, previamente vamos ordenar las filas del data frame “datos” según el valor que toma, en el caso correspondiente, la variable SALARIO y, si existen casos con el mismo valor de SALARIO, los ordenaremos por orden alfabético del nombre del caso (nombre del trabajador, o de la fila del data frame). Para realizar este reordenamiento de casos (filas) del data frame de un modo sencillo, vamos utilizar la función arrange() del paquete {deplyr}:Una vez que los casos están ordenados en el data frame de menor mayor valor de SALARIO, calcularemos, para cada valor de esta variable, el número de casos que lo poseen, es decir, su frecuencia absoluta. Para ello, vamos crear un objeto denominado “conteo”, que va ser de clase “table”, de la variable SALARIO. Todo ello lo realizamos mediante la función table(), que contabiliza el número de veces que aparece cada valor de la variable, como se detalla continuación:Al mostrar en la consola el “conteo”, vemos cómo se compone de dos filas de datos. La primera se corresponde con los valores que toma la variable SALARIO en los distintos casos, y la segunda es el número de casos (frecuencia absoluta) que se adopta cada valor. Es decir, el objeto “tabla” es la distribución de frecuencias de la variable SALARIO.Vamos convertir este objeto “tabla” en un data frame, llamado “conteo_df”, con el objeto de poder representar de un modo más elegante la distribución de frecuencias. Para ello, ejecutaremos el código:Al mostrar en la consola el data frame “conteo_df”, observamos que consta de dos columnas o variables. Var1 recoge los valores que toma la variable SALARIO en el grupo de casos, y Freq es el conjunto de frecuencias absolutas de los diferentes valores. Para que se entienda mejor qué es cada columna, las renombraremos:continuación, vamos calcular el resto de frecuencias que suelen calcularse para una variable. La frecuencia total, N, que es la suma de todas las frecuencias absolutas, es decir, el número total de casos, se puede calcular fácilmente como:La serie de frecuencias absolutas acumuladas se calcularán del siguiente modo:Como sabemos, la última frecuencia absoluta acumulada debe coincidir con la frecuencia total. Por último, calcularemos las frecuencias relativas, que son las frecuencias absolutas divididas por la frecuencia total, y recogen la proporción de casos correspondientes al valor de la variable:La suma de las frecuencias relativas es siempre 1 (el 100% de los casos). Además, la última frecuencia relativa acumulada siempre es, igualmente, 1.Ahora pasaremos construir una tabla que recoja la distribución de frecuencias de la variable SALARIO (con los diversos tipos de frecuencias). Para ello, simplemente hemos de aplicar al data frame “conteo_df” las funciones kable() del paquete knitr, y el resto de funciones auxiliares del paquete kableExtra:\nTable 4.3: Table 4.4: Distribución de frecuencias de salarios. Shuttlepod Movers\nHemos de advertir que en la función kable() se han insertado dos nuevos argumentos:digits = es un vector que establece, para cada columna, el número de decimales presentar en la tabla (si es una columna con datos categóricos, se indicará con el valor NA).digits = es un vector que establece, para cada columna, el número de decimales presentar en la tabla (si es una columna con datos categóricos, se indicará con el valor NA).format.args = es una “lista” que controla aspectos de formato como si los decimales se indican con un punto o una coma, o si en cifras muy grandes se debe utilizar notación científica.format.args = es una “lista” que controla aspectos de formato como si los decimales se indican con un punto o una coma, o si en cifras muy grandes se debe utilizar notación científica.En cuanto la interpretación de la distribución, podemos apreciar que la distribución de salarios en Shuttlepod Movers refleja una estructura claramente concentrada en los niveles intermedios. La mayor parte de los trabajadores percibe entre 12 y 20 cientos de PAVOs, lo que supone cerca del 70% de la plantilla, mientras que los salarios bajos (8–10) y los más altos (25–30) son minoritarios. Este patrón sugiere una organización con un núcleo amplio de personal con remuneraciones medias, posiblemente operativas o técnicas, y una proporción reducida de empleados en los extremos, que podrían corresponder personal de apoyo poco cualificado (en la base) y mandos o especialistas (en la parte alta). Desde el punto de vista económico, la distribución es relativamente equilibrada.Hay ocasiones en las que la cantidad de valores diferentes que toma la variable analizada para los diferentes casos es muy elevado. Esto puede deberse, por ejemplo, que el número de casos es muy elevado, o que la variable es de naturaleza continua, y puede tomar una gran variedad de posibles valores (incluso infinitos). En estos casos, un modo de representar la distribución de frecuencias de la variable en una tabla de dimensión reducida es agrupando los valores en intervalos. Esto es lo que vamos hacer ahora con la variable SALARIO.La primera tarea realizar será formar los intervalos. Para ello podemos usar la función cut(), que permite decir el número de intervalos (de la misma amplitud) en que queremos dividir el intervalo que va desde el menor valor de la distribución (menor salario) al mayor valor (mayor salario). Vamos controlar el número de intervalos en que deseamos dividir el rango de valores que puede tomar la variable estudiada mediante una constante k. Si en lugar de decidir nosotros este número, preferimos que sea calculado por un método objetivo, como el de Sturges, ligaremos k al método. Por ejemplo:El resultado del código anterior es una nueva columna en el data frame “datos”, llamada “intervalos”, que informa, para cada caso, cuál de los intervalos calculados lo contiene. El argumento lógico include.lowest = se especifica para indicar que el intervalo inferior es cerrados por la izquierda. Lo usual es que, salvo este, el resto sean abiertos, es decir, que los casos que toman como valor de la variable un extremo de intervalo se contabilicen dentro del intervalo donde ese valor es el extremo superior.La columna “intervalos” es de la clase factor. Precisamente, los posibles “niveles” de ese factor son los intervalos que se han creado con cut().Las siguientes líneas de código son similares las que vimos en el caso de distribuciones de frecuencias agrupadas: se creará un objeto “tabla” para contabilizar el número de casos que pertenecen cada intervalo (frecuencias absolutas), se transformará este objeto en un data frame para poder trabajar de un modo más fácil, y se cambiarán el nombre de las dos columnas para que se entienda mejor:Con todo lo anterior, se obtiene un data frame denominado “conteo_intervalos_df”, que contiene dos columnas: la columna “Intervalo”, con los intervalos calculados, y la columna “Frecuencia”, con el número de casos que tienen un salario incluido dentro de cada intervalo salarial.Antes de proceder diseñar la tabla de presentación de la distribución de frecuencia con kable(), vamos obtener, para incluir en la tabla, otras informaciones que suelen ser presentadas junto las frecuencias absolutas de cada intervalo.Una de estas informaciones es lo que denominamos marca de clase de un intervalo. La marca de clase de un intervalo de valores es simplemente el punto medio de dicho intervalo. La obtención en nuestro ejemplo de las marcas de clase puede resultar algo compleja, ya que hemos de recordar que los intervalos, tal y como están almacenados, son los niveles de una variable de clase factor:Explicaremos detenidamente el código anterior:conteo_intervalos_df$Intervalo: Aquí se está accediendo la columna “Intervalo” del data frame “conteo_intervalos_df”.conteo_intervalos_df$Intervalo: Aquí se está accediendo la columna “Intervalo” del data frame “conteo_intervalos_df”..character(conteo_intervalos_df$Intervalo): convierte los valores de la columna “Intervalo” caracteres (strings). Esto es necesario porque la función strsplit() trabaja con cadenas de texto..character(conteo_intervalos_df$Intervalo): convierte los valores de la columna “Intervalo” caracteres (strings). Esto es necesario porque la función strsplit() trabaja con cadenas de texto.strsplit(.character(conteo_intervalos_df$Intervalo), \",|\\\\[|\\\\(|\\\\]\"): strsplit() divide cada cadena de texto en partes usando los delimitadores especificados. En este caso, se están utilizando como delimitadores las comas “,”, los corchetes “[” y ”]”, y el paréntesis de apertura “(”. Cada separador queda “encerrado” entre una doble barra inclinada “\\\\” (o unas comillas, si es el primer separador que se pone), y una barra vertical “|” (o unas comillas, si es el último separador que se considera). El resultado es una lista de vectores de caracteres, donde cada vector contiene las partes de la cadena original que estaban separadas por los delimitadores.strsplit(.character(conteo_intervalos_df$Intervalo), \",|\\\\[|\\\\(|\\\\]\"): strsplit() divide cada cadena de texto en partes usando los delimitadores especificados. En este caso, se están utilizando como delimitadores las comas “,”, los corchetes “[” y ”]”, y el paréntesis de apertura “(”. Cada separador queda “encerrado” entre una doble barra inclinada “\\\\” (o unas comillas, si es el primer separador que se pone), y una barra vertical “|” (o unas comillas, si es el último separador que se considera). El resultado es una lista de vectores de caracteres, donde cada vector contiene las partes de la cadena original que estaban separadas por los delimitadores.sapply(..., function(x) { ... }): sapply() aplica una función cada elemento de una lista y simplifica el resultado un vector o matriz. Por otro lado, la función anónima function(x) { ... } se aplica cada vector resultante de strsplit().sapply(..., function(x) { ... }): sapply() aplica una función cada elemento de una lista y simplifica el resultado un vector o matriz. Por otro lado, la función anónima function(x) { ... } se aplica cada vector resultante de strsplit().function(x) { mean(.numeric(x[2:3])) }: Esta es la función anónima que se aplica cada vector “x”. Después, x[2:3] selecciona el segundo y tercer elemento del vector “x”. Estos elementos corresponden con los límites del intervalo. .numeric(x[2:3]) convierte estos elementos números. mean(.numeric(x[2:3])) calcula la media de estos dos números, que representa el punto medio del intervalo.function(x) { mean(.numeric(x[2:3])) }: Esta es la función anónima que se aplica cada vector “x”. Después, x[2:3] selecciona el segundo y tercer elemento del vector “x”. Estos elementos corresponden con los límites del intervalo. .numeric(x[2:3]) convierte estos elementos números. mean(.numeric(x[2:3])) calcula la media de estos dos números, que representa el punto medio del intervalo.marca_clase <- ...: Finalmente, el resultado de sapply() se asigna al vector”marca_clase”, que contendrá los puntos medios de los 4 intervalos.marca_clase <- ...: Finalmente, el resultado de sapply() se asigna al vector”marca_clase”, que contendrá los puntos medios de los 4 intervalos.El resto de código integra el vector “marca_clase” en el data frame “conteo_intervalo_df” como una variable más, reodena con la función select() del paquete {dplyr} el orden de las columnas del data frame, calcula el resto de frecuencias (absoluta acumulada, relativa, relativa acumulada), y diseña la tabla de presentación de la distribución de frecuencias de los salarios de los trabajadores de la empresa; pero agrupada en los intervalos de valores que se fijaron con la constante k:\nTable 4.5: Table 4.6: Distribución de frecuencias agrupadas en intervalos de salarios. Shuttlepod Movers\nLa distribución agrupada de salarios en Shuttlepod Movers, expresada en cientos de PAVOs, confirma la concentración de la plantilla en niveles intermedios de ingresos. En particular, los intervalos [7.98, 11.1] y (14.3, 17.4] concentran cada uno un 22% de los trabajadores, mientras que los tramos (11.1, 14.3] y (17.4, 20.6] aportan un 16% adicional cada uno. Esto refleja que casi tres cuartas partes de la fuerza laboral percibe salarios entre 8 y 20 cientos de PAVOs, lo que configura una estructura salarial predominantemente media. Los tramos superiores muestran menor densidad: solo un 10% supera los 23.7 cientos de PAVOs, y el salario máximo (30 cientos) corresponde un único trabajador.","code":"\n# Script para la construcción de tablas de datos \n# y trabajo con distribuciones de frecuencias univariantes.\n\n# Limpiando el Global Environment\nrm(list = ls())\n\n# Cargando paquetes\nlibrary(readxl)\nlibrary(gtExtras)\nlibrary (knitr)\nlibrary (kableExtra)\nlibrary (dplyr)\nlibrary(DescTools)\nlibrary(moments)\nlibrary (ggplot2)\n\n## DATOS\n\n# Importando datos desde Excel\ndatos <- read_excel(\"trabajadores.xlsx\",\n                    sheet = \"Datos\",\n                    na = c(\"n.d.\"))\ndatos <- data.frame(datos, row.names = 1)\n\n# visualizando el data frame de modo elegante con {gtExtras}\ndatos_df_graph <- gt_plt_summary(datos)\ndatos_df_graph\n# Tabla de datos\ndatos %>%\n  kable(caption = \"Trabajadores de Shuttlepod Movers\",\n        col.names = c(\"Trabajador\", \"Salario\", \"Nivel de estudios\",\n                      \"Departamento\")) %>%\n  kable_styling(full_width = F,\n                bootstrap_options = \"striped\", \"bordered\", \"condensed\",\n                position = \"center\",\n                font_size = 11) %>%\n  row_spec(0, bold= T, align = \"c\") %>%\n  row_spec(1:(nrow(datos)), bold= F, align = \"c\")\n# Distribución de frecuencias del salario de los trabajadores de la empresa.\n\n# Colocar los datos\ndatos <- datos %>% arrange(SALARIO, row.names(datos))\n# Contar frecuencias\nconteo <- table(datos$SALARIO)\nconteo## \n##  8  9 10 12 15 20 22 25 30 \n##  2  3  6  8 11  8  6  4  1\n# Convertir el resultado a un data frame para una mejor visualización\nconteo_df <- as.data.frame(conteo)\nconteo_df##   Var1 Freq\n## 1    8    2\n## 2    9    3\n## 3   10    6\n## 4   12    8\n## 5   15   11\n## 6   20    8\n## 7   22    6\n## 8   25    4\n## 9   30    1\n# Renombrar las columnas para mayor claridad\ncolnames(conteo_df) <- c(\"Valor\", \"Frecuencia\")\n# Calcular y guardar la frecuencia total\nN <- sum(conteo_df$Frecuencia)\n# Calcular frecuencias absolutas acumuladas\nconteo_df$Frecuencia_acum <- cumsum(conteo_df$Frecuencia)\n# Calcular frecuencias relativas\nconteo_df$Frecuencia_R <- conteo_df$Frecuencia / N\n\n# Calcular frecuencias relativas acumuladas\nconteo_df$Frecuencia_R_acum <- cumsum(conteo_df$Frecuencia_R)\nconteo_df %>%\n  kable(caption = \"Distribución de frecuencias de salarios. Shuttlepod Movers\",\n        col.names = c(\"x(i) = Salario\", \"Frecuencia absoluta n(i)\",\n                      \"Frecuencia absoluta acum. N(i)\", \"Frecuencia relativa f(i)\",\n                      \"Frecuencia relativa acum. F(i)\"),\n        digits  = c(0, 0, 0, 2, 2),\n        format.args = list(decimal.mark = \".\", scientific = FALSE)) %>%\n  kable_styling(full_width = F,\n                bootstrap_options = \"striped\", \"bordered\", \"condensed\",\n                position = \"center\",\n                font_size = 11) %>%\n  row_spec(0, bold= T, align = \"c\") %>%\n  row_spec(1:(nrow(conteo_df)), bold= F, align = \"c\")\n# Distribución de frecuencias agrupadas en intervalos\n# del salario de los trabajadores de la empresa.\n\n# Crear los intervalos (método de Sturges)\nk <- nclass.Sturges(datos$SALARIO)\ndatos$intervalos <- cut(datos$SALARIO, breaks = k, include.lowest = TRUE)\nlevels(datos$intervalos)## [1] \"[7.98,11.1]\" \"(11.1,14.3]\" \"(14.3,17.4]\" \"(17.4,20.6]\" \"(20.6,23.7]\" \"(23.7,26.9]\" \"(26.9,30]\"\n# Contar las frecuencias de cada intervalo\nconteo_intervalos <- table(datos$intervalos)\n\n# Convertir el resultado a un data frame para una mejor visualización\nconteo_intervalos_df <- as.data.frame(conteo_intervalos)\n\n# Renombrar las columnas para mayor claridad\ncolnames(conteo_intervalos_df) <- c(\"Intervalo\", \"Frecuencia\")\nmarca_clase <- sapply(strsplit(as.character(conteo_intervalos_df$Intervalo),\n                               \",|\\\\[|\\\\(|\\\\]\"),\n                      function(x) {mean(as.numeric(x[2:3]))})\n# Agregar la columna \"marca_clase\" al data frame\nconteo_intervalos_df$marca_clase <- marca_clase\n\n#Cambiar el orden de las columnas en el data frame con dplyr\nconteo_intervalos_df <- conteo_intervalos_df %>% select(Intervalo, marca_clase, Frecuencia)\n\n# Calcular y guardar la frecuencia total\nN_agre <- sum(conteo_intervalos_df$Frecuencia)\n\n# Calcular frecuencias absolutas acumuladas\nconteo_intervalos_df$Frecuencia_acum <- cumsum(conteo_intervalos_df$Frecuencia)\n\n# Calcular frecuencias relativas\nconteo_intervalos_df$Frecuencia_R <- conteo_intervalos_df$Frecuencia / N_agre\n\n# Calcular frecuencias relativas acumuladas\nconteo_intervalos_df$Frecuencia_R_acum <- cumsum(conteo_intervalos_df$Frecuencia_R)\n\n# Mostrar el resultado\nconteo_intervalos_df %>%\n  kable(caption = \"Distribución de frecuencias agrupadas en intervalos de salarios. Shuttlepod Movers\",\n        col.names = c(\"Intervalo salarial\",\n                      \"Marca de clase x(i)\",\n                      \"Frecuencia absoluta n(i)\",\n                      \"Frecuencia absoluta acum. N(i)\",\n                      \"Frecuencia relativa f(i)\",\n                      \"Frecuencia relativa acum. F(i)\"),\n        digits  = c(NA, 2, 0, 0, 2, 2),\n        format.args = list(decimal.mark = \".\", scientific = FALSE)) %>%\n  kable_styling(full_width = F,\n                bootstrap_options = \"striped\", \"bordered\", \"condensed\",\n                position = \"center\",\n                font_size = 11) %>%\n  row_spec(0, bold= T, align = \"c\") %>%\n  row_spec(1:(nrow(conteo_intervalos_df)), bold= F, align = \"c\")"},{"path":"estadística-descriptiva..html","id":"medidas-de-posición.","chapter":"4 Estadística descriptiva.","heading":"4.4  Medidas de posición.","text":"Las medidas de posición son instrumentos matemáticos que pretenden, mediante un único valor o muy pocos valores, caracterizar de modo global la distribución de frecuencias de una variable determinada.Las medidas de posición se pueden clasificar en medidas de posición central, y en medidas de posición central (principalmente, los llamados cuantiles).Las principales medidas de posición central son: la media, la mediana y la moda. Dentro de la media, podemos distinguir la media aritmética, la geométrica y la armónica. De ellas, nos centraremos en la más común: la media aritmética.La media aritmética de la distribución de frecuencias de una variable X se calcula como:\\[\n\\overline{x} = \\frac{1}{N} \\sum_{=1}^{h} x_i n_i\n\\]Hemos de tener en cuenta en la fórmula anterior que N es la frecuencia total, y h es el número de valores diferentes que toma la variable.Si las frecuencias de todos los valores de la variable son 1 (distribución de frecuencias unitarias), lógicamente la media pasará ser:\\[\n\\overline{x} = \\frac{1}{N} \\sum_{=1}^{N} x_i\n\\] En R, la función para obtener la media de una variable es mean(). Así, para obtener el salario medio de la variable SALARIO de los trabajadoes de la empresa, ejecutaremos el código:Como podemos observar, el salario medio de los trabajadores de la empresa, recogido en el valor “media”, es de 16.04 cientos de PAVOs, es decir, 1604 PAVOs.¿Qué significado tiene la media aritmética? La media aritmética es el “centro de gravedad” de la distribución, el punto de equilibrio, en el sentido de que, si todos los trabajadores ganaran el salario medio, habría diferencias salariales aun cuando la “masa” salarial invertida por la empresa permanecería invariable. Es decir, la media aritmética supone un reparto igualitario de la masa total de la variable.Entre sus ventajas destaca el que, para variables (escala métrica) es siempre calculable y única. Como inconvenientes, que pierde su representatividad ante la existencia de casos atípicos o outliers, y que se puede calcular en el caso de trabajar con atributos, variables cualitativas o factores (escalas nominal u ordinal).La mediana es el valor que se corresponde con el caso o casos que dividen la distribución en dos grupos con el mismo número de elementos (frecuencias), siempre teniendo en cuenta que, previamente, la distribución ha sido ordenada según los valores de la variable en estudio, de menor mayor. Si la distribución tiene frecuencia total par, los casos “frontera” entre los dos grupos en que queda dividida la distribución son dos, por lo que, si estos casos asumen valores diferentes en la variable estudiada, podría ocurrir que hubiera dos medianas diferentes. En tal circunstancia, se suele tomar, como convenio, el promedio de de ambos valores para tener una única mediana.En R, la función para obtener la mediana de una variable es median(). De este modo, para obtener el salario mediano de la variable SALARIO de los trabajadoes de la empresa, ejecutaremos el código:En el ejemplo de la variable SALARIO, la mediana es 15. Es decir, 15 cientos de PAVOs es el salario percibido por el caso 25, que es el trabajador que divide la distribución de frecuencias en dos grupos de 24 trabajadores: 24 que ganan un salario menor o igual que el caso 25 (menos o igual que 15 cientos de PAVOs), y otros 24 trabajadores que ganan más o lo mismo que el caso en la posición 25 (o sea, igual o más que 15 cientos de PAVOs).Como ventajas de la mediana, contamos con que es sensible la existencia de casos atípicos o outliers, y que se puede calcular en el caso de atributos o factores en escala ordinal. Como desventajas, tenemos que es necesariamente única, y que tiene en cuenta la totalidad de los valores de la distribución.Con la moda hacemos referencia al valor (o valores) que posee (o poseen) una mayor frecuencia absoluta.En R, la moda se calcula mediante la función Mode() del paquete DescTools, que habremos de activar con library() (si aún lo hemos hecho):Como podemos apreciar, la moda de la distribución es 15 (un salario de 1500 PAVOs), que aparece en la distribución en 11 ocasiones (la frecuencia absoluta de ese salario es 11).La moda puede ser calculada en atributos o factores en escala nominal. Como inconveniente principal, tenemos que necesariamente es un valor único (existen distribuciones multimodales).Existen otras medidas que son de posición central, principalmente lo que llamamos cuantiles. La naturaleza de los cuantiles es fácil de comprender si los consideramos como una generalización de la mediana. Ya sabemos que, ordenados los valores (y por tanto, los casos que toman dichos valores) de una distribución de frecuencias de una variable de menor mayor, la mediana es el valor (o valores, porque pueden existir dos medianas, aunque vamos suponer que solo hay una) de la variable correspondiente al caso que divide la distribución en dos grupos con el mismo número de frecuencias. Pues bien, si en lugar de dividir la distribución de frecuencias en dos grupos con el mismo número de elementos, la dividimos en 4 grupos, estaremos hablando de tres valores correspondientes los casos que delimitan esos cuatro grupos. Estos valores serán los cuartiles de la distribución.Si queremos dividir la distribución de 9 valores de la variable que toman los casos “frontera” que separan estos 10 grupos. Esos valores serán los deciles. Y si queremos dividir la distribución de frecuencias en 100 grupos con el mismo número de casos o individuos, estaríamos hablando de 99 valores de la variable que toman los casos “frontera” que separan estos 100 grupos. Esos valores serán los percentiles.En R, la función para calcular los diferentes cuantiles es quantile(). Para calcular, por ejemplo, los cuartiles de la variable SALARIO, procederemos así:El argumento probs = informa de la proporción de los casos que han de quedar por detrás (con valores menores o iguales) de cada uno de los casos que hacen de “frontera” entre los grupos. En el caso de los cuartiles, estos son 0.25, 0.5 (este cuartil es, su vez, la mediana de la distribución) y 0.75. Vemos cómo los cuartiles son 12, 15 y 20.","code":"\n## MEDIDAS\n\n# Media aritmética.\nmedia <- mean(datos$SALARIO)\nmedia## [1] 16.04082\n# Mediana\nmediana <- median(datos$SALARIO)\nmediana## [1] 15\n# Moda\nmoda <- Mode(datos$SALARIO)\nmoda## [1] 15\n## attr(,\"freq\")\n## [1] 11\n# Calcular los cuartiles\ncuartiles <- quantile(datos$SALARIO, probs = c(0.25, 0.5, 0.75))\ncuartiles## 25% 50% 75% \n##  12  15  20"},{"path":"estadística-descriptiva..html","id":"medidas-de-dispersión-o-variabilidad.","chapter":"4 Estadística descriptiva.","heading":"4.5  Medidas de dispersión o variabilidad.","text":"Las medidas de dispersión cuantifican lo cerca o lejos que, en general, los valores asumidos por los casos de una distribución de frecuencias se hallan respecto una medida de posición central. Si la medida de dispersión toma un valor muy elevado, querrá decir que la medida de posición central representa bien la distribución de frecuencias, ya que, en general, los casos toman valores alejados de dicha medida.La medida de posición central la que suelen hacer referencia las medidas de dispersión es la media aritmética.Existen múltiples medidas de dispersión, que principalmente se dividen en medidas absolutas (que se expresan en ciertas unidades, como por ejemplo euros, o euros al cuadrado) y medidas relativas (que carecen de unidades y siven, por tanto, para comparar la dispersión entre distribuciones de frecuencias expresadas en distintas unidades).La medida de dispersión absoluta más utilizada es la varianza, cuya fórmula es:\\[\nS^2 = \\frac{1}{N} \\sum_{=1}^{h} (x_i - \\overline{x})^2 n_i\n\\]Hemos de tener en cuenta en la fórmula anterior que N es la frecuencia total, y h es el número de valores diferentes que toma la variable.Si las frecuencias de todos los valores de la variable son 1 (distribución de frecuencias unitarias), lógicamente la varianza pasará ser:\\[\nS^2 = \\frac{1}{N} \\sum_{=1}^{N} (x_i - \\overline{x})^2\n\\]En realidad, la varianza es el promedio de las diferencias que existen entre los valores que toma la variable y la media aritmética de esta, diferencias que son elevadas al cuadrado para evitar la compensación entre diferencias por los signos.Una limitación de la varianza viene referida que, debido al exponente del paréntesis, puede tomar valores muy elevados. Para evitar el inconveniente, una medida alternativa es la desviación típica, que queda definida como la raíz cuadrada positiva de la varianza:\\[\nS = +\\sqrt{S^2}\n\\]Otra media de dispersión muy utilizada, sobre todo en Econometría, es la varianza insesgada o cuasivarianza, cuya fórmula es:\\[\n{\\overline{S}}^2 = \\frac{1}{N-1} \\sum_{=1}^{h} (x_i - \\overline{x})^2 n_i\n\\]Como siempre, si las frecuencias de todos los valores de la variable son 1 (distribución de frecuencias unitarias), la cuasivarianza pasará ser:\\[\n{\\overline{S}}^2 = \\frac{1}{N-1} \\sum_{=1}^{N} (x_i - \\overline{x})^2\n\\]En cuanto una medida de dispersión relativa, cabe nombrar al coeficiente de variación de Pearson, definido como el cociente entre la desviación típica y la media aritmética (en valor absoluto):\\[\nV = \\frac{S}{|\\overline{x}|}\n\\]El coeficiente de variación informa del número de medias aritméticas que “caben” en la desviación típica de una distribución de frecuencias. mayor coeficiente, mayor dispersión y menor representatividad de la media aritmética con respecto la distribución. Además, pueden compararse coeficientes de distribuciones expresadas en unidades diferentes (medida relativa).continuación, vamos calcular varianza, desviación típica, cuasivarianza, y coeficiente de variación en R. Para ello, hemos de tener en cuenta que la función var() de R, en realidad, calcula la cuasivarianza. Para obtener la varianza, pues, hemos de realizar una corrección (en realidad, para un número de casos muy grande, ambas medidas prácticamente coinciden):Según los resultados, puede destacarse que el coeficiente de variación indica que en la desviación típica de la variable SALARIO “cabe” el 34,4% de la media.","code":"\n# Varianza\nvarianza <- var(datos$SALARIO)*(N-1)/N # recordar que la frecuencia total N ya fue calculada\nvarianza## [1] 30.48813\n# Desviación típica\ndesv <- varianza ^ (1/2)\ndesv## [1] 5.521606\n# Cuasivarianza\ncuasivarianza <- var (datos$SALARIO)\ncuasivarianza## [1] 31.1233\n# Coeficiente de variación\ncvariacion <- desv / abs(media)\ncvariacion## [1] 0.3442222"},{"path":"estadística-descriptiva..html","id":"medidas-de-forma.","chapter":"4 Estadística descriptiva.","heading":"4.6  Medidas de forma.","text":"Las medidas de forma cuantifican el grado de deformación horizontal y vertical de la representación gráfica de una distribución de frecuencias. Son de dos tipos: medidas de asimetría y medidas de apuntamiento o curtosis.Las medidas de asimetría miden el grado de deformación horizontal con respecto un “eje de simetría”, que es aquel que pasa por el valor medio de la distribución. Si suponemos que la distribución es unimodal y campaniforme, tendremos los casos que se muestran en la figura:El tipo y grado de asimetría se puede obtener mediante el coeficiente de asimetría de Fisher. Este coeficiente toma valor negativo si la distribución es asimétrica negativa (mayores frecuencias la derecha de la media), valor positivo si la distribución es asimétrica positiva (mayores frecuencias la izquierda de la media), y se acerca 0 en caso de que la distribución sea aproximadamente simétrica, aunque pueden darse casos de distribuciones simétricas con coeficiente 0. En R, se puede obtener el coeficiente de asimetría mediante la función skewness() del paquete moments:El valor obtenido para la variable SALARIO de nuestra distribución de frecuencias de los trabajadores de la empresa es positivo, lo que indica asimetría positiva: las mayores frecuencias se localizan la derecha de la media. Esto se puede comprobar fácilmente construyendo el histograma de la variable SALARIO y trazando una línea vertical que pase por la media salarial. Para ello usamos el paquete ggplot2:Como comprobamos, la distribución es claramente asimétrica positiva.En cuanto las medidas de curtosis, miden el grado de deformación vertical con respecto una distribución “tipo”, la distribución normal. Suponemos previamente que la distribución de frecuencias estudiada es campaniforme, unimodal y simétrica (o con ligera asimetría). Pueden darse los casos que se muestran en la figura:El tipo y grado de apuntamiento o curtosis se puede obtener mediante el coeficiente de apuntamiento de Fisher. Este coeficiente toma valor negativo si la distribución es platicúrtica (más aplastada que la distribución normal), valor positivo si la distribución es leptocúrtica (más apuntada que la distribución normal), y se acerca 0 en caso de que la distribución sea aproximadamente igual de apuntada que la distribución normal. En R, se puede obtener el coeficiente de asimetría mediante la función kurtosis() del paquete moments. Hay que tener en cuenta que para que esta versión coincida con lo dicho anteriormente, al valor calculado hay que restarle el valor “3”:Se aprecia como el coeficiente (corregido) es menor que 0, por lo que la distribución de frecuencias de la variable SALARIO es platicúrtica (más “aplastada” que la distribución de frecuencias normal). También hay que tener en cuenta que debemos prestar atención la aplicación del coeficiente, ya que vimos con anterioridad que la distribución es aproximadamente simétrica. Gráficamente, podemos comprobar lo anterior representando el histograma y una curva normal que posea la misma moda (que ya calculamos anteriormente). Para que sean comparables, debemos transformar el eje “y” del gráfico, pasando de “frecuencias” “densidad”, para lo cuál se incluye en el geom_histogram() el argumento aes(y = ..density..):El histograma de salarios de Shuttlepod Movers muestra con claridad que la mayor densidad de trabajadores se concentra en los tramos intermedios, especialmente entre los 10 y 17 cientos de PAVOs, donde se alcanza el pico de la distribución. partir de ahí, la frecuencia va disminuyendo progresivamente hacia los intervalos superiores, lo que indica una asimetría positiva: la cola de la distribución se extiende hacia los salarios altos, aunque con muy pocos casos (apenas uno llega 30 cientos de PAVOs).Desde un punto de vista económico, la gráfica confirma que la empresa presenta una estructura salarial relativamente compacta, dominada por ingresos medios, lo que sugiere un modelo de retribución homogéneo para la mayor parte de la plantilla. Sin embargo, la escasez de salarios muy altos revela una débil diferenciación jerárquica, característica de empresas con poca estratificación en su organigrama. Esto puede favorecer la cohesión interna y el control de costes salariales, pero también limita los incentivos para atraer y retener perfiles altamente cualificados en un sector tecnológicamente exigente como el transporte interespacial.","code":"\n# Coeficiente de asimetría de Fisher\nasimetria <- skewness(datos$SALARIO)\nasimetria## [1] 0.413764\nggplot(data = datos,\n       map = aes(x = SALARIO)) +\n  geom_histogram(bins = k,            # k ya fue definida antes (Sturges)\n                 colour = \"red\",\n                 fill = \"orange\") +\n  geom_vline(aes(xintercept = mean(SALARIO)),\n             colour = \"blue\",\n             linetype = \"dashed\",\n             linewidth = 1) +\n  ggtitle(\"SALARIO MENSUAL\",\n          subtitle = \"trabajadores de Shuttlepod Movers\")+\n  xlab(\"Salario (cientos de PAVOs)\") +\n  ylab(\"Frecuencias\")\n# Coeficiente de apuntamiento o curtosis de Fisher\ncurtosis <- kurtosis(datos$SALARIO) - 3\ncurtosis## [1] -0.8232965\nggplot(data = datos, map = aes(x = SALARIO)) +\n  geom_histogram(bins = k,\n                 colour = \"red\",\n                 fill = \"orange\",\n                 aes(y = after_stat(density))) +\n  stat_function(fun = dnorm,\n                args = list(mean = moda, sd = sd(datos$SALARIO)),\n                colour = \"darkblue\",\n                linewidth = 1) +\n  ggtitle(\"SALARIO MENSUAL\",\n          subtitle = \"trabajadores de la empresa Shuttlepod Movers\")+\n  xlab(\"Salario (cientos de PAVOs)\") +\n  ylab(\"Densidad\")"},{"path":"estadística-descriptiva..html","id":"materiales-para-realizar-las-prácticas-del-capítulo.-2","chapter":"4 Estadística descriptiva.","heading":"4.7  Materiales para realizar las prácticas del capítulo.","text":"En esta sección se muestran los links de acceso los diferentes materiales (scripts, datos…) necesarios para llevar cabo los contenidos prácticos del capítulo.Datos (en formato Microsoft® Excel®):interestelar_100.xlsx (obtener aquí)Scripts:describe_rstars.R (obtener aquí)","code":""},{"path":"análisis-previo-de-datos..html","id":"análisis-previo-de-datos.","chapter":"5 Análisis previo de datos.","heading":"5 Análisis previo de datos.","text":"","code":""},{"path":"análisis-previo-de-datos..html","id":"introducción.-1","chapter":"5 Análisis previo de datos.","heading":"5.1  Introducción.","text":"Antes de la aplicación de técnicas complejas que permitan extraer de los datos conclusiones relevantes, es necesario realizar unas tareas previas destinadas conseguir dos objetivos:Preparar nuestros datos para que puedan ser procesados correctamente sin provocar distorsiones en los resultados. En especial, hay dos puntos clave: el tratamiento de los datos faltantes (missing values) y de los casos atípicos o outliers.Preparar nuestros datos para que puedan ser procesados correctamente sin provocar distorsiones en los resultados. En especial, hay dos puntos clave: el tratamiento de los datos faltantes (missing values) y de los casos atípicos o outliers.Obtener una visión inicial de la información que esconden los datos, fundamentalmente en cuanto las medidas básicas que caracterizan la distribución de frecuencias de las variables en las que se estructuran estos, y, en el caso de contar con más de una variable, de la relación estadística que existe entre ellas.Obtener una visión inicial de la información que esconden los datos, fundamentalmente en cuanto las medidas básicas que caracterizan la distribución de frecuencias de las variables en las que se estructuran estos, y, en el caso de contar con más de una variable, de la relación estadística que existe entre ellas.Además, es preciso tener en cuenta que, usualmente, es conveniente que estos rasgos iniciales que caracterizan nuestra muestra o población sean plasmados de un modo visualmente amigable, claro y conciso.En esta práctica, por medio de un ejemplo basado en una base de datos de las empresas dedicadas al transporte de mercancías interestelar, se mostrarán una serie de buenas prácticas y análisis básicos útiles la hora de preparar y analizar inicialmente nuestro conjunto de datos.Vamos suponer que trabajamos dentro de un proyecto que hemos creado previamente, de nombre “explora”. Dentro de la carpeta del proyecto guardaremos el script llamado “previo_rstars.R”, y el archivo de Microsoft® Excel® llamado “interestelar_100.xlsx”. Para decargar los ficheros, ve al final de este capítulo y pincha en los enlaces.Si abrimos “interestelar_100.xlsx”, comprobaremos que se compone de tres hojas. La primera muestra un aviso sobre el uso exclusivo que se debe dar los datos incorporados; la segunda recoge la descripción de las variables consideradas; y la tercera (hoja “Datos”) guarda los datos que debemos importar desde R-Studio. Estos datos se corresponden con diferentes variables económico-financieras y de diversa índole de una muestra de empresas que se dedican al transporte de mercancías interestelar.Es muy importante observar que, en la hoja de cálculo, existen variables con datos faltantes (missing values). En concreto, podemos identificar estas faltas de dato por la presencia de celdas en blanco; pero también por la existencia de celdas con el texto, por ejemplo, “n.d.” (dato). Así, es clave identificar el modo en que quedan recogidos los datos faltantes en la hoja de cálculo, ya que tendremos que aplicar código adicional en el comando de importación de R para que estos casos queden correctamente recogidos como NAs (available).Cerraremos el archivo de Microsoft® Excel®, “interestelar_100.xlsx” y volveremos RStudio. Después, abriremos nuestro script “previo_rstars.R” con File → Open File… Este script contiene el código que vamos ir ejecutando en la práctica.La primera línea / instrucción en el script es:La instrucción tiene como objeto limpiar el Global Environment (memoria) de objetos de anteriores sesiones de trabajo.Luego, si queremos despreocuparnos de la carga de los paquetes que utilizaremos en el script, podemos activarlos ahora:Para importar los datos que hay en la hoja “Datos” del archivo de Microsoft® Excel® y hacer una primera inspección de las variables que contiene, ejecutaremos el código:Por defecto, R considera las celdas en blanco de la hoja de cálculo como NAs; pero hemos de advertirle del resto de posibilidades que existen en la hoja para comunicar que falta un dato determinado, como ya se ha comentado. Para ello, hemos añadido en la función read_excel() el argumento na =, que recoge los contenidos de celda de la hoja de cálculo que indican que falta el dato en cuestión.Por otro lado, R ha tomado la primera columna como una variable categórica. En realidad, esta columna es una variable, sino que está formada por los nombres de los diferentes casos u observaciones. Para evitar que R vea la columna de los nombres de los casos como una variable más, hemos redefinido nuestro data frame diciéndole que tome esa primera columna como el conjunto de los nombres de los casos (filas).En la “tabla” de variables obtenida, se comprueba que 24 variables son métricas, mientras que 4 son categóricas.","code":"\n## Tratamiento y análisis previo de datos.\n\n# Limpiando el Global Environment\nrm(list = ls())\n# Cargando paquetes\nlibrary (readxl)\nlibrary (gtExtras)\nlibrary (dplyr)\nlibrary (visdat)\nlibrary (ggplot2)\nlibrary (knitr)\nlibrary (kableExtra)\nlibrary (moments) # paquete necesario para calcular la curtosis.\nlibrary (patchwork)\nlibrary (GGally)\n## DATOS\n\n# Importando datos desde Excel\ninterestelar_100 <- read_excel(\"interestelar_100.xlsx\",\n                               sheet = \"Datos\",\n                               na = c(\"n.d.\"))\ninterestelar_100 <- data.frame(interestelar_100, row.names = 1)\n\n# visualizando el data frame de modo elegante con {gtExtras}\ndatos_df_graph <- gt_plt_summary(interestelar_100)\ndatos_df_graph"},{"path":"análisis-previo-de-datos..html","id":"análisis-de-una-variable.","chapter":"5 Análisis previo de datos.","heading":"5.2  Análisis de una variable.","text":"","code":""},{"path":"análisis-previo-de-datos..html","id":"buscando-missing-values-y-outliers.","chapter":"5 Análisis previo de datos.","heading":"5.2.1  Buscando missing values y outliers.","text":"Vamos suponer que la variable que queremos estudiar es la variable Rentabilidad Económica (RENECO). Es una buena práctica el trabajar con una copia del data frame original. Con ello, conseguimos mantener la integridad de este data frame, y que el código sea fácilmente utilizable en otros scripts, al utilizar en la copia un nombre genérico. Utilizaremos la función select() con el argumento everything() del paquete dplyr:La primera acción que debe realizarse es comprobar que todos los casos (empresas) tienen su correspondiente dato o valor para la variable (RENECO), es decir, que existen valores perdidos o missing values.Para tener una idea general, se puede utilizar la función vis_miss() del paquete visdat, que localizará gráficamente los missing values de las diferentes variables, y calculará el porcentaje de casos que supone, con respecto al total de observaciones. Para limitar el análisis solo la variable RENECO, filtraremos de data frame con la función select() del paquete dplyr:La explicación detallada del código es la siguiente:muestra %>%: Usa el pipe de dplyr para encadenar operaciones de forma legible.muestra %>%: Usa el pipe de dplyr para encadenar operaciones de forma legible.select(RENECO): Te quedas solo con la columna RENECO. Así, el gráfico se centra en esa variable (una sola columna en la visualización).select(RENECO): Te quedas solo con la columna RENECO. Así, el gráfico se centra en esa variable (una sola columna en la visualización).vis_miss(): Función del paquete visdat que crea un mapa de celdas Present/Missing:\nEje Y = observaciones (filas del data frame).\nEje X = variables (aquí, solo RENECO).\nCada celda indica si el valor está presente o es NA.\nvis_miss(): Función del paquete visdat que crea un mapa de celdas Present/Missing:Eje Y = observaciones (filas del data frame).Eje Y = observaciones (filas del data frame).Eje X = variables (aquí, solo RENECO).Eje X = variables (aquí, solo RENECO).Cada celda indica si el valor está presente o es NA.Cada celda indica si el valor está presente o es NA.labs(...):\ntitle y subtitle: títulos del gráfico.\ny = \"Observación\": etiqueta del eje Y.\nfill = NULL: quita el título de la leyenda (por defecto aparecería algo tipo “valueType” o similar).\nlabs(...):title y subtitle: títulos del gráfico.title y subtitle: títulos del gráfico.y = \"Observación\": etiqueta del eje Y.y = \"Observación\": etiqueta del eje Y.fill = NULL: quita el título de la leyenda (por defecto aparecería algo tipo “valueType” o similar).fill = NULL: quita el título de la leyenda (por defecto aparecería algo tipo “valueType” o similar).scale_fill_manual(...): Personaliza los colores y las etiquetas de la leyenda:\nvalues = c(\"TRUE\" = \"red\", \"FALSE\" = \"grey\"): En vis_miss, internamente se codifica si falta el dato (TRUE) o (FALSE). Aquí dices: rojo para TRUE (hay NA) y gris para FALSE (dato presente).\nlabels = c(\"TRUE\" = \"NA\", \"FALSE\" = \"Presente\"): Cambias el texto mostrado en la leyenda: en lugar de TRUE/FALSE, se verá NA/Presente.\nscale_fill_manual(...): Personaliza los colores y las etiquetas de la leyenda:values = c(\"TRUE\" = \"red\", \"FALSE\" = \"grey\"): En vis_miss, internamente se codifica si falta el dato (TRUE) o (FALSE). Aquí dices: rojo para TRUE (hay NA) y gris para FALSE (dato presente).values = c(\"TRUE\" = \"red\", \"FALSE\" = \"grey\"): En vis_miss, internamente se codifica si falta el dato (TRUE) o (FALSE). Aquí dices: rojo para TRUE (hay NA) y gris para FALSE (dato presente).labels = c(\"TRUE\" = \"NA\", \"FALSE\" = \"Presente\"): Cambias el texto mostrado en la leyenda: en lugar de TRUE/FALSE, se verá NA/Presente.labels = c(\"TRUE\" = \"NA\", \"FALSE\" = \"Presente\"): Cambias el texto mostrado en la leyenda: en lugar de TRUE/FALSE, se verá NA/Presente.theme(plot.title = element_text(face = \"bold\", size = 14)): Pone el título en negrita y con tamaño 14.theme(plot.title = element_text(face = \"bold\", size = 14)): Pone el título en negrita y con tamaño 14.Puede observarse cómo, en el caso concreto de la variable RENECO, un 1% (aproximadamente) de los casos tienen dato (es decir, uno de los 104 casos). Para localizar el caso concreto, recurriremos las herramientas de manejo de data frames del paquete dplyr. En concreto, filtraremos los casos para detectar aquellos que carecen de valor en la variable RENECO:La función .na() comprueba si, en la posición correspondiente una fila o caso, para la variable escrita en el argumento; hay o un dato o valor. Como resultado se obtiene una empresa, para la que se puede corroborar que hay valor para la variable RENECO:Comprobamos que la empresa sin valor en RENECO es Photon Pack Freight.Ante la existencia de missing values, se puede actuar de varios modos. Por ejemplo, se puede intentar obtener por otro canal de información el conjunto de valores de RENECO que están disponibles, o recurrir alguna estimación para los mismos y asignarlos. En caso de que esto sea difícil, se puede optar, simplemente, por eliminar estos casos, en especial cuando representan un porcentaje muy reducido respecto al total. En nuestro ejemplo, vamos suponer que hemos optado por esta última vía, al conseguir unos valores más o menos verosímiles de RENECO para las empresas de las que se carece de dato. Esta eliminación de casos se podrá realizar mediante el código:El operador ! significa “”.Podemos comprobar cómo en el Global Environment aparece el data frame “muestra” con un caso menos (103).Una vez tratados los casos con valores perdidos o missing values, conviene detectar la posible presencia en la muestra de outliers o casos atípicos, que pudieran desvirtuar los resultados derivados de ciertos análisis. Al trabajar con una sola variable métrica (la rentabilidad económica, RENECO), podemos realizar esta tarea representando gráficamente la variable mediante un boxplot o gráfico de caja. Aplicaremos, por ejemplo, el código siguiente, que utiliza la gramática del paquete ggplot2:Obteniéndose el gráfico:La “caja” contiene el 50% de los valores de la variable que toman los casos centrales (los que van del primer cuartil al tercero, cuya diferencia se llama rango intercuartílico), y contiene una línea horizontal que es la mediana (segundo cuartil). Por arriba sobresale un segmento que llega al mayor valor de la variable que toma algún caso y que llega ser atípico; y por debajo de la caja otro segmento que llega al menor valor de la variable que toma algún caso y que llega ser atípico. Los casos atípicos o outliers son aquellos que toman valores que se alejan más de 1.5 veces del rango intercuartílico (altura de la caja) del tercer cuartil, por arriba; o del primer cuartil, por abajo. Se registran mediante puntos.En nuestro caso, el boxplot ratifica la existencia de dos casos atípicos. Para identificar esos dos casos concretos, podemos recurrir al paquete dplyr, y establecer un filtro con el siguiente código:En el código anterior, las dos primeras filas calculan los cuartiles primero (Q1) y tercero (Q3) mediante la función quantile(). En esta función, es preciso poner como segundo argumento la proporción de casos que van quedar por debajo del “cuantil” en cuestión (por ejemplo, el primer cuartil se calcula poniendo 0.25, dado que deja por debajo al 25% de casos con menor valor en la variable). Luego se filtran los outliers mediante la función filter() de dplyr , calculados como aquellos casos con valores de RENECO mayores que Q3 más 1,5 veces el rango intercuartílico de la variable; o menores que Q1 menos 1,5 veces dicho rango intercuartílico. Para calcular el rango intercuartílico se recurre la función IQR(). Finalmente, con select(), se muestran los casos en la consola de R-Studio:Los casos atípicos o outliers en la variable RENECO son Jovian Logistics y Sandworm Freight.Como ocurría con los missing values, el tratamiento de los outliers depende de la información que se tenga, y del enfoque que se quiera seguir en la aplicación de las diferentes técnicas, existiendo varias alternativas (corrección del dato, estimación, eliminación del caso, mantenimiento del caso tal y como está y utilización de técnicas robustas, etc.) Es un tema que veces resulta complejo, y que está exento de divergencias en la opinión de los investigadores.Por tanto, una opción posible es, si se tiene información fiable y los outliers representan una gran proporción respecto al total de casos, la eliminación de los casos que presentan valores atípicos en las variables en estudio. En este ejemplo, efectivamente, eliminaremos estas dos empresas con comportamiento atípico en la rentabilidad económica (RENECO), fin de que su presencia en la muestra distorsione los resultados en la aplicación posterior de ciertas técnicas (por ejemplo, un ANOVA o un análisis de regresión). Podemos hacerlo creando un nuevo data frame partir de “muestra”; pero sin esos dos casos. Ese nuevo data frame se llamará, por ejemplo, “muestra_so”:Es importante observar que, en el código de la función filter(), las desigualdades deben cambiar, así como el operador “|” por el operador “&”. En el Global Environment podemos comprobar cómo el data frame “muestra_so” posee el mismo número de variables que el data frame “muestra”; pero con dos observaciones o casos menos (101).","code":"\n# Copia de df\nmuestra<- select(interestelar_100, everything())\n## Analisis de una variable.\n\n# Localizando missing values.\nmuestra %>%\n  select (RENECO) %>%\n  vis_miss() +\n    labs(title = \"Rentabilidad Económica: valores ausentes\",\n      subtitle = \"Transporte de mercancías interestelar\",\n      y = \"Observación\",\n      fill = NULL) +\n    scale_fill_manual(\n      values = c(\"TRUE\" = \"red\", \"FALSE\" = \"grey\"),\n      labels = c(\"TRUE\" = \"NA\", \"FALSE\" = \"Presente\")) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14))\n# Mostrar casos concretos de NAs\nmuestra %>% filter(is.na(RENECO)) %>% select(RENECO)##                     RENECO\n## photon pack freight     NA\nmuestra <- muestra %>% filter(! is.na(RENECO))\n## Localizando outliers\nggplot(data = muestra, map = (aes(y = RENECO))) +\n    geom_boxplot(fill = \"orange\") +\n    ggtitle(\"Rentabilidad Económica\",\n            subtitle = \"Empresas de transporte interestelar\") +\n            ylab(\"Rentabilidad Económica (%)\") +\n    theme(plot.title = element_text(face = \"bold\", size = 14))\n# Mostrar casos concretos de outliers\nQ1 <- quantile (muestra$RENECO, c(0.25))\nQ3 <- quantile (muestra$RENECO, c(0.75))\n\nmuestra %>% \n  filter(RENECO > Q3 + 1.5*IQR(RENECO) |\n         RENECO < Q1 - 1.5*IQR(RENECO)) %>%\n  select(RENECO)##                    RENECO\n## jovian logistics 93.84748\n## sandworm freight 89.40171\n# Eliminar outliers\nmuestra_so <- muestra %>%\n              filter(RENECO <= Q3 + 1.5*IQR(RENECO) &\n                     RENECO >= Q1 - 1.5*IQR(RENECO))"},{"path":"análisis-previo-de-datos..html","id":"descripción-de-una-variable.","chapter":"5 Análisis previo de datos.","heading":"5.2.2  Descripción de una variable.","text":"Una vez que se tiene preparada la base de datos, con un tratamiento adecuado de los missing values y de los outliers, y antes de proceder la aplicación de una técnica adecuada según los objetivos perseguidos en el estudio; suelen presentarse una serie de gráficos básicos y medidas descriptivas que proporcionan una idea inicial de la estructura del sector para la variable o variables analizadas. Nos referimos medidas y/o gráficos de posición, dispersión y forma (asimetría y curtosis).Antes de ello, es conveniente presentar una tabla donde se recoja la distribución de frecuencias de la variable. Si son muchos los casos (en el ejemplo, 101), la distribución podría presentarse agrupada en intervalos, como ya se vio en el capítulo 4 de este libro. De este modo, el código para generar la tabla podría ser (recordar que tienen que estar activos los paquetes knitr y kableExtra):\nTable 5.1: Table 5.2: Distribución de frecuencias agrupadas en intervalos de Rentabilidad Económica\nEn conjunto, la rentabilidad económica de estas empresas se concentra en valores medios–altos: la gran mayoría se mueve en torno al 50–60%, y dos de cada cuatro intervalos centrales (47,4–53,9 y 53,9–60,3) reúnen la mayor parte de los casos. En términos prácticos, alrededor de ocho de cada diez compañías quedan entre 41% y 66,8%, lo que sugiere un sector relativamente estable, con resultados parecidos entre sí. En los extremos hay pocas: apenas un 5% presenta rentabilidades muy bajas (por debajo del 34,5%) y sólo un 2% llega niveles muy altos (por encima del 73,2%). Económicamente, esto indica que la competencia y la eficiencia operativa tienden “empujar” las empresas hacia un rendimiento razonable y sostenido, mientras que los casos de fracaso o de éxito excepcional son minoría.Por otro lado, el análisis gráfico suele dar una idea atractiva e intuitiva de la estructura de la distribución de frecuencias de nuestro conjunto de casos en relación con la variable analizar.Un gráfico fundamental es el histograma de la variable estudiada. Para ello, utilizaremos la gramática del paquete ggplot2:En el gráfico vemos de un modo nítido la distribución de frecuencias en cuanto la rentabilidad económica (RENECO). Se ha incorporado una línea vertical azul (mediante geom_vline()) para localizar la rentabilidad media. Entre otras cosas, se puede apreciar que la distribución de frecuencias es acampanada y prácticamente simétrica.Como complemento al histograma, podemos realizar un gráfico de densidad de RENECO, al que añadiremos una curva normal con la misma media y desviación típica que nuestra distribución de frecuencias, y que se añade mediante stat_function() y el argumento fun = dnorm. Este gráfico representa la distribución de probabilidad empírica de la muestra, es una especie de histograma “suavizado”. De este modo, se podrán verificar de un modo fácil algunas de las características avanzadas con la observación del histograma, como la asimetría positiva. El código es:La curva naranja muestra cómo se distribuye la rentabilidad económica: tiene forma de “campana”, con la mayoría de empresas concentradas alrededor del 50–60%. La línea vertical azul marca el valor central (media), en torno al 52%. Comparada con la curva azul de referencia (la normal), la distribución real es muy parecida, aunque algo más ancha y con un ligero peso extra en rentabilidades algo por debajo del centro; en el extremo alto hay menos casos de lo que predeciría una campana perfecta. En términos económicos: el sector muestra un rendimiento bastante estable y similar entre compañías, con pocas muy rezagadas o extraordinariamente rentables.Un tercer gráfico útil es el box-plot una vez se eliminaron los casos outliers, con la incorporación de los valores que toman los casos que componen la muestra, para lo cual se utiliza el geom_jitter:El box-plot confirma que la rentabilidad económica está bastante concentrada en la zona central. La mediana ronda el 50% (línea dentro de la caja) y la mitad de las empresas cae aproximadamente entre 46% y 59% (altura de la caja, IQR), lo que sugiere variabilidad moderada. El rombo azul (media) está muy cerca de la mediana, indicando casi simetría. Los bigotes se extienden hacia valores en torno 35–70% y aparecen pocos outliers tanto por abajo (≈30%) como por arriba (hasta ≈80%). En términos económicos: la mayoría de compañías rinde de forma similar y estable alrededor del 50–60%, con casos extremos poco frecuentes.Por otro lado, conviene tener conocimiento del valor de las principales medidas descriptivas (de posición, dispersión, forma) que caracterizan la distribución de la variable analizar. Para ello, vamos crear un data frame llamado, por ejemplo, “estadisticos”, que recogerá las diferentes medidas, calculadas al aplicar la variable RENECO del data frame “muestra_so” la función de {deplyr} llamada summarise(). Se calcularán la media, desviación típica, valor mínimo, mediana, valor máximo, el coeficiente de asimetría de Fisher, y el coeficiente de apuntamiento o curtosis de Fisher. Precisamente, para poder calcular esta última medida, es preciso activar el paquete moments, que contiene la función kurtosis(). La versión del coeficiente de apuntamiento de esta función dispone como distribución perfectamente mesocúrtica el valor de 3, por lo que se le restará 3 en la versión que manejaremos para que la distribución mesocúrtica se sitúe en un coeficiente de 0. El código es:La ventaja de volcar las medidas y estadísticos en el data frame (de un solo caso) “estadisticos” es que se pueden mostrar los valores en una tabla elegante generada partir de el mismo mediante las funciones knitr() y kableExtra(), como ya sabemos:\nTable 5.3: Table 5.4: Principales Estadísticos de la Rentabilidad Económica\nLa interpretación de estas medidas fueron comentadas en el capítulo 4. En general, puede decirse que:La rentabilidad típica del sector está en torno al 51–52% (media ≈ 51,6% y mediana ≈ 50,4%), lo que indica un nivel medio-alto y sin sesgos fuertes.La rentabilidad típica del sector está en torno al 51–52% (media ≈ 51,6% y mediana ≈ 50,4%), lo que indica un nivel medio-alto y sin sesgos fuertes.La variabilidad es moderada (desv. típica ≈ 10 p.p.): la mayoría de empresas se mueve, grosso modo, entre ~42% y ~62%. Es un sector relativamente estable.La variabilidad es moderada (desv. típica ≈ 10 p.p.): la mayoría de empresas se mueve, grosso modo, entre ~42% y ~62%. Es un sector relativamente estable.La asimetría ligeramente positiva (0,14) sugiere que hay algunas compañías con rentabilidades algo superiores lo habitual, pero dominan el panorama.La asimetría ligeramente positiva (0,14) sugiere que hay algunas compañías con rentabilidades algo superiores lo habitual, pero dominan el panorama.La curtosis negativa (−0,19, platicúrtica) implica una distribución más “plana” que la normal: menos extremos (tanto muy altos como muy bajos) y también algo menos de concentración en el centro. Económicamente: hay diversidad razonable de resultados, pero pocos casos extremos de fracaso o de éxito extraordinario.La curtosis negativa (−0,19, platicúrtica) implica una distribución más “plana” que la normal: menos extremos (tanto muy altos como muy bajos) y también algo menos de concentración en el centro. Económicamente: hay diversidad razonable de resultados, pero pocos casos extremos de fracaso o de éxito extraordinario.Los extremos observados (≈ 28% mínimo y ≈ 80% máximo) existen, pero son minoría.Los extremos observados (≈ 28% mínimo y ≈ 80% máximo) existen, pero son minoría.Conclusión: el negocio de transporte interestelar muestra una rentabilidad sostenida y relativamente homogénea; hay diferencias entre empresas (gestión, escala, rutas, mix tecnológico), pero el riesgo de resultados extremos parece contenido, y los “superéxitos” son poco frecuentes.Conclusión: el negocio de transporte interestelar muestra una rentabilidad sostenida y relativamente homogénea; hay diferencias entre empresas (gestión, escala, rutas, mix tecnológico), pero el riesgo de resultados extremos parece contenido, y los “superéxitos” son poco frecuentes.","code":"\n# Tabla de datos (distribución de frecuencias agrupadas en intervalos)\n# Colocando casos\nmuestra_so <- muestra_so %>% arrange(RENECO, row.names(muestra_so))\n\n# Fijar k como número de intervalos (método de Sturges)\nk <- nclass.Sturges(muestra_so$RENECO)\n\n# Crear los intervalos\nmuestra_so$intervalos <- cut(muestra_so$RENECO, breaks = k, include.lowest = TRUE)\n\n# Contar las frecuencias de cada intervalo\nconteo_intervalos <- table(muestra_so$intervalos)\n\n# Convertir el resultado a un data frame para una mejor visualización\nconteo_intervalos_df <- as.data.frame(conteo_intervalos)\n\n# Renombrar las columnas para mayor claridad\ncolnames(conteo_intervalos_df) <- c(\"Intervalo\", \"Frecuencia\")\n\n# Calcular y guardar la frecuencia total\nN_agre <- sum(conteo_intervalos_df$Frecuencia)\n\n# Calcular frecuencias absolutas acumuladas\nconteo_intervalos_df$Frecuencia_acum <- cumsum(conteo_intervalos_df$Frecuencia)\n\n# Calcular frecuencias relativas\nconteo_intervalos_df$Frecuencia_R <- conteo_intervalos_df$Frecuencia / N_agre\n\n# Calcular frecuencias relativas acumuladas\nconteo_intervalos_df$Frecuencia_R_acum <- cumsum(conteo_intervalos_df$Frecuencia_R)\n\n# Mostrar el resultado\nconteo_intervalos_df %>%\n  kable(caption = \"Distribución de frecuencias agrupadas en intervalos de Rentabilidad Económica\",\n        col.names = c(\"Intervalo rentabilidad\", \"Frecuencia absoluta n(i)\",\n                      \"Frecuencia absoluta acum. N(i)\", \"Frecuencia relativa f(i)\",\n                      \"Frecuencia relativa acum. F(i)\"),\n        digits    = c(NA, 0, 0, 2, 2),\n        format.args = list(decimal.mark = \".\", scientific = FALSE)) %>%\n  kable_styling(full_width = F, bootstrap_options = \"striped\",\n                \"bordered\", \"condensed\",\n                position = \"center\", font_size = 11) %>%\n  row_spec(0, bold= T, align = \"c\") %>%\n  row_spec(1:(nrow(conteo_intervalos_df)), bold= F, align = \"c\")\n## Descriptivos básicos\n\n# Gráficos básicos\ng1 <-\nggplot(data = muestra_so, map = aes(x = RENECO)) +\n  geom_histogram(bins = k,\n                 colour = \"red\",\n                 fill = \"orange\",\n                 alpha = 0.7) +\n  geom_vline(xintercept = mean(muestra_so$RENECO),\n             color = \"dark blue\",\n             linewidth = 1.2,\n             alpha = 0.8) +\n  ggtitle(\"Histograma\")+\n  xlab(\"Rentabilidad Económica (%)\") +\n  ylab(\"Frecuencias\")\n\ng1\ng2 <-\nggplot(data = muestra_so, map = aes(x = RENECO)) +\n  geom_density(colour = \"red\",\n               fill = \"orange\",\n               alpha = 0.7) +\n  geom_vline(xintercept = mean(muestra_so$RENECO),\n             color = \"dark blue\",\n             linewidth = 0.8,\n             alpha = 0.8) +\n  stat_function(fun = dnorm, args = list(mean = mean(muestra_so$RENECO),\n                                         sd = sd(muestra_so$RENECO)),\n                                         geom = \"area\",\n                                         color = \"darkblue\", \n                                         fill = \"yellow\",\n                                         alpha = 0.2) +\n  ggtitle(\"Gráfico de densidad vs curva normal\")+\n  xlab(\"Rentabilidad Económica (%)\") +\n  ylab(\"Densidad\")\n\ng2\ng3 <-\nggplot(data = muestra_so, map = (aes(x = \"\", y = RENECO))) +\n  geom_boxplot(color = \"red\",\n               fill = \"orange\",\n               outlier.shape = NA) +\n  stat_summary(fun = \"mean\",\n               geom = \"point\",\n               size = 3,\n               col = \"darkblue\") +\n  geom_jitter(width = 0.1,\n              size = 1,\n              col = \"darkred\",\n              alpha = 0.50) +\n  ggtitle(\"Box-Plot\") +\n  ylab(\"Rentabilidad Económica (%)\")\n\ng3\n# Calcular estadísticos\nestadisticos <- muestra_so %>% summarise( Media = mean(RENECO),\n                                          DT = sd(RENECO),\n                                          Mínimo = min(RENECO),\n                                          Mediana = median(RENECO),\n                                          Maximo = max(RENECO),\n                                          Asimetria = skewness(RENECO),\n                                          Curtosis = kurtosis(RENECO) - 3)\n# Mostrar estadisticos\nestadisticos %>%\n  kable(caption = \"Principales Estadísticos de la Rentabilidad Económica\",\n        col.names = c(\"Media\", \"Mediana\",\n                      \"Desviación Típica\", \"Valor mínimo\",\n                      \"Valor Máximo\", \"C. Asimetría Fisher\",\n                      \"C. Curtosis Fisher\"),\n        digits    = c(2, 2, 2, 2, 2, 2, 2),\n        format.args = list(decimal.mark = \".\", scientific = FALSE)) %>%\n  kable_styling(full_width = F, bootstrap_options = \"striped\",\n                \"bordered\", \"condensed\",\n                position = \"center\", font_size = 11) %>%\n  row_spec(0, bold= T, align = \"c\") %>%\n  row_spec(1:(nrow(estadisticos)), bold= F, align = \"c\")"},{"path":"análisis-previo-de-datos..html","id":"normalidad.","chapter":"5 Análisis previo de datos.","heading":"5.2.3  Normalidad.","text":"En muchas técnicas multivariantes basadas en métodos inferenciales (por ejemplo, análisis de la varianza, o en la regresión lineal), se requiere que las variables sigan una distribución normal. Para comprobarlo, se puede recurrir análisis gráficos o análisis formales, estos últimos basados en contrastar la hipótesis nula de normalidad.Vamos mostrar un método gráfico muy extendido. Comprobaremos la normalidad de la variable RENECO mediante un gráfico qq (cuantil-cuantil), que compara los cuantiles de nuestra muestra con los de una distribución normal teórica (con la misma media y desviación típica). Si los puntos se sitúan cercanos la diagonal, entonces se asumirá un comportamiento (aproximadamente) normal. El código para realizar el gráfico con las herramientas del paquete ggplot2 es:Y el resultado:veces, es difícil obtener una conclusión sólida con el gráfico qq; aunque en el ejemplo la separación de los puntos respecto al eje diagonal induce pensar en que podría seguirse una distribución normal.Si queremos ser más precisos, en lugar de un análisis gráfico se puede recurrir realizar un análisis formal, basado en la realización de contrastes de hipótesis. Una prueba muy usual es la prueba de normalidad de Shapiro y Wilk, que tiene un buen comportamiento en muestras relativamente reducidas. En esta prueba, la hipótesis nula equivale al supuesto de normalidad. Para un 5% de significación estadística, un p-valor superior 0.05 implicará el -rechazo de la hipótesis de normalidad. Para realizar la prueba, se ejecutará el código:El resultado obtenido en la consola es:Como el p-valor es (muy) superior 0.05, se rechaza la hipótesis nula de normalidad en la distribución, lo que implica que, para una significación estadística del 5%, la muestra apoya la hipótesis de que RENECO sigue un comportamiento normal, como ya se anticipó con el gráfico qq.","code":"\n## Normalidad\n\n# Grafico QQ\ng4 <-\nggplot(data = muestra_so, aes(sample = RENECO)) +\n  stat_qq(colour = \"red\") + \n  stat_qq_line(colour = \"dark blue\") +\n  ggtitle(\"QQ-Plot\")\n\ng4\n# Prueba de Shapiro-Wilk\n\nshapiro.test(x = muestra_so$RENECO)## \n##  Shapiro-Wilk normality test\n## \n## data:  muestra_so$RENECO\n## W = 0.9949, p-value = 0.9715"},{"path":"análisis-previo-de-datos..html","id":"resumen-4-gráficos-básicos-en-la-descripción-de-una-variable.","chapter":"5 Análisis previo de datos.","heading":"5.2.4  Resumen: 4 gráficos básicos en la descripción de una variable.","text":"En definitiva, para describir de un modo inicial una distribución de frecuencias de una variable (en escala métrica), podrían analizarse los 4 gráficos que se han comentado anteriormente. Estos gráficos se pueden presentar conjuntamente, para ahorrar espacio en un informe, utilizando el paquete patchwork, que permite combinar e integrar en una sola imagen varios gráficos generados con ggplot2. En nuestro ejemplo, vamos generar una figura que integra los 4 gráficos anteriores (que hemos denominado “g1”, “g2”, “g3” y “g4”). Para ello creamos el objeto “resumen”, que integra los gráficos, mediante una asignación con la sintaxis del paquete patchwork: El operador / indica que los gráficos siguientes se dispondrán inmediatamente debajo; mientras que | indica que el gráfico siguiente se dispone al lado del anterior. Luego, se le añade también un título y un subtítulo personalizados:La composición gráfica se guarda en el Global Environment con el nombre “resumen”. El bloque plot_annotation() determina y caracteriza el título y subtítulo del conjunto de gráficos.","code":"\n## Resumen gráfico\n\nresumen <- (g1 | g2)/(g3 | g4)\nresumen <- resumen + \n  plot_annotation(\n    title = \"Rentabilidad Económica\",\n    subtitle = \"Empresas TMI (sin outliers)\",\n    theme = theme(\n      # TÍTULO de la composición\n      plot.title = element_text(\n        size = 16,          # tamaño\n        face = \"bold\",      # negrita\n      ),\n      # SUBTÍTULO de la composición\n      plot.subtitle = element_text(\n        size = 12\n      )))\nresumen"},{"path":"análisis-previo-de-datos..html","id":"análisis-de-múltiples-variables.","chapter":"5 Análisis previo de datos.","heading":"5.3  Análisis de múltiples variables.","text":"Son muchas las técnicas aplicadas al análisis de datos económicos basadas en una distribución de frecuencias multivariante. En este apartado nos centraremos en el caso de variables métricas, ya que al caso de atributos, variables categóricas o factores; le dedicaremos un capítulo en exclusiva. Algunas técnicas multivariantes son: el análisis de componentes principales, el análisis de regresión o el análisis clúster…Todas estas metodologías requieren, de nuevo, de una fase inicial que ponga punto la base de datos y ofrezca una fotografía de cómo es la situación en cuanto las variables en estudio. En este sentido, es conveniente aplicar, para cada variable por separado, algunos de los análisis gráficos básicos vistos anteriormente.estos análisis básicos hay que añadir, principalmente, algún análisis previo adicional, destinado fundamentalmente cuantificar el grado de intensidad en la relación estadística entre las variables implicadas, mediante el estudio de la correlación. Antes de abordar esta cuestión, hemos de pararnos en una casuística específica que se presenta cuando trabajamos con numerosas variables: la detección de casos atípicos o outliers.","code":""},{"path":"análisis-previo-de-datos..html","id":"localización-de-missing-values-y-outliers.","chapter":"5 Análisis previo de datos.","heading":"5.3.1  Localización de missing values y outliers.","text":"Para trabajar con múltiples variables, en primer lugar es preciso localizar los casos con valores perdidos o missing values, para decidir cómo procesarlos (eliminación del caso, estimación del valor faltante, etc.)Vamos imaginar que queremos realizar un análisis en el que tendremos en cuenta las variables RENECO (rentabilidad económica), ACTIVO (volumen de activos de la empresa), MARGEN (margen de beneficio) y RES (resultado del ejercicio).En primer lugar, generaremos una copia del data frame original, “interestelar_100”, para preservar su integridad. esa copia la hemos llamado “muestra2”. Ya vimos cómo el siguiente código nos aporta gráficamente una idea de la posible existencia de valores faltantes:Podemos apreciar cómo existen varios casos con missing values en las variables objeto de estudio.Para localizar los casos concretos con missing values, podemos recurrir al siguiente código. En él, hemos sometido “muestra2” un filtro para detectar los casos en los que hay valor para alguna (o varias) de las variables analizadas. El operador | significa “o”. Posteriormente, hemos decidido eliminar esos casos (podría optarse por otro tipo de tratamiento, como la estimación de valores). Para ello asignamos “muestra2” el resultado de pasar un filtro en el que se eligen los casos que tienen valores faltantes en ninguna de las variables. El operador & significa “y”:El data frame “muestra2” contiene los mismos datos que “interestelar_100”, salvo los 3 casos con missing values que han sido eliminados (101): Vega Transport, Photon Pack Freight y Poe Dameron Cargo.Para la detección de outliers, si las variables que entran en el análisis son numerosas, podría ser poco operativo estudiar las variables una una. Una alternativa consiste en calcular la distancia de Mahalanobis de las variables del estudio, como “resumen” del comportamiento de cada caso en todas las variables del análisis, consideradas conjuntamente. En concreto, se puede considerar como una suerte de “z-score multivariante”: mide cuántas desviaciones típicas se aleja una observación del “centro” del conjunto, teniendo en cuenta todas las variables la vez y cómo se relacionan entre sí.Su utilización, intuitivamente, se puede justificar de este modo:Con una sola variable, el “raro” es el que está muchos z-scores de la media.Con una sola variable, el “raro” es el que está muchos z-scores de la media.Con varias variables (p. ej., rentabilidad, activo, margen, resultado), el “centro” es el vector de medias y la forma de la nube viene dada por la covarianza (las variables pueden estar en escalas distintas y estar correlacionadas).Con varias variables (p. ej., rentabilidad, activo, margen, resultado), el “centro” es el vector de medias y la forma de la nube viene dada por la covarianza (las variables pueden estar en escalas distintas y estar correlacionadas).La distancia de Mahalanobis:\nEstandariza las escalas (una variable en millones pesa más que otra en porcentajes).\nGira y estira el espacio según las correlaciones (si dos variables están muy correlacionadas, moverte lo largo de su diagonal “cuesta” menos que moverte perpendicularmente).\nLa distancia de Mahalanobis:Estandariza las escalas (una variable en millones pesa más que otra en porcentajes).Estandariza las escalas (una variable en millones pesa más que otra en porcentajes).Gira y estira el espacio según las correlaciones (si dos variables están muy correlacionadas, moverte lo largo de su diagonal “cuesta” menos que moverte perpendicularmente).Gira y estira el espacio según las correlaciones (si dos variables están muy correlacionadas, moverte lo largo de su diagonal “cuesta” menos que moverte perpendicularmente).En 2D lo verías como elipses alrededor del centro (círculos). Los puntos fuera de la elipse “grande” son potenciales outliers.En 2D lo verías como elipses alrededor del centro (círculos). Los puntos fuera de la elipse “grande” son potenciales outliers.La distancia de Mahalanobis de un vector \\(\\mathbf{x}\\\\mathbb{R}^p\\) respecto del centro \\(\\boldsymbol{\\mu}\\) y la matriz de covarianzas \\(\\mathbf{\\Sigma}\\) es:\\[\nD_M(\\mathbf{x})\n= \\sqrt{(\\mathbf{x}-\\boldsymbol{\\mu})^{\\top}\\,\\mathbf{\\Sigma}^{-1}\\,(\\mathbf{x}-\\boldsymbol{\\mu})}.\n\\]Así, primero vamos calcular una columna más en el data frame “muestra2” con los valores de la distancia de Mahalanobis del conjunto de las 4 variables en cada uno de los casos. Esta columna o variable la denominaremos, por ejemplo, MAHALANOBIS. Para ello, se utiliza las funciones mutate() y pick() del paquete dplyr, y como argumento de esta la función mahalanobis(), en la que hay que especificar:Las columnas de “muestra2” para las que se van calcular las distancias.Las columnas de “muestra2” para las que se van calcular las distancias.El vector de medias de las variables para las que se calcula la distancia (argumento center =).El vector de medias de las variables para las que se calcula la distancia (argumento center =).La matriz de varianzas y covarianzas de las variables para las que se calcula la distancia (argumento cov =).La matriz de varianzas y covarianzas de las variables para las que se calcula la distancia (argumento cov =).En definitiva, el código es:Posteriormente, se puede construir el diagrama de caja de la variable MAHALANOBIS, como cualquier otra variable:Se observa cómo existen varios casos outliers. Para saber de qué casos concretos se trata, se podrá ejecutar el código:En la consola se obtendrá el listado:Si se opta por eliminar estos casos cara al análisis aplicar posteriormente, se podrá crear un nuevo data frame, por ejemplo “muestra2_so”, con el código siguiente:El data frame “muestra2_so” será una réplica de “muestra2”, aunque sin incluir los casos detectados como atípicos o outliers (91 casos).","code":"\n## Trabajando con multiples variables.\n\n# Copia de df original.\nmuestra2<- select(interestelar_100, everything())\n\n# Localizando missing values.\nmuestra2 %>%\n  select (RENECO, ACTIVO, MARGEN, RES) %>%\n  vis_miss() +\n    labs(title = \"Rentabilidad Económica: valores ausentes\",\n      subtitle = \"Transporte de mercancías interestelar\",\n      y = \"Observación\",\n      fill = NULL) +\n    scale_fill_manual(\n      values = c(\"TRUE\" = \"red\", \"FALSE\" = \"grey\"),\n      labels = c(\"TRUE\" = \"NA\", \"FALSE\" = \"Presente\")) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14))\n## Trabajando con multiples variables.\n\n# Localizando y descartando casos con missing values.\nmuestra2 %>% filter(is.na(RENECO) |\n                      is.na(ACTIVO) |\n                      is.na(MARGEN) |\n                      is.na(RES))%>%\n             select(RENECO, ACTIVO, MARGEN, RES)##                       RENECO ACTIVO  MARGEN    RES\n## vega transport      50.42554     NA 48.4567 104.87\n## photon pack freight       NA  57.78 48.7883  24.36\n## poe dameron cargo   54.20692  49.68      NA  26.93\nmuestra2 <- muestra2 %>%\n            filter(! is.na(RENECO) &\n                     ! is.na(ACTIVO) &\n                     ! is.na(MARGEN) &\n                     ! is.na(RES))\n# Identificando y descartando outliers con distancia de Mahalanobis.\nmuestra2 <- muestra2 %>%\n  mutate(\n    MAHALANOBIS = {\n      X <- pick(RENECO, ACTIVO, MARGEN, RES)\n      mahalanobis(as.matrix(X),\n                  center = colMeans(X),\n                  cov = cov(X))\n    }\n  )\nggplot(data = muestra2, map = (aes(y = MAHALANOBIS))) +\n    geom_boxplot(fill = \"orange\") +\n    ggtitle(\"DISTANCIA DE MAHALANOBIS\",\n            subtitle = \"RENECO, ACTIVO, MARGEN, RES. Empresas TMI.\") +\n    ylab(\"MAHALANOBIS\")\nQ1M <- quantile (muestra2$MAHALANOBIS, c(0.25))\nQ3M <- quantile (muestra2$MAHALANOBIS, c(0.75))\n\nmuestra2 %>%\n  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |\n         MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%\n  select(MAHALANOBIS, RENECO, ACTIVO, MARGEN, RES) ##                        MAHALANOBIS   RENECO  ACTIVO\n## hyperion star haulage     8.443881 79.68646  211.14\n## jovian logistics         19.407213 93.84748  220.56\n## sandworm freight         23.338920 89.40171   17.55\n## chakotay cargo systems   34.189074 37.09704 2158.69\n## moya cargo systems        7.987575 66.05547  520.29\n## io star transport         8.143146 34.26908  498.00\n## dagobah freightlines     11.049984 66.83549  511.39\n## hyperdrive express       27.755438 45.20668 2499.94\n## home one cargo           14.276454 53.21612 1145.48\n## arrakis freight          44.863023 40.10962 2966.52\n## \n##                          MARGEN     RES\n## hyperion star haulage  41.51348  168.25\n## jovian logistics       32.11189  206.99\n## sandworm freight       35.12424   15.69\n## chakotay cargo systems 44.83819  800.81\n## moya cargo systems     63.31031  343.68\n## io star transport      17.41820  170.66\n## dagobah freightlines   91.97546  341.79\n## hyperdrive express     63.19775 1130.14\n## home one cargo         48.62131  609.58\n## arrakis freight        51.42027 1189.86\nmuestra2_so <- muestra2 %>%\n  filter(MAHALANOBIS <= Q3M + 1.5*IQR(MAHALANOBIS) &\n           MAHALANOBIS >= Q1M - 1.5*IQR(MAHALANOBIS)) "},{"path":"análisis-previo-de-datos..html","id":"correlación-entre-variables.","chapter":"5 Análisis previo de datos.","heading":"5.3.2  Correlación entre variables.","text":"Cuando trabajamos con más de una variable, una característica muy importante viene dada por la intensidad con la que tales variables están relacionadas estadísticamente entre sí, es decir, el estudio de las correlaciones. Un modo atractivo y rápido de visualizar la matriz de correlaciones de las variables es través de la función ggpairs() del paquete GGally. Para aplicar la función, hemos creado el data frame “temporal” con las variables (métricas) del estudio, que borramos tras general el gráfico:La interpretación detallada del código es la siguiente:ggpairs(temporal): crea una cuadrícula con una fila/columna por variable del data frame temporal.\nTriángulo inferior (lower): lo personalizamos para que muestre números de correlación.\nggpairs(temporal): crea una cuadrícula con una fila/columna por variable del data frame temporal.Triángulo inferior (lower): lo personalizamos para que muestre números de correlación.lower = list(continuous = wrap(\"cor\", ...))\nEn las celdas del triángulo inferior, cuando ambas variables son numéricas (“continuous”):\n\"cor\": escribe el coeficiente de correlación entre esas dos variables.\nmethod = \"pearson\": usa la correlación de Pearson (relación lineal).\nsize = 4.5: tamaño del número que aparece.\nstars = TRUE: añade asteriscos de significación estadística junto al número. Habitual: * p < 0.05, ** p < 0.01, *** p < 0.001 (más asteriscos ⇒ relación menos probable por azar).\nlower = list(continuous = wrap(\"cor\", ...))\nEn las celdas del triángulo inferior, cuando ambas variables son numéricas (“continuous”):\"cor\": escribe el coeficiente de correlación entre esas dos variables.\"cor\": escribe el coeficiente de correlación entre esas dos variables.method = \"pearson\": usa la correlación de Pearson (relación lineal).method = \"pearson\": usa la correlación de Pearson (relación lineal).size = 4.5: tamaño del número que aparece.size = 4.5: tamaño del número que aparece.stars = TRUE: añade asteriscos de significación estadística junto al número. Habitual: * p < 0.05, ** p < 0.01, *** p < 0.001 (más asteriscos ⇒ relación menos probable por azar).stars = TRUE: añade asteriscos de significación estadística junto al número. Habitual: * p < 0.05, ** p < 0.01, *** p < 0.001 (más asteriscos ⇒ relación menos probable por azar).title = \"Matriz de Correlación sin outliers\": título del conjunto.title = \"Matriz de Correlación sin outliers\": título del conjunto.Asignación corr_plot_so <- ...: guarda la figura en el objeto corr_plot_so. Para verla, basta con escribir corr_plot_so en la consola.Asignación corr_plot_so <- ...: guarda la figura en el objeto corr_plot_so. Para verla, basta con escribir corr_plot_so en la consola.Un coeficiente de correlación puede tomar un valor entre -1 (fuerte relación, en sentido opuesto) y +1 (fuerte relación, en el mismo sentido). Como puede apreciarse en el gráfico, las variables ACTIVO y RES mantienen una relación muy intensa y en sentido positivo. Entre MARGEN y RENECO existe también una relación de intensidad destacable. En cambio, ACTIVO y MARGEN; y RENECO y ACTIVO apenas están estadísticamente relacionadas.Para finalizar, vamos comparar las correlaciones anteriores con las que se dan si se incluyen los casos outliers en la muestra:Se aprecia cómo la presencia de outliers puede variar la relación entre las variables. Salvo el caso de la correlación entre ACTIVO y RES, que se fortalece; las correlaciones entre RENECO y MARGEN, y RENECO y RES se debilitan. Esto redunda en la idea, ya expuesta, de que la decisión de trabajar con o sin los casos outliers es una cuestión, veces, muy relevante y compleja.","code":"\n## Correlaciones entre variables.\ntemporal <- muestra2_so %>% select(RENECO, ACTIVO, MARGEN, RES)\ncorr_plot_so <- ggpairs(temporal, \n                        lower = list(continuous = wrap(\"cor\",\n                        size = 4.5,\n                        method = \"pearson\",\n                        stars = TRUE)),\n                        title = \"Matriz de Correlación (sin outliers)\")\nrm(temporal)\ncorr_plot_so"},{"path":"análisis-previo-de-datos..html","id":"materiales-para-realizar-las-prácticas-del-capítulo.-3","chapter":"5 Análisis previo de datos.","heading":"5.4  Materiales para realizar las prácticas del capítulo.","text":"En esta sección se muestran los links de acceso los diferentes materiales (scripts, datos…) necesarios para llevar cabo los contenidos prácticos del capítulo.Datos (en formato Microsoft (R) Excel (R)):interestelar_100.xlsx (obtener aquí)Scripts:previo_rstars.R (obtener aquí)","code":""},{"path":"componentes-principales..html","id":"componentes-principales.","chapter":"6 Componentes principales.","heading":"6 Componentes principales.","text":"","code":""},{"path":"componentes-principales..html","id":"introducción.-2","chapter":"6 Componentes principales.","heading":"6.1  Introducción.","text":"veces, menos es más. Esta es la filosofía que subyace las técnicas de reducción de la dimensión de la información.Imaginemos una serie de casos (por ejemplo, las empresas de un sector económico) caracterizados por múltiples variables. Puede ocurrir que, paradójicamente, el contar con tantas variables haga difícil la caracterización de los casos. Esto ocurre cuando algunas de las variables aportan una información muy parecida sobre tales casos. Por ejemplo, es más difícil hacerse una idea del comportamiento global en el ámbito económico o financiero de un grupo de empresas si tenemos que atender los valores que toman en un conjunto de 10 variables, que si solo tenemos que atender un par de indicadores.Las técnicas de reducción de la dimensión de la información tratan, precisamente, de disminuir el número de variables necesarias para caracterizar un grupo de casos, aprovechando la posibilidad de que las (múltiples) variables originales compartan información sobre estos. Es decir, la idea fundamental es pasar de un planteamiento basado en manejar muchas variables con información compartida o redundante (variables que “dicen lo mismo” sobre el comportamiento de los casos) un planteamiento en el que hay menos variables, pero que comparten información (variables que “dicen” cosas diferentes sobre el comportamiento de los casos). En este proceso es importante que la pérdida de información sea mínima, y que solo se pierda la información redundante o repetida.La principal técnica de reducción de la dimensión de la información es la de componentes principales, y es la que se expondrá y ejemplificará en el resto del capítulo. Pero antes, es preciso concretar la relación entre dos conceptos muy presentes en esta técnica: información y varianza.","code":""},{"path":"componentes-principales..html","id":"información-y-varianza.","chapter":"6 Componentes principales.","heading":"6.2  Información y varianza.","text":"En el apartado anterior hemos hablado de la posibilidad de que algunas variables compartan “información” sobre el comportamiento de los casos que constituyen nuestra muestra u objeto de estudio. Pero, ¿qué es, en este contexto, la “información”?La información que una variable contiene sobre un conjunto de casos puede entenderse como su capacidad para diferenciar unos casos de otros.Observemos este ejemplo, en el que se representan los valores que toman un grupo de 20 empresas en 3 variables.En la variable 1, todas las empresas toman el mismo valor. Por tanto, la capacidad que tiene la variable para distinguir los casos (empresas), unos de otros, es nula. Eso es debido que, en definitiva, esta variable contiene información sobre el grupo de 20 empresas.En la variable 2, existe cierta dispersión, aunque reducida, en los valores que adoptan los casos. Esto permite distinguir unos de otros, aunque veces con cierta dificultad. Por ejemplo, la empresa 17 se distingue del resto por ser la que tiene un valor (un poco) mayor. Aun así, como la dispersión es reducida, se distinguen algunos casos de otros demasiado bien. En definitiva, la variable 2 contiene cierta cantidad de información sobre el conjunto de empresas de la muestra, aunque demasiado grande.Por último, la variable 3 muestra una dispersión considerablemente mayor que las otras dos variables. Existe un amplio abanico de valores que toman los diferentes casos (empresas). Esto hace que puedan diferenciarse con facilidad, en general, unos de otros. Esta variable posee, por tanto, una cantidad de información superior respecto las empresas, ya que observando los valores que toman en la variable pueden diferenciarse con facilidad unas de otras.Como conclusión, podemos establecer que cuanto mayor dispersión muestra una variable para un grupo de casos, mayor cantidad de información contiene sobre ellos, en el sentido de disfrutar de un mayor “poder” de diferenciación de unos casos respecto otros.Una medida de la dispersión de una variable usualmente utilizada es la varianza. Por tanto, en cierta manera, la varianza sirve para medir la cantidad de información que contiene la variable: mayor varianza, mayor dispersión. Y mayor dispersión, mayor cantidad de información.En el ejemplo, puede observarse cómo la variable 3 es la que mayor varianza tiene, luego la variable 2, y la variable 1 tiene una varianza de 0 (y posee información sobre las 20 empresas). Esta comparación de varianzas es válida siempre y cuando las tres variables estén expresadas en las mismas unidades, ya que la varianza es una medida de dispersión absoluta. Por ello, para poder comparar, hemos añadido también en el ejemplo una medida de dispersión relativa: el coeficiente de variación. Podemos comprobar cómo el mayor coeficiente de variación pertenece la variable 3 (que es la que tiene una mayor cantidad de información), luego la variable 2 (que cuenta con menor cantidad de información), y por último la variable 1, con un coeficiente de 0 (contiene información sobre las empresas).","code":""},{"path":"componentes-principales..html","id":"la-elección-de-arg-us-korp.","chapter":"6 Componentes principales.","heading":"6.3  La elección de Arg-Us Korp.","text":"Vamos considerar el caso del sector del “Transporte Interestelar”. Tenemos una selección de 104 empresas o compañías. El “magnate” de los negocios escala interplanetaria, Arg-us Korp en la imagen), está de compras: quiere adquirir una de las empresas que mejor proyección futuro en el sector tenga.Hay varias variables disponibles que pueden ser interpretadas como indicadores de la preparación de las diferentes compañías de transporte para afrontar el futuro, como por ejemplo:IMD (Gasto en +D)\nJustificación: Una inversión alta en investigación y desarrollo indica que la empresa está preparándose para innovaciones tecnológicas, lo que le permitirá mejorar su eficiencia, reducir costos y mantenerse competitiva en el futuro.\nIMD (Gasto en +D)Justificación: Una inversión alta en investigación y desarrollo indica que la empresa está preparándose para innovaciones tecnológicas, lo que le permitirá mejorar su eficiencia, reducir costos y mantenerse competitiva en el futuro.IDIG (Índice de Digitalización)\nJustificación: La digitalización es un factor clave en la eficiencia operativa y la adaptabilidad nuevas tecnologías. Un alto IDIG sugiere que la empresa está invirtiendo en automatización, software avanzado y optimización de procesos.\nIDIG (Índice de Digitalización)Justificación: La digitalización es un factor clave en la eficiencia operativa y la adaptabilidad nuevas tecnologías. Un alto IDIG sugiere que la empresa está invirtiendo en automatización, software avanzado y optimización de procesos.EFLO (Edad Media de la Flota)\nJustificación: Contar con una flota renovada significa menor riesgo de fallas mecánicas, menores costos de mantenimiento y mayor eficiencia en las operaciones, lo que permite mantener ventajas competitivas largo plazo.\nEFLO (Edad Media de la Flota)Justificación: Contar con una flota renovada significa menor riesgo de fallas mecánicas, menores costos de mantenimiento y mayor eficiencia en las operaciones, lo que permite mantener ventajas competitivas largo plazo.CAPEX (Gastos de Capital)\nJustificación: Empresas que invierten en infraestructura y equipamiento moderno están mejor preparadas para el crecimiento y la adaptación nuevas demandas del mercado.\nCAPEX (Gastos de Capital)Justificación: Empresas que invierten en infraestructura y equipamiento moderno están mejor preparadas para el crecimiento y la adaptación nuevas demandas del mercado.IDIVERSE (Índice de Diversificación)\nJustificación: Empresas con operaciones diversificadas tienen mayor resiliencia ante cambios del mercado, ya que dependen de una única fuente de ingresos o de un solo tipo de carga.\nIDIVERSE (Índice de Diversificación)Justificación: Empresas con operaciones diversificadas tienen mayor resiliencia ante cambios del mercado, ya que dependen de una única fuente de ingresos o de un solo tipo de carga.RUTAS (Número de Rutas Atendidas)\nJustificación: La expansión de rutas refleja una empresa con visión de crecimiento y acceso mercados emergentes, lo que fortalece su sostenibilidad largo plazo.\nRUTAS (Número de Rutas Atendidas)Justificación: La expansión de rutas refleja una empresa con visión de crecimiento y acceso mercados emergentes, lo que fortalece su sostenibilidad largo plazo.SOLVENCIA\nJustificación: Empresas con una solvencia alta tienen mayor capacidad de enfrentar crisis económicas o períodos de baja demanda sin comprometer su estabilidad financiera.\nSOLVENCIAJustificación: Empresas con una solvencia alta tienen mayor capacidad de enfrentar crisis económicas o períodos de baja demanda sin comprometer su estabilidad financiera.BMAL (Beneficio Medio por Año Luz)\nJustificación: Un alto BMAL indica eficiencia en la operación y rentabilidad sostenible, lo que contribuye la capacidad de la empresa para reinvertir y mejorar su competitividad.\nBMAL (Beneficio Medio por Año Luz)Justificación: Un alto BMAL indica eficiencia en la operación y rentabilidad sostenible, lo que contribuye la capacidad de la empresa para reinvertir y mejorar su competitividad.IFIDE (Índice de Fidelización)\nJustificación: Un alto nivel de fidelización sugiere que la empresa ha construido relaciones sólidas con sus clientes, lo que le proporciona estabilidad de ingresos y ventajas competitivas futuro.\nIFIDE (Índice de Fidelización)Justificación: Un alto nivel de fidelización sugiere que la empresa ha construido relaciones sólidas con sus clientes, lo que le proporciona estabilidad de ingresos y ventajas competitivas futuro.Estos indicadores reflejan la capacidad de innovación, estabilidad financiera, expansión de mercado y eficiencia operativa de las empresas, factores esenciales para su sostenibilidad en el tiempo y preparación para los desafíos futuros.De entre ellas, Korp ha seleccionado IDIVERSE, IFIDE e IDIG como aspectos que le importan especialmente. Aun así, su equipo sabe que su jefe quiere una respuesta precisa. Un nombre de una empresa.Así, el equipo ha pensado en crear un ranking de compañías basándose en estos tres indicadores. Pero, si tienen 3 variables, ¿cómo combinar su análisis para obtener un solo ranking de modo objetivo? ¿Cómo ponderar las tres variables?La respuesta ha venido al comprobar que las tres variables guardan entre sí unas correlaciones relativamente altas (en valor absoluto). Es decir: aportan una información bastante parecida sobre cada una de las empresas de la selección. Esto es importante porque, si en gran medida comparten información “redundante”, pueden ser, seguramente, “resumidas” en un solo indicador, cuyo valor o puntuación para cada caso (empresa) podría servir para establecer el ranking de compañías candidatas ser adquiridas.El método para obtener este indicador partir de las tres variables originales se llama Análisis de Componentes Principales (PCA), y puede ser fácilmente desarrollado con unas líneas de código de R. Ese indicador, que es la clave de toda la estrategia, será la primera “componente” del PCA, una combinación lineal de las tres variables originales; siempre y cuando su poder para “retener” la información global ofrecida por esas tres variables sea lo suficientemente alto.","code":""},{"path":"componentes-principales..html","id":"preparación-previa-de-datos.","chapter":"6 Componentes principales.","heading":"6.4  Preparación previa de datos.","text":"Vamos suponer que trabajamos dentro de un proyecto que hemos creado previamente, de nombre “componentes”. Dentro de la carpeta del proyecto guardaremos estos dos elementos:El script llamado “componentes_rstars.R”.El script llamado “componentes_rstars.R”.El archivo de Microsoft® Excel® llamado “interestelar_100.xlsx”. Si abrimos este último archivo, comprobaremos que se compone de tres hojas. La primera muestra un mensaje sobre el uso de los datos, la segunda recoge la descripción de las variables consideradas, y la tercera (hoja “Datos”) almacena los datos que debemos importar. Estos datos se corresponden con diferentes variables económicas y financieras de 104 empresas dedicadas los servicios de transporte interestelar de mercancías.El archivo de Microsoft® Excel® llamado “interestelar_100.xlsx”. Si abrimos este último archivo, comprobaremos que se compone de tres hojas. La primera muestra un mensaje sobre el uso de los datos, la segunda recoge la descripción de las variables consideradas, y la tercera (hoja “Datos”) almacena los datos que debemos importar. Estos datos se corresponden con diferentes variables económicas y financieras de 104 empresas dedicadas los servicios de transporte interestelar de mercancías.Comenzaremos ejecutar el código usual en cualquier script, esto es, limpiar el Global Environment, cargar los paquetes necesarios, importar los datos del archivo de Excel®, y tratar los casos con datos faltantes o missing values, y los casos que, para las variables estudiadas, se comportan como ouliers. En cuanto los primeros pasos, tenemos:Básicamente, en el código anterior se han almacenado los datos de la hoja de cálculo en el data frame “interestelar_100”. Luego, se ha creado un nuevo data frame más reducido con las únicas variables del análisis que vamos realizar (IDIVERSE, IFIDE e IDIG), al que hemos llamado “seleccion”. Estas tres variables han sido exploradas traves de una tabla gráfica partir de la función gt_plt_summary() del paquete gtExtras.El siguiente consistirá en localizar los posibles missing values, ya que para obtener componentes principales es necesario que todos los casos posean dato en todas las variables del análisis. Para tener una idea general, se puede utilizar la función vis_miss() del paquete visdat, que localizará gráficamente los missing values de las diferentes variables, y calculará el porcentaje de casos que supone, con respecto al total de observaciones:Del gráfico anterior se desprende que existen 2 missing values repartidos en 2 de las 3 variables del estudio. Para localizarlos, podemos filtrar nuestro data frame con las herramientas de dplyr:Los casos con datos faltantes son las empresas Ezra Bridger Haulage y Alderaan Freight.Ante la existencia de missing values, se puede actuar de varios modos. Por ejemplo, se puede intentar obtener el conjunto de valores que están disponibles por otro canal de información, o recurrir alguna estimación. En caso de que esto sea difícil, se puede optar, simplemente, por eliminar estos casos, en especial cuando representan un porcentaje muy reducido respecto al total de observaciones. En nuestro ejemplo, supondremos que hemos optado por esta última vía, y eliminaremos estos casos con el código:Verificamos en el Global Environment que el data frame “seleccion” ha pasado tener 102 casos.Por otro lado, la técnica de componentes principales es muy sensible la existencia de outliers. En concreto, las observaciones atípicas pueden afectar los resultados través de tres vías:Distorsión de las componentes principales: pueden influir en la dirección de las componentes principales, haciendo que estas se ajusten más los valores atípicos que la mayoría de los datos. Esto puede llevar una interpretación incorrecta de las relaciones entre las variables.Distorsión de las componentes principales: pueden influir en la dirección de las componentes principales, haciendo que estas se ajusten más los valores atípicos que la mayoría de los datos. Esto puede llevar una interpretación incorrecta de las relaciones entre las variables.Afectación las cargas de las variables: estas cargas son relevantes porque indican la importancia relativa de cada variable original en la formación de la componente, es decir, cuánto contribuye o se correlaciona una variable específica con una componente principal.Afectación las cargas de las variables: estas cargas son relevantes porque indican la importancia relativa de cada variable original en la formación de la componente, es decir, cuánto contribuye o se correlaciona una variable específica con una componente principal.Modificación de la varianza explicada: pueden inflar la varianza total de los datos, lo que puede afectar la proporción de varianza explicada por cada componente principal. Esto puede ocasionar una selección incorrecta del número de componentes retener.Modificación de la varianza explicada: pueden inflar la varianza total de los datos, lo que puede afectar la proporción de varianza explicada por cada componente principal. Esto puede ocasionar una selección incorrecta del número de componentes retener.En consecuencia, deberán ser identificados y, en su caso, eliminados. Para realizar este proceso, y dado que en nuestro análisis contamos con 4 variables, primero “resumiremos” el valor que toman dichas variables para cada observación (empresa), mediante el cálculo de la distancia de Mahalanobis. De hecho, las distancias de los diferentes casos se almacenarán en una nueva columna o variable de nuestro data frame, la que llamaremos MAHALANOBIS:Dentro de los argumentos de la función mahalanobis() incluida en la función mutate() hay unos puntos entre paréntesis. Recordemos que estos puntos deben ser añadidos cuando una función es la primera del operador “pipe” (%>%), para indicar que las variables de los paréntesis hacen referencia al data frame “seleccion” (o, en general, el objeto que fluye través del “pipe”).continuación, hemos construido un box-plot o diagrama de caja de la variable MAHALANOBIS, como si fuera cualquier otra variable, partir de la función ggplot() del paquete ggplot2:En el gráfico se aprecia que existen, por encima de la caja, varios outliers. Para identificarlos de modo concreto, hemos de calcular los cuartiles primero y tercero de la variable MAHALANOBIS y pasar el correspondiente filtro:Las compañías que se comportan como outliers, considerando conjuntamente las tres variables (través de la distancia de Mahalanobis), son 13. La eliminación de estos casos puede realizarse fácilmente con el código:Se ha creado un nuevo data frame llamado “seleccion_so” con los casos (89) que son outliers (y que contienen missing values), y se ha eliminado la variable MAHALANOBIS, puesto que su única utilidad era la de localizar y filtrar los outliers. Con este data frame “seleccion_so” es con el que se procederá al cálculo de las componentes.","code":"\n### Análisis de Componentes Principales ###\n\n# Limpiando el Global Environment\nrm(list = ls())\n\n# Cargando paquetes\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(visdat)\nlibrary(ggplot2)\nlibrary(gtExtras)\nlibrary (GGally)\nlibrary (knitr)\nlibrary (kableExtra)\nlibrary (patchwork)\n\n## DATOS\n\n# Importando datos desde Excel\ninterestelar_100 <- read_excel(\"interestelar_100.xlsx\",\n                               sheet = \"Datos\",\n                               na = c(\"n.d.\"))\ninterestelar_100 <- data.frame(interestelar_100, row.names = 1)\n\n# Seleccionando variables metricas para el analisis.\nseleccion <- interestelar_100 %>%\n  select(IDIVERSE, IFIDE, IDIG)\nseleccion_df_graph <- gt_plt_summary(seleccion)\nseleccion_df_graph\n# Localizando missing values.\nseleccion %>%\n  vis_miss() +\n  labs(title = \"Indicadores: Diversificación, Fidelidad, Digitalización\",\n       subtitle = \"Transporte de mercancías interestelar\",\n       y = \"Observación\",\n       fill = NULL) +\n  scale_fill_manual(\n    values = c(\"TRUE\" = \"red\", \"FALSE\" = \"grey\"),\n    labels = c(\"TRUE\" = \"NA\", \"FALSE\" = \"Presente\")) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14))\nseleccion %>% filter(is.na(IDIVERSE) |\n                     is.na(IFIDE) |\n                     is.na(IDIG)) %>%\n              select(IDIVERSE, IFIDE, IDIG)##                      IDIVERSE    IFIDE     IDIG\n## ezra bridger haulage       NA 34.11209 29.06697\n## alderaan freight     35.96067       NA 19.10291\nseleccion <- seleccion %>%\n             filter(! is.na(IDIVERSE) &\n                    ! is.na(IFIDE) &\n                    ! is.na(IDIG))\n# Identificando y descartando outliers con distancia de Mahalanobis.\nseleccion <- seleccion %>%\n  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),\n                                   center = colMeans(.),\n                                   cov    = cov(.)))\nggplot(data = seleccion, map = (aes(y = MAHALANOBIS))) +\n  geom_boxplot(fill = \"orange\") +\n  ggtitle(\"DISTANCIA DE MAHALANOBIS\",\n          subtitle = \"IDIVERSE, IFIDE, IDIG. Empresas TMI.\") +\n  ylab(\"MAHALANOBIS\")\nQ1M <- quantile (seleccion$MAHALANOBIS, c(0.25))\nQ3M <- quantile (seleccion$MAHALANOBIS, c(0.75))\n\nseleccion %>%\n  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |\n           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%\n  select(MAHALANOBIS, IDIVERSE, IFIDE, IDIG) ##                             MAHALANOBIS   IDIVERSE    IFIDE       IDIG\n## hyperion star haulage          5.212257 11.7150226 26.44375 13.7407440\n## jovian logistics               6.337391 22.3920052 29.77401 27.4342295\n## ripley interstellar freight   25.147548 71.3507415 40.12974  4.4464628\n## chakotay cargo systems        16.548312 54.3681496 64.26013 42.7197240\n## io star transport              7.380454 38.0206319 33.82858 35.1177708\n## hal intergalactic freight      7.174817  7.4887170 31.61145 28.1566284\n## andromeda cargo                6.445906 46.0541586 44.98949 44.3415811\n## kyber crystal shipping         8.476561 22.6273372 30.97667 34.0135033\n## tannhäuser freight             7.869429 47.8884591 42.64052  8.2092915\n## hyperdrive express            24.045602 35.7898130 68.14784 33.6619031\n## home one cargo                20.784784 22.1760155 46.77635 60.8325142\n## arrakis freight               34.687429 38.8813669 73.30230 58.5101128\n## kobol cargo systems            5.449331  0.7736944 39.46283  0.6007237\n# Creando nuevo df sin outliers.\nseleccion_so <- seleccion %>%\n  filter(MAHALANOBIS <= Q3M + 1.5*IQR(MAHALANOBIS) &\n           MAHALANOBIS >= Q1M - 1.5*IQR(MAHALANOBIS))  \n\n# Eliminando variable MAHALANOBIS de los df\nseleccion    <- seleccion    %>% select(-MAHALANOBIS)\nseleccion_so <- seleccion_so %>% select(-MAHALANOBIS)"},{"path":"componentes-principales..html","id":"cálculo-de-componentes.","chapter":"6 Componentes principales.","heading":"6.5  Cálculo de componentes.","text":"La condición previa para el cálculo de componentes es que las variables originales del análisis contengan información redundante, es decir, que en buena medida tengan una capacidad para diferenciar los casos (empresas) parecida. Esto se verifica con la existencia de altas correlaciones, en valor absoluto, entre ellas (al menos, entre algunas). Por tanto, hemos de calcular la matriz de correlaciones correspondiente. Un modo gráfico visualmente efectivo es utilizar las posibilidades que nos ofrece el paquete GGally, mediante la función ggpairs():Puede apreciarse cómo existen altas correlaciones (en valor absoluto) entre todas las variables. Por tanto, tiene sentido hacer un análisis de componentes principales, ya que hay variables que parecen compartir información.La obtención de las componentes se va realizar mediante la función prcomp() del paquete {stats}, que es un paquete cargado por defecto al abrir R. Es conveniente que activemos el argumento scale = con “T” (true) para que las variables originales sean consideradas en sus versiones tipificadas. Vamos asignar los resultados un objeto de nombre, por ejemplo, “componentes”. Por último, guardaremos el summary() o resumen de los resultados con un nombre provisional, por ejemplo, “temporal”. El código es el siguiente:La “Standard deviation” es la raíz cuadrada de los autovalores asociados cada componente. “Proportion Variance” nos dice la proporción de la suma de varianzas de las variables originales (comunalidad) recogida por cada componente, proporción que se acumula en “Cumulative Proportion”. Nótese que las componentes aparecen ordenadas de más menos importantes en función de la cantidad de varianza que capturan.En este caso, partir de la tabla anterior podemos destacar que la primera componente recoge más del 71% de la varianza (comunalidad) o información puesta en juego globalmente por las tres variables originales. Las dos primeras componentes, en conjunto, aglutinan casi el 89% de la información de las tres variables ofrecen sobre el comportamiento de las empresas. Entre las tres componentes, lógicamente se recoge el 100% de la comunalidad o varianza global.Si el elemento “importance” del summary() o resumen “temporal” lo convertimos en un data frame, por ejemplo “summary_df”, podremos presentar los resultados por medio de una tabla estéticamente más atractiva, partir de la función kable() del paquete knitr, y las funciones complementarias del paquete kableExtra:\nTable 6.1: Table 6.2: Resumen de Componentes\nLos coeficientes o cargas de cada componente se obtienen pidiendo nuestro objeto “componentes” el elemento “rotation”. Estas cargas las vamos guardar en un nuevo objeto que llamaremos, por ejemplo, “cargas”, que presentaremos mediante una pequeña tabla diseñada con la función kable() del paquete knitr y otras funciones del paquete kableExtra:\nTable 6.3: Table 6.4: Cargas de las componentes obtenidas\nEn la tabla, se muestran las cargas (loads), que son los coeficientes que intervienen en las combinaciones lineales que definen cada componente, partir de las variables originales (tipificadas). Por tanto, con base en las cargas se pueden explicitar las ecuaciones correspondientes cada componente. Por ejemplo, para la primera componente, la ecuación será:\\[\n\\text{CP}_{i1} = 0.6006 \\cdot \\text{IDIVERSE}_{i1} + 0.5442 \\cdot \\text{IFIDE}_{i1} + 0.5858 \\cdot \\text{IDIG}_{i1}\n\\]Puede observarse que, en cuanto la primera componente, que es la que especialmente nos interesa como indicador de la “preparación de las diferentes compañías de transporte para afrontar el futuro”, las 3 cargas tienen signo positivo, lo que implica que, cuanto mayores sean los valores de una empresa en las variables IDIVERSE (índice de diversificación), IFIDE (índice de fidelidad) e IDIG (índice de digitalización), mayor será el valor del indicador y, por tanto, su preparación. Además, como las variables fueron tipificadas, los valores de las cargas son comparables. De este modo, vemos cómo, dentro de la primera componente, que es la que adoptamos como indicador, la mayor importancia la tiene IDIVERSE, seguido de IDIG y, por último, IFIDE.","code":"\n# Correlaciones.\ncorr_plot_so <- ggpairs(seleccion_so, \n                        lower = list(continuous = wrap(\"cor\",\n                                                       size = 4.5,\n                                                       method = \"pearson\",\n                                                       stars = TRUE)),\n                        title = \"Matriz de Correlación (sin outliers).\")\ncorr_plot_so\n# Obtencion de componentes.\ncomponentes <- prcomp (seleccion_so, scale=T)\ntemporal <- summary (componentes)\ntemporal## Importance of components:\n##                           PC1    PC2    PC3\n## Standard deviation     1.4614 0.7261 0.5807\n## Proportion of Variance 0.7119 0.1757 0.1124\n## Cumulative Proportion  0.7119 0.8876 1.0000\n# Convertir el resumen en un data frame\nsummary_df <- as.data.frame(temporal$importance)\nsummary_df <- t(summary_df)  # Transponer para mejor visualización\nrm (temporal)\n\n# Crear la tabla con kable y personalizarla con kableExtra\nsummary_df %>%\n  kable(caption = \"Resumen de Componentes\",\n        col.names = c(\"Componente\", \n                      \"Desv. típica\",\n                      \"Proporción de varianza (comunalidad)\",\n                      \"Proporción de varianza (comunalidad) acumulada\"),\n        digits = c(2, 2, 2),\n        format.args = list(decimal.mark = \".\", scientific = FALSE)) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"bordered\", \"condensed\"),\n                full_width = F, \n                position = \"center\") %>%\n  row_spec(0, bold= T, align = \"c\") %>%\n  row_spec(1:(nrow(summary_df)), bold= F, align = \"c\") %>%\n  column_spec(1, bold = TRUE, extra_css = \"text-align: center;\")\n# Cargas de cada componente.\ncargas <- componentes$rotation\ncargas %>%\n  kable(caption = \"Cargas de las componentes obtenidas\",\n        digits = c(3, 3, 3),\n        format.args = list(decimal.mark = \".\", scientific = FALSE)) %>%\n  kable_styling(full_width = F,\n                bootstrap_options = c(\"striped\", \"bordered\", \"condensed\"),\n                position = \"center\") %>%\n  row_spec(0, bold= T, align = \"c\") %>%\n  row_spec(1:(nrow(cargas)), bold= F, align = \"c\") %>%\n  column_spec(1, bold = TRUE, extra_css = \"text-align: left;\")"},{"path":"componentes-principales..html","id":"retención-de-componentes.","chapter":"6 Componentes principales.","heading":"6.6  Retención de componentes.","text":"La etapa de retención de componentes consiste en decidir cuántas de las componentes generadas (recordemos que, en un principio, se calculan tantas componentes como variables originales) consideramos que resumen de un modo aceptable la información contenida en las variables originales. Estas componentes “retenidas” se convertirán en las componentes principales.La primera componente siempre es retenida y, por tanto, es una “componente principal”. El resto, que van capturando proporciones cada vez menores de la varianza común de las variables originales (comunalidad), podrán o retenerse; aunque, siempre, la retención de una componente implica que se han retenido todas las anteriores. En este caso práctico, buscamos un único indicador de la “preparación de las diferentes compañías de transporte para afrontar el futuro”, por lo que solo vamos “retener” la primera componente. En otras aplicaciones, podría ser necesario retener varias para recoger la suficiente comunalidad.Hay varios procedimientos o criterios para decidir cuántas componentes retener. Uno de ellos, comúnmente aplicado, es el de retener aquellas componentes cuyo autovalor es mayor que 1 (suponiendo que se ha trabajado con las variables en sus versiones tipificadas). Los autovalores son el cuadrado de los elementos “Desviación típica” (valores “Standard deviation” (sdev) del objeto “componentes” que hemos generado partir de la función prcomp()).Hemos creado un data frame con estos autovalores calculados (y su orden de importancia, al que hemos llamado variable o columna “orden”, y que es un vector de números enteros consecutivos que va desde uno hasta número de variables originales o de componentes) y los hemos dispuesto en un gráfico de barras:Respecto al gráfico, conviene recordar que, al ser de barras, si se quieren representar las frecuencias sino los valores que toma una variable (en este caso, “autovalor”) para cada valor de la otra variable (en este caso, “orden”); en el geom_bar() habrá que añadir el argumento stat = con el valor “identity”. Además, se utiliza el elemento scale_x_continuous() para pesonalizar la escala del eje x, y que se divida dicho eje en tantos tramos como componentes hay.En el gráfico obtenido, las componentes cuyas barras atraviesan la línea que pasa por el valor “1” son las que deberían ser retenidas, ya que son las componentes que resumen de modo suficiente la información ofrecida sobre el comportamiento de los casos por las variables originales. En este caso, basta con retener solo la primera componente (por lo tanto, solo contamos con una componente principal). Este es un resultado favorable cara nuestro propósito, ya que nos indica que podemos usar como indicador solo una componente, la primera, y con ella resumiremos suficientemente las tres variables originales. Por tanto, es un buen indicador, en ese sentido.Un gráfico complementario útil es el que muestra, para cada componente, el porcentaje de varianza total (comunalidad) acumulada al ir reteniendo las sucesivas componentes, un resultado que ya se obtuvo anteriormente en forma de tabla:Para obtener el gráfico anterior, se comienza añadiendo al data frame “autovalores” una columna o variable que es la suma acumulada del porcentaje de comunalidad recogido por las sucesivas componentes, que están ordenadas de mayor menor autovalor. Para calcular el porcentaje, se usa la función cumsum(), y se tiene en cuenta que, como las variables fueron tipificadas para calcular las componentes, la comunalidad, que coincide con la suma de las varianzas de las componentes (autovalores), es igual al número de variables o componentes (valor que toma la función nrow()).Después, se ha creado un vector que contiene tantos elementos como variables o componentes hay en el análisis (vector “checkcp”). Con la función condicional ifelse() se consigue que los elementos de “checkcp” sean “CP” o “NCP” según los correspondientes autovalores sean mayores o que 1. Finalmente, según sea el valor de cada elemento de “checkcp”, las barras del gráfico se colorearán de uno u otro modo.Posteriormente, mediante el paquete patchwork, se han unido los dos gráficos creados en esta fase, poniendo uno debajo del otro:Un vez confirmado el hecho de que la primera componente es suficiente para contar con un buen indicador de “preparación de las diferentes compañías de transporte para afrontar el futuro”, considerando los aspectos de digitalización, diversificación y fidelización de clientes, pasaremos estudiar qué casos concretos ofrecen mejores (mayores) valores en el indicador, para lo cual hemos de calcular sus puntuaciones.","code":"\n# Determinacion Componentes a retener.\n# Criterio del Autovalor mayor que 1.\norden <- c(1:ncol(seleccion_so))\nautovalor <- componentes$sdev^2\nautovalores <- data.frame(orden, autovalor)\n\nautograph <- ggplot(data = autovalores, map = (aes(x = orden,\n                                                   y = autovalor))) +\n             geom_bar(stat = \"identity\",\n                      colour = \"red\",\n                      fill = \"orange\",\n                      alpha = 0.7) +\n             scale_x_continuous(breaks=c(1:nrow(autovalores)))+\n             geom_hline(yintercept = 1,\n                        colour = \"dark blue\") +\n             geom_text(aes(label = round(autovalor,2)),\n                       vjust = 1,\n                       colour = \"dark blue\",\n                       size = 3) +\n             ggtitle(\"AUTOVALORES DE LAS COMPONENTES\",\n                     subtitle = \"Empresas TMI\") +\n             xlab (\"Número de componente\") +\n             ylab(\"Autovalor\")\n\nautograph\n# Comunalidad acumulada.\nautovalores <- autovalores %>%\n  mutate(variacum = 100*(cumsum((autovalor/nrow(autovalores)))))\ncheckcp <- ifelse(autovalores$autovalor >= 1, \"CP\", \"NCP\")\ncheckcp## [1] \"CP\"  \"NCP\" \"NCP\"\nvacumgraph <- ggplot(data = autovalores, map = (aes(x = orden,\n                                                    y = variacum))) +\n              geom_bar(stat = \"identity\",\n                       aes(fill = checkcp),\n                       colour = \"red\",\n                       alpha = 0.7) +\n              scale_x_continuous(breaks=c(1:nrow(autovalores)))+\n              geom_text(aes(label = round(variacum,2)),\n                        vjust = 1,\n                        colour = \"dark blue\",\n                        size = 3) +\n              ggtitle(\"COMUNALIDAD ACUMULADA POR COMPONENTES\",\n                      subtitle = \"Empresas TMI\") +\n              xlab (\"Número de componente\") +\n              ylab(\"Varianza acumulada\")\nvacumgraph\ncombinado <- autograph / vacumgraph\ncombinado <- combinado + \n  plot_annotation(\n    title = \"Retención de componentes (Autovalor >1)\",\n    subtitle = \"Empresas TMI (sin outliers)\",\n    theme = theme(\n      # TÍTULO de la composición\n      plot.title = element_text(\n        size = 16,          # tamaño\n        face = \"bold\",      # negrita\n      ),\n      # SUBTÍTULO de la composición\n      plot.subtitle = element_text(\n        size = 12\n      )))\ncombinado"},{"path":"componentes-principales..html","id":"puntuaciones-de-los-casos-scores.","chapter":"6 Componentes principales.","heading":"6.7  Puntuaciones de los casos (scores).","text":"Para obtener las puntuaciones de cada caso (empresa) en el indicador de “preparación de las diferentes compañías de transporte para afrontar el futuro” (y que es nuestra componente principal, que su vez coincide con la primera componente), simplemente debemos tener en cuenta que tales puntuaciones están guardadas en la matriz “x” del objeto prcomp() creado. Vamos renombrar las primera columna (componente) de esta matriz como “scores” y vamos recolocar las filas (empresas) de mayor menor valor de la puntuación (lo que se consigue mediante la función arrange() del paquete dplyr. Finalmente, mostraremos en una tabla el ranking de las 10 mejores empresas (según sus puntuaciones en el indicador), “cortando” el data frame con la función slice():\nTable 6.5: Table 6.6: Puntuaciones emporesas TMI (Top-10, sin outliers)\nDe la tabla y gráfico anteriores podemos concluir que las empresas más preparadas para afrontar el futuro, según nuestro indicador (primera componente del análisis PCA) son, por este orden, Shuttlepod Movers, Kamino Movers e Ícarus Star Transport.¿Seguro?Este ranking se ha elaborado partir de las compañías que formaron la muestra para realizar el análisis de componentes principales. Estas empresas eran aquellas que tenían dato en las tres variables originales y que habían sido calificadas como outliers. Los outliers se apartaron de la muestra para evitar distorsiones y sesgos en el análisis. Pero una cuestión que podríamos plantearnos es si, una vez calculadas las combinaciones lineales que son las componentes sin su influencia, podrían ser ahora, en virtud de esas componentes calculadas, puntuadas. Que una compañía se comporte como outlier en una o varias de las variables originales, y se cuente con ella la hora de calcular las componentes; quiere decir necesariamente que sea una buena candidata ser la elegida para ser adquirida por su preparación para afrontar el futuro (incluso podría concluirse que en algún aspecto está “especialmente preparada”).\nTable 6.7: Table 6.8: Puntuaciones emporesas TMI (Top-10, con outliers)\nVemos como el ranking cambia radicalmente, y pasa estar encabezado por las compañías Arrakis Freight, Chakotay Cargo Systems e Hyperdrive Express. Si un análisis particularizado de estas empresas por parte del equipo encargado del estudio da como resultado que sus datos son correctos, serían, seguramente, las candidatas para ser adquiridas por el magnate Arg-Us Korp.","code":"\n## Puntuaciones o Scores\nscores <- componentes$x[,1]  #tantas columnas como componentes retenidas\nscores_df <- as.data.frame(scores)\nscores_df <- cbind(scores_df,seleccion_so)\nscores_top10 <- scores_df %>%\n  arrange(desc(scores)) %>%\n  slice(1:10)\n\nscores_top10 %>%\n  kable(caption = \"Puntuaciones emporesas TMI (Top-10, sin outliers)\",\n        col.names = c(\"Empresa\",\n                      \"Puntuación\",\n                      \"I. Diversif.\",\n                      \"I. Fidelizac.\",\n                      \"I. Digitalizac.\"),\n        digits = c(3, 3, 3, 3),\n        format.args = list(decimal.mark = \".\",\n                           scientific = FALSE)) %>%\n  kable_styling(full_width = F,\n                bootstrap_options = \"striped\",\n                                    \"bordered\",\n                                    \"condensed\",\n                position = \"center\",\n                font_size = 12) %>%\n  row_spec(0, bold= T, align = \"c\") %>%\n  row_spec(1:(nrow(scores_top10)),\n           bold= F,\n           align = \"c\") %>%\n  column_spec(1, bold = TRUE,\n              extra_css = \"text-align: left;\")\n# Scores puntuando outliers\nscores_all <- predict(componentes, newdata = seleccion)\nscores_all <- scores_all[,1]\nscores_all_df <- as.data.frame(scores_all)\nscores_all_df <- cbind(scores_all_df,seleccion)\nscores_all_top10 <- scores_all_df %>%\n  arrange(desc(scores_all)) %>%\n  slice(1:10)\n\nscores_all_top10 %>%\n  kable(caption = \"Puntuaciones emporesas TMI (Top-10, con outliers)\",\n        col.names = c(\"Empresa\",\n                      \"Puntuación\",\n                      \"I. Diversif.\",\n                      \"I. Fidelizac.\",\n                      \"I. Digitalizac.\"),\n        digits = c(3, 3, 3, 3),\n        format.args = list(decimal.mark = \".\",\n                           scientific = FALSE)) %>%\n  kable_styling(full_width = F,\n                bootstrap_options = \"striped\",\n                \"bordered\",\n                \"condensed\",\n                position = \"center\",\n                font_size = 12) %>%\n  row_spec(0, bold= T, align = \"c\") %>%\n  row_spec(1:(nrow(scores_all_top10)),\n           bold= F,\n           align = \"c\") %>%\n  column_spec(1, bold = TRUE,\n              extra_css = \"text-align: left;\")"},{"path":"componentes-principales..html","id":"puntuaciones-de-los-casos-scores.-1","chapter":"6 Componentes principales.","heading":"6.8  Puntuaciones de los casos (scores).","text":"Cabe hacerse una pregunta antes de cerrar esta historia. ¿Qué hubiera ocurrido si se hubieran calculado las componentes sin eliminar los outliers previamente? ¿Hubieran cambiado los resultados? ¿era tan relevante el efecto de estos casos sobre los resultados, como comentamos al comienzo del tema, en la parte teórica?En R, el código es fácil de adaptar: basta con sustituir el data frame “seleccion_so” por el data frame “seleccion”, desde el apartado del cálculo de componentes hasta la formación del primer ranking basado en las puntuaciones de las compañías en la primera componente.El ranking obtenido de este modo es el siguiente:\nTable 6.9: Table 6.10: Puntuaciones emporesas TMI (Top-10, outliers en cálculo)\nComo puede repararse, los cambios, en este caso, parecen ser relevantes. Solo partir de la séptima posición se registran cambios en el orden de las empresas.obstante, hay una tercera vía para proceder al análisis de componentes principales que nos libera de la necesidad de tener que decidir si es conveniente incluir en el cálculo los outliers o : la utilización de técnicas robustas.","code":""},{"path":"componentes-principales..html","id":"utilización-de-técnicas-robustas-método-de-hubert.","chapter":"6 Componentes principales.","heading":"6.9  Utilización de técnicas robustas: método de Hubert.","text":"La presencia de outliers, según comentamos, puede distorsionar los resultados del análisis. Esto se debe, en esencia, los efectos que estos elementos tienen en el cálculo de la matriz de varianzas-covarianzas de las variables originales, básica en el cálculo de componentes. Este efecto se manifiesta, por ejemplo, en un incremento artificial de las varianzas, debido la mayor dispersión inducida por los outliers.En la literatura se han desarrollado métodos de cálculo de esta matriz de varianzas-covarianzas que minimizan este efecto distorsionador de los outliers. Son los métodos robustos. Este es el caso de la variante del análisis de componentes principales de Hubert, que emplea estimadores robustos de la matriz de varianzas y covarianzas tales como MCD o S-estimators.El siguiente código permite obtener las componentes por Hubert, y realizar el ranking de empresas de acuerdo las puntuaciones de la primera componente. Hay que tener en cuenta que, en esta ocasión, las cargas de la primera componente son negativas, luego las mejores compañías en términos de diversificación de operaciones, fidelización de clientes y digitalización serán aquellas que presenten menores puntuaciones:\nTable 6.11: Table 6.12: Puntuaciones emporesas TMI (Top-10, Hubert)\nEn la tabla se comprueba que existen leves variaciones con respecto las clasificaciones anteriores. destacar la inclusión en el top-10 de Ripley Interestellar Freight, y la desparición de Kamino Movers.¿cuál de los 3 ránkings elegirías tú?","code":"\n# --- Obtención de componentes (ROBPCA de Hubert) ---\n# \"seleccion\" debe ser un data.frame/matriz numérica (obs x vars)\nlibrary(rrcov)\n\ncomponentes3 <- PcaHubert(\n  x        = seleccion,\n  k        = ncol(seleccion),        # 0 => deja que el método elija nº de comp.\n  scale    = TRUE,     # estandariza como en prcomp(scale=TRUE)\n  mcd      = TRUE,     # fase inicial robusta\n)\n\nsummary(componentes3)\n\n# --- Cargas de cada componente ---\ncargas3 <- as.data.frame(unclass(componentes3@loadings))\ncargas3    # Atención! Las cargas de CP1 son negativas: el ranking debe ascender\n\n# --- Puntuaciones (Scores) ---\n# Si quieres la 1ª componente (equivalente a componentes2$x[,1]):\nscores3 <- as.numeric(componentes3@scores[, 1])\n# (Si quieres todas: scores3_all <- as.data.frame(componentes3@scores))\n\nscores3_df <- data.frame(scores3 = scores3) %>%\n  cbind(seleccion)\n\n# --- Top-10 por la 1ª componente (orden descendente, como en tu código original) ---\nscores3_top10 <- scores3_df %>%\n  arrange(scores3_df) %>%      # orden ascendente porque las cargas de PC1 son negativas (mejor empresa => menor puntuación)\n  slice(1:10)\n\nscores3_top10 %>%\n  kable(caption = \"Puntuaciones emporesas TMI (Top-10, Hubert)\",\n        col.names = c(\"Empresa\",\n                      \"Puntuación\",\n                      \"I. Diversif.\",\n                      \"I. Fidelizac.\",\n                      \"I. Digitalizac.\"),\n        digits = c(3, 3, 3, 3),\n        format.args = list(decimal.mark = \".\",\n                           scientific = FALSE)) %>%\n  kable_styling(full_width = F,\n                bootstrap_options = \"striped\",\n                \"bordered\",\n                \"condensed\",\n                position = \"center\",\n                font_size = 12) %>%\n  row_spec(0, bold= T, align = \"c\") %>%\n  row_spec(1:(nrow(scores3_top10)),\n           bold= F,\n           align = \"c\") %>%\n  column_spec(1, bold = TRUE,\n              extra_css = \"text-align: left;\")## \n## Call:\n## PcaHubert(x = seleccion, k = ncol(seleccion), mcd = TRUE, scale = TRUE)\n## Importance of components:\n##                           PC1     PC2    PC3\n## Standard deviation     1.2003 0.30914 0.2684\n## Proportion of Variance 0.8958 0.05942 0.0448\n## Cumulative Proportion  0.8958 0.95520 1.0000##                 PC1         PC2        PC3\n## IDIVERSE -0.7929785  0.07582335 -0.6045129\n## IFIDE    -0.2189863  0.89044224  0.3989456\n## IDIG     -0.5685332 -0.44873537  0.6894973"},{"path":"componentes-principales..html","id":"materiales-para-realizar-las-prácticas-del-capítulo.-4","chapter":"6 Componentes principales.","heading":"6.10  Materiales para realizar las prácticas del capítulo.","text":"En esta sección se muestran los links de acceso los diferentes materiales (scripts, datos…) necesarios para llevar cabo los contenidos prácticos del capítulo.Datos (en formato Microsoft (R) Excel (R)):interestelar_100.xlsx (obtener aquí)Scripts:componentes_rstars.R (obtener aquí)","code":""},{"path":"análisis-clúster..html","id":"análisis-clúster.","chapter":"7 Análisis Clúster.","heading":"7 Análisis Clúster.","text":"","code":""},{"path":"análisis-clúster..html","id":"introducción.-3","chapter":"7 Análisis Clúster.","heading":"7.1  Introducción.","text":"El análisis de conglomerados —o análisis de clústeres (AC)— agrupa casos con características similares en función de un conjunto de variables clasificadoras. El objetivo es que:Los casos de un mismo clúster sean lo más homogéneos posible entre sí.Los clústeres entre sí sean lo más heterogéneos posible, de acuerdo con las variables consideradas.En general, el proceso de determinación de los grupos, conglomerados o clústeres de casos es el siguiente:Se parte de un conjunto de n casos, y para cada uno de ellos se cuenta con el valor de m variables clasificadoras.Se establece una medida de distancia que cuantifica lo que dos casos se parecen, considerando en conjunto los valores que poseen para las variables clasificadoras.Se crean los grupos, conglomerados o clústeres con los casos que poseen entre sí una menor distancia. Existen dos enfoques principales la hora de crear los grupos de casos partir de las distancias observadas entre los casos: los métodos jerárquicos y los métodos -jerárquicos.Finalmente, se caracterizan los grupos, conglomerados o clústeres obtenidos, y se comparan unos con otros para extraer conclusiones.En lo que respecta la medida de distancia entre los casos, la medida más habitual es la distancia euclídea. Así, la distancia euclídea entre dos caso, e ’, para las m variables clasificadoras x, será:\\[\nd(, ') = \\sqrt{\\sum_{j=1}^{m} (x_{ij} - x_{'j})^2}\n\\] Esta distancia es muy sensible la escala de las variables clasificadoras. Para evitar este inconveniente, se trabaja con las variables previamente tipificadas.","code":""},{"path":"análisis-clúster..html","id":"métodos-de-agrupación-jerárquicos.","chapter":"7 Análisis Clúster.","heading":"7.2  Métodos de agrupación jerárquicos.","text":"Como se acaba de comentar, existen dos enfoques fundamentales de realizar el análisis clúster, dependiendo de cómo son los métodos de agrupación de los casos (y grupos de casos): el enfoque de los métodos jerárquicos, y el enfoque que reúne los métodos -jerárquicos.Ambos enfoques tienen sus ventajas e inconvenientes, y pueden adaptarse mejor cada problema concreto. Es importante seleccionar un buen método de agrupación, puesto que pueden proporcionar soluciones muy diferentes entre sí.En los métodos jerárquicos, se van formando sucesivamente grupos como agrupación de otros grupos precedentes, hasta llegar un único grupo que recoge todos los individuos; tomando el proceso una estructura piramidal (también existen métodos jerárquicos descendientes, que parten de un único grupo que contiene todos los casos, para acabar el n grupos de un solo caso, aunque son menos frecuentes).Estos métodos suelen aplicarse cuando hay un número reducido de casos. También, cuando nuestro objetivo pasa por crear grupos que recojan todos los casos, más que definir simplemente tipologías más o menos homogéneas de casos (lo que se obtiene caracterizando los grupos obtenidos). Es decir, cuando se incluyen en el análisis todos los individuos, incluidos los outliers. De hecho, estos métodos pueden emplearse, de por sí, como técnicas de localización de outliers. Por último, también se suelen emplearse cuando se desconoce priori el número de grupos, conglomerados o clústeres formar.Entre los métodos jerárquicos de agrupación más extendidos, figuran los siguientes:Método del vecino más cercano (single linkage): la distancia que se considera entre grupos es la distancia entre sus elementos más próximos.Método del vecino más cercano (single linkage): la distancia que se considera entre grupos es la distancia entre sus elementos más próximos.Método del vecino más lejano (complete linkage): la distancia que se considera entre grupos es la distancia entre sus elementos más lejanos.Método del vecino más lejano (complete linkage): la distancia que se considera entre grupos es la distancia entre sus elementos más lejanos.Método de Ward (Ward method): se unen los grupos que dan lugar otro grupo cuyos casos tienen una menor suma de los cuadrados de sus distancias respecto al centro de dicho grupo (menor varianza intra-clúster).Método de Ward (Ward method): se unen los grupos que dan lugar otro grupo cuyos casos tienen una menor suma de los cuadrados de sus distancias respecto al centro de dicho grupo (menor varianza intra-clúster).Otros métodos: vinculación intergrupos (average linkage groups), vinculación intragrupos (whithin-group)…Otros métodos: vinculación intergrupos (average linkage groups), vinculación intragrupos (whithin-group)…De entre ellos, ¿cuál elegir?La cuestión es fácil de resolver, y tiene por qué tener una única respuesta. Por otro lado, cada método proporciona soluciones que pueden variar mucho entre sí. Una estrategia puede pasar por probar con varios métodos y se seleccionar la solución que parezca más coherente desde el punto de vista teórico, y estable desde el punto de vista empírico.En la práctica, uno de los métodos más utilizados es el método de Ward, porque proporciona grupos muy homogéneos, ya que se basa en la minimización de la varianza o dispersión de los elementos que componen cada grupo con respecto su centro de gravedad o centroide. Precisamente, este método será aplicado en el ejemplo práctico que desarrollaremos en R continuación.","code":""},{"path":"análisis-clúster..html","id":"el-informe-bluebird.","chapter":"7 Análisis Clúster.","heading":"7.2.1  El Informe Bluebird.","text":"La Agencia Interplanetaria de Transporte de Mercancías es un organismo dedicado estudiar el funcionamiento del sector. Entre sus actividades, hay una consistente en la selección de un grupo de empresas para su segmentación en términos de fidelidad de los clientes (IFIDE), diversificación del negocio (IDIVERSE), y digitalización de la compañía (IDIG). En esta ocasión, se ha elegido un panel de 25 empresas o compañías. La investigación corre cargo de una de las investigadoras de la agencia, la doctora Xelia Bluebird (en la imagen), por lo que al informe que contiene los resultados de la segmentación se le denomina Informe Bluebird.En esta edición del informe, y tras la reciente compra de la compañía Home One Cargo por parte del magnate Arg-us Korp, la doctora Bluebird está especialmente interesada en cómo queda encuadrada dicha empresa.","code":""},{"path":"análisis-clúster..html","id":"preparación-de-los-datos.","chapter":"7 Análisis Clúster.","heading":"7.2.2  Preparación de los datos.","text":"Dado que son pocos los casos (empresas) segmentar, vamos utilizar un método jerárquico de agrupación de casos. En concreto, utilizaremos el método de Ward.Vamos suponer que trabajamos dentro de un proyecto que hemos creado previamente, de nombre “cluster”. Dentro de la carpeta del proyecto guardaremos estos dos elementos:El script llamado “cluster_rstars.R”.El script llamado “cluster_rstars.R”.El archivo de Microsoft® Excel® llamado “interestelar_25.xlsx”. Si abrimos este último archivo, comprobaremos que se compone de tres hojas. La primera muestra un mensaje sobre el uso de los datos, la segunda recoge la descripción de las variables consideradas, y la tercera (hoja “Datos”) almacena los datos que debemos importar. Estos datos se corresponden con diferentes variables económicas y financieras de 25 empresas dedicadas los servicios de transporte interestelar de mercancías.El archivo de Microsoft® Excel® llamado “interestelar_25.xlsx”. Si abrimos este último archivo, comprobaremos que se compone de tres hojas. La primera muestra un mensaje sobre el uso de los datos, la segunda recoge la descripción de las variables consideradas, y la tercera (hoja “Datos”) almacena los datos que debemos importar. Estos datos se corresponden con diferentes variables económicas y financieras de 25 empresas dedicadas los servicios de transporte interestelar de mercancías.Comenzaremos ejecutar el código del script. En primer lugar, el código se ocupa de limpiar el Global Environment y cargar los paquetes necesarios:La siguiente sección de código es algo especial. Se trata de una función, en la que se entrará en detalle, dada la complejidad de su código. Basta decir que el input de la función es una lista de gráficos generados con el paquete ggplot2. El output será una serie de composiciones de dimensión 4X4 realizada con los gráficos de la lista de modo automático, dejando los huecos en blanco necesarios en caso de que el número de gráficos sea múltiplo de 4. La función se denomina create_patchwork().lo largo del script se llamará dos veces esta función, lo que ahorrará una buena cantidad de código. Se ha ubicado al comienzo del script para asegurar su ejecución previa sus llamadas, aunque, en realidad, lo más adecuado sería integrar la función en un paquete, instalarlo y activarlo, y así evitar alargar el script con su código (se deja esta opción para usuarios avanzados).Posteriormente importaremos los datos del archivo de Excel®, y trataremos los casos con datos faltantes o missing values:se ha localizado ninguna observación con missing values, luego se ha de realizar ningún tipo de tratamiento.El siguiente paso es la identificación de outliers. Para realizar este proceso, y dado que en nuestro análisis contamos con 3 variables, primero “resumiremos” el valor que toman dichas variables para cada caso, mediante el cálculo de la distancia de Mahalanobis. De hecho, las distancias de los diferentes casos se almacenarán en una nueva variable, la que llamaremos MAHALANOBIS, que se incorporará al data frame “originales” por medio de la función mutate() de dplyr, y la función mahalanobis(). Recordemos que, en los diferentes argumentos de esta función, el punto “.” hace referencia al data frame que está delante del operador pipe (%>%).Una vez creada la variable MAHALANOBIS, se estudia la existencia en sus valores de outliers mediante la construcción de un diagrama de caja o boxplot:En el gráfico se observa que existen, por encima de la caja, 4 outliers. Para identificarlos de modo concreto, hemos de calcular los cuartiles primero y tercero de la variable MAHALANOBIS y pasar el correspondiente filtro:De la tabla anterior se desprende que los outliers identificados son las empresas Chakotay Cargo Systems, Home One Cargo, y de modo más moderado, Tannhäuser Freight e Hyperion Star Haulage. Estos son, en definitiva, los 4 casos en los que nos fijaremos específicamente más adelante, al analizar los resultados del análisis clúster.Estas empresas cuentan con un valor atípico en la variable MAHALANOBIS, lo que, su vez, implica que muestren valores atípicos en una o varias de las variables originales (IDIVERSE, IFIDE, IDIG). En el desarrollo de otras técnicas, en este punto localizaríamos y eliminaríamos los outliers. En este caso lo vamos hacer, ya que queremos agrupar todos los casos que tenemos en el análisis. Precisamente, si hay algún caso que permanece aislado, sin agruparse con otros en el proceso de agrupación hasta las últimas etapas, quizá se trate de un candidato outlier, por lo que el análisis clúster también puede considerarse una técnica de localización de casos atípicos.Por último, borramos la variable MAHALANOBIS del data frame “seleccion”, puesto que ya ha cumplido la función de localizar los casos atípicos:La siguiente etapa se refiere la aplicación propia del análisis clúster al grupo de 25 compañías que toman valores para las tres variables incluidas en el análisis.","code":"\n### CLUSTER jerárquico 25 empresas TMI.###\n\n# Limpiando el Global Environment\nrm(list = ls())\n\n# Cargando paquetes\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gtExtras)\nlibrary(visdat)\nlibrary (factoextra)\nlibrary (knitr)\nlibrary (kableExtra)\nlibrary (patchwork)\n##### Función para crear composiciones de gráficos con patchwork ###############\ncreate_patchwork <- function(plot_list) {\n  n <- length(plot_list)\n  if (n == 0) return(NULL)\n  full_rows <- n %/% 4\n  remaining <- n %% 4\n  patchworks <- list()\n  \n  if (full_rows > 0) {\n    for (i in seq(1, full_rows * 4, by = 4)) {\n      patchworks <- c(patchworks, list((plot_list[[i]] + plot_list[[i+1]]) / \n                                         (plot_list[[i+2]] + plot_list[[i+3]])))\n    }\n  }\n  \n  if (remaining > 0) {\n   last_plots <- plot_list[(full_rows * 4 + 1):n]\n   empty_plots <- lapply(1:(4 - remaining), function(x) ggplot() + theme_void())\n   last_patchwork <- do.call(patchwork::wrap_plots, c(last_plots, empty_plots))\n   patchworks <- c(patchworks, list(last_patchwork))\n  }\n  return(patchworks)\n}\n################################################################################\n## DATOS\n\n# Importando datos desde Excel\ninterestelar_25 <- read_excel(\"interestelar_25.xlsx\",\n                               sheet = \"Datos\",\n                               na = c(\"n.d.\"))\ninterestelar_25 <- data.frame(interestelar_25, row.names = 1)\n\n# Seleccionando variables metricas para el analisis.\nseleccion <- interestelar_25 %>%\n  select(IDIVERSE, IFIDE, IDIG)\nseleccion_df_graph <- gt_plt_summary(seleccion)\nseleccion_df_graph\n# Localizando missing values.\nseleccion %>%\n  vis_miss() +\n  labs(title = \"Indicadores: Diversificación, Fidelidad, Digitalización\",\n       subtitle = \"Transporte de mercancías interestelar\",\n       y = \"Observación\",\n       fill = NULL) +\n  scale_fill_manual(\n    values = c(\"TRUE\" = \"red\", \"FALSE\" = \"grey\"),\n    labels = c(\"TRUE\" = \"NA\", \"FALSE\" = \"Presente\")) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14))\nseleccion %>% filter(is.na(IDIVERSE) |\n                       is.na(IFIDE) |\n                       is.na(IDIG)) %>%\n  select(IDIVERSE, IFIDE, IDIG)## [1] IDIVERSE IFIDE    IDIG    \n## <0 rows> (o 0- extensión row.names)\nseleccion <- seleccion %>%\n  filter(! is.na(IDIVERSE) &\n           ! is.na(IFIDE) &\n           ! is.na(IDIG)) \n# Identificando y descartando outliers con distancia de Mahalanobis.\nseleccion <- seleccion %>%\n  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),\n                                   center = colMeans(.),\n                                   cov    = cov(.)))\n# Identificando outliers con distancia de Mahalanobis.\nseleccion <- seleccion %>%\n  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),\n                                   center = colMeans(.),\n                                   cov    = cov(.)))\n\nggplot(data = seleccion, map = (aes(y = MAHALANOBIS))) +\n  geom_boxplot(fill = \"orange\") +\n  ggtitle(\"DISTANCIA DE MAHALANOBIS\",\n          subtitle = \"IDIVERSE, IFIDE, IDIG. Empresas TMI.\") +\n  ylab(\"MAHALANOBIS\")\nQ1M <- quantile (seleccion$MAHALANOBIS, c(0.25))\nQ3M <- quantile (seleccion$MAHALANOBIS, c(0.75))\n\nseleccion %>%\n  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |\n           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%\n  select(MAHALANOBIS, IDIVERSE, IFIDE, IDIG)##                        MAHALANOBIS IDIVERSE    IFIDE      IDIG\n## hyperion star haulage      8.27971 11.71502 26.44375 13.740744\n## chakotay cargo systems    15.65839 54.36815 64.26013 42.719724\n## tannhäuser freight        11.91949 47.88846 42.64052  8.209291\n## home one cargo            15.25357 22.17602 46.77635 60.832514\n# Eliminando variable MAHALANOBIS del df\nseleccion    <- seleccion    %>% select(-MAHALANOBIS)"},{"path":"análisis-clúster..html","id":"aplicación-del-método-de-ward.","chapter":"7 Análisis Clúster.","heading":"7.2.3  Aplicación del método de Ward.","text":"Los métodos de agrupación usualmente se basan en la distancia euclídea. Como la distancia euclídea es sensible las unidades de medida de las diferentes variables clasificadoras, es preciso trabajar con las variables tipificadas, lo que lograremos creando, por ejemplo, un data frame “zseleccion” con la función scale(). Luego, aplicaremos el método elegido este data frame, en lugar de al data frame que contiene los datos originales sin tipificar:Este nuevo data frame contiene las mismas variables del análisis; pero tipificadas (obsérvese, en el summary(), las medias de las variables).Previamente aplicar un método de agrupación concreto, es necesario calcular la matriz de distancias entre los casos, la que llamaremos, por ejemplo, “d”. Esta matriz se calcula con la función dist(). Para visualizarla, una opción es representarla mediante el gráfico de temperatura que ofrece la función fviz_dist() del paquete factoextra:Los casos con intersecciones en tonos anaranjados tenderán agruparse con mayor facilidad (o agruparse antes); mientras que los casos cuyas intersecciones están en tonos azulados tenderán pertenecer grupos diferentes (o agruparse más tarde). Cabe destacar los colores azulados asociados las empresas Chakotay Cargo Systems, Home One Cargo, que precisamente eran las dos compañías identificadas más claramente como outliers.Vamos realizar el análisis clúster jerárquico mediante uno de los métodos más habituales, el de Ward, como es común en las aplicaciones prácticas, ya que este método proporciona grupos muy homogéneos (mínima varianza). La función utilizar es hclust(). La solución la guardaremos en el objeto (lista) que hemos llamado, por ejemplo, “cluster_j”. Luego se visualizará el dendograma construido con la función fviz_dend() del paquete factoextra, que permite personalizar el gráfico con una gramática similar la utilizada con los gráficos del paquete ggplot2:En el código anterior:cluster_j: Es el objeto que contiene el dendrograma que se desea visualizar.cluster_j: Es el objeto que contiene el dendrograma que se desea visualizar.cex = 0.6: Este argumento ajusta el tamaño del texto de las etiquetas en el dendrograma. Un valor de 0.6 significa que el texto será más pequeño que el tamaño predeterminado.cex = 0.6: Este argumento ajusta el tamaño del texto de las etiquetas en el dendrograma. Un valor de 0.6 significa que el texto será más pequeño que el tamaño predeterminado.rect = FALSE: Este argumento indica si se deben dibujar rectángulos alrededor de los clústeres en el dendrograma. FALSE significa que se dibujarán rectángulos.rect = FALSE: Este argumento indica si se deben dibujar rectángulos alrededor de los clústeres en el dendrograma. FALSE significa que se dibujarán rectángulos.labels_track_height = 5.5: Este argumento ajusta la altura de la pista de etiquetas, que es el espacio reservado para las etiquetas de los objetos en el dendrograma. Un valor de 5.5 proporciona más espacio para las etiquetas.labels_track_height = 5.5: Este argumento ajusta la altura de la pista de etiquetas, que es el espacio reservado para las etiquetas de los objetos en el dendrograma. Un valor de 5.5 proporciona más espacio para las etiquetas.Además, el código incluye funciones adicionales para mejorar la visualización:labs(title = \"Empresas TMI.\", subtitle = \"Método de Ward. Variables originales tipificadas.\"): Esta función añade un título y un subtítulo al gráfico.labs(title = \"Empresas TMI.\", subtitle = \"Método de Ward. Variables originales tipificadas.\"): Esta función añade un título y un subtítulo al gráfico.theme_grey(): Esta función aplica un tema gris al gráfico, que es el tema predeterminado en ggplot2, proporcionando un fondo gris claro y un estilo de texto específico.theme_grey(): Esta función aplica un tema gris al gráfico, que es el tema predeterminado en ggplot2, proporcionando un fondo gris claro y un estilo de texto específico.El eje vertical del dendograma recoge las distancias (o disimilitud) entre los casos y/o grupos previos que se van agrupando sucesivamente. La escala depende de cada método empleado. En el caso del método de Ward, la escala refleja la suma de cuadrados de la distancia de los casos dentro del clúster.Por otro lado, en este ejemplo, es interesante observar que las empresas Chakotay Cargo Systems, Home One Cargo, las outliers comentadas anteriormente, permanecen sin agruparse hasta una zona muy avanzada del proceso de agrupación, en coherencia con el gráfico de temperatura de la matriz de distancias euclídeas. En cambio, compañías como Betazoid Transport y Skywalker Freight Co. se han unido en el mismo grupo casi inmediatamente, lo que cuadra con el tono anaranjado de su intersección en la matriz de distancias.Una cuestión importante consiste en determinar con cuántos grupos hemos de quedarnos. Aunque existen algoritmos y paquetes de R que aconsejan un número (por ejemplo, la función NbClust() del paquete NbClust); veces puede ser preferible que el propio investigador decida el número de grupos crear, mediante la observación del dendograma, y de acuerdo los objetivos de su propia investigación.Dentro de los métodos objetivos, uno muy extendido es el del método de la anchura media de silueta, muy utilizado en este tipo de análisis.Para cada observación \\(\\), la anchura de la silueta \\(s_i\\) se calcula como:\\[\ns_i = \\frac{b_i - a_i}{\\max(a_i, b_i)}\n\\]Donde:\\(a_i\\) es la distancia promedio de la observación \\(\\) todas las demás observaciones dentro de su propio clúster.\\(b_i\\) es la distancia promedio de la observación \\(\\) al clúster más cercano.La anchura de la silueta varía entre -1 y 1. Un valor cercano 1 indica que la observación está bien agrupada dentro de su clúster; un valor cercano 0 quiere decir que la observación está en el límite entre dos clústeres; y un valor negativo implica que la observación podría estar mal clasificada en su clúster actual. De este modo, para evaluar cuál es el mejor número de conglomerados retener:Se prueban diferentes valores de \\(k\\) (número de clústeres).Se prueban diferentes valores de \\(k\\) (número de clústeres).Se calcula el promedio de los valores de silueta \\(k\\) .Se calcula el promedio de los valores de silueta \\(k\\) .Se selecciona el \\(k\\) que maximiza el promedio de la silueta, lo que indica que la partición es más adecuada.Se selecciona el \\(k\\) que maximiza el promedio de la silueta, lo que indica que la partición es más adecuada.De todos modos, es necesario insistir en que estos métodos llevan una conclusión única ni irrevocable; por lo que puede ser preferible que el propio investigador decida el número de grupos crear, mediante la observación del dendograma, y de acuerdo los objetivos de su propia investigación, tomando los métodos cuantitativos solo como orientación.En nuestro caso, el gráfico del método de la silueta se obtiene partir de la función fviz_nbclust del paquete factoextra, con el código:El método aconseja una división de los casos en 8 grupos o conglomerados diferentes. Sin embargo, parece un número excesivo si se quieren caracterizar posteriormente los grupos e incidir en sus diferencias. Un segundo número de grupos apropiado según el método es 2; pero podría ocurrir lo contrario que el caso anterior: sería un valor demasiado bajo, lo que provocaría que tan solo se distinguiese un grupo con los dos outliers de otro con los 23 casos restantes. Así, el investigador puede decidir qué número de grupos parece equilibrado y se ajusta sus intereses. En este ejemplo, un número de grupos razonable podría ser 5, que contaría con el aval de mantener individualizadas 2 de las empresas etiquetadas como outliers.Si se acepta esta opción, se podrá visualizar de nuevo el dendograma coloreando los grupos formados, con el código siguiente (debe modificarse el código para igualar el argumento k = al número de grupos seleccionado):En el código anterior:k = 5: Especifica el número de grupos o clústeres que se desea formar en el dendrograma. En este caso, se han decidido formar 5 grupos.k = 5: Especifica el número de grupos o clústeres que se desea formar en el dendrograma. En este caso, se han decidido formar 5 grupos.k_colors = \"black\": Define el color de las etiquetas de los clústeres. Aquí, se ha elegido el color negro para las etiquetas.k_colors = \"black\": Define el color de las etiquetas de los clústeres. Aquí, se ha elegido el color negro para las etiquetas.rect = TRUE:Significa que se dibujarán rectángulos delimitando los grupos formados.rect = TRUE:Significa que se dibujarán rectángulos delimitando los grupos formados.rect_border = \"npg\": Define el color del borde de los rectángulos que rodean los clústeres. \"npg\" es un conjunto de colores predefinidos (el de las publicaciones del Nature Publishing Group) en el paquete ggsci, que proporciona paletas de colores científicas.rect_border = \"npg\": Define el color del borde de los rectángulos que rodean los clústeres. \"npg\" es un conjunto de colores predefinidos (el de las publicaciones del Nature Publishing Group) en el paquete ggsci, que proporciona paletas de colores científicas.rect_fill = TRUE: Indica si los rectángulos que rodean los clústeres deben estar rellenos. TRUE significa que los rectángulos estarán rellenos con el color especificado.rect_fill = TRUE: Indica si los rectángulos que rodean los clústeres deben estar rellenos. TRUE significa que los rectángulos estarán rellenos con el color especificado.Al observar el gráfico, pueden destacarse varios conglomerados:Primer grupo (rojo, izquierda):Integrado por Hyperion Star Haulage y Event Horizon Haulage. Estas dos empresas están muy próximas entre sí, indicando un alto grado de similitud en sus características. Se separan tempranamente del resto, lo que sugiere que poseen un perfil muy diferenciado respecto al resto de compañías (quizá por especialización extrema o tamaño atípico).Segundo grupo (azul, central):Es el grupo más numeroso (14), e incluye empresas como Bib Fortuna Haulage, Razor Freight Lines, Kashyyyk Logistics o Tatooine Movers, entre otras. Aglutina compañías con comportamientos relativamente homogéneos, posiblemente representando el segmento “medio” del sector. Dentro de este grupo se distinguen subgrupos internos, lo que refleja cierta diversidad en estrategias o mercados específicos.Tercer grupo (verde, derecha):Formado por empresas (7) como Terminator Freight, Blizzard Transport y Rebel Alliance Transport. Su distancia con respecto al grupo azul indica una diferenciación moderada, quizá por operar en mercados más especializados o disponer de tecnologías más avanzadas.Por último, aparecen la derecha las compañías Chakotay Cargo Systems y Home One Cargo, que son nuestros outliers más destacados en el análisis previo y en el estudio de la matriz de distancias. Su aislamiento sugiere perfiles muy particulares.continuación vamos identificar con mayor detalle los casos que integran cada uno de los grupos, así como caracterizar tales grupos en función de los valores medios de las variables originales. Para ello, crearemos el vector de valores enteros que indica el grupo al que pertenece cada caso (empresa). este vector se le llamará, por ejemplo, “whatcluster_j”, y se construirá mediante la función cutree(), donde el primer argumento es el nombre del objeto que guarda la solución del análisis clúster (“cluster_j”), y el segundo argumento es el número de grupos que hemos decidido crear (k = 5).Lamentablemente, el “número de grupo” que cutree() asigna cada caso tiene por qué coincidir con el orden de grupos del dendograma (es decir, el segundo grupo del dendograma puede ser denominado por cutree(), por ejemplo, grupo “4”). Para que los grupos se llamen según el orden en que se disponen en el dendograma, hay que hacer varios ajustes adicionales, como se muestra en el código.Además, conviene convertir esta variable “whatcluster_j” en un factor con la función .factor(), para que deje de ser variable métrica, efectos de incorporar una leyenda en gráficos posteriores, . Finalmente, ese factor se incorporará al data frame “seleccion” (importante: ”zseleccion”; sino al data frame que contiene las variables tipificadas):","code":"\n# CLUSTER JERARQUICO CON VARIABLES ORIGINALES.\n\n# Tipificando variables\nzseleccion <- data.frame(scale(seleccion))\nsummary(zseleccion)##     IDIVERSE           IFIDE               IDIG        \n##  Min.   :-1.2276   Min.   :-2.02660   Min.   :-1.0673  \n##  1st Qu.:-0.9035   1st Qu.:-0.35844   1st Qu.:-0.6273  \n##  Median :-0.2048   Median :-0.02907   Median :-0.3250  \n##  Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000  \n##  3rd Qu.: 0.7956   3rd Qu.: 0.22847   3rd Qu.: 0.5436  \n##  Max.   : 2.2679   Max.   : 3.71756   Max.   : 3.1734\n# Matriz de distancias\nd <- dist(zseleccion)\nfviz_dist(d, lab_size = 8)  # Del paquete {factoextra}\n# Método de Ward.\ncluster_j<-hclust(d, method=\"ward.D2\")\n\nfviz_dend(cluster_j,\n          cex = 0.6,\n          rect = FALSE,\n          labels_track_height = 5.5) +\n  labs(title = \"Empresas TMI.\",\n       subtitle = \"Método de Ward. Variables originales tipificadas.\") +\n  theme_grey()\n# Método de obtención de número de grupos (k) del Ancho de Silueta.\np <- fviz_nbclust(\n  x = zseleccion, \n  FUNcluster = hcut, \n  method = \"silhouette\",\n  hc_method = \"ward.D2\",\n  k.max = 15\n)\np\nngrupos = 5   # números de conglomerados decidido! #############################\nfviz_dend(cluster_j,\n          cex = 0.6,\n          k = ngrupos, # número de conglomerados que se ha decidido formar!\n          k_colors = \"black\",\n          labels_track_height = 5.5,\n          rect = TRUE,\n          rect_border = \"npg\",\n          rect_fill = TRUE) +\n  labs(title = \"Empresas TMI.\",\n       subtitle = \"Método de Ward. Variables originales tipificadas.\") +\n  theme_grey()\n## CARACTERIZACIÓN Y COMPOSICIÓN DE GRUPOS.\n\n# cortando árbol en grupos\ncl <- cutree(cluster_j, k = ngrupos)\n\n# etiquetas en el orden exacto del dendrograma (izq→der)\nord_labels <- cluster_j$labels[cluster_j$order]\n\n# posición (1..n) de cada caso según ese orden\npos <- match(names(cl), ord_labels)\n\n# mapa de id antiguo -> id nuevo (1..ngrupos) según el bloque más a la izquierda\nmins       <- tapply(pos, cl, min)             # posición más a la izq por clúster\nold_order  <- names(sort(mins))                # ids antiguos ordenados izq→der\nmap        <- setNames(seq_along(old_order), old_order)\n\n# Formar etiqueta de grupos (factor \"whatcluster\")\nwhatcluster_j <- factor(map[as.character(cl)], levels = 1:ngrupos)\nseleccion$whatcluster_j <- whatcluster_j"},{"path":"análisis-clúster..html","id":"caracterización-de-los-grupos-o-conglomerados.","chapter":"7 Análisis Clúster.","heading":"7.2.4  Caracterización de los grupos o conglomerados.","text":"Una vez incorporado el grupo de pertenencia de cada empresa al data frame “seleccion”, se podrán calcular y almacenar las medias de cada grupo de las distintas variables originales, usando las funciones by_group() y summarise() de dplyr. Toda la información se asigna al data frame “tablamedias” para poder representarla en una tabla mediante las facilidades que ofrecen los paquetes knitr y kableExtra:\nTable 7.1: Table 7.2: Método de Ward. 5 grupos. Medias de variables\nObviamente, también se podrían comparar las medias de los grupos, para cada variable, con un simple gráfico de barras. fin de crear un método que valga para cualquier número de variables, realizaremos la tarea con un bucle. El código es el siguiente:En el código anterior, setdiff() crea un vector “variables” que contiene todos los nombres de las columnas del data frame “tablamedias”, excepto “whatcluster_j” y “obs”, que son las variables originales. Luego se crea una lista vacía “graficos.centroides” para almacenar los gráficos generados. Con (seq_along(variables)) comienza el bucle, que recorre cada elemento del vector “variables”. En el código de gráfico, “var1” toma el nombre, en cada iteración, de la variable representar. En el “mapeo”, es importante utilizar aes_string(), que requiere que los nombres de las variables se pasen como cadenas de texto (entre comillas), lo que es útil cuando los nombres de las variables se generan dinámicamente o se pasan como argumentos de función, y cuando se necesitan construir mapeos estéticos de manera programática. Finalmente, con graficos.centroides[[paste0(\"grafico_\", var1)]] <- grafico se guarda el gráfico en la lista “graficos.centroides” con un nombre basado en “var1”.Los gráficos guardados en la lista “gráficos.centroides” se pueden agrupar en composiciones, de, por ejemplo, 2x2, utilizando el paquete patchwork, empleando la función create.patchwork()que ya incorporamos al comienzo del script. Con esta función, crearemos la lista de composiciones de gráficos denominada, por ejemplo, “grupos.graficos.centroides”:Como solo hay 3 variables originales en el análisis; “grupos.graficos.centroides” solo cuenta con un elemento o composición, que contiene tres gráficos (valores medios de IDIVERSE, IFIDE e IDIG, por conglomerado).Hablando siempre en términos de la media (centroides), puede comprobarse cómo el grupo 4 (compañía Chakotay Cargo Systems) destaca por su alto valor en el índice de diversificación y en el Fidelización, quedando en segunda posición en el índice de digitalización. El otro grupo de una única compañía, el grupo 5, que contiene la empresa Home One Cargo, destca por poseer el mayor índice de digitalización. En adición, ocupa la segunda posición en el índice de fidelización. Por último, en cuanto al índice de diversificación, ocupa una posición media. En cuanto al grupo 3, em promedio es un conglomerado formado por empresas con índices de fidelización y digitalización medios, mientras que en cuando la diversificación ocupa una destacada segunda posición, solo por detrás del grupo 4. Por su parte, las empresas del grupo 2, en promedio, presentan un índice de fidelización medio, mientras que los índices de diversificación y digitalización son más bien bajos. Por último, el grupo 1, compuesto por las compañías Hyperion Star Haulage y Event Horizon Haulage, se caracteriza por mostrar los menores valores medios en los tres indicadores.Por otro lado, se pueden presentar en diferentes tablas las informaciones de cada grupo. Vamos automatizar de nuevo el proceso de generación de las tablas mediante el empleo de un bucle. Las diferentes tablas se irán guardando en una lista de nombre, por ejemplo, “tablascompo”:Las tablas obtenidas serán:Para representar los grupos gráficamente, tenemos la dificultad de contar con más de dos variables clasificadoras. Una idea es generar todas las combinaciones de variables posibles y los correspondientes gráficos de dispersión con los casos coloreados de modo diferente según el grupo de pertenencia. Finalmente, los gráficos de modo compacto utilizando la función create_patchwork() que se incluyó anteriormente en el script para hacer mediante patchwork.composiciones de 2x2 gráficos.Vamos realizar la tarea de generar todos los gráficos de modo automatizado. Primero generaremos un vector con el nombre de todas las variables (excluyendo al factor whatcluster_j) mediante la función setdiff(). Luego, crearemos una lista para ir almacenando los gráficos (lista “graficos”). Por último, calcularemos todas las combinaciones de nombres de variables posibles, con la función combn(), y las almacenaremos en la lista “combinaciones”. En esta función, el argumento simplify = FALSE le dice la función que vuelque el resultado una matriz. En su lugar, devuelve una lista donde cada elemento de la misma es una combinación de los elementos del vector original.El siguiente paso consiste en utilizar un bucle para generar los gráficos de dispersión de acuerdo las combinaciones de variables obtenidas y guardarlos en la lista “graficos”. El bucle itera tantas veces como elementos guarda la lista “combinaciones” (argumento/función seq_along()):Una vez generados los gráficos de todas las combinaciones de variables (6 gráficos en el ejemplo), y almacenados en la lista “graficos”, se podrán reagrupar y presentar en composiciones de 2x2 mediante el empleo de la función create_patchwork():Pueden extraerse algunas conclusiones, partir de estos gráficos, sobre cada uno de los grupos, que confirman las ideas anteriores:1. Grupos 1 y 2: empresas tradicionales de baja diversificación.Los clústeres 1 y 2 se sitúan en la parte baja del eje de diversificación (IDIVERSE < 30).El clúster 2, el más numeroso, agrupa empresas con niveles medios de fidelidad (IFIDE ≈ 37-42) y baja digitalización (IDIG < 20). Estas compañías parecen representar un modelo tradicional y estable, con una clientela relativamente fiel pero escasa inversión tecnológica. Son negocios consolidados, pero con riesgo de perder competitividad en entornos cada vez más digitalizados.El clúster 1, aunque pequeño, muestra menor fidelidad y ligeramente más digitalización, sugiriendo empresas que están experimentando cambios estructurales o en transición, probablemente buscando nuevos nichos o adaptándose la digitalización con dificultades.2. Grupo 3: empresas equilibradas y diversificadas.El clúster 3 reúne compañías con diversificación y fidelidad moderadamente altas (IDIVERSE e IFIDE entre 35 y 45) y digitalización intermedia (IDIG entre 20 y 35). Estas firmas parecen situarse en una posición estratégica óptima, combinando una cartera de servicios variada con clientelas estables y cierto grado de modernización. Representan el núcleo competitivo del sector, con potencial para liderar la transición tecnológica sin perder estabilidad comercial.3. Grupo 4: empresa líder altamente diversificada y digitalizadaEl clúster 4, compuesto por una única empresa (Chakotay Cargo Systems), se sitúa en los valores máximos de los tres indicadores: diversificación, fidelidad y digitalización. Esta empresa se distingue como referente del sector, con una clara orientación la innovación y una base de clientes sólida. Su posición sugiere una estrategia de diferenciación tecnológica y de servicio, que le otorga ventajas competitivas sostenibles.4. Grupo 5: empresa digital pero poco diversificadaFinalmente, el clúster 5 (Home One Cargo) presenta muy alta digitalización (IDIG > 60) y fidelidad elevada, pero una diversificación reducida. Se trataría de una compañía especializada en un nicho muy digitalizado, posiblemente centrado en transporte premium o servicios automatizados. Su modelo es eficiente y tecnológicamente avanzado, aunque dependiente de un ámbito de negocio limitado.En cuanto las tres variables consideradas, se pueden inferir algunos patrones de relación también interesantes:1. Relación entre IDIVERSE e IFIDE (diversificación y fidelidad):En el gráfico IDIVERSE–IFIDE, los puntos muestran una ligera pendiente ascendente, aunque muy marcada. Esto sugiere una correlación positiva débil moderada: las empresas con una mayor diversificación tienden mantener una clientela algo más fiel.En el clúster 2 (el grupo mayoritario y más tradicional), la nube de puntos es bastante compacta y casi horizontal, indicando poca relación entre ambas variables. En este segmento, la fidelidad parece más determinada por la trayectoria o reputación que por la diversificación.En cambio, el clúster 3 (empresas más diversificadas y equilibradas) sí muestra una relación más consistente: conforme aumenta la diversificación, también se mantiene o eleva la fidelidad.El clúster 4 (chakotay cargo systems) refuerza esa tendencia, al situarse en los valores más altos de ambas variables — es decir, una empresa muy diversificada y con gran fidelidad, un ejemplo de sinergia entre ambas dimensiones.EN conclusión, existe una correlación positiva leve, que se hace más fuerte en las empresas modernas y diversificadas.2. Relación entre IDIVERSE e IDIG (diversificación y digitalización):En el gráfico IDIVERSE–IDIG la relación parece algo más heterogénea. En general, hay una correlación lineal clara entre ambas variables para el conjunto total, pero sí aparecen patrones diferenciados por clúster.En los clústeres 1 y 2, la digitalización es baja y relativamente independiente de la diversificación: empresas poco diversificadas pueden tener tanto baja como media digitalización.En el clúster 3, se aprecia una correlación positiva más nítida: las empresas con mayor diversificación presentan también mayores niveles de digitalización. Esto sugiere que estas firmas han apostado por diversificar apoyándose en la tecnología.Los casos extremos (clústeres 4 y 5) marcan los límites de esa relación:Clúster 4: alta diversificación + alta digitalización → perfil de liderazgo.Clúster 5: baja diversificación + digitalización muy alta → modelo de especialización tecnológica.EN definitiva, la relación entre diversificación y digitalización es positiva en los grupos más avanzados, pero inexistente o débil en los tradicionales.3. Relación entre IFIDE e IDIG (fidelidad y digitalización):El gráfico IFIDE–IDIG muestra una dispersión considerable, aunque con algunas señales interesantes. En la mayor parte de los casos (clústeres 1 y 2), mayor digitalización necesariamente corresponde mayor fidelidad; las empresas digitalizadas parecen haber consolidado aún esa ventaja comercial.Sin embargo, en el clúster 3 la nube de puntos tiende ligeramente al alza: las empresas con niveles medios-altos de digitalización también presentan fidelidad superior, lo que podría indicar una mejor experiencia de cliente o una oferta más personalizada gracias la tecnología.Los casos atípicos refuerzan la idea de estrategias distintas:Clúster 4 (Chakotay Cargo Systems) combina altos niveles de ambas, señal de madurez digital y consolidación de la relación con clientes.Clúster 5 (Home One Cargo) muestra muy alta digitalización pero fidelidad algo menor, típico de modelos disruptivos o muy especializados donde la clientela puede fluctuar.Se puede concluir que la correlación global entre fidelidad y digitalización es débil, pero se vuelve positiva en empresas más equilibradas o maduras tecnológicamente.","code":"\n# Tabla con centroides de grupos.\ntablamedias <- seleccion %>%\n               group_by(whatcluster_j) %>%\n               summarise(obs = length(whatcluster_j),\n                                      Idiverse = mean(IDIVERSE),\n                                      Ifide = mean(IFIDE),\n                                      Idig = mean(IDIG))\n\ntablamedias %>%\n  kable(caption = \"Método de Ward. 5 grupos. Medias de variables\",\n        col.names = c(\"Clúster\",\n                      \"Observaciones\",\n                      \"I. Diversif.\",\n                      \"I. Fidelizac.\",\n                      \"I. Digitalizac.\"),\n        digits = c(0, 0, 3, 3, 3),\n        format.args = list(decimal.mark = \".\",\n                           scientific = FALSE)) %>%\n  kable_styling(full_width = F,\n                bootstrap_options = \"striped\",\n                                    \"bordered\",\n                                    \"condensed\",\n                position = \"center\",\n                font_size = 11) %>%\n  row_spec(0, bold= T,\n           align = \"c\") %>%\n  row_spec(1:nrow(tablamedias),\n           bold= F,\n           align = \"c\")\n# Gráficos de centroides\n\n  # Vector de nombre de variables excluyendo la variable no deseada\n    variables <- setdiff(names(tablamedias), c(\"whatcluster_j\", \"obs\"))\n\n  # Lista para almacenar los gráficos\n    graficos.centroides <- list()\n\n  # Bucle para crear y almacenar los gráficos\n    for (i in seq_along(variables)) {\n    var1 <- variables[[i]]\n    grafico <- ggplot(data= tablamedias,\n                      map = (aes_string(y = var1, x = \"whatcluster_j\"))) +\n               geom_bar(stat = \"identity\",\n                        colour = \"red\",\n                        fill = \"orange\",\n                        alpha = 0.7) +\n               ggtitle(paste0(var1, \". Media por grupos.\"),\n                       subtitle = \"Empresas eólicas\")+\n               xlab (\"Grupo\") +\n               ylab(var1)\n    graficos.centroides[[paste0(\"grafico_\", var1)]] <- grafico\n}  \n# Aplicar función de composiciones a gráficos de centroides.\n  grupos.graficos.centroides <- create_patchwork(graficos.centroides)\n  \n# Presentar las composiciones\n  for (n in 1:length(grupos.graficos.centroides)){\n       print(grupos.graficos.centroides[[n]])\n  }\n# Tablas con composiciones de grupos\n\n  # Número de tablas y lista para guardarlas\n\n  numclusters <- nlevels(seleccion$whatcluster_j)\n  tablascompo <- list()\n\n  # Bucle para generar las tablas\n\n  for (n in 1:numclusters){\n      tabla <- seleccion %>%\n      filter(whatcluster_j == as.character(n)) %>%\n      select(IDIVERSE, IFIDE, IDIG) %>%\n      kable(caption = paste(\"Método de Ward. Grupo \", n, \".\"),\n            col.names = c(\"I. Diversificación\",\n                          \"I. Fidelización\",\n                          \"I. Digitalización\"),\n            digits = c(0, 3, 3, 3),\n            format.args = list(decimal.mark = \".\",\n                               scientific = FALSE)) %>%\n      kable_styling(full_width = FALSE, \n                    bootstrap_options = c(\"striped\",\n                                          \"bordered\",\n                                          \"condensed\"),\n                    position = \"center\",\n                    font_size = 12) %>%\n      row_spec(0, bold = TRUE, align = \"c\")\n      tablascompo[[n]] <- tabla\n  }\n\n  # Presentar las tablas\n  for (n in 1:numclusters){\n    print(tablascompo[[n]])\n  }\n# Gráficos Variable vs Variable\n\n  # Lista de variables excluyendo la variable no deseada\n    variables <- setdiff(names(seleccion), \"whatcluster_j\")\n\n  # Lista para almacenar los gráficos\n    graficos <- list()\n\n  # Generar todas las combinaciones posibles de pares de variables\n    combinaciones <- combn(variables, 2, simplify = FALSE)\n  # Bucle para crear y almacenar los gráficos\n    for (i in seq_along(combinaciones)) {\n      var1 <- combinaciones[[i]][1]\n      var2 <- combinaciones[[i]][2]\n      grafico <- ggplot(seleccion,\n                        map = aes_string(x = var1,\n                                         y = var2,\n                                         color = \"whatcluster_j\")) +\n                 geom_point() +\n                 labs(title = paste(\"GRÁFICO\", var1, \"-\", var2),\n                      subtitle = \"Empresas TMI\") +\n                 xlab (var1) +\n                 ylab (var2) +\n                 scale_color_brewer(palette = \"Set1\") \n      graficos[[paste0(\"grafico_\", var1, \"_\", var2)]] <- grafico\n}\n  # Hacer agrupaciones con la función de patchworks creada anteriormente\n    gruposgraficos <- create_patchwork(graficos)\n\n  # Presentar las composiciones\n    for (n in 1:length(gruposgraficos)){\n      print(gruposgraficos[[n]])\n    }"},{"path":"análisis-clúster..html","id":"métodos-de-agrupación-no-jerárquicos.","chapter":"7 Análisis Clúster.","heading":"7.3  Métodos de agrupación no-jerárquicos.","text":"Dentro del análisis clúster, los métodos de agrupación -jerárquicos se utilizan en situaciones en las que hay un elevado número de casos que clasificar. Son especialmente útiles cuando nuestro objetivo pasa por crear grupos que definan una tipología de casos o individuos, más que clasificar casos o individuos concretos. En definitiva, identificar patrones de subpoblaciones partir de una muestra. Por eso es conveniente, previamente, detectar los outliers y, en su caso, eliminarlos; ya que podrían distorsionar las características de los grupos debido la sensibilidad de los algoritmos la presencia de casos atípicos.Una diferencia clave con respecto los métodos jerárquicos es que es necesario decidir priori el número de conclomerados o grupos de casos formar. Por otro lado, son métodos más eficientes, y permiten el traslado de casos de unos grupos otros.Aunque existen otros métodos, la técnica -jerárquica más común es la de k-medias. Este es un método iterativo. Se establece un centroide inicial (“semilla”) para cada uno de los k grupos que se quieren crear, y se van asignando cada grupo los casos que se sitúen más cerca de su centro. Una vez asignados los casos, se recalculan los centroides de los grupos, y se repite el proceso en una nueva iteración. El procedimiento termina cuando el algoritmo encuentra la solución convergente (estable). Precisamente, la elección de las “semillas” iniciales es otro de las debilidades que presenta el método, ya que de ello puede depender la obtención de soluciones diferentes.¿Cómo fijar el número de grupos o conglomerados formar? Hay ocasiones en que las que el investigador establecerá un número que le sea manejable o útil según los objetivos que persiga. Si esto es así, y se tiene claro el número de grupos construir, se podrá optar por probar con varios números, y evaluar las soluciones obtenidas. También puede ayudar el realizar previamente un análisis jerárquico para estudiar el dendograma. Además, existen métodos como el del Ancho de Silueta; o algoritmos, como NbClust en R, que sugieren un número de grupos en función de una batería de pruebas presentes en la literatura.La segunda cuestión clave es cómo determinar las “semillas” o centroides iniciales. Una opción es generar las semillas de modo aleatorio, aunque es un método muy conveniente. De hecho, cada vez que se aplicara el algoritmo de k-medias, podría obtenerse una solución diferente. Otra alternativa es la fijación de las “semillas” por parte del investigador. Una idea, en este sentido, es hacer un clúster jerárquico previo, y tomar los centroides de la solución final como “semillas” de k-medias. Otra posibilidad interesante es aplicar el método del centroide más lejano: se fija el primer centroide al azar, pero luego el 2º centroide coincidirá con el punto de datos más alejado de él. En general, el jº centroide coincidirá con el punto cuya distancia mínima los centroides precedentes sea mayor. Se pretende que los centroides estén bien separados unos de otros. Una versión mejorada de este procedimiento es el método k-medias++.Para nuestro ejemplo práctico, vamos volver utilizar como variables clasificadoras las del ejemplo desarrollado para los métodos jerárquicos; pero esta vez para una muestra de 300 empresas de transporte de mercancías interestelar. Estas variables son, de nuevo, los índices de diversificación (IDIVERSE), fidelización de clientes (IFIDE) y de digitalización (IDIG). Con base en ellos, queremos establecer una serie de perfiles o tipologías de las empresas que componen el sector.Trabajaremos, como en el ejemplo de clúster jerarquizado, en el proyecto llamado “cluster”. Vamos ir la carpeta del proyecto y vamos guardar en ella los dos archivos de esta práctica: un archivo de Microsoft® Excel® llamado “interestelar_300.xlsx” y un script denominado “kmedias_rstars.R”. En la última sección del capítulo dispones de los enlaces dicho material.Si abrimos el archivo de Microsoft® Excel®, “interestelar_300.xlsx”, comprobaremos que tiene la misma estructura y “hojas” que nos encontramos en “interestelar_25.xlsx”; pero extendido 300 empresas o compañías.","code":""},{"path":"análisis-clúster..html","id":"preparación-de-los-datos.-1","chapter":"7 Análisis Clúster.","heading":"7.3.1  Preparación de los datos.","text":"La primera parte del script es semejante la del script de la parte de “clúster jerárquico”, cluster_rstars.R. Solo cambia en los siguientes aspectos:utiliza el paquete factoextra, y sí utiliza los paquetes clustery ClusterR.El archivo importar es interestelar_300.xlsx, y los datos se almacenan en el data frame “interestelar_300”.Los outliers, en esta ocasión, sí son eliminados, con lo que se trabajará con un data frame sin casos atípicos, denominado “seleccion_so”.Dicho esto, pasamos transcribir el código correspondiente la limpieza del Global Environment, carga de paquetes, función para crear composiciones de 2X2 gráficos con patchwork, importación de datos:Aunque se ha incluido el código para identificar y eliminar los casos con missing values, el gráfico generado con la función vis_miss() ya informó de que había ningún dato ausente en las tres variables de interés, almacenadas en el data frame “seleccion”, luego se siguen manteniendo los 300 casos.Para la detección de outliers, se ha utilizado también el mismo código que en el análisis clúster jerárquico: obtención de las distancias de Mahalanobis, y aplicación de diagrama de caja o boxplot:Una vez se constata que existen casos outliers, se pueden identificar con el código:Puede comprobarse que son 26 casos. En esta ocasión, y diferencia del caso del análisis jerárquico, vamos eliminar estas observaciones, ya que lo que deseamos es agrupar casos concretos, sino identificar patrones que caractericen la mayor parte de las empresas. Para eliminar los outliers, ejecutaremos:Hemos creado un data frame llamado “seleccion_so” con las variables de nuestro análisis; pero sin los casos considerados atípicos. Por último, eliminaremos la variable MAHALANOBIS, ya que será necesaria para el resto del análisis:","code":"\n### CLUSTER de k-medias empresas TMI. ###\n\n# Limpiando el Global Environment\nrm(list = ls())\n\n# Cargando paquetes\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gtExtras)\nlibrary(visdat)\nlibrary (cluster)\nlibrary (ClusterR)\nlibrary (knitr)\nlibrary (kableExtra)\nlibrary (patchwork)\nlibrary (pgirmess)\n\n##### Función para crear composiciones de gráficos con patchwork ###############\ncreate_patchwork <- function(plot_list) {\n  n <- length(plot_list)\n  if (n == 0) return(NULL)\n  full_rows <- n %/% 4\n  remaining <- n %% 4\n  patchworks <- list()\n  \n  if (full_rows > 0) {\n    for (i in seq(1, full_rows * 4, by = 4)) {\n      patchworks <- c(patchworks, list((plot_list[[i]] + plot_list[[i+1]]) / \n                                         (plot_list[[i+2]] + plot_list[[i+3]])))\n    }\n  }\n  \n  if (remaining > 0) {\n    last_plots <- plot_list[(full_rows * 4 + 1):n]\n    empty_plots <- lapply(1:(4 - remaining), function(x) ggplot() + theme_void())\n    last_patchwork <- do.call(patchwork::wrap_plots, c(last_plots, empty_plots))\n    patchworks <- c(patchworks, list(last_patchwork))\n  }\n  return(patchworks)\n}\n################################################################################\n\n## DATOS\n\n# Importando datos desde Excel\ninterestelar_300 <- read_excel(\"interestelar_300.xlsx\",\n                              sheet = \"Datos\",\n                              na = c(\"n.d.\"))\ninterestelar_300 <- data.frame(interestelar_300, row.names = 1)\n\n# Seleccionando variables metricas para el analisis.\nseleccion <- interestelar_300 %>%\n  select(IDIVERSE, IFIDE, IDIG)\nseleccion_df_graph <- gt_plt_summary(seleccion)\nseleccion_df_graph\n# Localizando missing values.\nseleccion %>%\n  vis_miss() +\n  labs(title = \"Indicadores: Diversificación, Fidelidad, Digitalización\",\n       subtitle = \"Transporte de mercancías interestelar\",\n       y = \"Observación\",\n       fill = NULL) +\n  scale_fill_manual(\n    values = c(\"TRUE\" = \"red\", \"FALSE\" = \"grey\"),\n    labels = c(\"TRUE\" = \"NA\", \"FALSE\" = \"Presente\")) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14))\nseleccion %>% filter(is.na(IDIVERSE) |\n                       is.na(IFIDE) |\n                       is.na(IDIG)) %>%\n  select(IDIVERSE, IFIDE, IDIG)## [1] IDIVERSE IFIDE    IDIG    \n## <0 rows> (o 0- extensión row.names)\nseleccion <- seleccion %>%\n  filter(! is.na(IDIVERSE) &\n           ! is.na(IFIDE) &\n           ! is.na(IDIG)) \n# Identificando outliers con distancia de Mahalanobis.\nseleccion <- seleccion %>%\n  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),\n                                   center = colMeans(.),\n                                   cov    = cov(.)))\n\nggplot(data = seleccion, map = (aes(y = MAHALANOBIS))) +\n  geom_boxplot(fill = \"orange\") +\n  ggtitle(\"DISTANCIA DE MAHALANOBIS\",\n          subtitle = \"IDIVERSE, IFIDE, IDIG. Empresas TMI.\") +\n  ylab(\"MAHALANOBIS\")\nQ1M <- quantile (seleccion$MAHALANOBIS, c(0.25))\nQ3M <- quantile (seleccion$MAHALANOBIS, c(0.75))\n\nseleccion %>%\n  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |\n         MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%\n  select(MAHALANOBIS, IDIVERSE, IFIDE, IDIG)##                             MAHALANOBIS  IDIVERSE     IFIDE       IDIG\n## arrakis freight               34.783752 38.881367 73.302303 58.5101128\n## vader and company             23.777077 34.387492 68.770819 42.4288883\n## excalibur freight systems     43.615267 36.183108 79.153830 45.4123278\n## hyperdrive express            22.159690 35.789813 68.147843 33.6619031\n## kaiju haulage co.             25.590991 49.848485 71.375275 52.4964487\n## starship express              14.235065 33.533204 62.362973 37.5822888\n## chakotay cargo systems        15.212105 54.368150 64.260135 42.7197240\n## home one cargo                26.705677 22.176015 46.776346 60.8325142\n## ezra bridger haulage           5.949267 12.491941 34.112087 29.0669725\n## peacekeeper freightlines       6.587697 17.375887 33.460326 31.7773652\n## kyber crystal shipping         8.893282 22.627337 30.976669 34.0135033\n## hal intergalactic freight      8.338086  7.488717 31.611448 28.1566284\n## europa cargo systems          13.192201  8.675048 24.277964 27.0609190\n## yavin cargo lines              5.779355 47.624113 46.842087 41.9147891\n## io star transport              7.573304 38.020632 33.828582 35.1177708\n## bb-8 haulage                   5.646624 43.168923 42.636597 40.7575527\n## andromeda cargo                6.870547 46.054159 44.989487 44.3415811\n## rey skywalker logistics       16.895933  5.412637 16.301331 16.0120976\n## togruta freight                5.959823 45.828498 44.763678 42.1676280\n## martian dust cargo            15.657514 32.675693 39.509961 52.2607912\n## jovian logistics               6.229850 22.392005 29.774005 27.4342295\n## outer rim logistics           31.997819  2.082527  5.260711  7.5809886\n## tannhäuser freight            11.305381 47.888459 42.640524  8.2092915\n## pandora cargo                  6.013632  0.000000 23.391777  0.3378664\n## endor movers                  43.420347 78.297872 40.329272  5.9251666\n## ripley interstellar freight   36.179175 71.350741 40.129739  4.4464628\n# Eliminando outliers.\nseleccion_so <-seleccion %>%\n  filter(MAHALANOBIS <= Q3M + 1.5*IQR(MAHALANOBIS) &\n         MAHALANOBIS >= Q1M - 1.5*IQR(MAHALANOBIS))%>%\n  select(IDIVERSE, IFIDE, IDIG)\n# Eliminando variable MAHALANOBIS del df con outliers\nseleccion <- seleccion %>% select(-MAHALANOBIS)"},{"path":"análisis-clúster..html","id":"determinación-del-número-de-grupos-o-clústeres-a-formar.","chapter":"7 Análisis Clúster.","heading":"7.3.2  Determinación del número de grupos o clústeres a formar.","text":"Una vez preparados los datos, vamos proceder aplicar la técnica de formación de conglomerados -jerárquica de k-medias. Primero, y puesto que se basa en el cálculo de las distancias euclídeas entre casos, procederemos tipificar los valores de las variables, utilizando la función scale(), y creando el data frame “zseleccion_so”. Después, calcularemos la matriz de distancias d:Una cuestión clave es la determinación previa del número de grupos formar. Si se tiene decidido un número de conglomerados priori como consecuencia del interés o de los objetivos de la propia investigación, se puede recurrir como orientación algún método o algoritmo “objetivo”. De nuevo, puede recurrirse al **método de la *anchura media de silueta**, proponiendo diferentes números de grupos (parámetro k).La función KMeans_rcpp() del paquete ClusterR permite calcular la anchura media de Silueta para cada número de grupos propuesto, estimando los grupos mediante el método de k-medias++. Aplicaremos el método para cada número de grupos k propuesto, mediante una función que guarda el ancho medio de silueta en un vector:El código anterior ejecuta la siguiente secuencia:set.seed(123) fija la “semilla” aleatoria. k-medias (incluso con kmeans++) usa un poco de azar al empezar; fijar la semilla hace que siempre se obtengan los mismos resultados al repetir el código (reproducibilidad).set.seed(123) fija la “semilla” aleatoria. k-medias (incluso con kmeans++) usa un poco de azar al empezar; fijar la semilla hace que siempre se obtengan los mismos resultados al repetir el código (reproducibilidad).sapply(2:10, function(k) { ... }):sapply(2:10, function(k) { ... }):2:10 es la secuencia de valores de k que vamos probar (de 2 10 clústeres).2:10 es la secuencia de valores de k que vamos probar (de 2 10 clústeres).sapply(..., function(k) { ... }) aplica la función cada k y devuelve el vector numérico res_sil con los resultados.sapply(..., function(k) { ... }) aplica la función cada k y devuelve el vector numérico res_sil con los resultados.Dentro de la función anónima function(k) { ... }:KMeans_rcpp() (del paquete ClusterR) ejecuta k-medias sobre los datos del data frame “zseleccion_so”.KMeans_rcpp() (del paquete ClusterR) ejecuta k-medias sobre los datos del data frame “zseleccion_so”.clusters = k: número de grupos que queremos formar.clusters = k: número de grupos que queremos formar.num_init = 10: ejecuta el algoritmo 10 veces con diferentes inicios y se queda con la mejor solución (esto reduce la mala suerte de una mala inicialización).num_init = 10: ejecuta el algoritmo 10 veces con diferentes inicios y se queda con la mejor solución (esto reduce la mala suerte de una mala inicialización).max_iters = 100: tope de iteraciones por ejecución.max_iters = 100: tope de iteraciones por ejecución.initializer = \"kmeans++\": estrategia de inicio que suele dar mejores resultados que un inicio totalmente aleatorio.initializer = \"kmeans++\": estrategia de inicio que suele dar mejores resultados que un inicio totalmente aleatorio.El objeto km contiene, entre otras cosas, km$clusters, que es el vector de asignaciones: dice qué cluster pertenece (1, 2, 3, …, k) cada caso.silhouette(...) (del paquete cluster) calcula, para cada observación, su ancho de silueta. Necesita:km$clusters: las etiquetas de clúster.km$clusters: las etiquetas de clúster.d: matriz de distancias entre observaciones.d: matriz de distancias entre observaciones.El resultado de silhouette es una matriz con columnas; una de ellas es \"sil_width\" =, que es el ancho de silueta de cada punto.[...] [, \"sil_width\"] extrae esa columna.[...] [, \"sil_width\"] extrae esa columna.mean(...) hace la media: se obtiene el ancho medio de la silueta para ese k.mean(...) hace la media: se obtiene el ancho medio de la silueta para ese k.El resultado final es el vector “res_sil”, con 9 valores (para k = 2, 3, …, 10). Cada valor es el ancho medio de silueta correspondiente. Servirá para crear, junto cada valor de “k”, un pequeño data frame de nombre “df_sil” que será la base, su vez, para representar los anchos de silueta gráficamente con ggplot2:","code":"\n## CLUSTER K-MEDIAS CON VARIABLES ORIGINALES\n\n# Tipificando variables\nzseleccion_so <- data.frame(scale(seleccion_so))\nsummary(zseleccion_so)##     IDIVERSE           IFIDE              IDIG        \n##  Min.   :-1.7930   Min.   :-3.4256   Min.   :-1.7221  \n##  1st Qu.:-0.8533   1st Qu.:-0.3813   1st Qu.:-0.8467  \n##  Median :-0.1200   Median : 0.1941   Median :-0.0536  \n##  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n##  3rd Qu.: 0.8489   3rd Qu.: 0.4783   3rd Qu.: 0.7754  \n##  Max.   : 2.2253   Max.   : 3.5385   Max.   : 2.7621\nd <- dist(zseleccion_so)\n# --- Calcular el ancho medio de silueta para distintos k ---\nset.seed(123)\nres_sil <- sapply(2:10, \n                  function(k) {\n                    km <- KMeans_rcpp(zseleccion_so,\n                    clusters = k,\n                    num_init = 10,\n                    max_iters = 100,\n                    initializer = \"kmeans++\")\n  mean(silhouette(km$clusters, d)[, \"sil_width\"])\n}\n)\n# --- Crear data frame para graficar ---\ndf_sil <- data.frame(\n  k = 2:10,\n  Silhouette = res_sil\n)\n\n# --- Gráfico con ggplot2 ---\nggplot(df_sil, aes(x = k, y = Silhouette)) +\n  geom_line(color = \"steelblue\", linewidth = 1) +\n  geom_point(size = 3, color = \"darkred\") +\n  labs(title = \"Ancho medio de Silueta para distintos k (k-means++)\",\n       x = \"Número de clústeres (k)\",\n       y = \"Silueta media\") +\n  theme_minimal(base_size = 13)"},{"path":"análisis-clúster..html","id":"aplicación-del-método-de-k-medias-con-determinación-de-semillas-kmeans.","chapter":"7 Análisis Clúster.","heading":"7.3.3  Aplicación del método de k-medias con determinación de semillas kmeans++.^","text":"Una vez decidido el número de grupos (en el ejemplo, 7), se aplica (de nuevo) el método de k-medias, en la versión de la función KMeans_rcpp() del paquete ClusterR, que permite obtener las “semillas” (centroides iniciales) mediante el algoritmo kmeans++, que ofrece buenos resultados frente otras posibilidades de obtención de “semillas”, como la generación puramente aleatoria. La solución final se guarda en el objeto “cluster_k”:Como se acaba de explicar, en la función KMeans_rcpp(), el primer argumento es el data frame con las variables clasificadoras (en sus versiones tipificadas). El segundo es el número de clústeres o grupos obtener (que lo hemos asignado anteriormente al parámetro “k”). El tercero, num_init =, es el número de veces que se repite el procedimiento fin de retener la mejor solución. Max_iters = fija el máximo de iteraciones del procedimiento de k-medias hasta obtener una solución estable. Initializer = define el método de obtención de las semillas (en nuestro caso, k-means++). La solución final se guarda en el objeto “cluster_k”, como se puede apreciar en el Environment.Dentro de la solución, el vector con el grupo de pertenencia de cada empresa se obtiene con el elemento “$clusters”. Conviene guardar ese vector como factor. En concreto, se ha añadido como el factor “whatcluster_k”, integrado dentro del data frame “seleccion_so”.","code":"\n  # Aplicación definitiva de k-medias.\n  k <- 7  # poner aquí número de grupos decidido!!!!\n\n  # Aplicando k-means con inicializacion kmeans++\n\n  cluster_k <-KMeans_rcpp (zseleccion_so,\n                           clusters = k,\n                           num_init = 10,\n                           max_iters = 100,\n                           initializer = \"kmeans++\")\n  seleccion_so$whatcluster_k <- as.factor(cluster_k$clusters)"},{"path":"análisis-clúster..html","id":"caracterización-de-los-grupos-o-conglomerados.-1","chapter":"7 Análisis Clúster.","heading":"7.3.4  Caracterización de los grupos o conglomerados.","text":"continuación vamos caracterizar los grupos formados en función de las medias de las variables originales (coordenadas de los centroides). Así, se podrán mostrar en pantalla las medias de cada grupo de las distintas variables originales, usando las funciones by_group() y summarise() de dplyr. Toda la información se asigna al data frame “tablamedias”; para poder, posteriormente, representarla en una tabla mediante las facilidades que ofrecen los paquetes knitr y kableExtra:\nTable 7.3: Table 7.4: Método de k-medias. 7 grupos. Medias de variables\nObviamente, también se podrían comparar las medias de los grupos, para cada variable, con un gráfico de barras. El código es el siguiente:Para hacer composiciones de 4 gráficos (que en este caso será solo una, dado que hemos almacenado en la lista “graficos.centroides” 4 elementos correspondientes las 4 variables clasificadoras), volveremos utilizar la función create_patchwork(), que ya se mostró en el ejemplo de clúster jerárquico. De nuevo se incluye su código y se aplica la lista de gráficos “graficos.centroides”:La lista “grupos.graficos.centroides” almacena las composiciones de 4 (o menos) gráficos generadas. En este ejemplo, solo tiene un elemento, que es mostrado al ejecutar el bucle de presentación de composiciones.Por medio de la tabla y los gráficos anteriores se pueden trazar algunas conclusiones, comparando las medias de las variables para cada grupo de casos formado (coordenadas de los centroides). El análisis revela una clara heterogeneidad estructural entre las empresas de transporte de mercancías interestelar. Las tres dimensiones —diversificación, fidelización y digitalización— se combinan de manera diferente en cada grupo, reflejando distintos modelos de negocio y grados de madurez estratégica. Así, podemos distinguir tres grandes perfiles:Empresas avanzadas e innovadoras (altos niveles en las tres dimensiones).Empresas avanzadas e innovadoras (altos niveles en las tres dimensiones).Empresas especializadas tradicionales, con baja digitalización y diversificación.Empresas especializadas tradicionales, con baja digitalización y diversificación.Empresas en transición o de nicho, con fortalezas parciales (por ejemplo, alta fidelidad pero baja digitalización).Empresas en transición o de nicho, con fortalezas parciales (por ejemplo, alta fidelidad pero baja digitalización).Analizando cada grupo de modo individualizado:Grupo 1 (57 empresas). Alta diversificación (35.8), alta fidelidad (40.2), digitalización media (22.2). En general, el grupo se compone de empresas sólidas, con una base de clientes fiel y estrategias diversificadas que les permiten operar en distintos segmentos o rutas. Aunque su digitalización es moderada, su estructura sugiere estabilidad y resiliencia. Perspectiva futura: Si invierten más en tecnología, podrían convertirse en líderes del sector, con ventajas competitivas sostenibles largo plazo.Grupo 1 (57 empresas). Alta diversificación (35.8), alta fidelidad (40.2), digitalización media (22.2). En general, el grupo se compone de empresas sólidas, con una base de clientes fiel y estrategias diversificadas que les permiten operar en distintos segmentos o rutas. Aunque su digitalización es moderada, su estructura sugiere estabilidad y resiliencia. Perspectiva futura: Si invierten más en tecnología, podrían convertirse en líderes del sector, con ventajas competitivas sostenibles largo plazo.Grupo 2 (17 empresas). Diversificación media-baja (17.9), muy alta fidelidad (43.1), alta digitalización (26.0). Se compone de empresas tecnológicamente dinámicas y con gran fidelidad, pero con un negocio poco diversificado. Probablemente dominan un nicho específico del transporte interestelar.\nPerspectiva futura: Su foco estratégico y digitalización pueden asegurarles crecimiento, aunque deberían explorar una mayor diversificación para reducir riesgos sectoriales.Grupo 2 (17 empresas). Diversificación media-baja (17.9), muy alta fidelidad (43.1), alta digitalización (26.0). Se compone de empresas tecnológicamente dinámicas y con gran fidelidad, pero con un negocio poco diversificado. Probablemente dominan un nicho específico del transporte interestelar.\nPerspectiva futura: Su foco estratégico y digitalización pueden asegurarles crecimiento, aunque deberían explorar una mayor diversificación para reducir riesgos sectoriales.Grupo 3 (17 empresas). Baja diversificación (15.8), fidelidad media (34.8), digitalización media (21.8). Las empresas que conforman este conglomerado se encuentran, en general, en fase de consolidación, con cierto desarrollo digital pero sin un posicionamiento claro ni fidelización destacable. Perspectiva futura: Si fortalecen su propuesta de valor o amplían su mercado, podrían quedar rezagadas frente competidores más innovadores.Grupo 3 (17 empresas). Baja diversificación (15.8), fidelidad media (34.8), digitalización media (21.8). Las empresas que conforman este conglomerado se encuentran, en general, en fase de consolidación, con cierto desarrollo digital pero sin un posicionamiento claro ni fidelización destacable. Perspectiva futura: Si fortalecen su propuesta de valor o amplían su mercado, podrían quedar rezagadas frente competidores más innovadores.Grupo 4 (30 empresas). Muy baja diversificación (7.5), fidelidad baja (31.6), muy baja digitalización (7.8). Este grupo reúne las empresas más rezagadas, con escasa adaptación tecnológica y un modelo de negocio poco flexible.Perspectiva futura: Riesgo elevado de obsolescencia o pérdida de cuota de mercado. Necesitan transformarse urgentemente mediante digitalización y estrategias de retención de clientes.Grupo 4 (30 empresas). Muy baja diversificación (7.5), fidelidad baja (31.6), muy baja digitalización (7.8). Este grupo reúne las empresas más rezagadas, con escasa adaptación tecnológica y un modelo de negocio poco flexible.Perspectiva futura: Riesgo elevado de obsolescencia o pérdida de cuota de mercado. Necesitan transformarse urgentemente mediante digitalización y estrategias de retención de clientes.Grupo 5 (74 empresas, grupo más numeroso). Diversificación baja (11.1), fidelidad media-alta (38.1), muy baja digitalización (6.8). Grupo formado, en término medio, por empresas tradicionales, que conservan buena relación con sus clientes pero han avanzado en digitalización. Representan el núcleo clásico del sector TMI. Perspectiva futura: Son vulnerables la disrupción tecnológica. Sin inversión en digitalización y diversificación, podrían perder relevancia en mercados más competitivos.Grupo 5 (74 empresas, grupo más numeroso). Diversificación baja (11.1), fidelidad media-alta (38.1), muy baja digitalización (6.8). Grupo formado, en término medio, por empresas tradicionales, que conservan buena relación con sus clientes pero han avanzado en digitalización. Representan el núcleo clásico del sector TMI. Perspectiva futura: Son vulnerables la disrupción tecnológica. Sin inversión en digitalización y diversificación, podrían perder relevancia en mercados más competitivos.Grupo 6 (18 empresas). Alta diversificación (35.9), muy alta fidelidad (43.7), muy alta digitalización (33.7). Grupo constituido por las empresas punteras del sector, con máximos en las tres dimensiones. Combinan diversificación de servicios, lealtad de clientes y transformación digital. Perspectiva futura: Tienen el liderazgo asegurado y marcan el estándar competitivo. Probablemente sean grandes corporaciones interplanetarias con capacidad de expansión internacional.Grupo 6 (18 empresas). Alta diversificación (35.9), muy alta fidelidad (43.7), muy alta digitalización (33.7). Grupo constituido por las empresas punteras del sector, con máximos en las tres dimensiones. Combinan diversificación de servicios, lealtad de clientes y transformación digital. Perspectiva futura: Tienen el liderazgo asegurado y marcan el estándar competitivo. Probablemente sean grandes corporaciones interplanetarias con capacidad de expansión internacional.Grupo 7 (61 empresas). Diversificación media (24.6), fidelidad alta (39.3), digitalización baja-media (14.8). Grupo formado por empresas en transición digital, con buen posicionamiento comercial y cierta diversificación. Perspectiva futura: Si continúan su proceso de digitalización, pueden convertirse en empresas de alto rendimiento similares al grupo 1 o incluso al 6.Grupo 7 (61 empresas). Diversificación media (24.6), fidelidad alta (39.3), digitalización baja-media (14.8). Grupo formado por empresas en transición digital, con buen posicionamiento comercial y cierta diversificación. Perspectiva futura: Si continúan su proceso de digitalización, pueden convertirse en empresas de alto rendimiento similares al grupo 1 o incluso al 6.Conclusión general: El Grupo 6 es el referente: altamente competitivo, innovador y sostenible. El Grupo 4 y parte del Grupo 5 enfrentan los mayores desafíos estructurales. Los Grupos 1 y 7 son prometedores si logran acelerar su digitalización. Los Grupos 2 y 3 podrían especializarse o asociarse para sobrevivir en un mercado cada vez más concentrado y tecnológico. En conjunto, el análisis sugiere que la digitalización se convierte en el factor diferenciador clave para la supervivencia y crecimiento en el transporte de mercancías interestelar, actuando como catalizador de la fidelidad y la diversificación.El análisis gráfico anterior se puede complementar de un modo más formal, fin de verificar si las diferencias observadas entre los valores medios de los grupos, para cada variable, son significativas. Para ello, y teniendo en cuenta que los grupos se pueden considerar submuestras que representan subpoblaciones, se puede aplicar alguna prueba de comparaciones múltiples de las medias de los grupos formados, de modo que se pueda confirmar, para cierta significación estadística (usualmente 0,05), si las diferencias en las medias de los grupos para cada variable son estadísticamente relevantes (significativas) o .Una prueba clásica para llevar cabo esta tarea es aplicar el test de comparaciones múltiples de Tuckey. obstante, y dado que esta prueba requiere del cumplimiento de ciertos requisitos previos (normalidad de los grupos, varianzas homogéneas); hemos optado por la prueba robusta de Kruskal-Wallis. Para cada una de las variable originales, realizaremos un gráfico múltiple de diagramas de caja, y procederemos mostrar los resultados de la prueba.En primer lugar, vamos definir el vector con el nombre de las variables que entran en el análisis (vector “variables”), e inicializaremos las listas para guardar los gráficos y los resultados de la prueba para cada variable (“graficos_kw” y “tablas_kw”, respectivamente):Luego, haremos un bucle para que se realice el gráfico de caja para cada una de las variables. Los gráficos se almacenan en la lista “graficos_kw”, que luego pasan la función create_patchwork()para agruparse en elementos de la lista “gruposgraficos_kw” (que solo tendrá un elemento, puesto que únicamente hay tres gráficos que agrupar en composiciones de 4 o menos.En cuanto la prueba de comparaciones múltiples, la función que se ocupa de llevarla cabo es kruskalmc(), del paquete pgirmess. la solución se guarda en un objeto, por ejemplo, “datos_kmc”, y los argumentos son, por un lado, la variable analizada y el factor que se utiliza para denominar los grupos (“whatcluster_k”), unidos por el símbolo “~”. La función se aplica dentro del bucle que permite aplicar la prueba cada una de las tres variables. Además, los resultados se pasan en tablas que son almacenadas en la lista “tablas_kw”:partir del código anterior se obtienen los siguientes resultados, concernientes al análisis de comparaciones múltiples (de medias) de Kruskal-Wallis. Cada fila compara dos grupos. Las columnas indican:Diferencias centros: diferencia absoluta entre los rangos medios de esos dos grupos.Diferencias centros: diferencia absoluta entre los rangos medios de esos dos grupos.Diferencias críticas: umbral mínimo que esa diferencia debe superar para considerarse significativa.Diferencias críticas: umbral mínimo que esa diferencia debe superar para considerarse significativa.Significación (TRUE/FALSE): indica si la diferencia es estadísticamente significativa (TRUE) o (FALSE).Significación (TRUE/FALSE): indica si la diferencia es estadísticamente significativa (TRUE) o (FALSE).En el caso de la variable IDIVERSE (grado de diversificación del negocio), se aprecia que los grupos 1 y 6, con los mayores valores medios de diversificación (~35 puntos), difieren entre sí, pero sí del resto. Son empresas altamente diversificadas, posiblemente grandes corporaciones con presencia en múltiples rutas galácticas y segmentos de carga. Por otro lado, los grupos 4 y 5, con valores muy bajos (≈7–11), se sitúan en el extremo opuesto, significativamente diferentes de casi todos los demás. Representan compañías especializadas o locales, con operaciones limitadas y modelos menos flexibles. Los grupos 2, 3 y 7 ocupan posiciones intermedias, sin diferencias significativas entre ellos. Podrían estar en una fase de transición o exploración de nuevos mercados, con estrategias aún en consolidación.En cuanto la variable IFIDE (grado de fidelización de los clientes del negocio), se puede deducir que los grupos 2 y 6 son los “campeones de fidelización”: muestran diferencias significativas entre sí, pero sí con casi todos los demás grupos. Representan empresas muy consolidadas comercialmente, con redes de clientes leales. Económicamente, disfrutan de ingresos estables y bajos costes de adquisición de clientes, lo que les confiere ventaja competitiva sostenida. Por otro lado, los grupos 1 y 7 exhiben una “alta fidelidad consolidada”, similar entre ambos, sin diferencias significativas con los campeones (2 y 6) en algunos casos. Estas compañías probablemente combinan buen servicio y reputación con cierto margen para mejorar la digitalización o la personalización de su oferta. Adicionalmente, el grupo 5 se caracteriza por una fidelidad media, es decir, alcanzan un nivel aceptable, pero significativamente inferior al de los grupos líderes. Finalmente, los grupos 3 y 4 adolecen de una fidelización débil. difieren entre sí, pero sí del resto. Son, en general grupos formados por empresas con poca orientación al cliente o escaso conocimiento del mismo, probablemente con modelos operativos más básicos y precios competitivos. Desde una perspectiva de futuro, son las más vulnerables la pérdida de clientes y la competencia.PO último, en relación con la variable IDIG (grado de digitalización de cada empresa: integración de sistemas inteligentes, automatización logística, uso de IA en gestión de flotas, plataformas de clientes, etc.), podemos distinguir cuatro niveles tecnológicos:Grupo 6 (media 33.7). Difere significativamente de casi todos los grupos, menos 1, 2 y 3. Representa las empresas más avanzadas tecnológicamente, probablemente grandes corporaciones con sistemas integrados de navegación, inteligencia artificial en mantenimiento predictivo y plataformas de comercio galáctico.Grupo 6 (media 33.7). Difere significativamente de casi todos los grupos, menos 1, 2 y 3. Representa las empresas más avanzadas tecnológicamente, probablemente grandes corporaciones con sistemas integrados de navegación, inteligencia artificial en mantenimiento predictivo y plataformas de comercio galáctico.Grupos 1 (22.2), 2 (26.0), 3 (21.8). hay diferencias significativas entre ellos, lo que indica un bloque intermedio tecnológicamente.Han avanzado en digitalización (probablemente con sistemas de trazabilidad o automatización parcial), pero aún alcanzan la madurez total del grupo 6.Grupos 1 (22.2), 2 (26.0), 3 (21.8). hay diferencias significativas entre ellos, lo que indica un bloque intermedio tecnológicamente.Han avanzado en digitalización (probablemente con sistemas de trazabilidad o automatización parcial), pero aún alcanzan la madurez total del grupo 6.Grupo 7 (14.8). Difiere significativamente de casi todos los grupos más avanzados, pero de 3. Empresas con digitalización parcial o básica (gestión digital de clientes o control de flota limitado).Grupo 7 (14.8). Difiere significativamente de casi todos los grupos más avanzados, pero de 3. Empresas con digitalización parcial o básica (gestión digital de clientes o control de flota limitado).Grupos 4 (7.8) y 5 (6.8). difieren entre sí, pero sí de todos los demás.Son los más rezagados tecnológicamente: probablemente dependientes de procesos manuales, escasa automatización y baja conectividad.Grupos 4 (7.8) y 5 (6.8). difieren entre sí, pero sí de todos los demás.Son los más rezagados tecnológicamente: probablemente dependientes de procesos manuales, escasa automatización y baja conectividad.Finalmente, es interesante mostrar cómo los elementos de cada grupo se disponen en los diagramas de dispersión producto de cruzar las variables clasificadoras entre sí. Para generar estos gráficos de un modo automatizado, primero generaremos un vector con el nombre de todas las variables (excluyendo al factor whatcluster_k mediante la función setdiff(). Luego, crearemos una lista para ir almacenando los gráficos (lista “graficos”).continuación, calcularemos todas las combinaciones de nombres de variables posibles, con la función combn(), y las almacenaremos en la lista “combinaciones”. En esta función, el argumento simplify = FALSE le dice la función que vuelque el resultado una matriz:Una vez almacenados los elementos anteriores, procederemos crear los gráficos de dispersión mediante un bucle. Estos gráficos se guardarán en la lista “graficos”, que se pasará por la función create_patchwork() para que se sinteticen en composiciones de 4 (o menos) gráficos. Estas composiciones se almacenan en la lista “gruposgraficos”, y se presentan mediante un nuevo bucle:Los gráficos de dispersión muestran con claridad la estructura espacial de los siete clústeres y su correspondencia con los niveles de diversificación, fidelización y digitalización de las empresas. Los grupos 4 y 5 se concentran en la zona inferior izquierda de los diagramas, caracterizados por los valores más bajos en todas las variables —empresas con escasa diversificación y digitalización, y menor fidelidad de clientes—, mientras que el grupo 6 se sitúa de forma destacada en el extremo superior derecho, evidenciando su liderazgo tecnológico y su alta diversificación. Los grupos 1 y 7 se ubican en posiciones intermedias, aunque algo más desplazados hacia niveles medios-altos de fidelidad y diversificación, lo que los perfila como empresas con potencial de crecimiento. Finalmente, los grupos 2 y 3, más concentrados en la parte superior central, presentan altos niveles de fidelización y moderada digitalización, pero con menor dispersión en diversificación, lo que sugiere estrategias más focalizadas.En conjunto, los diagramas reflejan relaciones positivas entre las tres variables, especialmente entre diversificación y digitalización (IDIVERSE–IDIG), donde se observa un patrón casi lineal: las empresas más diversificadas tienden también estar más digitalizadas. La relación entre fidelización y digitalización (IFIDE–IDIG) es más dispersa, aunque se vislumbra una pendiente positiva: la digitalización parece acompañarse de cierta mejora en la fidelidad de los clientes. En cambio, la asociación fidelidad–diversificación (IFIDE–IDIVERSE) resulta más compleja, con un grupo principal en torno valores medios-altos de fidelización independientemente de la diversificación, lo que podría indicar que la fidelidad depende directamente de la amplitud del negocio, sino de otros factores como la calidad del servicio o la relación cliente–empresa.","code":"\n# CARACTERIZANDO GRUPOS FORMADOS\n  \n  # Tabla con centroides de grupos.\n  tablamedias <- seleccion_so %>%\n    group_by(whatcluster_k) %>%\n    summarise(obs = length(whatcluster_k),\n              Idiverse = mean(IDIVERSE),\n              Ifide = mean(IFIDE),\n              Idig = mean(IDIG))\n  \n  tablamedias %>%\n    kable(caption = \"Método de k-medias. 7 grupos. Medias de variables\",\n          col.names = c(\"Clúster\",\n                        \"Observaciones\",\n                        \"I. Diversif.\",\n                        \"I. Fidelizac.\",\n                        \"I. Digitalizac.\"),\n          digits = c(NA, 0, 3, 3, 3),\n          format.args = list(decimal.mark = \".\",\n                             scientific = FALSE)) %>%\n    kable_styling(full_width = F,\n                  bootstrap_options = \"striped\",\n                  \"bordered\",\n                  \"condensed\",\n                  position = \"center\",\n                  font_size = 11) %>%\n    row_spec(0, bold= T,\n             align = \"c\") %>%\n    row_spec(1:nrow(tablamedias),\n             bold= F,\n             align = \"c\")\n# Gráficos de centroides\n  \n  # Vector de nombre de variables excluyendo la variable no deseada\n  variables <- setdiff(names(tablamedias), c(\"whatcluster_k\", \"obs\"))\n  \n  # Lista para almacenar los gráficos\n  graficos.centroides <- list()\n  \n  # Bucle para crear y almacenar los gráficos\n  for (i in seq_along(variables)) {\n    var1 <- variables[[i]]\n    grafico <- ggplot(data= tablamedias,\n                      map = (aes_string(y = var1, x = \"whatcluster_k\"))) +\n      geom_bar(stat = \"identity\",\n               colour = \"red\",\n               fill = \"orange\",\n               alpha = 0.7) +\n      ggtitle(paste0(var1, \". Media por grupos.\"),\n              subtitle = \"Empresas TMI.\")+\n      xlab (\"Grupo\") +\n      ylab(var1)\n    graficos.centroides[[paste0(\"grafico_\", var1)]] <- grafico\n  }          \n  # Aplicar función de composiciones a gráficos de centroides.\n  grupos.graficos.centroides <- create_patchwork(graficos.centroides)\n  \n  # Presentar las composiciones\n  for (n in 1:length(grupos.graficos.centroides)){\n    print(grupos.graficos.centroides[[n]])\n  }\n  # ¿Centroides estadísticamente significativos?\n\n    # Vector de nombre de variables excluyendo la variable no deseada\n      variables <- setdiff(names(seleccion_so), \"whatcluster_k\")\n\n    # Inicializar listas para almacenar gráficos y tablas\n      graficos_kw <- list()\n      tablas_kw <- list()\n  # ¿Diferencias entre centroides estadísticamente significativas?\n\n   # Vector de nombre de variables excluyendo la variable no deseada\n      variables <- setdiff(names(seleccion_so), \"whatcluster_k\")\n\n   # Bucle para generar gráficos\n     for (i in seq_along(variables)) {\n          variable <- variables[i]\n  \n    # Crear el gráfico\n      p <- ggplot(data = seleccion_so,\n                  aes_string(x = \"whatcluster_k\",\n                             y = variable,\n                             fill = \"whatcluster_k\")) +\n           geom_boxplot(outlier.shape = NA) +\n           stat_summary(fun = \"mean\",\n                        geom = \"point\",\n                        size = 3,\n                        col = \"red\") +\n           stat_summary(fun = \"mean\",\n                        geom = \"line\",\n                        col = \"red\",\n                        aes(group = TRUE)) +   \n           geom_jitter(width = 0.1,\n                       size = 1,\n                       col = \"red\",\n                       alpha = 0.40) +\n           ggtitle(paste(variable, \". Comparación de grupos.\"),\n                   subtitle = \"Empresas TMI.\") +\n           ylab(\"Valor\")\n  \n    # Almacenar el gráfico en la lista\n      graficos_kw[[i]] <- p\n    }\n\n    # Crear composiciones de gráficos.\n            gruposgraficos_kw <- create_patchwork(graficos_kw)    \n\n            for (i in seq_along(gruposgraficos_kw)) {\n              print(gruposgraficos_kw[[i]])\n    }   \n    # Realizar el análisis de Kruskal-Wallis  y llevar resultados a tablas\n\n      tablas_kw <- list()\n      for (i in seq_along(variables)) {\n         variable <- variables[i]\n         datos_kmc <- kruskalmc(as.formula(paste(variable, \"~ whatcluster_k\")),\n                                 data = seleccion_so)\n              \n         tabla <- datos_kmc$dif.com %>%\n         kable(caption = paste(\"k-medias. Diferencias de Centroides\", variable),\n               col.names = c(\"Diferencias centros\",\n                             \"Diferencias críticas\",\n                             \"Significación\"),\n               digits = c(3, 3, NA),\n               format.args = list(decimal.mark = \".\",\n                                  scientific = FALSE)) %>%\n         kable_styling(full_width = F,\n                bootstrap_options = c(\"striped\",\n                                      \"bordered\",\n                                      \"condensed\"),\n                position = \"center\",\n                font_size = 11) %>%\n         row_spec(0, bold = T, align = \"c\") %>%\n         row_spec(1:nrow(datos_kmc$dif.com), bold = F, align = \"c\")\n\n    # Almacenar la tabla en la lista\n      \n      tablas_kw[[i]] <- tabla\n      }\n  \n    # Mostrar tablas almacenadas\n\n      for (i in seq_along(tablas_kw)) {\n           print(tablas_kw[[i]])\n      }\n# GRÁFICOS Variable vs Variable\n\n  # Lista de variables excluyendo la variable no deseada\n    variables <- setdiff(names(seleccion_so), \"whatcluster_k\")\n\n  # Lista para almacenar los gráficos\n    graficos <- list()\n\n  # Generar todas las combinaciones posibles de pares de variables\n    combinaciones <- combn(variables, 2, simplify = FALSE)\n  # Bucle para crear y almacenar los gráficos\n    for (i in seq_along(combinaciones)) {\n         var1 <- combinaciones[[i]][1]\n         var2 <- combinaciones[[i]][2]\n         grafico <- ggplot(seleccion_so,\n                           map = aes_string(x = var1,\n                                            y = var2,\n                                            color = \"whatcluster_k\")) +\n         geom_point() +\n         labs(title = paste(\"GRÁFICO\", var1, \"-\", var2),\n              subtitle = \"Empresas eólicas\") +\n         xlab (var1) +\n         ylab (var2) +\n         scale_color_brewer(palette = \"Set1\") \n         graficos[[paste0(\"grafico_\", var1, \"_\", var2)]] <- grafico\n    }\n\n  # Aplicar función de composiciones de patchwork\n\n    gruposgraficos <- create_patchwork(graficos)\n\n  # Presentar las composiciones\n    for (n in 1:length(gruposgraficos)){\n         print(gruposgraficos[[n]])\n    }"},{"path":"análisis-clúster..html","id":"clustering-basado-en-densidad-dbscan.","chapter":"7 Análisis Clúster.","heading":"7.4  Clustering basado en densidad (DBSCAN).","text":"El método DBSCAN —acrónimo de Density-Based Spatial Clustering Applications Noise— pertenece la familia de algoritmos de agrupación basados en densidad.diferencia de los métodos jerárquicos o de partición, que buscan minimizar distancias o varianzas internas, DBSCAN identifica regiones del espacio donde los puntos están suficientemente “apiñados” y las considera clústeres naturales, mientras que los puntos aislados son clasificados como ruido o outliers.La idea central es muy intuitiva. Imaginemos que recorremos el espacio de datos con un círculo (en tres dimensiones, una esfera) de radio eps. Si ese círculo abarca al menos minPts observaciones, el punto central se considera parte de una zona densa. Si la densidad se propaga los vecinos, se forma un clúster; si , el punto se etiqueta como ruido.Así, el algoritmo forma clústeres partir de regiones densas y deja fuera los puntos dispersos.En este método, cada punto o caso se clasifica según su vecindad:Punto núcleo (core point): tiene al menos minPts vecinos una distancia menor que eps.Punto borde (border point): cumple esa condición, pero pertenece la vecindad de un punto núcleo.Ruido: punto que pertenece ningún clúster.El proceso se repite hasta que todos los puntos han sido etiquetados.\nDe este modo, DBSCAN necesita especificar el número de clústeres priori: el número total emerge de los patrones de densidad del propio conjunto de datos.Los dos hiperparámetros clave de la técnica son:Como ventajas frente los métodos jerárquicos y de k-medias podemos destacar:requiere fijar k: diferencia de k-medias, donde el número de grupos se define manualmente, DBSCAN detecta automáticamente cuántos clústeres existen.requiere fijar k: diferencia de k-medias, donde el número de grupos se define manualmente, DBSCAN detecta automáticamente cuántos clústeres existen.Identifica ruido y outliers : los métodos clásicos fuerzan la asignación de todos los casos algún grupo; DBSCAN puede dejarlos fuera, lo que resulta muy útil cuando hay observaciones anómalas.Identifica ruido y outliers : los métodos clásicos fuerzan la asignación de todos los casos algún grupo; DBSCAN puede dejarlos fuera, lo que resulta muy útil cuando hay observaciones anómalas.Detecta clústeres de forma irregular: k-medias tiende producir grupos esféricos; DBSCAN permite clústeres de formas arbitrarias y de tamaños distintos.Detecta clústeres de forma irregular: k-medias tiende producir grupos esféricos; DBSCAN permite clústeres de formas arbitrarias y de tamaños distintos.Escala bien con datos medianos o grandes: es eficiente para conjuntos de varios miles de observaciones.Escala bien con datos medianos o grandes: es eficiente para conjuntos de varios miles de observaciones.En cuanto sus limitaciones, y las precauciones que se han de tener en cuenta, hemos de advertir:Sensibilidad los parámetros eps y minPts : su resultado depende fuertemente de estos valores. Si eps es demasiado pequeño, surgen muchos puntos de ruido; si es demasiado grande, los clústeres se fusionan.Sensibilidad los parámetros eps y minPts : su resultado depende fuertemente de estos valores. Si eps es demasiado pequeño, surgen muchos puntos de ruido; si es demasiado grande, los clústeres se fusionan.Dificultad en densidades heterogéneas: cuando existen regiones con densidades muy distintas, un único eps puede ser inadecuado para todas (en esos casos, el algoritmo HDBSCAN, su versión jerárquica, ofrece mejores resultados).Dificultad en densidades heterogéneas: cuando existen regiones con densidades muy distintas, un único eps puede ser inadecuado para todas (en esos casos, el algoritmo HDBSCAN, su versión jerárquica, ofrece mejores resultados).Escalado y distancia: requiere que las variables estén en la misma escala y que la métrica de distancia sea representativa del problema.Escalado y distancia: requiere que las variables estén en la misma escala y que la métrica de distancia sea representativa del problema.Una de las mayores virtudes de DBSCAN es su robustez ante outliers. En los métodos jerárquicos o de partición (k-medias, Ward), un punto extremo puede distorsionar el cálculo de distancias o medias y alterar significativamente la estructura de los grupos. En cambio, DBSCAN identifica explícitamente los puntos atípicos como ruido, y los utiliza para formar ni expandir clústeres. Dicho de otro modo: los outliers influyen en los límites de los clústeres; y el algoritmo intenta “forzarlos” pertenecer algún grupo, sino que los deja fuera de la estructura principal. Esto convierte DBSCAN en una técnica intrínsecamente más robusta frente observaciones anómalas, especialmente cuando los datos han sido previamente tipificados.En el ejemplo práctico volveremos utilizar la misma muestra de 300 empresas de transporte de mercancías interestelar empleada en el apartado de k-medias. Las variables clasificadoras son, de nuevo, los tres índices ya conocidos: diversificación (IDIVERSE), fidelización de clientes (IFIDE) y digitalización (IDIG).Trabajaremos dentro del proyecto denominado “cluster”, donde guardaremos los dos archivos necesarios para esta práctica: el archivo de datos en formato Microsoft® Excel® (interestelar_300.xlsx) y el script de R denominado dbscan_rstars.R. En la última sección del capítulo se facilitan los enlaces ambos materiales.","code":""},{"path":"análisis-clúster..html","id":"preparación-de-los-datos.-2","chapter":"7 Análisis Clúster.","heading":"7.4.1  Preparación de los datos.","text":"La primera parte del script reproduce exactamente el mismo código que vimos en el ejemplo de k-medias: limpieza de Global Environment, activación de paquetes, código de la función create_patchwork(), importación de los datos, selección y presentación de las variables del análisis, tratamiento de missing values e identificación de outliers:Aunque el script incluye el código para detectar y eliminar posibles missing values, el gráfico generado por vis_miss() confirma que existen datos ausentes en las tres variables de interés. Por tanto, el data frame “seleccion” mantiene los 300 casos originales:En la detección de outliers, se calcularon las distancias de Mahalanobis y se representaron mediante un boxplot. El gráfico evidenció la presencia de casos atípicos, que fueron identificados mediante el filtro habitual (26 observaciones):partir de aquí iniciaremos un procedimiento de análisis diferente. DBSCAN es una técnica robusta frente los outliers, ya que estos afectan la formación de los clústeres, sino que el propio algoritmo los identifica y clasifica como “ruido”. Por tanto, mantendremos las 300 observaciones en el análisis.","code":"\n### CLUSTER por algoritmo DBScan empresas TMI. ###\n\n# Limpiando el Global Environment\nrm(list = ls())\n\n# Cargando paquetes\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gtExtras)\nlibrary(visdat)\nlibrary(dbscan)\nlibrary (knitr)\nlibrary (kableExtra)\nlibrary (patchwork)\n\n##### Función para crear composiciones de gráficos con patchwork ###############\ncreate_patchwork <- function(plot_list) {\n  n <- length(plot_list)\n  if (n == 0) return(NULL)\n  full_rows <- n %/% 4\n  remaining <- n %% 4\n  patchworks <- list()\n  \n  if (full_rows > 0) {\n    for (i in seq(1, full_rows * 4, by = 4)) {\n      patchworks <- c(patchworks, list((plot_list[[i]] + plot_list[[i+1]]) / \n                                         (plot_list[[i+2]] + plot_list[[i+3]])))\n    }\n  }\n  \n  if (remaining > 0) {\n    last_plots <- plot_list[(full_rows * 4 + 1):n]\n    empty_plots <- lapply(1:(4 - remaining), function(x) ggplot() + theme_void())\n    last_patchwork <- do.call(patchwork::wrap_plots, c(last_plots, empty_plots))\n    patchworks <- c(patchworks, list(last_patchwork))\n  }\n  return(patchworks)\n}\n################################################################################\n\n## DATOS\n\n# Importando datos desde Excel\ninterestelar_300 <- read_excel(\"interestelar_300.xlsx\",\n                               sheet = \"Datos\",\n                               na = c(\"n.d.\"))\ninterestelar_300 <- data.frame(interestelar_300, row.names = 1)\n\n# Seleccionando variables metricas para el analisis.\nseleccion <- interestelar_300 %>%\n  select(IDIVERSE, IFIDE, IDIG)\nseleccion_df_graph <- gt_plt_summary(seleccion)\nseleccion_df_graph\n\n# Localizando missing values.\nseleccion %>%\n  vis_miss() +\n  labs(title = \"Indicadores: Diversificación, Fidelidad, Digitalización\",\n       subtitle = \"Transporte de mercancías interestelar\",\n       y = \"Observación\",\n       fill = NULL) +\n  scale_fill_manual(\n    values = c(\"TRUE\" = \"red\", \"FALSE\" = \"grey\"),\n    labels = c(\"TRUE\" = \"NA\", \"FALSE\" = \"Presente\")) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14))\n\nseleccion %>% filter(is.na(IDIVERSE) |\n                       is.na(IFIDE) |\n                       is.na(IDIG)) %>%\n  select(IDIVERSE, IFIDE, IDIG)\nseleccion <- seleccion %>%\n  filter(! is.na(IDIVERSE) &\n           ! is.na(IFIDE) &\n           ! is.na(IDIG)) \n\n# Identificando outliers con distancia de Mahalanobis.\nseleccion <- seleccion %>%\n  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),\n                                   center = colMeans(.),\n                                   cov    = cov(.)))\n\nggplot(data = seleccion, map = (aes(y = MAHALANOBIS))) +\n  geom_boxplot(fill = \"orange\") +\n  ggtitle(\"DISTANCIA DE MAHALANOBIS\",\n          subtitle = \"IDIVERSE, IFIDE, IDIG. Empresas TMI.\") +\n  ylab(\"MAHALANOBIS\")\n\nQ1M <- quantile (seleccion$MAHALANOBIS, c(0.25))\nQ3M <- quantile (seleccion$MAHALANOBIS, c(0.75))\n\nseleccion %>%\n  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |\n           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%\n  select(MAHALANOBIS, IDIVERSE, IFIDE, IDIG)\n\n# Eliminando variable MAHALANOBIS del df con outliers\nseleccion <- seleccion %>% select(-MAHALANOBIS)"},{"path":"análisis-clúster..html","id":"aplicación-de-la-técnica.","chapter":"7 Análisis Clúster.","heading":"7.4.2  Aplicación de la técnica.","text":"Antes de calcular distancias, es imprescindible que todas las variables estén en la misma escala. Si lo hacemos, aquellas con un rango numérico más amplio dominarán el cálculo de distancias y distorsionarán la formación de los grupos. Para evitarlo, aplicamos una tipificación estándar (z-score) cada variable: restamos su media y dividimos entre su desviación típica.Como ya se precisó, DBSCAN depende de dos parámetros principales que determinan cómo se definen las regiones densas del espacio de datos:minPts: número mínimo de observaciones que deben encontrarse dentro de un vecindario para que se considere suficientemente denso como para formar parte de un clúster.minPts: número mínimo de observaciones que deben encontrarse dentro de un vecindario para que se considere suficientemente denso como para formar parte de un clúster.eps: radio del vecindario, es decir, la distancia máxima que define si dos puntos son vecinos.eps: radio del vecindario, es decir, la distancia máxima que define si dos puntos son vecinos.En la práctica, una regla empírica útil es minPts = 2 × p, donde p es el número de variables métricas. Este valor garantiza que el algoritmo considere suficiente densidad en función de la dimensionalidad del problema. Como en nuestro caso p = 3, usaremos minPts = 6.Además, definiremos una proporción objetivo de ruido, que representa el porcentaje aproximado de observaciones que esperamos que el algoritmo clasifique como outliers o pertenecientes ningún clúster. partir de este nivel deseado de ruido, el procedimiento buscará automáticamente el valor de eps que lo produzca.En este bloque de código, los parámetros cumplen las siguientes funciones:minPts0: define la densidad mínima requerida para formar un clúster (en este caso, seis puntos).minPts0: define la densidad mínima requerida para formar un clúster (en este caso, seis puntos).ruido_obj: fija la proporción deseada de observaciones que se considerarán ruido o casos atípicos.ruido_obj: fija la proporción deseada de observaciones que se considerarán ruido o casos atípicos.tol_ruido: margen de tolerancia aceptado respecto al valor objetivo de ruido (±2 puntos porcentuales).tol_ruido: margen de tolerancia aceptado respecto al valor objetivo de ruido (±2 puntos porcentuales).max_iter: número máximo de iteraciones permitidas para ajustar eps y alcanzar la proporción de ruido deseada.max_iter: número máximo de iteraciones permitidas para ajustar eps y alcanzar la proporción de ruido deseada.Para comenzar la búsqueda de un valor adecuado para el radio eps, analizamos la distribución de las distancias al (minPts − 1)-ésimo vecino más cercano de cada observación. Este gráfico, conocido como curva kNN, suele presentar una primera zona plana (puntos en regiones densas) y, partir de cierto valor, un “codo” o cambio brusco de pendiente que marca la transición hacia las zonas menos densas o de ruido. Ese punto de inflexión nos orienta sobre el rango de valores razonables para eps.partir de esta información inicial, el siguiente paso consistirá en acotar automáticamente la búsqueda dentro de un intervalo que incluya tanto la zona densa como la de transición.EL código anterior calcula un rango inicial de búsqueda para el parámetro eps partir de las distancias entre observaciones más cercanas. La función kNNdist() (del paquete dbscan) obtiene, para cada punto del conjunto tipificado “zseleccion”, la distancia su (minPts − 1)-ésimo vecino más próximo. Estas distancias reflejan cuán denso es el entorno de cada observación: los puntos en zonas compactas tienen distancias pequeñas, mientras que los puntos aislados presentan distancias más grandes.continuación:cknn_d guarda todas esas distancias.cknn_d guarda todas esas distancias.d_sorted las ordena de menor mayor para facilitar su análisis.d_sorted las ordena de menor mayor para facilitar su análisis.eps_lo y eps_hi definen, respectivamente, los límites inferior y superior del rango dentro del cual se buscará el valor óptimo de eps.eps_lo y eps_hi definen, respectivamente, los límites inferior y superior del rango dentro del cual se buscará el valor óptimo de eps.El límite inferior (eps_lo) se fija ligeramente por debajo de la distancia mínima observada. El límite superior (eps_hi) se amplía un 50 % por encima del valor máximo, para garantizar que el rango incluya tanto las zonas densas como las menos densas del espacio de datos.En conjunto, este paso permite acotar de forma inteligente la búsqueda de eps, evitando probar valores arbitrarios y asegurando que el algoritmo explore un intervalo realista basado en la estructura de los propios datos.En el paso o código siguiente se define una función auxiliar denominada noise_rate(), que servirá para evaluar la proporción de ruido generada por el modelo DBSCAN para distintos valores del parámetro eps. Su objetivo es permitirnos ajustar automáticamente el radio óptimo que produce el nivel de ruido deseado:La lógica del código anterior es sencilla. Cada vez que se ejecute noise_rate() con un valor concreto de eps, la función aplicará internamente el algoritmo dbscan() sobre el conjunto de datos tipificados “zseleccion”. Se mantienen constantes el parámetro minPts (almacenado en minPts0) y el número de variables.El argumento eps se actualiza en cada llamada, de modo que se prueban distintos radios de vecindad. La función calcula el porcentaje de observaciones clasificadas como ruido, es decir, aquellas que el modelo identifica con la etiqueta cluster = 0.La expresión mean(fit$cluster == 0) devuelve el proporción de puntos fuera de los clústeres, ya que cuenta cuántos casos cumplen esa condición y los divide entre el total.En resumen, este paso crea una herramienta compacta que nos permitirá, más adelante, probar de forma sistemática diferentes valores de eps y comprobar qué proporción de ruido generan. Gracias ello, podremos automatizar la elección de eps en lugar de depender exclusivamente de la inspección visual del gráfico kNN.En le siguiente paso pretendemos encontrar de forma automática el valor de eps que produzca una proporción de ruido cercana al objetivo fijado (ruido_obj). Para ello, este bloque de código implementa una búsqueda iterativa que ajusta progresivamente el valor de eps hasta aproximarse al nivel deseado de ruido. El procedimiento se basa en una estrategia conocida como búsqueda binaria (binary search), un método eficiente que va dividiendo el intervalo de búsqueda por la mitad en cada paso, descartando la zona que cumple el criterio.En este código:Se crea primero el data frame “tested”, que almacenará los pares de valores eps y la proporción de ruido (ruido) obtenidos en cada iteración.Se crea primero el data frame “tested”, que almacenará los pares de valores eps y la proporción de ruido (ruido) obtenidos en cada iteración.En cada paso del bucle :\nSe calcula un valor intermedio del radio, eps_mid, como el punto medio entre los límites actuales del rango (eps_lo y eps_hi).\nSe ejecuta la función noise_rate() para ese valor de eps, obteniendo la proporción de ruido r_mid.\nEl resultado se guarda en el data frame “tested” para poder consultarlo o graficarlo posteriormente.\nEn cada paso del bucle :Se calcula un valor intermedio del radio, eps_mid, como el punto medio entre los límites actuales del rango (eps_lo y eps_hi).Se calcula un valor intermedio del radio, eps_mid, como el punto medio entre los límites actuales del rango (eps_lo y eps_hi).Se ejecuta la función noise_rate() para ese valor de eps, obteniendo la proporción de ruido r_mid.Se ejecuta la función noise_rate() para ese valor de eps, obteniendo la proporción de ruido r_mid.El resultado se guarda en el data frame “tested” para poder consultarlo o graficarlo posteriormente.El resultado se guarda en el data frame “tested” para poder consultarlo o graficarlo posteriormente.continuación, el código compara el ruido obtenido r_mid con el ruido objetivo ruido_obj:\nSi la diferencia está dentro de la tolerancia admitida (tol_ruido), el bucle se detiene porque se ha encontrado un eps adecuado.\nSi el ruido actual es demasiado alto, significa que los clústeres son demasiado pequeños; para corregirlo, se aumenta eps, lo que amplía los vecindarios y une más puntos.\nSi el ruido es demasiado bajo, se reduce eps, haciendo los clústeres más estrictos y separando más puntos como atípicos.\ncontinuación, el código compara el ruido obtenido r_mid con el ruido objetivo ruido_obj:Si la diferencia está dentro de la tolerancia admitida (tol_ruido), el bucle se detiene porque se ha encontrado un eps adecuado.Si la diferencia está dentro de la tolerancia admitida (tol_ruido), el bucle se detiene porque se ha encontrado un eps adecuado.Si el ruido actual es demasiado alto, significa que los clústeres son demasiado pequeños; para corregirlo, se aumenta eps, lo que amplía los vecindarios y une más puntos.Si el ruido actual es demasiado alto, significa que los clústeres son demasiado pequeños; para corregirlo, se aumenta eps, lo que amplía los vecindarios y une más puntos.Si el ruido es demasiado bajo, se reduce eps, haciendo los clústeres más estrictos y separando más puntos como atípicos.Si el ruido es demasiado bajo, se reduce eps, haciendo los clústeres más estrictos y separando más puntos como atípicos.Este proceso se repite hasta alcanzar la proporción deseada de ruido o hasta completar el número máximo de iteraciones definido en max_iter.El resultado final es un conjunto de valores eps probados con sus correspondientes niveles de ruido, entre los cuales se encontrará el radio más adecuado para aplicar el modelo DBSCAN con los parámetros óptimos.Una vez completadas las iteraciones del paso anterior, disponemos de varios valores de eps junto con las proporciones de ruido que generaron. El siguiente paso consiste en seleccionar, pues, el valor óptimo de eps, es decir, aquel que produce un nivel de ruido más próximo al objetivo fijado (ruido_obj).Este bloque de código realiza esa selección de forma automática y guarda los resultados finales para ser utilizados en el modelo definitivo:En este fragmento de código:“tested” es el data frame generado en la etapa anterior, que contiene todas las combinaciones de eps evaluadas junto con su proporción de ruido.“tested” es el data frame generado en la etapa anterior, que contiene todas las combinaciones de eps evaluadas junto con su proporción de ruido.La instrucción .min(abs(tested$ruido - ruido_obj)) identifica el índice (ix_best) del valor de eps cuya proporción de ruido está más cerca del objetivo deseado. partir de ese índice:\neps_final guarda el valor de radio óptimo que se empleará finalmente en el algoritmo DBSCAN.\nruido_est almacena la proporción real de ruido que produce ese valor óptimo.\nLa instrucción .min(abs(tested$ruido - ruido_obj)) identifica el índice (ix_best) del valor de eps cuya proporción de ruido está más cerca del objetivo deseado. partir de ese índice:eps_final guarda el valor de radio óptimo que se empleará finalmente en el algoritmo DBSCAN.eps_final guarda el valor de radio óptimo que se empleará finalmente en el algoritmo DBSCAN.ruido_est almacena la proporción real de ruido que produce ese valor óptimo.ruido_est almacena la proporción real de ruido que produce ese valor óptimo.Este paso resume todo el proceso de búsqueda en un resultado claro y cuantitativo: el radio eps_final, ajustado para lograr la densidad de agrupación coherente con el nivel de ruido que se considera adecuado para el análisis.Con el valor óptimo de eps ya determinado, podemos ajustar el modelo DBSCAN definitivo. En este paso se aplica el algoritmo utilizando los parámetros seleccionados (eps_final y minPts0) sobre el conjunto de datos tipificados “zseleccion”. El objetivo es obtener la estructura final de clústeres, junto con la asignación de cada observación su correspondiente grupo o al conjunto de ruido.En este bloque:La función dbscan() ejecuta el algoritmo con los parámetros óptimos:\neps = eps_final, el radio de vecindad que determina qué puntos se consideran cercanos.\nminPts = minPts0, el número mínimo de observaciones necesarias en un vecindario para formar un clúster.\nLa función dbscan() ejecuta el algoritmo con los parámetros óptimos:eps = eps_final, el radio de vecindad que determina qué puntos se consideran cercanos.eps = eps_final, el radio de vecindad que determina qué puntos se consideran cercanos.minPts = minPts0, el número mínimo de observaciones necesarias en un vecindario para formar un clúster.minPts = minPts0, el número mínimo de observaciones necesarias en un vecindario para formar un clúster.El resultado se guarda en el objeto “modelo_db”, que contiene toda la información del agrupamiento: número de clústeres detectados, tamaño de cada grupo y proporción de ruido. AL mostrarlo en pantalla, R presenta un resumen con el conteo de puntos por clúster y el porcentaje de observaciones asignadas (ruido).Finalmente, se muestra un resumen sintético de los resultados obtenidos por el modelo DBSCAN y se asocian las etiquetas de clúster las observaciones originales. Este paso permite comprobar de un vistazo la coherencia entre los parámetros elegidos y los resultados alcanzados:En este bloque:La función cat() imprime un resumen limpio con los valores finales:La función cat() imprime un resumen limpio con los valores finales:eps_final, el radio óptimo de vecindad.eps_final, el radio óptimo de vecindad.ruido_obj, la proporción de ruido objetivo definida por el usuario.ruido_obj, la proporción de ruido objetivo definida por el usuario.ruido_est, el porcentaje real de observaciones que el modelo ha clasificado como ruido.ruido_est, el porcentaje real de observaciones que el modelo ha clasificado como ruido.Este breve informe permite verificar si el algoritmo alcanzó el nivel de ruido esperado o si se desvió ligeramente dentro del margen de tolerancia.La función table() genera un recuento del número de observaciones asignadas cada clúster, incluido el clúster 0, que representa las observaciones agrupadas o de baja densidad.Finalmente, la instrucción seleccion$whatcluster_dbs <- .factor(modelo_db$cluster) incorpora al data frame original una nueva columna denominada whatcluster_dbs, que almacena la etiqueta de clúster asignada por DBSCAN cada empresa. Esta variable será muy útil en la siguiente fase del análisis, cuando visualicemos los clústeres y analicemos sus características internas.","code":"\n## APLICACION DE DBSCAN\n\n# 1) Tipificación (z-scores)\nzseleccion <- scale(seleccion)\nzseleccion <- as.matrix(zseleccion)  # dbscan espera matriz/numérico\n# 2) Determinación de minPts y eps\nminPts0 <- 6  # regla general: 2*p (p=3)\nruido_obj <- 0.20 # proporción objetivo de ruido (p. ej., 0.10, 0.20, 0.30)\ntol_ruido <- 0.02 # tolerancia (±2 p.p.)\nmax_iter  <- 30   # tope de iteraciones\n# 3) Rango inicial inteligente para eps usando kNN (k = minPts0-1)\nknn_d <- kNNdist(zseleccion, k = minPts0 - 1)\nd_sorted <- sort(as.numeric(knn_d))\n\neps_lo <- max(min(d_sorted) * 0.9, 1e-6)      # límite inferior\neps_hi <- max(d_sorted) * 1.5                  # límite superior amplio\n# 4) Función auxiliar: calcula proporción de ruido para un eps\nnoise_rate <- function(eps) {\n  fit <- dbscan::dbscan(zseleccion, eps = eps, minPts = minPts0)\n  mean(fit$cluster == 0)\n}\n# 5) Búsqueda binaria para acercarnos a ruido_obj\ntested <- data.frame(eps = numeric(0), ruido = numeric(0))\n\nfor (i in seq_len(max_iter)) {\n  eps_mid <- (eps_lo + eps_hi) / 2\n  r_mid   <- noise_rate(eps_mid)\n  tested  <- rbind(tested, data.frame(eps = eps_mid, ruido = r_mid))\n  \n  if (abs(r_mid - ruido_obj) <= tol_ruido) break\n  if (r_mid > ruido_obj) {\n    # demasiado ruido -> aumentar eps para unir vecindarios y reducir ruido\n    eps_lo <- eps_mid\n  } else {\n    # poco ruido -> disminuir eps para ser más estricto\n    eps_hi <- eps_mid\n  }\n}\n# 6) Elegimos el eps con ruido más cercano a la diana\nix_best   <- which.min(abs(tested$ruido - ruido_obj))\neps_final <- tested$eps[ix_best]\nruido_est <- tested$ruido[ix_best]\n# 7) Modelo final\nmodelo_db <- dbscan::dbscan(zseleccion, eps = eps_final, minPts = minPts0)\nmodelo_db## DBSCAN clustering for 300 objects.\n## Parameters: eps = 0.407873467683963, minPts = 6\n## Using euclidean distances and borderpoints = TRUE\n## The clustering contains 4 cluster(s) and 65 noise points.\n## \n##   0   1   2   3   4 \n##  65   8  10 197  20 \n## \n## Available fields: cluster, eps, minPts, metric, borderPoints\n# 8) Resumen rápido\ncat(\"\\n---\\n\",\n    \"eps_final =\", round(eps_final, 3), \n    \"| ruido_obj =\", scales::percent(ruido_obj),\n    \"| ruido_logrado =\", scales::percent(ruido_est), \"\\n\")## \n## ---\n##  eps_final = 0.408 | ruido_obj = 20% | ruido_logrado = 22%\ntable(Cluster = modelo_db$cluster)## Cluster\n##   0   1   2   3   4 \n##  65   8  10 197  20\nseleccion$whatcluster_dbs <- as.factor(modelo_db$cluster)"},{"path":"análisis-clúster..html","id":"caracterización-de-los-clústeres-o-grupos-formados.","chapter":"7 Análisis Clúster.","heading":"7.4.3  Caracterización de los clústeres o grupos formados.","text":"Una vez obtenidos los clústeres, y asignados los elementos (teniendo en cuenta que el grupo 0\nes realmente un clúster; sino que reúne los casos considerados atípicos, ouliers o ruido), puede aplicarse el mismo código que se empleó en el caso de k-medias la hora de caracterizar tales grupos (omitimos por extender el código la prueba de comparaciones múltiples de Kruskal-Wallis):Con el código anterior, por ejemplo, podemos construir la tabla de número de elementos y medias de cada clúster:\nTable 7.5: Table 7.6: Método DBSCAN. Medias de variables (Grupo 0 = Ruido)\nAdemás, obtenemos los gráficos de las medias de los clústeres y de dispersión de las variables 2 2:\nSe deja al lector, como ejercicio práctico, la interpretación de estos gráficos fin de caracterizar los 4 clústeres identificados (ha de tenerse en cuenta que el grupo 0 es un clúster, sino que recoge los elementos ruido, y por tanto, su interpretación carece de sentido, al menos en térmisnos de valores medios de las variables).Para concluir, conviene hacerse una pregunta clave. ¿Cómo ha clasificado DBSCAN los casos 26 que fueron identificados como outliers? ¿Los ha etiquetado como “ruido”, como sería lo coherente?La siguiente tabla nos muestra la respuesta:En efecto, todas las compañías que fueron identificadas como outliers han sido clasificadas como ruido (es decir, pertenecen ninguno de los clústeres identificados). De hecho, DBSCAN ha sido más exigente, y ha clasificado como ruido otras empresas que fueron localizadas, por el método tradicional, como outliers.","code":"\n# CARACTERIZANDO GRUPOS FORMADOS\n\n# Tabla con centroides de grupos.\ntablamedias <- seleccion %>%\n  group_by(whatcluster_dbs) %>%\n  summarise(obs = length(whatcluster_dbs),\n            Idiverse = mean(IDIVERSE),\n            Ifide = mean(IFIDE),\n            Idig = mean(IDIG))\n\ntablamedias %>%\n  kable(caption = \"Método DBSCAN. Medias de variables (Grupo 0 = Ruido)\",\n        col.names = c(\"Clúster\",\n                      \"Observaciones\",\n                      \"I. Diversif.\",\n                      \"I. Fidelizac.\",\n                      \"I. Digitalizac.\"),\n        digits = c(NA, 0, 3, 3, 3),\n        format.args = list(decimal.mark = \".\",\n                           scientific = FALSE)) %>%\n  kable_styling(full_width = F,\n                bootstrap_options = \"striped\",\n                \"bordered\",\n                \"condensed\",\n                position = \"center\",\n                font_size = 11) %>%\n  row_spec(0, bold= T,\n           align = \"c\") %>%\n  row_spec(1:nrow(tablamedias),\n           bold= F,\n           align = \"c\")\n\n# Gráficos de centroides\n\n# Vector de nombre de variables excluyendo la variable no deseada\nvariables <- setdiff(names(tablamedias), c(\"whatcluster_dbs\", \"obs\"))\n\n# Lista para almacenar los gráficos\ngraficos.centroides <- list()\n\n# Bucle para crear y almacenar los gráficos\nfor (i in seq_along(variables)) {\n  var1 <- variables[[i]]\n  grafico <- ggplot(data= tablamedias,\n                    map = (aes_string(y = var1, x = \"whatcluster_dbs\"))) +\n    geom_bar(stat = \"identity\",\n             colour = \"red\",\n             fill = \"orange\",\n             alpha = 0.7) +\n    ggtitle(paste0(var1, \". Media por grupos.\"),\n            subtitle = \"Empresas TMI.\")+\n    xlab (\"Grupo\") +\n    ylab(var1)\n  graficos.centroides[[paste0(\"grafico_\", var1)]] <- grafico\n}                 \n\n# Aplicar función de composiciones a gráficos de centroides.\ngrupos.graficos.centroides <- create_patchwork(graficos.centroides)\n\n# Presentar las composiciones\nfor (n in 1:length(grupos.graficos.centroides)){\n  print(grupos.graficos.centroides[[n]])\n}\n\n# GRÁFICOS Variable vs Variable\n\n# Lista de variables excluyendo la variable no deseada\nvariables <- setdiff(names(seleccion), \"whatcluster_dbs\")\n\n# Lista para almacenar los gráficos\ngraficos <- list()\n\n# Generar todas las combinaciones posibles de pares de variables\ncombinaciones <- combn(variables, 2, simplify = FALSE)\n\n# Bucle para crear y almacenar los gráficos\nfor (i in seq_along(combinaciones)) {\n  var1 <- combinaciones[[i]][1]\n  var2 <- combinaciones[[i]][2]\n  grafico <- ggplot(seleccion,\n                    map = aes_string(x = var1,\n                                     y = var2,\n                                     color = \"whatcluster_dbs\")) +\n    geom_point() +\n    labs(title = paste(\"GRÁFICO\", var1, \"-\", var2),\n         subtitle = \"Empresas TMI.\") +\n    xlab (var1) +\n    ylab (var2) +\n    scale_color_brewer(palette = \"Set1\") \n  graficos[[paste0(\"grafico_\", var1, \"_\", var2)]] <- grafico\n}\n\n# Aplicar función de composiciones de patchwork\n\ngruposgraficos <- create_patchwork(graficos)\n\n# Presentar las composiciones\nfor (n in 1:length(gruposgraficos)){\n  print(gruposgraficos[[n]])\n}\n# ¿Los outliers son ruido?\n\nseleccion <- seleccion %>%\n  mutate(MAHALANOBIS = mahalanobis(select(., IDIVERSE, IFIDE, IDIG),\n                                   colMeans(select(., IDIVERSE, IFIDE, IDIG)),\n                                   cov(select(., IDIVERSE, IFIDE, IDIG))))\nQ1M <- quantile (seleccion$MAHALANOBIS, c(0.25))\nQ3M <- quantile (seleccion$MAHALANOBIS, c(0.75))\n\nseleccion_out <- seleccion %>%\n  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |\n           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%\n  select(whatcluster_dbs, MAHALANOBIS, IDIVERSE, IFIDE, IDIG)\n\nseleccion_out %>%\n  kable(caption = \"¿Outliers son ruido? (Grupo 0 = Ruido)\",\n        col.names = c(\"Caso\",\n                      \"Observaciones\",\n                      \"Grupo (0=ruido)\",\n                      \"D. Mahalanobis\",\n                      \"I. Diversificación\",\n                      \"I. Fidelizac.\",\n                      \"I. Digitalizac.\"),\n        digits = c(NA, 0, 3, 3, 3),\n        format.args = list(decimal.mark = \".\",\n                           scientific = FALSE)) %>%\n  kable_styling(full_width = F,\n                bootstrap_options = \"striped\",\n                \"bordered\",\n                \"condensed\",\n                position = \"center\",\n                font_size = 11) %>%\n  row_spec(0, bold= T,\n           align = \"c\") %>%\n  row_spec(1:nrow(seleccion_out),\n           bold= F,\n           align = \"c\")\n# ¿Los outliers son ruido?\n\nseleccion <- seleccion %>%\n  mutate(MAHALANOBIS = mahalanobis(select(., IDIVERSE, IFIDE, IDIG),\n                                   colMeans(select(., IDIVERSE, IFIDE, IDIG)),\n                                   cov(select(., IDIVERSE, IFIDE, IDIG))))\nQ1M <- quantile (seleccion$MAHALANOBIS, c(0.25))\nQ3M <- quantile (seleccion$MAHALANOBIS, c(0.75))\n\nseleccion_out <- seleccion %>%\n  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |\n           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%\n  select(whatcluster_dbs, MAHALANOBIS, IDIVERSE, IFIDE, IDIG)\n\ntipo_output <- c(\"html\") # pdf, html, docx\nknitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)\n\nif (knitr::opts_knit$get(\"rmarkdown.pandoc.to\") == \"html\") {\nseleccion_out %>%\n  kable(caption = \"¿Outliers son ruido? (Grupo 0 = Ruido)\",\n        col.names = c(\"Caso\",\n                      \"Observaciones\",\n                      \"Grupo (0=ruido)\",\n                      \"D. Mahalanobis\",\n                      \"I. Diversificación\",\n                      \"I. Fidelizac.\",\n                      \"I. Digitalizac.\"),\n        digits = c(NA, 0, 3, 3, 3),\n        format.args = list(decimal.mark = \".\",\n                           scientific = FALSE)) %>%\n  kable_styling(full_width = F,\n                bootstrap_options = \"striped\",\n                \"bordered\",\n                \"condensed\",\n                position = \"center\",\n                font_size = 11) %>%\n  row_spec(0, bold= T,\n           align = \"c\") %>%\n  row_spec(1:nrow(seleccion_out),\n           bold= F,\n           align = \"c\")\n}else if (knitr::opts_knit$get(\"rmarkdown.pandoc.to\") == \"docx\") {\n  seleccion_out %>%\n  kable(caption = \"¿Outliers son ruido? (Grupo 0 = Ruido)\",\n        col.names = c(\"Caso\",\n                      \"Observaciones\",\n                      \"Grupo (0=ruido)\",\n                      \"D. Mahalanobis\",\n                      \"I. Diversificación\",\n                      \"I. Fidelizac.\",\n                      \"I. Digitalizac.\"),\n        digits = c(NA, 0, 3, 3, 3),\n        format.args = list(decimal.mark = \".\",\n                           scientific = FALSE))\n}"},{"path":"análisis-clúster..html","id":"materiales-para-realizar-las-prácticas-del-capítulo.-5","chapter":"7 Análisis Clúster.","heading":"7.5 Materiales para realizar las prácticas del capítulo.","text":"En esta sección se muestran los links de acceso los diferentes materiales (scripts, datos…) necesarios para llevar cabo los contenidos prácticos del capítulo.Datos (en formato Microsoft (R) Excel (R)):interestelar_25.xlsx (obtener aquí)interestelar_300.xlsx (obtener aquí)Scripts:cluster_rstars.R (obtener aquí)kmedias_rstars.R (obtener aquí)dbscan_rstars.R (obtener aquí)","code":""},{"path":"análisis-de-la-varianza..html","id":"análisis-de-la-varianza.","chapter":"8 Análisis de la varianza.","heading":"8 Análisis de la varianza.","text":"","code":""},{"path":"análisis-de-la-varianza..html","id":"introducción.-4","chapter":"8 Análisis de la varianza.","heading":"8.1 Introducción.","text":"El análisis de la varianza (ANOVA) puede considerarse una generalización del contraste de hipótesis de medias poblacionales iguales para el caso de poblaciones normales y con varianzas desconocidas, pero iguales. La generalización consiste en poder considerar más de dos poblaciones. La hipótesis nula será la que afirma que las medias poblacionales de la variable métrica en estudio, para todas las poblaciones, son iguales. La hipótesis alternativa, por su lado, afirmará que existe al menos dos poblaciones con medias diferentes. Como todo contraste, para llevarlo acabo hemos de tener una muestra representativa de cada población, y fijar un nivel de significación (usualmente 0.05).También se puede considerar el ANOVA como un tipo especial de análisis de regresión, en la que la variable dependiente es una variable métrica, y las variables explicativas son atributos o factores (en escala nominal u ordinal). La misión de los factores es clasificar los casos que constituyen nuestra muestra en distintas submuestras, cada una representativa de una de las subpoblaciones cuyas medias en la variable en estudio se quiere comparar.","code":""},{"path":"análisis-de-la-varianza..html","id":"anova-de-un-solo-factor.","chapter":"8 Análisis de la varianza.","heading":"8.2 ANOVA de un solo factor.","text":"Aunque se pueden realizar ANOVAs con más de un atributo o factor, en este ejemplo nos ceñiremos al caso más simple, en el que solo hay un atributo o factor que se ocupa de distribuir los casos de la muestra entre los distintos grupos o submuestras (partir de las categoría o nivel que toma cada caso).En concreto, en esta práctica, comprobaremos si la dimensión del grupo empresarial al que pertenecen las empresas eólicas (medida en función del número de empresas integradas en el grupo empresarial, y concretada en el factor DIMENSION) tiene una influencia significativa sobre la rentabilidad económica (variable RENECO), en términos medios. Para ello se ha seleccionado una muestra constituida por 50 empresas productoras de electricidad mediante tecnología eólica. Así, la población, constituida por todas las empresas de generación eléctrica eólica de España, queda dividida en tres subpoblaciones: la subpoblación de empresas que pertenecen grupos empresariales de DIMENSION (según el número de filiales contenidas) “GRANDE”, la subpoblación de empresas que pertenecen grupos empresariales de DIMENSION “MEDIA”, y la subpoblación de empresas que pertenecen grupos empresariales de DIMENSION “PEQUEÑA”. Cada una de estas subpoblaciones tendrán sus respectivas rentabilidades económicas medias, que desconocemos (ya que tenemos los datos de la población, es decir, de todas las empresas eólicas del país; sino solo de una muestra de 50 empresas). Lo que si tenemos para cada subpoblación es una submuestra que la representa (parte de las 50 empresas de la muestra, que queda fraccionada en tres según el factor DIMENSION). Y de cada submuestra, tenemos la correspondiente rentabilidad media muestral. Lo que comprobaremos con el contraste de ANOVA, en definitiva, es si las diferencias observadas entre las rentabilidades medias de cada submuestra son lo suficientemente amplias como para pensar que, puede considerarse que existen diferencias importantes (significativas) entre las rentabilidades medias de las subpoblaciones (considerando todas las empresas eólicas que conforman la población).Los datos están almacenados en el archivo de Microsoft Excel “eolica_50.xls”, y el script con el código del ejemplo se halla contenido en el fichero “anova_cluster.R”. Vamos suponer que trabajaremos en un proyecto de RStudio al que denominaremos “anova”.Una vez abierto el script en el editor de RStudio , comprobaremos que la primera parte del código está dedicada la limpieza de la memoria (Environment) y la importación de los datos. Para ello, activaremos el paquete readxl y utilizaremos la función read_excel(), indicando en los argumentos el archivo explorar, y la hoja en la cual se encuentran los datos (hoja “Datos”). También hemos de prestar atención la cuestión de si existen en la hoja de Excel anotaciones en las celdas donde haya dato, para completar adecuadamente el argumento na= . Los datos se almacenarán en el data frame “datos”. En este data frame, la primera columna es una verdadera variable, sino que se compone de los nombres de los casos o empresas. Con una línea de código adicional transformaremos esa primera columna en el nombre de las filas, de modo que tal columna abandona su rol de variable. En definitiva, el código para importar los datos es:Antes de proceder al análisis de la varianza propiamente dicho, hemos de preparar nuestros datos mediante la localización de missing values y outliers. Los outliers, en el ANOVA, pueden tener una gran influencia (ya que se trabaja en términos medios) sobre los resultados, por lo que deben ser tratados convenientemente.Para localizar los casos concretos de missing values, puede recurrirse utilizar las herramientas de manejo de data frames del paquete dplyr. Previamente, realizaremos una copia del data frame original, “datos”, la que llamaremos “muestra”, que es con la que trabajaremos (para mantener la integridad del primer data frame). Con la función vismiss() del paquete visdat podemos tener una visión gráfica general de los valores faltantes, en especial en el caso de la variable RENECO y el factor DIMENSION. Si hay casos faltantes en una de estas variables, los identificaremos filtrando el data frame con la función filter() de dplyr. Ante la existencia de missing values, se puede actuar de varios modos. Por ejemplo, se puede intentar obtener por otro canal de información el conjunto de valores de RENECO que están disponibles, o recurrir alguna estimación para los mismos y asignarlos. En caso de que esto sea difícil, se puede optar, simplemente, por eliminar estos casos, en especial cuando representan un porcentaje muy reducido respecto al total de casos. En nuestro ejemplo, vamos suponer que hemos optado por esta última vía, al conseguir unos valores más o menos verosímiles de RENECO para las empresas de las que se carece de dato. Así, estos casos, finalmente, será exluidos del análisis, utilizando para ello un nuevo filtro:Tras el código anterior, se han eliminado del data frame “muestra” las empresas “Sargon Energías S. L. U.” y “Viesgo Renovables S. L.”, empresas pertenecientes grupos empresariales de dimensión media (DIMENSION), debido que carecían de dato de rentabilidad económica (RENECO).Una vez tratados los casos con valores perdidos o missing values, es necesario detectar la presencia de outliers o casos atípicos en la muestra, que pudieran desvirtuar los resultados derivados del ANOVA. Para ello, realizaremos un boxplot o gráfico de caja. Aplicaremos, por ejemplo, el código siguiente, que utiliza la gramática del paquete ggplot2:En el gráfico de caja se aprecia claramente que existe un outlier. Para identificar tal empresa, calcularemos, respecto la variable RENECO, el primer y tercer cuartiles, que nos servirán para construir el filtro que capturará el caso atípico. Así, en el código, las dos primeras filas calculan los cuartiles primero (Q1) y tercero (Q3) mediante la función quantile(). Luego se filtran, mediante la función filter() de dplyr, los outliers, calculadoscomo aquellos casos con valores de RENECO mayores que Q3 más 1,5 veces el rango intercuartílico de la variable; o menores que Q1 menos 1,5 veces dicho rango intercuartílico. Para calcular el rango intercuartílico se recurre la función IQR(). Finalmente, con select() se muestran los casos en la consola de RStudio:La empresa identificada como outlier es “Molinos del Ebro S. ”. Como ocurría con los missing values, el tratamiento de los outliers depende de la información que se tenga, existiendo varias alternativas (corrección del dato, estimación, etc.) Si se tiene información fiable, y los outliers representan una gran proporción respecto al total de casos, puede optarse por su eliminación de la muestra, como haremos en este ejemplo. Podemos hacerlo creando un nuevo data frame partir de “muestra”; pero sin ese caso. Ese nuevo data frame se llamará, por ejemplo, “muestra_so”:Una vez preparados los datos, vamos presentar los grupos de empresas eólicas y sus rentabilidades económicas medias. Para ello, diseñaremos una tabla con la función kable() del paquete knitr, y personalizada con algunas funciones incluidas en el paquete kableExtra. Para construir la tabla, hemos de crear anteriormente un pequeño data frame, llamado por ejemplo “tablamedias”, en el que cada caso o fila sea uno de los grupos en que queda dividida la muestra partir de los niveles del factor “DIMENSION”, y que contenga tres variables: la DIMENSION de cada grupo o submuestra, su número de casos contenidos (variable “observaciones”), y las respectivas rentabilidades medias (variable “media”):\nTable 8.1: Table 8.2: Rentabilidad Económica. Medias por grupos (tamaño matriz).\nGráficamente, las tres submuestras de empresas pueden caracterizarse, en cuanto la rentabilidad económica (RENECO), mediante dos tipos de gráficos: gráficos de densidad y gráficos de caja. Los gráficos serán construidos utilizando las facilidades del paquete ggplot2. Además, combinaremos los gráficos en una composición utilizando el paquete patchwork. El código es el siguiente:Cada grupo o submuestra “representa” una subpoblación, según la dimensión que tenga la matriz empresarial. El ANOVA lo que intenta determinar es si, observadas las diferencias en las medias muestrales de la variable en estudio (aquí, RENECO), esas diferencias pueden considerarse o estadísticamente significativas nivel poblacional. En realidad, la hipótesis nula contrastar es que ninguna de las diferencias entre las medias de la variable de las subpoblacionales es estadísticamente significativa; mientras que la hipótesis alternativa es que existe al menos una diferencia significativa entre las medias de las subpoblaciones. En definitiva, la hipótesis nula contrastar sugiere que existen diferencias entre las rentabilidades económicas medias de los grupos (subpoblaciones) de empresas eólicas (discriminadas por el tamaño del grupo empresarial al que pertenecen). De ser así, el factor DIMENSION tendría una influencia estadísticamente significativa sobre el valor medio de la variable RENECO.Las conclusiones las que lleguemos con el contraste F de ANOVA serán válidas en la medida en que se cumplan las hipótesis de normalidad y homogeneidad en las varianzas de la variable dependiente o métrica (RENECO), en los tres grupos o subpoblaciones en que queda dividida la población atendiendo los niveles o categorías del factor (DIMENSION). Hemos de contrastar, pues, ambas hipótesis, partir de la información de las tres submuestras que tenemos y que representan, respectivamente, cada una de esas subpoblaciones.En cuanto la hipótesis de normalidad, se pueden usar dos vías para verificar su cumplimiento: el análisis gráfico y la realización de contrastes estadísticos.El análisis gráfico puede llevarse cabo mediante la realización de gráficos QQ de la variable RENECO, partir de las submuestras o grupos de empresas en que queda fragmentada la muestra partir de la variable DIMENSION. Estos gráficos pueden generarse con la gramática del paquete ggplot2:Los gráficos QQ parecen inidicar que la variable RENECO tiene un comportamiento distante la Ley Normal en las submuestras (y, por tanto, en las correspondientes subpoblaciones), al localizarse, algunos de los puntos, relativamente alejados de la diagonal, especialmente en el caso de las empresas pertenecientes matrices de dimensión “grande”. Para extraer una conclusión de un modo más preciso, vamos realizar el contraste de normalidad de Shapiro-Wilk:En el código anterior, se crea un pequeño data frame denominado “normalidad”, que incluye, para cada submuestra definida por los niveles del factor DIMENSION, la variable “decide” con los p-valores de la prueba de Shapiro-Wilk, y un atributo llamado “decide”, creado mediante la función mutate() de dplyr, que adopta la categoría “NORMALIDAD” o “-NORMALIDAD” dependiendo de los p-valores. Posteriormente, el data frame “normalidad” se presenta como una tabla de nombre “tablashapiro”.\nTable 8.3: Table 8.4: Normalidad (Shapiro-Wilks).\nEn esta prueba, la hipótesis nula equivale al supuesto de normalidad. Para un 5% de significación estadística, un p-valor superior 0,05 implicará el -rechazo de la hipótesis nula de normalidad. En el ejemplo, la submuestra de empresas pertenecientes matrices de dimensión “grande” llevan pensar que esta subpoblación sigue una distribución normal. En los otros dos casos, en cambio, podemos aceptar la existencia de normalidad.En cuanto la homogeneidad de las varianzas, contrastamos este supuesto mediante la prueba de Bartlett.El p-valor es menor que 0,05; luego se rechaza la hipótesis nula de homogeneidad de las varianzas de los grupos (subpoblaciones).Puesto que en uno de los grupos (subpoblaciones) se rechaza la hipótesis de normalidad, y (sobre todo) puesto que se puede considerar una dispersión similar en las tres subpoblaciones (varianzas homogéneas), los resultados de la prueba F de ANOVA pierden validez, y habría que optar por una alternativa robusta. obstante, modo ilustrativo, seguiremos adelante con la prueba F de ANOVA.El contraste F de ANOVA de igualdad en las medias de la variable en estudio (rentabilidad económica, RENECO) de las distintas (sub)poblaciones (grupos de empresas según el tamaño del grupo empresarial de pertenencia) se realiza en R mediante la función aov(), que guardaremos, por ejemplo, como el objeto “Datos.aov”, y que se almacenará en forma resumida en la lista “summary_aov”, de un solo elemento. Este elemento, que contiene la solución resumida del contraste ANOVA, se convierte en un data frame (de nombre “aov_table”) con el objetivo de presentarlo como una tabla de kable():\nTable 8.5: Table 8.6: Resultados del ANOVA\nEl valor del estadístico F de ANOVA es de 8,721; con un p-valor asociado de 0,0006. Como el p-valor es menor que 0,05, se rechaza la hipótesis nula de medias iguales; por lo que podremos afirmar (para una significación del 5%) que el tamaño o dimensión del grupo empresarial de pertenencia influye, en media, en la rentabilidad económica obtenida.","code":"\n# Análisis ANOVA de un factor.\n\nrm(list = ls())\n\n# DATOS\n\nlibrary (readxl)\ndatos <- read_excel(\"eolica_50.xlsx\", sheet = \"Datos\",\n                    na = c(\"n.d.\", \"s.d.\"))\ndatos <- data.frame(datos, row.names = 1)\nsummary (datos)##       RES              ACTIVO            FPIOS        \n##  Min.   :-5268.6   Min.   :  25354   Min.   : -10985  \n##  1st Qu.:  919.6   1st Qu.:  35511   1st Qu.:   1897  \n##  Median : 2321.2   Median :  50301   Median :  17256  \n##  Mean   : 4986.7   Mean   : 189142   Mean   :  83164  \n##  3rd Qu.: 4380.4   3rd Qu.: 101035   3rd Qu.:  44871  \n##  Max.   :42737.0   Max.   :2002458   Max.   :1740487  \n##                    NA's   :1                          \n## \n##      RENECO           RENFIN            LIQUIDEZ       \n##  Min.   :-2.708   Min.   :-263.639   Min.   :  0.0140  \n##  1st Qu.: 1.817   1st Qu.:   1.251   1st Qu.:  0.6462  \n##  Median : 3.957   Median :  14.322   Median :  1.1550  \n##  Mean   : 5.758   Mean   :  32.071   Mean   :  4.2370  \n##  3rd Qu.: 8.038   3rd Qu.:  36.193   3rd Qu.:  1.9185  \n##  Max.   :35.262   Max.   : 588.190   Max.   :128.4330  \n##  NA's   :2                                             \n## \n##     ENDEUDA            MARGEN           SOLVENCIA      \n##  Min.   :  0.917   Min.   :-2248.16   Min.   :-24.465  \n##  1st Qu.: 37.272   1st Qu.:   14.74   1st Qu.:  4.327  \n##  Median : 74.683   Median :   23.86   Median : 25.317  \n##  Mean   : 66.646   Mean   :  -17.77   Mean   : 33.353  \n##  3rd Qu.: 95.672   3rd Qu.:   45.88   3rd Qu.: 62.727  \n##  Max.   :124.465   Max.   :  400.90   Max.   : 99.082  \n## \n##     APALANCA            MATRIZ           DIMENSION        \n##  Min.   :-6905.772   Length:50          Length:50         \n##  1st Qu.:    7.586   Class :character   Class :character  \n##  Median :  126.207   Mode  :character   Mode  :character  \n##  Mean   :  784.429                                        \n##  3rd Qu.:  763.952                                        \n##  Max.   :12244.351\n  # Missing values\n\n  library (dplyr)\n  library(visdat)\n  vis_miss(datos)\n  datos %>% filter(is.na(RENECO) | is.na(DIMENSION)) %>%\n            select(RENECO, DIMENSION)##                       RENECO DIMENSION\n## Sargon Energias SLU       NA     MEDIA\n## Viesgo Renovables SL.     NA     MEDIA\n  muestra <- datos %>%\n             filter(! is.na(RENECO) & ! is.na(DIMENSION))\n  # Outliers\n\n  library (ggplot2)\n  ggplot(data = muestra,\n         map = (aes(y = RENECO))) +\n  geom_boxplot(fill = \"orange\") +\n  ggtitle(\"RENTABILIDAD ECONÓMICA\",\n          subtitle = \"100 empresas eólicas\") +\n  ylab(\"Rentabilidad Económica (%)\")\n  Q1 <- quantile (muestra$RENECO, c(0.25))\n  Q3 <- quantile (muestra$RENECO, c(0.75))\n\n  muestra %>%\n    filter(RENECO > Q3 + 1.5*IQR(RENECO) |\n           RENECO < Q1 - 1.5*IQR(RENECO)) %>%\n    select(RENECO)##                     RENECO\n## Molinos Del Ebro SA 35.262\n  muestra_so <- muestra %>% filter(RENECO <= Q3 + 1.5*IQR(RENECO) &\n                                   RENECO >= Q1 - 1.5*IQR(RENECO))\n  # Visualizando número de frecuencias y medias de los grupos con dplyr:\n  \n  library (knitr)\n  library (kableExtra)\n  knitr.table.format = \"html\" \n\n  tablamedias <-  muestra_so %>%\n                    group_by(DIMENSION) %>%\n                    summarise (observaciones = length(DIMENSION),\n                               media = mean(RENECO))\n\n  tablamedias %>%\n    kable(format = knitr.table.format,\n      caption = \"Rentabilidad Económica. Medias por grupos (tamaño matriz).\",\n      col.names = c(\"Tamaño\", \"Observaciones\", \"Rentabilidad Económica\")) %>%\n  kable_styling(full_width = F,\n                bootstrap_options = \"striped\", \"bordered\", \"condensed\",\n                position = \"center\",\n                font_size = 11) %>%\n  row_spec(0, bold= T, align = \"c\") %>%\n  row_spec(1:nrow(tablamedias), bold= F, align = \"c\")\n  # Graficando casos y medias.\n\n  # Crear el gráfico de densidad\n  gdensidad <- ggplot(data= muestra_so, aes(x = RENECO, color = DIMENSION, fill = DIMENSION)) +\n    geom_density(alpha = 0.3) +\n    geom_vline(data = tablamedias, aes(xintercept = media, color = DIMENSION), linetype = \"dashed\", size = 1) +\n    labs(title = \"Diagramas de Densidad por Grupo con Medias\",\n         x = \"Rentabilidad Económica (%)\",\n         y = \"Densidad\") +\n    theme_grey()\n  \n  # Crear box-plot\n  gbox <- ggplot(data = muestra_so,\n         map = (aes(y = DIMENSION,\n                    x = RENECO,\n                    color = DIMENSION,\n                    fill = DIMENSION))) +\n  geom_boxplot(outlier.shape = NA,\n               alpha = 0.3) +\n  stat_summary(fun = \"mean\",\n               geom = \"point\",\n               size = 3,\n               map = aes(col = DIMENSION),\n               alpha = 0.60) +\n  geom_jitter(width = 0.1,\n              size = 1,\n              map = (aes(col = DIMENSION)),\n              alpha = 0.40) +\n  labs(tittle =\"Diagramas de caja por Grupo con Medias\",\n       xlab = \"Rentabilidad Económica (%)\",\n       ylab = \"Submuestras\")\n\n  # Combinar gráficos.\n  \n  library(patchwork)\n  \n  gdensidad / gbox\n# PRERREQUISITOS / HIPÓTESIS ANOVA  \n\n    # Normalidad / Gráfico QQ\n\n    ggplot(data = muestra_so,\n           aes(sample = RENECO)) +\n    stat_qq(colour = \"red\") + \n    stat_qq_line(colour = \"dark blue\") +\n    ggtitle(\"RENTABILIDAD ECONÓMICA: QQ-PLOT\",\n            subtitle = \"Empresas eólicas\") +\n    facet_grid(. ~ DIMENSION)\n    # Normalidad: Shapiro-Wilk para cada grupo\n    normalidad <- muestra_so %>%\n      group_by(DIMENSION) %>%\n    summarise(shapiro_p_value = round(shapiro.test(RENECO)$p.value, 3)) %>%\n    mutate(decide = if_else(shapiro_p_value > 0.05,\n                            \"NORMALIDAD\",\n                            \"NO-NORMALIDAD\"))\n\n    tablashapiro <- normalidad %>%\n    kable(format = knitr.table.format,\n          caption = \"Normalidad (Shapiro-Wilks)\",\n          col.names = c(\"Dimensión\", \"p-valor\", \"Conclusión\")) %>%\n    kable_styling(full_width = F,\n                  bootstrap_options = \"striped\", \"bordered\", \"condensed\",\n                  position = \"center\",\n                  font_size = 11) %>%\n    row_spec(0, bold= T, align = \"c\") %>%\n    row_spec(1:nrow(normalidad), bold= F, align = \"c\")\n\n    tablashapiro\n    # Homogeneidad en las varianzas\n\n    bartlett.test(muestra_so$RENECO ~ muestra_so$DIMENSION)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  muestra_so$RENECO by muestra_so$DIMENSION\n## Bartlett's K-squared = 9.389, df = 2, p-value = 0.009146\n# Test F de ANOVA\n\nDatos.aov <- aov(muestra_so$RENECO ~ muestra_so$DIMENSION)\nsummary_aov <- summary(Datos.aov)\n\n  # Extraer los resultados del ANOVA\n  aov_table <- as.data.frame(summary_aov[[1]])\n\n  # Convertir la tabla en una tabla de kable\n  aov_table %>%\n    kable(format = knitr.table.format,\n          caption = \"Resultados del ANOVA\",\n          col.names = c(\"Grados Libertad\",\n                        \"Suma cuadrados\",\n                        \"Media suma cuadrados\",\n                        \"Estadístico F\",\n                        \"p-valor\")) %>%\n    kable_styling(full_width = F,\n                  bootstrap_options = \"striped\", \"bordered\", \"condensed\",\n                  position = \"center\",\n                  font_size = 11) %>%\n    row_spec(0, bold= T, align = \"c\") %>%\n    row_spec(1:nrow(aov_table), bold= F, align = \"c\")"},{"path":"análisis-de-la-varianza..html","id":"comparaciones-múltiples.","chapter":"8 Análisis de la varianza.","heading":"8.3 Comparaciones múltiples.","text":"Cuando el resultado del contraste F de ANOVA es de rechazo de la hipótesis nula, se presenta otra cuestión interesante. La hipótesis alternativa dice que existe al menos una diferencia entre las medias de dos grupos que es “importante”. Pero tienen porque ser todas. Entonces, cabe preguntarse que diferencias concretas entre medias son estadísticamente significativas, y cuáles . Para dilucidar esta cuestión se han desarrollado diversas pruebas de comparaciones múltiples. Una de ellas es la prueba HSD de Tuckey. Un modo de obtener los resultados de esta prueba es ejecutarla partir de las funciones del paquete emmeans. En concreto, los resultados de la prueba HSD de Tuckey se obtendrán aplicando la función pairs() un objeto creado previamente con la función emmeans(), al que hemos llamado, por ejemplo, “medias”, y que tiene como argumentos el nombre de nuestra solución del contraste ANOVA anterior y, entrecomillado, el nombre de la variable que actúa como factor que divide la muestra en los tres grupos (submuestras) comparados (DIMENSION). Los resultados se han guardado en el objeto “pares”, que posteriormente se ha transformado en un data frame para poder ser presentado como una tabla de kable():\nTable 8.7: Table 8.8: Resultados comparaciones múltiples\nEn la tabla generada, cada fila recoge la diferencia entre la rentabilidad media de las empresas incluidas en las submuestras de los distintos niveles de dimensión de los grupos empresariales los que perteneces dichas empresas. En la última columna, se muestran los p-valores. La hipótesis nula implica que la diferencia entre las rentabilidades medias de los dos grupos implicados son tan pequeñas que pueden considerarse nulas. Por tanto, p-valores muy pequeños (menores que 0,05) implican un rechazo de esta hipótesis, y la admisión de que esas difrencias son significativamente distintas 0, o sea, “importantes”. Por tanto, en el ejemplo se concluye que las rentabilidad media del grupo de empresas de matrices de dimensión “grande” difiere significativamente con respecto las rentabilidades medias de los otros dos grupos. En cambio, las rentabilidades medias de los grupos de empresas de matrices de dimensión “media” y “pequeña” difieren significativamente.","code":"\n# COMPARACIONES MÚLTIPLES\n\nlibrary(emmeans)\nmedias <- emmeans(Datos.aov, \"DIMENSION\")\npares <- pairs(medias)\npares_df <- as.data.frame(pares)\npares_df %>%\n  kable(format = knitr.table.format,\n        caption = \"Resultados comparaciones múltiples\",\n        col.names = c(\"Grupos\",\n                      \"Diferencia estimada\",\n                      \"Desviación Típica\",\n                      \"Grados de libertad\",\n                      \"Estadístico t\",\n                      \"p-valor\")) %>%\n  kable_styling(full_width = F,\n                bootstrap_options = \"striped\", \"bordered\", \"condensed\",\n                position = \"center\",\n                font_size = 11) %>%\n  row_spec(0, bold= T, align = \"c\") %>%\n  row_spec(1:nrow(pares_df), bold= F, align = \"c\")"},{"path":"análisis-de-la-varianza..html","id":"y-si-no-se-cumplen-las-condiciones-para-realizar-el-contraste-f-de-anova","chapter":"8 Análisis de la varianza.","heading":"8.4 ¿Y si no se cumplen las condiciones para realizar el contraste F de ANOVA?","text":"En el ejemplo anterior hemos visto cómo, aunque hemos seguido el procedimiento de realización del contraste F de ANOVA y la prueba HSD de Tuckey de comparaciones múltiples; sé cumplían algunos de los requisitos necesarios para confiar en los resultados de estos contrastes: la normalidad de las subpoblaciones (medida través de las submuestras) y la homogeneidad de las varianzas de estas.Cuando esto ocurre, es necesario recurrir técnicas robustas, puesto que el comportamiento de la variable analizada en las subpoblaciones (grupos) se ajusta al necesario para poder aplicar los contrastes anteriores.Una prueba robusta que puede suplir al contraste F de ANOVA es la de Kruskal-Wallis. En esta prueba, la hipótesis nula vuelve ser que existen diferencias significativas entre las medias de la variable estudiada de las diferentes subpoblaciones (representadas por las correspondientes submuestras), mientras que la hipótesis alternativa apuesta porquela existencia al menos una diferencia significativa. Para aplicar la prueba de Kruskal-Wallis en R, puede recurrirse la función kruskal.test() del paquete pgirmess. Así, en nuestro ejemplo, tenemos:Puede comprobarse cómo el p-valor es muy pequeño (menor 0,05), por lo que hemos de rechazar la hipótesis nula de medias iguales y admitir que existe al menos dos grupos (subpoblaciones) en los que la diferencia entre la rentabilidad económica media es significativa.De nuevo, si se rechaza la hipótesis nula, cabe preguntarse cuáles son los grupos o subpoblaciones concretas cuyas medias de rentabilidad económica son significativamente diferentes. Kruskal-Wallis desarrollaron también la versión robusta de la prueba HSD de Tuckey, que se incluye en pgirmess, con la función kruskalmc(). En el siguiente código se aplica la función, cuya solución se guarda en el objeto “Datos.kmc”. El elemento de la solución “dif.com”, que contiene las comparaciones entre las medias, se pasa un data frame para poder ser mostrado como una tabla de kable():\nTable 8.9: Table 8.10: Kruskal-Wallis. Múltiples diferencias\nEn la tabla generada se comprueba cómo las diferencias que la media de la rentabilidad económica de la subpoblación de empresas pertenecientes matrices de dimensión “grande” son significativas (para un 0,05 de significación). En cambio, las rentabilidades económicas medias de las empresas pertenecientes matrices de dimensiones “media” y “pequeña” difieren entre sí de modo significativo.","code":"\n#Cuando se incumplen las hipotesis de ANOVA (test robusto)\n\nlibrary(pgirmess)\n\nDatos.K <- kruskal.test(muestra_so$RENECO ~ muestra_so$DIMENSION)\nDatos.K## \n##  Kruskal-Wallis rank sum test\n## \n## data:  muestra_so$RENECO by muestra_so$DIMENSION\n## Kruskal-Wallis chi-squared = 14.718, df = 2, p-value = 0.0006368\n  # Comparaciones múltiples\n\n  Datos.kmc <- kruskalmc(muestra_so$RENECO ~ muestra_so$DIMENSION)\n\n  # Convertir los resultados a un data frame\n\n  Datos.kmc.df <- as.data.frame(Datos.kmc$dif.com)\n\n  Datos.kmc.df %>%\n    kable(format = knitr.table.format,\n          caption = \"Kruskal-Wallis. Múltiples diferencias\",\n          col.names = c(\"Diferencias medias\",\n                        \"Diferencias críticas\",\n                        \"Significación\")) %>%\n    kable_styling(full_width = F,\n                  bootstrap_options = c(\"striped\",\n                                        \"bordered\",\n                                        \"condensed\"),\n                  position = \"center\",\n                  font_size = 11) %>%\n    row_spec(0, bold = T, align = \"c\") %>%\n    row_spec(1:nrow(Datos.kmc.df), bold = F, align = \"c\")"},{"path":"análisis-de-la-varianza..html","id":"materiales-para-realizar-las-prácticas-del-capítulo.-6","chapter":"8 Análisis de la varianza.","heading":"8.5 Materiales para realizar las prácticas del capítulo.","text":"En esta sección se muestran los links de acceso los diferentes materiales (scripts, datos…) necesarios para llevar cabo los contenidos prácticos del capítulo.Datos (en formato Microsoft (R) Excel (R)):eolica_50.xlsx (obtener aquí)Scripts:anova_eolica.R (obtener aquí)","code":""},{"path":"análisis-de-regresión-lineal-múltiple..html","id":"análisis-de-regresión-lineal-múltiple.","chapter":"9 Análisis de Regresión Lineal Múltiple.","heading":"9 Análisis de Regresión Lineal Múltiple.","text":"","code":""},{"path":"análisis-de-regresión-lineal-múltiple..html","id":"introducción.-5","chapter":"9 Análisis de Regresión Lineal Múltiple.","heading":"9.1 Introducción.","text":"El análisis de regresión (lineal) múltiple es una de las técnicas de análisis de dependencias más profusamente utilizadas. En el modelo de regresión múltiple la variable dependiente tiene escala métrica. Las variables explicativas pueden ser métricas o ser atributos.Según los datos de los que se alimenta el modelo, se aplicarán diferentes métodos de estimación, especificaciones y pruebas:Series temporales.Series temporales.Datos de corte transversal.Datos de corte transversal.Paneles de datos.Paneles de datos.En este capítulo nos centraremos en los modelos estimados con base en datos de corte transversal (es decir, las variables tienen datos referentes distintos casos o individuos: personas, empresas, países, etc.)La construcción de un modelo de regresión cuenta con una serie de etapas, que son:Especificación del modelo: establecer las variables que entrarán formar parte del modelo (dependiente, explicativas).Especificación del modelo: establecer las variables que entrarán formar parte del modelo (dependiente, explicativas).Estimación: calcular el valor de los parámetros o coeficientes estructurales del modelo.Estimación: calcular el valor de los parámetros o coeficientes estructurales del modelo.Contraste y validación: verificar si el modelo estimado cumple con las hipótesis que garantizan unas buenas propiedades y si es adecuado para representar la realidad.Contraste y validación: verificar si el modelo estimado cumple con las hipótesis que garantizan unas buenas propiedades y si es adecuado para representar la realidad.Utilización del modelo: efectos de previsión, análisis estructural o simulación de escenarios.Utilización del modelo: efectos de previsión, análisis estructural o simulación de escenarios.Vamos partir del modelo básico de regresión (MBR). Es cierto que, para superar ciertas carencias de este, se ha procedido desarrollar especificaciones y métodos de estimación más elaborados; pero es menos cierto que es conveniente “quemar” etapas sin conocer las características del modelo fundamental, como cimiento donde se posan modelados más complejos.En el MBR vamos suponer que existen:Una variable dependiente y.Una variable dependiente y.k variables explicativas \\(x_j\\).k variables explicativas \\(x_j\\).Variable o perturbación aleatoria u, que recoge el efecto conjunto de todas aquellas variables que afectan al comportamiento de y pero que están explicitadas en la especificación como variables x.Variable o perturbación aleatoria u, que recoge el efecto conjunto de todas aquellas variables que afectan al comportamiento de y pero que están explicitadas en la especificación como variables x.El tamaño de la muestra es n.El tamaño de la muestra es n.El modelo que se plantea es:\\[\ny_i=\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots+\\beta_kx_{ik}+u_i\n\\]con \\(=1,2,...,n\\) .O, en notación matricial:\\[\ny=X\\beta+u\n\\]Donde y es un vector (nx1), X una matriz (nxk), \\(\\beta\\) un vector (kx1), y u un vestor (nx1).En el MBR, la perturbación aleatoria u debe cumplir con una serie de hipótesis básicas: normalidad en su comportamiento, esperanza nula, homoscedasticidad o varianza constante, y ausencia de autocorrelación (covarianza nula entre diferentes elementos del vector de la perturbación). Estas hipótesis, junto las de permanencia estructural (valores de los elementos de \\(\\beta\\) constantes lo largo de la muestra), endogeneidad o regresores -estocásticos (covarianza nula entre la matriz X y el vector u), y rango pleno (las columnas de la matriz X o variables explicativas han de ser combinaciones lineales unas de otras); permiten que el MBR pueda ser estimado por el método de mínimos cuadrados ordinarios (MCO), obteniendo estimadores con las mejores propiedades: insesgadez, eficiencia, consistencia.En la medida en que alguna o algunas de las hipótesis básicas se cumplan, la calidad de los estimadores MCO perderan calidad, en el sentido de gozar de las propiedades deseables, desde un punto de vista inferencial. En tal caso, podrán aplicarse otros métodos de estimación, diversos métodos econométricos, o asumir que los estimadores carecen de algunas de las propiedades deseables.El modelo estimado será:\\[\n\\hat{y}_i=\\hat{\\beta}_1x_{i1}+\\hat{\\beta}_2x_{i2}+\\cdots+\\hat{\\beta}_kx_{ik}\n\\]Y el error o residuo será, para cada observación, \\(\\hat{u}_i=y_i-\\hat{y}_i\\). El vector de residuos se considera una estimación del vector de perturbaciones aleatorias. Es por ello que el vector de residuos se utiliza para verificar el cumplimiento de las hipótesis del modelo básico referentes al comportamiento de la perturbación (normalidad, homoscedasticidad, ausencia de autocorrelación…)Tras estas breves notas formales del MBR, pasaremos construir un modelo que intentará explicar el comportamiento de la rentabilidad económica de un grupo de empresas en función de una serie de variables aleatorias.","code":""},{"path":"análisis-de-regresión-lineal-múltiple..html","id":"especificación-del-mbr.-datos-de-corte-transversal.","chapter":"9 Análisis de Regresión Lineal Múltiple.","heading":"9.2 Especificación del MBR. Datos de corte transversal.","text":"Vamos explicar, mediante un modelo de regresión múltiple, el comportamiento de la rentabilidad económica (RENECO) de las empresas de producción eléctrica mediante tecnología eólica en función del resultado del ejercicio (RES), el activo (ACTIVO), del grado de endeudamiento (ENDEUDA), del grado de apalancamiento (APALANCA), y del tamaño del grupo corporativo (matriz) al que pertenece (DIMENSION). Para ello se ha seleccionado una muestra constituida por 50 empresas.Supondremos que trabajamos en un proyecto de RStudio de nombre, por ejemplo, “regresion”. Los datos de las empresas se encuentran en la hoja “Datos” del archivo de Microsoft® Excel® “eolica_50.xlsx”. El script con el código que vamos ir ejecutando se llama “regresion_eolica.R”.En primer lugar, como de costumbre, hemos de preparar los datos: importarlos en R y gestionar missing values y outliers.Así, una vez abierto el script en el editor de RStudio , comprobaremos que la primera parte del código está dedicada la limpieza de la memoria (Environment) y la importación de los datos. Para ello, activaremos el paquete readxl y utilizaremos la función read_excel(), indicando en los argumentos el archivo explorar, y la hoja en la cual se encuentran los datos (hoja “Datos”). También hemos de prestar atención la cuestión de si existen en la hoja de Excel anotaciones en las celdas donde haya dato, para completar adecuadamente el argumento na= . Los datos se almacenarán en el data frame “datos”. En este data frame, la primera columna es una verdadera variable, sino que se compone de los nombres de los casos o empresas. Con una línea de código adicional transformaremos esa primera columna en el nombre de las filas, de modo que tal columna abandona su rol de variable. En definitiva, el código para importar los datos es:Posteriormente, seleccionaremos las variables que vamos utilizar en el análisis, almacenándolas en otro data frame de nombre, por ejemplo, “originales”:Para localizar los casos concretos de missing values, puede recurrirse utilizar las herramientas de manejo de data frames del paquete dplyr. Con la función vismiss() del paquete visdat podemos tener una visión gráfica general de los valores faltantes. Si hay casos faltantes en una de las variables, los identificaremos filtrando el data frame con la función filter() de dplyr. Ante la existencia de missing values, se puede actuar de varios modos. Por ejemplo, se puede intentar obtener por otro canal de información el conjunto de valores de las variables que están disponibles, o recurrir alguna estimación para los mismos y asignarlos. En caso de que esto sea difícil, se puede optar, simplemente, por eliminar estos casos, en especial cuando representan un porcentaje muy reducido respecto al total de casos. En nuestro ejemplo, vamos suponer que hemos optado por esta última vía. Así, estos casos, finalmente, será exluidos del análisis, utilizando para ello un nuevo filtro:Tras el código anterior, se han eliminado del data frame “originales” las empresas “Sargon Energías S. L. U.” y “Viesgo Renovables S. L.”, debido que carecían de dato de rentabilidad económica (RENECO), y “La Caldera Energía Burgos, S. L.”, al tener dato sobre el valor de sus activos (ACTIVO).Una vez tratados los casos con valores perdidos o missing values, es necesario detectar la presencia de outliers o casos atípicos en la muestra, que pudieran desvirtuar los resultados del análisis de regresión. Para realizar esta etapa, y dado que en nuestro análisis contamos con 5 variables, primero “resumiremos” el valor que toman dichas variables para cada caso, mediante el cálculo de la distancia de Mahalanobis. De hecho, las distancias de los diferentes casos se almacenarán en una nueva variable, la que llamaremos MAHALANOBIS, que se incorporará al data frame “originales” por medio de la función mutate() de dplyr, y la función mahalanobis(). Recordemos que, en los diferentes argumentos de esta función, el punto “.” hace referencia al data frame que está delante del operador pipe (%>%). Posteriormente, construiremos el diagrama de caja de MAHALANOBIS para verificar la existencia de outliers (puntos), e identificaremoslos casos correspondientes con el filtro adecuado. Obtaremos por crear un nuevo data frame, “originales_so”, aplicando un nuevo filtro que elimine esos casos localizados como outliers:Se han localizado 7 empresas con valores de la distancia de Mahalanobis atípicos, lo que hace pensar que, su vez, estas empresas registran valores atípicos en una o varias de las variables originales. Como ya hemos señalado, eliminamos estos 7 casos de la muestra. Así, el data frame “originales_so”, que es el que emplearemos para estimar el modelo, cuenta con 40 observaciones.Una de las variables explicativas es un atributo o factor, llamado DIMENSION, que cuenta con 3 niveles: “GRANDE”, “MEDIANA” y “PEQUEÑA”, según el número de empresas integradas en la matriz la que pertenece cada empresa de la muestra. Hemos de informar R de la condición de atributo o factor de esta variable (pues de momento solo la contempla como una variable cualitativa o alfanumérica). Para ello ejecutaremos el código:Una vez preparadas todas las variables, es el momento de especificar y estimar una regresión múltiple inicial, que contenga todas las variables explicativas candidatas formar parte de la versión final.Para especificar y estimar este modelo inicial, deben de cargarse previamente algunos paquetes que es necesario utilizar: knitr y kableExtra para construir tablas con los resultados de la regresión, y broom. Este último paquete es fundamental, ya que permite disponer de un modo cómodo de todos los elementos de los que se compone un modelo estimado (solo lineal, como es nuestro caso).Por otro lado, El MBR lineal, estimado mediante el método de mínimos cuadrados ordinarios (MCO), se obtendrá mediante la función lm(). Los resultados los guardaremos en un objeto de nombre, por ejemplo, “ecua0”. Luego, se realizará un summary() para ver dicha estimación.El resultado es:Podemos observar, en cuanto la bondad del modelo, cómo es capaz de recoger el 52% de la varianza o comportamiento de la rentabilidad económica (RENECO), atendiendo al valor del coeficiente de determinación lineal corregido (Adjusted R-squared). Los coeficientes o parámetros estimados, en su conjunto, son estadísticamente significativos para una significación de 0,05 (p-valor muy pequeño, en el contraste F de significación conjunta). En cuanto los coeficientes estimados considerados individualmente, encontramos que son estadísticamente significativos tanto el término independiente (intercept), como los asociados las variables RES y ACTIVO, juzgar por los p-valores correspondientes al contraste t de significación individual. En ambos casos, además, son coeficientes con signo positivo, lo que se interpreta como que ambas variables influyen sobre RENECO de modo que, mayor valor de estas variables, en general se obtiene una mayor rentabilidad económica.Por último, es conveniente advertir que el factor DIMENSION se especifica mediante dos variables dicotómicas que representan dos de los niveles del factor (“MEDIA” y “PEQUEÑA”). sus coeficientes muestran el efecto relativo de ese nivel en relación con el nivel que es especificado explicitamente (“GRANDE”), ya que se pueden especificar todos los niveles de un factor para generar un problema de multicolinealidad perfecta.Hay otras informaciones importantes la hora de valorar la especificación (inicial) del modelo que se recogen en el summary() del mismo. Además, vamos presentar todo en modo de tablas diseñadas con kable(). Para poder hacer esto solo con este modelo inicial, sino con cualquier otra estimación, vamos integrar el código correspondiente en una función de R. Llamaremos esta función presenta_modelo(), y recibirá como input la estimación lineal de un modelo, ofreciendo como output tres tablas contenidas en una lista: la primera es una versión del summary(), la segunda es una tabla con otras informaciones adicionales, como el valor del Criterio de Información de Akaike (AIC), y la tercera contiene los valores del factor de inflación de la varianza (VIF) de las variables del modelo. Previamente mostrar el código de la función, fijaremos un parámetro de nombre, por ejemplo, “knitr.table.format”, para recoger el formato en el que se generarán las tablas diseñadas:El código de la función se basa en el aprovechamiento, su vez, de dos de las funciones del paquete broom. Para realizar la versión en tabla del summary() del modelo, se aplica la función tidy(). Esta función crea un data frame donde se almacenan las columnas con las distintas informaciones (coeficientes, desviaciones típicas, valores del estadístico t, p-valores…) con tantas filas como variables explicativas especificadas. La función glance(), por su lado, extre las siguientes informaciones:r.squared: El coeficiente de determinación R², que indica el porcentaje de variación explicada por el modelo.r.squared: El coeficiente de determinación R², que indica el porcentaje de variación explicada por el modelo.adj.r.squared: El R² ajustado, que toma en cuenta los grados de libertad.adj.r.squared: El R² ajustado, que toma en cuenta los grados de libertad.sigma: El error estándar de los residuos.sigma: El error estándar de los residuos.statistic: El estadístico F del modelo.statistic: El estadístico F del modelo.p.value: El valor p asociado con el estadístico F, que indica la significancia global del modelo.p.value: El valor p asociado con el estadístico F, que indica la significancia global del modelo.df: Los grados de libertad del numerador del estadístico F.df: Los grados de libertad del numerador del estadístico F.logLik: El logaritmo de la verosimilitud del modelo.logLik: El logaritmo de la verosimilitud del modelo.AIC: El criterio de información de Akaike.AIC: El criterio de información de Akaike.BIC: El criterio de información bayesiano.BIC: El criterio de información bayesiano.deviance: La desviación del modelo.deviance: La desviación del modelo.El código es, en definitiva:Una vez definida la función, podemos aplicarla al modelo estimado inicial, guardando las tres tablas generadas en la lista “modelo_0”, que pueden ser visualizadas:\nTable 9.1: Table 9.2: Modelo Lineal\n\nTable 9.1: Table 9.1: Estadísticos del modelo\n\nTable 9.1: Table 9.1: Factor de inflación de la varianza\nLos resultados de la primera y segunda tabla ya fueron comentados en casi su totalidad anteriormente, al comentar el summary() del modelo. Se ha añadido el valor del Criterio de Información de Akaike (AIC). Esta es una medida basada en la función de verosimilitud que permite comparar la adecuación de especificaciones alternativas para representar la realidad, de modo que, menor AIC, mejor especificación.La tercera tabla muestra los valores del factor de la inflación de la varianza (VIF) para cada variable explicativa. El VIF mide el riesgo de que, debido la influencia de la variable en cuestión, exista un problema de multicolinealidad entre las variables explicativas del modelo. Un valor de 5/10 (dependiendo de los autores) sugiere que puede existir un problema de multicolinealidad importante. En el caso del modelo inicial, ningún valor del VIF sugiere un posible problema de multicolinealidad.La búsqueda de una especificación alternativa que sea más adecuada puede atender múltiples estrategias del analista, y del propio propósito con el que se quiere utilizar el modelo. Por tanto, implica una gran carga de subjetividad. obstante, existen métodos automatizados para que, una vez se tiene la estimación inicial, se obtenga una especificación más sencilla (Principio de Parsimonia) sin una pérdida grande de bondad de la regresión. Por ejemplo, un método es el step / backward que, en función del Criterio de Información de Akaike (AIC), irá probando estimar especificaciones más simples que disminuyan de AIC (lo que implica una mejor especificación). En nuestro caso, se aplicará con el código:El resultado del algoritmo es una especificación del modelo, cuya estimación se almacena en memoria con el nombre, por ejemplo, “ecuaDEF”. Se ha mostrado el summary() del modelo. También se puede aplicar la función presenta_modelo(), para obtener un output de la estimación más detallado:\nTable 9.3: Table 9.4: Modelo Lineal\n\nTable 9.3: Table 9.3: Estadísticos del modelo\n\nTable 9.3: Table 9.3: Factor de inflación de la varianza\nEn el modelo definitivo (ecuaDEF), tan solo permanecen 3 variables explicativas: RES, ACTIVO (ambas significativas para una significación de 0,05) y APALANCA (significativa para una significación de 0,1). Una diferencia importante respecto al modelo inicial es que, el signo del coeficiente asociado ACTIVO pasa ser negativo. También el grado de apalancamiento tiene asociado un coeficiente negativo (mayor apalancamiento, menor rentabilidad económica). SI observamos la segunda tabla, puede observarse que el valor de AIC es de 216,20, inferior al valor de AIC del modelo inicial (220,4). La bondad del ajuste, medida por medio del R2 ajustado, es de 0,541; superior al de modelo inicial (0,521). Finalmente, en la tercera tabla se muestran unos valores de VIF bajos, por lo que se descartan problemas de multicolinealidad.Una vez decidida la especificación (final) del modelo, es necesario desarrollar la etapa de contrastación de las hipótesis básicas del modelo, con la intención de determinar el grado en que los estimadores obtenidos mediante MCO gozan de buenas propiedades, o si es necesario aplicar métodos de estimación alternativos, métodos econométricos específicos, o incluso re-especificar el modelo).Es conveniente, previamente al análisis de cada hipótesis, generar varios gráficos de gran utilidad.El primer paso, obstante, es recurrir la función augment() del paquete broom. Esta función, aplicada un modelo, genera algunas series de datos fundamentales relacionadas con el mismo, y las almacena junto las variables del modelo especificado en un data frame. En nuestro caso, hemos llamado al data frame “series_ecuaDEF”, y hemos cambiado el nombre las series que vamos utilizar posteriormente: los valores ajustados de la variable dependiente (RENECO.est), los residuos (residuos), y los valores de la distancia de Cook (cooksd). Además, hemos creado una variable llamada ORDEN para asignar un valor correlativo cada observación o caso de la muestra:Una vez obtenidas todas las series necesarias, diseñaremos los gráficos que facilitarán la comprensión y contraste del modelo final. El primero compara los valores reales de RENECO con las estimaciones del modelo, RENECO.est. Lógicamente, es deseable que, para cada caso, la distancia entre ambos puntos sea mínima. El segundo muestra los residuos. El tercero es el gráfico de densidad de los residuos, y el cuarto representa, para cada caso, el valor de la distancia de Cook. Los valores altos de la distancia de Cook en un modelo de regresión indican observaciones que tienen una influencia significativa en los coeficientes estimados del modelo. Son considerados “altos” los valores que superan el valor inverso de 4 por el número de observaciones muestrales.Los 4 gráficos se agrupan mediante la gramática del paquete {patchwork}. Finalmente, se genera una tabla partir de un filtro para identificar los casos concretos que tienen valores “altos” de la distancia de Cook. En definitiva, el código es:\nTable 9.5: Table 9.6: Casos destacados distancia de Cook\nDe los gráficos anteriores se desprende, en general, que los residuos parecen mantener un comportamiento conforme una distribución normal, y que hay una observación o caso con una distancia de Cook que puede influir de modo relevante en el valor de los coeficientes estimados. Esta empresa es identificada como “Innogy Spain S..” Podría estudiarse en detalle este caso o incluso reestimar el modelo sin su presencia, fin de comprobar el efecto que tiene esta observación sobre la estimación en general.Tras estudiar los gráficos, vamos pasar contrastar las hipótesis básicas del MBR referidas la forma funcional, y la normalidad y comportamiento homoscedástico de la perturbación aleatoria. Para ello se aplicarán las pruebas Ramsey-Reset, Shapiro-Wilk y Breusch-Pagan, respectivamente. Los resultados de las tres pruebas se presentarán condensados en una tabla, en la que además se incluirá la conclusión del contraste, para una significación de 0,05. Para crear la tabla, primero se generará un data frame con sus elementos, denominado, por ejemplo, “check_hipotesis”. El código es:\nTable 9.7: Table 9.8: Contrastes de hipótesis del MBR\nEn la tabla de resultados del contraste de las hipótesis básicas del MBR, encontramos que el modelo plantea problemas de falta de normalidad o heteroscedasticidad en la perturbación aleatoria. En cambio, la prueba de Ramsey-Reset rechaza la hipótesis nula de especificación lineal correcta. En concreto, este contraste plantea estas dos hipótesis:Hipótesis nula (H0): El modelo está correctamente especificado, es decir, hay errores de especificación. En términos más técnicos, esto significa que las combinaciones lineales de los valores ajustados tienen poder explicativo adicional sobre la variable dependiente.Hipótesis nula (H0): El modelo está correctamente especificado, es decir, hay errores de especificación. En términos más técnicos, esto significa que las combinaciones lineales de los valores ajustados tienen poder explicativo adicional sobre la variable dependiente.Hipótesis alternativa (H1): El modelo está mal especificado, lo que implica que las combinaciones lineales de los valores ajustados sí tienen poder explicativo adicional sobre la variable dependiente.Hipótesis alternativa (H1): El modelo está mal especificado, lo que implica que las combinaciones lineales de los valores ajustados sí tienen poder explicativo adicional sobre la variable dependiente.Si se asume que las variables especificadas son las correctas, y hay omisión de variables relevantes; un rechazo de la hipótesis nula implica que la relación funcional planteada (lineal), es correcta.El incumplimiento de la hipótesis de linealidad podría acarrear que los estimadores MCO obtenidos adolecen de la pérdida de la propiedad de insesgadez.Si el modelo supera razonablemente la fase de contraste de las hipótesis básicas; podría ser utilizado para los objetivos planteados en la investigación: análisis estructural, previsión, simulación. En este ejemplo, vamos realizar un ejercicio de simulación.Vamos obtener la respuesta de la rentabilidad económica (RENECO), ante 3 escenarios alternativos. Dichos escenarios están guardados en la hoja “Simula” del archivo de Microsoft® Excel® “eolica_escenarios.xlsx”. Hay que tener en cuenta que en los escenarios se aportan valores para algunas variables que aparecen en el modelo final. Lógicamente, lo importante son los datos del escenario correspondientes las variables que sí están especificadas. Los escenarios se importan y se almacenan en el data frame de nombre, por ejemplo, “escenario”:Mediante la función predict(), se generará la respuesta los escenarios propuestos, por parte del modelo definitivo (ECUADEF). Esta respuesta se guardará en el data frame de nombre, por ejemplo, “estimación”, que luego uniremos al data frame “escenario” mediante la función cbind(), creando un único data frame llamado, por ejemplo, “simulacion”:Finalmente, vamos presentar la simulación en forma de tabla. En primer lugar, vamos dar un formato específico cada variable, cara su volcado la tabla, con la función format(). El argumento nsmall= indica el número mínimo de decimales que habrá la derecha del punto decimal:Tras definir el formato numérico de las variables, se construirá la tabla, de nombre, por ejemplo, “tablasimula”:\nTable 9.9: Table 9.10: Simulación Modelo Rentabilidad Económica\n","code":"\n## Regresion multiple empresas eolicas. Disculpen la falta de tildes.\n\nrm(list = ls())\n\n# DATOS\n\n  # Importando\n\n    library(readxl)\n    eolicos <- read_excel(\"eolica_50.xlsx\", sheet = \"Datos\",\n                          na = c(\"n.d.\", \"s.d.\"))\n    eolicos <- data.frame(eolicos, row.names = 1)\n    summary (eolicos)##       RES              ACTIVO            FPIOS        \n##  Min.   :-5268.6   Min.   :  25354   Min.   : -10985  \n##  1st Qu.:  919.6   1st Qu.:  35511   1st Qu.:   1897  \n##  Median : 2321.2   Median :  50301   Median :  17256  \n##  Mean   : 4986.7   Mean   : 189142   Mean   :  83164  \n##  3rd Qu.: 4380.4   3rd Qu.: 101035   3rd Qu.:  44871  \n##  Max.   :42737.0   Max.   :2002458   Max.   :1740487  \n##                    NA's   :1                          \n## \n##      RENECO           RENFIN            LIQUIDEZ       \n##  Min.   :-2.708   Min.   :-263.639   Min.   :  0.0140  \n##  1st Qu.: 1.817   1st Qu.:   1.251   1st Qu.:  0.6462  \n##  Median : 3.957   Median :  14.322   Median :  1.1550  \n##  Mean   : 5.758   Mean   :  32.071   Mean   :  4.2370  \n##  3rd Qu.: 8.038   3rd Qu.:  36.193   3rd Qu.:  1.9185  \n##  Max.   :35.262   Max.   : 588.190   Max.   :128.4330  \n##  NA's   :2                                             \n## \n##     ENDEUDA            MARGEN           SOLVENCIA      \n##  Min.   :  0.917   Min.   :-2248.16   Min.   :-24.465  \n##  1st Qu.: 37.272   1st Qu.:   14.74   1st Qu.:  4.327  \n##  Median : 74.683   Median :   23.86   Median : 25.317  \n##  Mean   : 66.646   Mean   :  -17.77   Mean   : 33.353  \n##  3rd Qu.: 95.672   3rd Qu.:   45.88   3rd Qu.: 62.727  \n##  Max.   :124.465   Max.   :  400.90   Max.   : 99.082  \n## \n##     APALANCA            MATRIZ           DIMENSION        \n##  Min.   :-6905.772   Length:50          Length:50         \n##  1st Qu.:    7.586   Class :character   Class :character  \n##  Median :  126.207   Mode  :character   Mode  :character  \n##  Mean   :  784.429                                        \n##  3rd Qu.:  763.952                                        \n##  Max.   :12244.351\n  # Seleccionando variables clasificadoras para el analisis\n\n    library(dplyr)\n    originales<-select(eolicos,\n                       RENECO,\n                       RES,\n                       ACTIVO,\n                       ENDEUDA,\n                       APALANCA,\n                       DIMENSION)\n    summary (originales)##      RENECO            RES              ACTIVO       \n##  Min.   :-2.708   Min.   :-5268.6   Min.   :  25354  \n##  1st Qu.: 1.817   1st Qu.:  919.6   1st Qu.:  35511  \n##  Median : 3.957   Median : 2321.2   Median :  50301  \n##  Mean   : 5.758   Mean   : 4986.7   Mean   : 189142  \n##  3rd Qu.: 8.038   3rd Qu.: 4380.4   3rd Qu.: 101035  \n##  Max.   :35.262   Max.   :42737.0   Max.   :2002458  \n##  NA's   :2                          NA's   :1        \n## \n##     ENDEUDA           APALANCA          DIMENSION        \n##  Min.   :  0.917   Min.   :-6905.772   Length:50         \n##  1st Qu.: 37.272   1st Qu.:    7.586   Class :character  \n##  Median : 74.683   Median :  126.207   Mode  :character  \n##  Mean   : 66.646   Mean   :  784.429                     \n##  3rd Qu.: 95.672   3rd Qu.:  763.952                     \n##  Max.   :124.465   Max.   :12244.351\n  # Identificando missing values.\n\n    library(visdat)\n    vis_miss(originales)\n    originales %>%\n      filter(is.na(RENECO) | is.na(RES) | is.na(ACTIVO) |\n             is.na(ENDEUDA) | is.na(APALANCA) | is.na(DIMENSION)) %>%\n      select(RENECO, RES, ACTIVO, ENDEUDA, APALANCA, DIMENSION)  ##                              RENECO       RES ACTIVO ENDEUDA  APALANCA DIMENSION\n## La Caldera Energia Burgos SL  2.643   511.304     NA 110.636 -1019.288    GRANDE\n## Sargon Energias SLU              NA -2216.000  85745 112.811  -879.289     MEDIA\n## Viesgo Renovables SL.            NA  4609.000 269730  34.116    13.330     MEDIA\n    originales <- originales %>%\n      filter(! is.na(RENECO) & ! is.na(RES) & ! is.na(ACTIVO) &\n             ! is.na(ENDEUDA) & ! is.na(APALANCA) & ! is.na(DIMENSION))\n  # Identificando outliers.\n\n    originales <- originales %>%\n      mutate(MAHALANOBIS = mahalanobis(select(.,\n                                          RENECO,\n                                          RES,\n                                          ACTIVO,\n                                          ENDEUDA,\n                                          APALANCA),\n                                        center = colMeans(select(.,\n                                                            RENECO,\n                                                            RES,\n                                                            ACTIVO,\n                                                            ENDEUDA,\n                                                            APALANCA)),\n                                        cov=cov(select(.,\n                                                  RENECO,\n                                                  RES,\n                                                  ACTIVO,\n                                                  ENDEUDA,\n                                                  APALANCA))))\n\n    library (ggplot2)\n    ggplot(data = originales, map = (aes(y = MAHALANOBIS))) +\n    geom_boxplot(fill = \"orange\") +\n    ggtitle(\"DISTANCIA DE MAHALANOBIS\", subtitle = \"Empresas eólicas\") +\n    ylab(\"MAHALANOBIS\")\n    Q1M <- quantile (originales$MAHALANOBIS, c(0.25))\n    Q3M <- quantile (originales$MAHALANOBIS, c(0.75))\n    \n    originales %>% filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |\n                      MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS)) %>%\n                   select(MAHALANOBIS)##                                       MAHALANOBIS\n## Corporacion Acciona Eolica SL            13.67263\n## Parque Eolico Sierra De Las Carbas SL    13.34797\n## Naturgy Renovables SLU                   19.45720\n## Global Power Generation SA.              19.88409\n## Saeta Yield SA.                          20.40512\n## Molinos Del Ebro SA                      21.73955\n## Elecdey Lezuza SA                        18.97057\n    originales_so <- originales %>%\n                     filter(MAHALANOBIS <= Q3M + 1.5*IQR(MAHALANOBIS) &\n                            MAHALANOBIS >= Q1M - 1.5*IQR(MAHALANOBIS)) \n    originales <- originales %>% select(-MAHALANOBIS)\n    originales_so <- originales_so %>% select(-MAHALANOBIS)\n  # Convertir variable DIMENSION en Factor.\n\n    originales_so$DIMENSION <- as.factor(originales_so$DIMENSION)\n    levels(originales_so$DIMENSION)## [1] \"GRANDE\"  \"MEDIA\"   \"PEQUEÑA\"\n# ESPECIFICACION Y ESTIMACION\n\n  # Cargar las librerías necesarias\n\n    library (knitr)\n    library (kableExtra)\n    library (broom)\n    library (car) # para obtener el vif\n\n  # Especificar el modelo de regresión lineal\n    ecua0 <- lm(data = originales_so,\n                RENECO ~ RES + ACTIVO + ENDEUDA + APALANCA + DIMENSION)\n    summary(ecua0)## \n## Call:\n## lm(formula = RENECO ~ RES + ACTIVO + ENDEUDA + APALANCA + DIMENSION, \n##     data = originales_so)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.0302 -2.1468 -0.1034  1.5821  6.7527 \n## \n## Coefficients:\n##                    Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)       5.045e+00  1.575e+00   3.203  0.00301 ** \n## RES               9.962e-04  2.137e-04   4.661 4.99e-05 ***\n## ACTIVO           -3.060e-05  8.938e-06  -3.423  0.00167 ** \n## ENDEUDA          -6.297e-03  1.859e-02  -0.339  0.73698    \n## APALANCA         -4.197e-04  3.508e-04  -1.196  0.24007    \n## DIMENSIONMEDIA    1.462e+00  1.516e+00   0.964  0.34195    \n## DIMENSIONPEQUEÑA  1.781e+00  1.563e+00   1.139  0.26282    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.43 on 33 degrees of freedom\n## Multiple R-squared:  0.5943, Adjusted R-squared:  0.5205 \n## F-statistic: 8.056 on 6 and 33 DF,  p-value: 2.122e-05\n  #diseña salida ordenador\n\n    knitr.table.format = \"html\"\n  # Definir la función de presentación de resultados:  presenta_modelo() #####\n\n    presenta_modelo <- function(modelo) {\n\n    # Lista de piezas\n      modelo_piezas <-list()\n  \n    # Aplicar la función tidy() al modelo\n      resultados <- tidy(modelo)\n  \n    # Seleccionar las columnas deseadas\n      resultados <- resultados[, c(\"term\",\n                                   \"estimate\",\n                                   \"std.error\",\"statistic\",\n                                   \"p.value\")]\n    # Añadir la columna 'stars' según los valores de 'p.value'\n      resultados$stars <- cut(resultados$p.value,\n                            breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n                            labels = c(\"***\", \"**\", \"*\", \"·\", \" \"),\n                            right = FALSE)\n    # formatear los valores de la columna \"estimate\" a 5 decimales\n      resultados$estimate <- formatC(resultados$estimate,\n                                     format = \"f\",\n                                     digits = 5)\n  \n    # Crear la tabla con kable\n      tabla1 <- resultados %>%\n        kable(format = knitr.table.format,\n          caption = \"Modelo Lineal\",\n          col.names = c(\"Variable\", \"Coeficiente\", \"Desv. Típica\",\n                        \"Estadístico t\", \"p-valor\", \"Sig.\"),\n          digits = 3,\n          align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\")) %>%\n        kable_styling(full_width = F,\n                      bootstrap_options = \"striped\",\n                                          \"bordered\",\n                                          \"condensed\",\n                      position = \"center\",\n                      font_size = 11)\n \n      modelo_piezas[[1]] <- tabla1\n  \n    # Aplicar la función glance\n      estadisticos <- glance(modelo)\n      estadisticos <- estadisticos[,c(\"r.squared\",\n                                      \"adj.r.squared\",\n                                      \"sigma\",\n                                      \"statistic\",\n                                      \"p.value\",\n                                      \"AIC\",\n                                      \"nobs\")]\n    # Crear la tabla con kable\n      tabla2 <- estadisticos %>%\n        kable(format = knitr.table.format,\n          caption = \"Estadísticos del modelo\",\n          col.names = c(\"R2\", \"R2 ajustado\", \"Sigma\", \"Estadístico F\",\n                        \"p-valor\", \"AIC\", \"num. observaciones\"),\n          digits = 3,\n          align = \"c\") %>%\n        kable_styling(full_width = F,\n                      bootstrap_options = \"striped\",\n                                          \"bordered\",\n                                          \"condensed\",\n                      position = \"center\",\n                      font_size = 11)\n      \n      modelo_piezas[[2]] <- tabla2  \n\n    # Obtener VIF\n      vif_df <- as.data.frame(vif(modelo))\n\n    # Añadir nombres de filas\n      library(tibble)\n      vif_df <- vif_df %>%\n      rownames_to_column(var = \"Variable\")\n  \n    # Crear tabla con kable\n      tabla3 <- vif_df[,1:2] %>%\n        kable(format = knitr.table.format,\n              caption = \"Factor de inflación de la varianza\",\n              col.names = c(\"Variable\",\"Valor VIF\"),\n              digits = 3,\n              align = \"c\") %>%\n        kable_styling(full_width = F,\n                      bootstrap_options = \"striped\",\n                                          \"bordered\",\n                                          \"condensed\",\n                      position = \"center\",\n                      font_size = 11)\n      \n      modelo_piezas[[3]] <- tabla3\n  \n    return(modelo_piezas)\n  }\n############################################################################\n  modelo_0 <- presenta_modelo(ecua0)\n\n  modelo_0[[1]]\n  modelo_0[[2]]\n  modelo_0[[3]]\n  ecuaDEF <- step(ecua0, scale = 0,\n                  direction = c(\"backward\"),\n                  trace = 1, steps = 1000, k = 2)## Start:  AIC=104.92\n## RENECO ~ RES + ACTIVO + ENDEUDA + APALANCA + DIMENSION\n## \n##             Df Sum of Sq    RSS    AIC\n## - DIMENSION  2    17.324 405.63 102.66\n## - ENDEUDA    1     1.350 389.65 103.06\n## - APALANCA   1    16.843 405.14 104.61\n## <none>                   388.30 104.92\n## - ACTIVO     1   137.886 526.19 115.07\n## - RES        1   255.671 643.97 123.15\n## \n## Step:  AIC=102.66\n## RENECO ~ RES + ACTIVO + ENDEUDA + APALANCA\n## \n##            Df Sum of Sq    RSS    AIC\n## - ENDEUDA   1      0.27 405.90 100.69\n## <none>                  405.63 102.66\n## - APALANCA  1     27.08 432.71 103.25\n## - ACTIVO    1    278.04 683.67 121.54\n## - RES       1    386.92 792.55 127.45\n## \n## Step:  AIC=100.69\n## RENECO ~ RES + ACTIVO + APALANCA\n## \n##            Df Sum of Sq    RSS    AIC\n## <none>                  405.90 100.69\n## - APALANCA  1     34.56 440.46 101.96\n## - ACTIVO    1    280.94 686.83 119.73\n## - RES       1    394.36 800.26 125.84\n  summary (ecuaDEF)## \n## Call:\n## lm(formula = RENECO ~ RES + ACTIVO + APALANCA, data = originales_so)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6.3668 -2.5872 -0.3775  1.4791  7.3184 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  5.759e+00  8.530e-01   6.752 6.97e-08 ***\n## RES          1.110e-03  1.877e-04   5.914 9.05e-07 ***\n## ACTIVO      -3.644e-05  7.301e-06  -4.992 1.54e-05 ***\n## APALANCA    -5.376e-04  3.070e-04  -1.751   0.0885 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.358 on 36 degrees of freedom\n## Multiple R-squared:  0.5759, Adjusted R-squared:  0.5406 \n## F-statistic:  16.3 on 3 and 36 DF,  p-value: 7.444e-07\n  modelo_DEF <- presenta_modelo(ecuaDEF)\n\n  modelo_DEF[[1]]\n  modelo_DEF[[2]]\n  modelo_DEF[[3]]\n# MODELO FINAL: CONTRASTACIÓN.\n\n  series_ecuaDEF <- augment(ecuaDEF)\n  series_ecuaDEF <- series_ecuaDEF %>%\n  rename(RENECO.est = .fitted,\n         residuos = .resid,\n         cooksd = .cooksd)\n  series_ecuaDEF$ORDEN = c(1:nrow(series_ecuaDEF))\n  summary (series_ecuaDEF)##   .rownames             RENECO            RES         \n##  Length:40          Min.   :-2.708   Min.   :-5268.6  \n##  Class :character   1st Qu.: 2.103   1st Qu.:  892.2  \n##  Mode  :character   Median : 4.404   Median : 2321.2  \n##                     Mean   : 5.547   Mean   : 2812.2  \n##                     3rd Qu.: 8.270   3rd Qu.: 3951.2  \n##                     Max.   :15.882   Max.   :12819.0  \n## \n##      ACTIVO          APALANCA         RENECO.est    \n##  Min.   : 25354   Min.   :-3037.8   Min.   :-8.564  \n##  1st Qu.: 33162   1st Qu.:   21.4   1st Qu.: 3.970  \n##  Median : 45903   Median :  146.5   Median : 5.620  \n##  Mean   : 78664   Mean   :  868.9   Mean   : 5.547  \n##  3rd Qu.: 81312   3rd Qu.: 1080.1   3rd Qu.: 7.432  \n##  Max.   :443467   Max.   : 8049.4   Max.   :12.464  \n## \n##     residuos            .hat             .sigma     \n##  Min.   :-6.3668   Min.   :0.02906   Min.   :3.107  \n##  1st Qu.:-2.5872   1st Qu.:0.03774   1st Qu.:3.350  \n##  Median :-0.3775   Median :0.05202   Median :3.385  \n##  Mean   : 0.0000   Mean   :0.10000   Mean   :3.356  \n##  3rd Qu.: 1.4791   3rd Qu.:0.10120   3rd Qu.:3.403  \n##  Max.   : 7.3184   Max.   :0.53601   Max.   :3.405  \n## \n##      cooksd            .std.resid          ORDEN      \n##  Min.   :1.433e-05   Min.   :-1.9446   Min.   : 1.00  \n##  1st Qu.:7.106e-04   1st Qu.:-0.7888   1st Qu.:10.75  \n##  Median :8.387e-03   Median :-0.1147   Median :20.50  \n##  Mean   :5.144e-02   Mean   : 0.0116   Mean   :20.50  \n##  3rd Qu.:2.711e-02   3rd Qu.: 0.4557   3rd Qu.:30.25  \n##  Max.   :1.485e+00   Max.   : 2.4566   Max.   :40.00\n  # Gráficos.\n  \n    g_real_pred <- ggplot(data = series_ecuaDEF) +\n      geom_point(aes(x = ORDEN, y = RENECO.est),\n                 size= 2,\n                 alpha= 0.6,\n                 color = \"blue\") +    \n      geom_point(aes(x = ORDEN, y = RENECO),\n                 size= 2,\n                 alpha= 0.6,\n                 color = \"red\") +\n      geom_line(aes(x = ORDEN, y = RENECO.est),\n                color = \"blue\",\n                linetype = \"dashed\",\n                size= 1) +\n      geom_line(aes(x = ORDEN, y = RENECO),\n                color = \"red\",\n                linetype = \"dashed\",\n                size= 1) +\n      geom_segment(aes(x = ORDEN, xend = ORDEN, y = RENECO.est, yend = RENECO),\n                   color = \"orange\") +\n      ggtitle(\"RENTABILIDAD ECONÓMICA.\",\n              subtitle= \"VALORES REALES (rojo) vs PREDICCIONES (azul).\") +\n      xlab(\"Casos\") + \n      ylab(\"Rentabilidad Económica: Real y Predicción\")\n\n  g_resid <- ggplot(data = series_ecuaDEF, aes(x = ORDEN, y = residuos)) +\n    geom_point(size=2, alpha= 0.6, color = \"blue\") +\n    geom_smooth(color = \"firebrick\", span = 0.5) +\n    geom_hline(yintercept = 0, color = \"red\")+\n    ggtitle(\"RENTABILIDAD ECONÓMICA.\", subtitle= \"Residuos.\")+\n    xlab(\"Casos\") + \n    ylab(\"Residuos\")\n\n  g_hresid <- ggplot(data = series_ecuaDEF, map = aes(x = residuos)) +\n    geom_density(colour = \"red\", fill = \"orange\", alpha = 0.6) +\n    ggtitle(\"RENTABILIDAD ECONÓMICA\", subtitle = \"Densidad Residuos\")+\n    xlab(\"Rentabilidad Económica\") +\n    ylab(\"Densidad\")\n\n  g_cook <- ggplot(data = series_ecuaDEF, aes(x = ORDEN, y = cooksd)) +\n    geom_bar(stat = \"identity\") +\n    geom_hline(yintercept = 4/nrow(series_ecuaDEF),\n               linetype = \"dashed\",\n               color = \"red\") +\n    ggtitle(\"RENTABILIDAD ECONÓMICA.\", subtitle= \"Distancia de Cook.\")+\n    xlab(\"Casos\") + \n    ylab(\"Distancias\")\n\n  library (patchwork)\n\n  (g_real_pred | g_hresid) / (g_resid | g_cook)\n\n  tablaCook <- series_ecuaDEF %>%\n    filter ( cooksd > 4/nrow(series_ecuaDEF)) %>%\n    select (.rownames, cooksd) %>%\n    kable(format = knitr.table.format,\n          caption = \"Casos destacados distancia de Cook\",\n          col.names = c(\"Caso\",\"Distancia de Cook\"),\n          digits = 3,\n          align = c(\"l\",\"c\")) %>%\n    kable_styling(full_width = F,\n                  bootstrap_options = \"striped\",\n                                      \"bordered\",\n                                      \"condensed\",\n                  position = \"center\",\n                  font_size = 11)\n\n    tablaCook\n  # Hipótesis básicas MBR.\n\n    library (lmtest)\n    reset_test <- resettest(ecuaDEF, data= originales_so) # F. Funcional\n    shapiro_test <- shapiro.test(series_ecuaDEF$residuos) # Normalidad\n    bp_test <- bptest(ecuaDEF) # Homoscedasticidad\n\n  # Tabla resultados\n\n    # Crear un data frame con los resultados\n\n      check_hipotesis <- data.frame(\n        \"Tipo_de_prueba\" = c(\"Forma Funcional\",\n                             \"Normalidad de perturbación aleatoria\",\n                             \"Homoscedasticidad de perturbación aleatoria\"),\n        \"Prueba\" = c(\"Ramsey-Reset\",\n                     \"Shapiro-Wilk\",\n                     \"Breusch-Pagan\"),\n        \"Estadistico\" = c(reset_test$statistic,\n                          shapiro_test$statistic,\n                          bp_test$statistic),\n        \"P_valor\" = c(reset_test$p.value,\n                      shapiro_test$p.value,\n                      bp_test$p.value),\n        \"Conclusion\" = c(ifelse(reset_test$p.value >= 0.05,\n                                 \"F. funcional correcta\",\n                                 \"F. funcional incorrecta\"),\n                         ifelse(shapiro_test$p.value >= 0.05,\n                                \"Normalidad\",\n                                \"No-Normalidad\"),\n                         ifelse(bp_test$p.value >= 0.05,\n                                \"Homoscedasticidad\",\n                                \"Heteroscedasticidad\")))\n\n    row.names(check_hipotesis) <- NULL\n\n  # Crear la tabla con kable\n    \n    tabla_check <- check_hipotesis %>%\n      kable(format = knitr.table.format,\n            caption = \"Contrastes de hipótesis del MBR\",\n            col.names = c(\"Tipo de prueba\",\n                          \"Prueba\",\n                          \"Estadístico\",\n                          \"P-valor\",\n                          \"Conclusión\"),\n            digits = 3,\n            align = c(\"l\", \"c\", \"c\", \"c\", \"c\")) %>%\n      kable_styling(full_width = F,\n                    bootstrap_options = \"striped\",\n                                        \"bordered\",\n                                        \"condensed\",\n                    position = \"center\",\n                    font_size = 11)\n\n    tabla_check\n# SIMULACIÓN\n\n  # Cargar escenario de Excel\n\n    escenario <- read_excel(\"eolica_escenarios.xlsx\", sheet = \"Simula\")\n    escenario <- data.frame(escenario, row.names = 1)\n    escenario##              RES ACTIVO ENDEUDA APALANCA DIMENSION\n## ESCENARIO A 2500  50000   20.25    0.238    GRANDE\n## ESCENARIO B 2500  25000   50.00   50.000     MEDIA\n## ESCENARIO C 3000   5000   90.00  100.000   PEQUEÑA\n  # Simulación con el modelo\n\n    estimacion <-predict (object= ecuaDEF,\n                          newdata = escenario,\n                          interval=\"prediction\",\n                          level=0.95)\n    estimacion##                  fit        lwr      upr\n## ESCENARIO A 6.711704 -0.2161380 13.63955\n## ESCENARIO B 7.596000  0.6413923 14.55061\n## ESCENARIO C 8.852940  1.8523982 15.85348\n    simulacion <- cbind(escenario, estimacion)\n  # Formatear las columnas con el número mínimo de decimales deseado\n\n    simulacion$ENDEUDA <- format(simulacion$ENDEUDA, nsmall = 3)\n    simulacion$APALANCA <- format(simulacion$APALANCA, nsmall = 3)\n    simulacion$fit <- format(simulacion$fit, nsmall = 3)\n    simulacion$lwr <- format(simulacion$lwr, nsmall = 3)\n    simulacion$upr <- format(simulacion$upr, nsmall = 3)\n  # Tabla\n\n    tablasimula <- simulacion %>%\n      kable(format = knitr.table.format,\n            caption = \"Simulación Modelo Rentabilidad Económica\",\n            col.names = c(\"Escenario\",\n                          \"Resultado\",\n                          \"Activo\",\n                          \"Endeuda\",\n                          \"Apalancamiento\",\n                          \"Dimensión\",\n                          \"Previsión\",\n                          \"Inferior 95%\",\n                          \"Superior 95%\"),\n        digits = 3) %>%\n      kable_styling(full_width = F,\n                    bootstrap_options = \"striped\",\n                    \"bordered\",\n                    \"condensed\",\n                    position = \"center\",\n                    font_size = 11) %>%\n      row_spec(0, bold= T, align = \"c\") %>%\n      row_spec(1:(nrow(simulacion)), bold= F, align = \"c\")\n\n  tablasimula"},{"path":"análisis-de-regresión-lineal-múltiple..html","id":"materiales-para-realizar-las-prácticas-del-capítulo.-7","chapter":"9 Análisis de Regresión Lineal Múltiple.","heading":"9.3 Materiales para realizar las prácticas del capítulo.","text":"En esta sección se muestran los links de acceso los diferentes materiales (scripts, datos…) necesarios para llevar cabo los contenidos prácticos del capítulo.Datos (en formato Microsoft (R) Excel (R)):eolica_50.xlsx (obtener aquí)eolica_50.xlsx (obtener aquí)eolica_escenarios.xlsx (obtener aquí)eolica_escenarios.xlsx (obtener aquí)Scripts:Scripts:regresion_eolica.R (obtener aquí)regresion_eolica.R (obtener aquí)","code":""},{"path":"análisis-de-datos-cualitativos..html","id":"análisis-de-datos-cualitativos.","chapter":"10 Análisis de Datos Cualitativos.","heading":"10 Análisis de Datos Cualitativos.","text":"","code":""},{"path":"análisis-de-datos-cualitativos..html","id":"introducción.-6","chapter":"10 Análisis de Datos Cualitativos.","heading":"10.1 Introducción.","text":"En numerosas ocasiones la información con que el analista debe enfrentarse es de naturaleza cualitativa, esto es, la información se recoge en características numéricas o atributos (o factores o variables categóricas o cualitativas). Cuando ocurre esto, es necesario recurrir técnicas de explotación específicas para este tipo de datos. Así, en este capítulo vamos explorar algunos métodos para extraer información útil cuando los datos de los que disponemos son categóricos.En estos casos, la información suele sintetizarse y presentarse mediante las denominadas “tablas de contingencia”. En este tipo de tablas, se muestran las frecuencias conjuntas, es decir, el número de casos que comparten los distintos niveles o categorías de los diferentes factores.Cuando se trabaja con varios factores o atributos, representados en su correspondiente tabla de contingencia, uno de los análisis más interesantes es determinar si existe asociación entre los factores o atributos. Esto es, si se aprecia algún tipo de relación estadística entre estas variables categóricas o cualitativas, en el sentido de si se puede afirmar que el hecho de que los casos de la muestra tomen ciertos niveles o categorías en unos factores, hace que estos mismos casos tiendan tomar ciertos niveles o categorías de otro u otros factores.","code":""},{"path":"análisis-de-datos-cualitativos..html","id":"tablas-de-contingencia-y-asociación-entre-dos-atributos-o-factores.","chapter":"10 Análisis de Datos Cualitativos.","heading":"10.2 Tablas de contingencia, y asociación entre dos atributos o factores.","text":"Comenzaremos con el caso más sencillo, en el que los casos de nuestra muestra se distribuyen entre las categorías o niveles de dos atributos o factores. En este caso, la tabla de contingencia que presente los datos de esta situación será una simple tabla de doble entrada. En las filas de la tabla se dispondrán las categorías de uno de los factores, y en las columnas de la tabla se situarán las categorías del otro factor.Para ilustrar el tratamiento de una tabla de contingencia bidimensional, vamos trabajar con los datos correspondientes 51 empresas de generación eléctrica presentes en el archivo de Microsoft® Excel® “eolica_contingencia.xlsx”. Este archivo cuenta con 3 hojas, los datos en cuestión se encuentran en la que tiene por nombre “Datos”.Dentro de “Datos”, existen dos variables categóricas, atributos o factores: la variable DIMENSION, que tiene, su vez, tres categorías dependiendo del número de empresas que integradas en la matriz de la empresa en cuestión (“GRANDE”, “MEDIA” o “REDUCIDA”); y la variable VALORACION, que cuenta con tres categorías dependiendo de la opnión de un panel de expertos en cuanto la situación económica de la empresa (“OPTIMA”, “NORMAL”, “PESIMA”). Nuestra intención es analizar si existe evidencia sobre asociación entre las dos variables categóricas, en el sentido de que los casos tienden concentrarse con cierta facilidad en ciertas combinaciones de categorías de uno y otro factor, y/o posicionarse en otras combinaciones; o si por el contrario se puede afirmar que exista asociación de este tipo.El código de R que iremos aplicando se encuentra en el script “correspondencias_eolica.R”. Trabajaremos en un proyecto específico de RStudio, si así lo valoramos como conveniente (en este ejemplo, el proyecto “correspondencias”).Al abrir el script en el editor de RStudio, lo primero que veremos será la instrucción para limpiar la memoria de objetos. Tras ello, se procede la importación de los datos del fichero de Excel. Tras importar las variables, se procede ajustar la importación para que la primera columna pase ser el nombre de los casos (filas) del data frame que recibe y almacena los datos. Este data frame se denomina, por ejmplo, “eolicas”. Con un summary() comprobamos que las variables se han importado correctamente:continuación, creamos otro data frame, de nombre, por ejemplo, “originales”, con los dos atributos analizados: el factor DIMENSION y el factor VALORACION. Se comprueba si existen observaciones con missing values y, en tal caso, se eliminan dichas observaciones aplicando el filtro apropiado, partir de la función filter() del paquete dplyr:En el ejemplo, se comprueba que existen casos con missing values, por lo que la tabla queda en blanco y se descarta ninguna observación.La siguiente etapa es la construcción de la “tabla de contingencia”. La función fundamental es table() que, como sabemos, hace un recuento de los casos (frecuencias) en los que una variable toma un determinado valor (o en los que un atributo adopta una determinada categoría o nivel). Cuando esta función se aplica más de una variable o atributo, hace un recuento de los casos que adoptan las posibles combinacionesse entre valores/categorías o niveles de las variables o atributos implicados. Cuando se trata de dos atributos, por tanto, table() construye una tabla de contingencia mediante la formulación de una tabla de doble entrada. En nuestro ejemplo, la tabla de contingencia la hemos denominado, por ejemplo, “tab.originales”. Luego, la hemos presentado con un formato de tabla elaborado con la función kable() del paquete knitr, y algunas funciones adicionales del paquete kableExtra:\nTable 10.1: Table 10.2: Empresas eólicas\nEs de destacar que, en el código de la tabla, se ha añadido la función addmargin() para que se añadan las frecuencias marginales de las categorias de ambos atributos (sumas de filas y columnas). Además, se ha utilizado la función add_header_above() del paquete kableExtra para añadir filas superiores al encabezado, que ocupen distintas cantidades de columnas.La tabla también se puede representar gráficamente mediante la función mosaic() de la librería {vcd}, con lo que se percibirá mejor la magnitud de las frecuencias conjuntas (celdas de la tabla). mayor frecuencia, mayor área del rectángulo correspondiente:Los argumentos de la función mosaic() juegan el siguiente papel:tab.originales: Es la tabla de contingencia que contiene los datos visualizar en el gráfico mosaico. Debe ser una tabla de contingencia creada con la función table() o una matriz con datos categóricos.tab.originales: Es la tabla de contingencia que contiene los datos visualizar en el gráfico mosaico. Debe ser una tabla de contingencia creada con la función table() o una matriz con datos categóricos.main: Título principal del gráfico.main: Título principal del gráfico.shade: Si se establece en TRUE, aplica un coloreado las celdas del mosaico; si es FALSE todas los rectángulos serán del mismo color.shade: Si se establece en TRUE, aplica un coloreado las celdas del mosaico; si es FALSE todas los rectángulos serán del mismo color.gp: Parámetros gráficos para personalizar la apariencia del gráfico. En este caso, se usa shading_Marimekko() para aplicar un sombreado específico. Hay otras funciones de parámetros gráficos como shading_hcl(), shading_max, o cualquier función personalizada que devuelva un objeto de clase gpar.gp: Parámetros gráficos para personalizar la apariencia del gráfico. En este caso, se usa shading_Marimekko() para aplicar un sombreado específico. Hay otras funciones de parámetros gráficos como shading_hcl(), shading_max, o cualquier función personalizada que devuelva un objeto de clase gpar.main_gp: Parámetros gráficos para el título principal, como el tamaño de la fuente. Se pueden especificar atributos como, por medio de la función gpar(), como fontsize, fontfamily, col, etc.main_gp: Parámetros gráficos para el título principal, como el tamaño de la fuente. Se pueden especificar atributos como, por medio de la función gpar(), como fontsize, fontfamily, col, etc.sub_gp: Igual que el caso anterior; pero para el subtítulo del gráfico de mosaico.sub_gp: Igual que el caso anterior; pero para el subtítulo del gráfico de mosaico.En nuestro ejemplo, podemos apreciar con claridad como una importante proporción de los casos (empresas) se concentran en la combinación de valoración óptima y dimensión de la empresa matriz reducida. También es destacable la combinación de valoración normal y dimensión de la compañía matriz grande. Por el lado opuesto, destaca la combinación de valoración normal y dimensión de la matriz media, que posee ningún caso (frecuencia 0), y valoración pésima y dimesión de la matriz reducida.Otro modo de visualizar la estructura de la tabla es hacer gráficos de barras que muestren las frecuencias de las categorías de cada factor (frecuencias marginales), aunque en cada barra se pueda diferenciar, además, los casos o frecuencias que pertenecen las categorías del otro factor. En nuestro caso:Se observa cómo, en cuanto al atributo o factor DIMENSION, la categoría más frecuente es la dimensión “REDUCIDA”, mientras que la categoría que menos se da en la muestra es, destacadamente, “MEDIA”. Además, como ya se ha comentado, destaca que la mayor parte de los casos de la categoría “REDUCIDA” tienen una valoración de “OPTIMA” en el factor VALORACION. También llama la atención que en la categoría “MEDIA” existen casos con valoración “NORMAL”. En cuanto al gráfico del atributo o factor “VALORACION”, el mayor número de frecuencias, de modo muy destacado, se concentran en la categoría “OPTIMA”, estando las otras dos categorías bastante igualadas en cuanto al número de casos. Es reseñable también que, en la categoría “OPTIMA”, la mayor parte de casos tienen una dimensión “REDUCIDA”.Como ya hemos dicho, una de las cuestiones más importantes en el análisis de tablas de contingencia es determinar si existe asociación entre ambos atributos o factores (valoración de las empresas y dimensión de las compañías matrices), en el sentido de poder plantear que los casos (empresas) concentradas en ciertas categorías concretas de uno de los factores o atributos tienden concentrarse, simultáneamente, en ciertas categorías concretas del otro factor o atributo; o al revés: que el hecho de que los casos tiendan concentrarse en ciertas categorías de uno de los factores o atributos está relacionado con que se concentren en ciertas categorías del otro factor o atributo. En nuestro ejemplo, ya hemos señalado algunas combinaciones de categorías que podrían llevar pensar que existe cierto grado de asociación entre los atributos DIMENSION y VALORACION.Existen diferentes pruebas para verificar la posible existencia de asociación entre dos factores. Una de ellas es el contraste o prueba de asociación de Pearson. Este contraste se base en un estadístico del contraste, que bajo la hipótesis nula de que existe asociación sigue una distribución Chi/Ji Cuadrado. El estadístico, en realidad, es una medida global de lo distante que está la tabla de contingencia observada (sus frecuencias conjuntas) respecto la estructura “ideal” que tendría que tener si existiera independencia “total” entre ambos atributos. Así, el estadístico del contraste de la prueba de asociación de Pearson es:\\[\n\\chi^2 = \\sum_{=1}^{r} \\sum_{j=1}^{c} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]donde:( \\(O_{ij}\\) ) es la frecuencia observada en la celda ( (, j) ).( \\(O_{ij}\\) ) es la frecuencia observada en la celda ( (, j) ).( \\(E_{ij}\\) ) es la frecuencia esperada en la celda ( (, j) ), calculada como ( \\(E_{ij} = \\frac{R_i \\cdot C_j}{N}\\) ).( \\(E_{ij}\\) ) es la frecuencia esperada en la celda ( (, j) ), calculada como ( \\(E_{ij} = \\frac{R_i \\cdot C_j}{N}\\) ).( \\(R_i\\) ) es el total de la fila ( ).( \\(R_i\\) ) es el total de la fila ( ).( \\(C_j\\) ) es el total de la columna ( j ).( \\(C_j\\) ) es el total de la columna ( j ).( \\(N\\) ) es el total general de todas las observaciones.( \\(N\\) ) es el total general de todas las observaciones.Los residuos estandarizados son una medida de la desviación de las frecuencias observadas respecto las frecuencias teóricas o esperadas (en caso de independencia perfecta entre atributos) en una tabla de contingencia. Se utilizan para identificar celdas que contribuyen significativamente la asociación entre las variables. La fórmula para calcular los residuos estandarizados es:\\[\n r_{ij} = \\frac{O_{ij} - E_{ij}}{\\sqrt{E_{ij} \\left(1 - \\frac{R_i}{N}\\right) \\left(1 - \\frac{C_j}{N}\\right)}}\n\\]donde:( \\(r_{ij}\\) ) es el residuo estandarizado para la celda en la fila ( ) y la columna ( j ).El paquete {stats} de R (que se carga automáticamente, por defecto, al iniciar R, por lo que hay que “activarlo”), contiene la función chisq.test() que efectúa la prueba de asociación de Pearson. El código y resultado de la prueba es:En nuestro caso, el p-valor es menor que 0,05, luego se rechaza la hipótesis nula de independencia de los atributos o factores, y admitimos que existe asociación entre ambos.Precisamente, el paquete {vcd} ofrece la posibilidad de, para cada frecuencia conjunta, visualizar la diferencia estandarizada entre el valor observado y el que debería darse en el caso de que existiera independencia perfecta entre los dos atributos o factores (residuo de Pearson). Para ello, se aplica la función assoc(), destinada construir un gráfico con los residuos de Pearson de la tabla de contingencia. Los residuos, si son estadísticamente significativos para una significación de 0,05, se colorean de naranja, y en caso contrario de gris. Para determinar los colores concretos, se ha creado la función custom_shading(), que se pasa en el argumento gp= para personalizar el color concreto de cada barra del gráfico:Puede observarse cómo en las combinaciones GRANDE/NORMAL, GRANDE/OPTIMA y MEDIA/NORMAL los residuos de Pearson son estadísticamente significativos (lo que, su vez, indica que las frecuencias correspondientes están muy alejadas de las que debería haber en caso de independencia perfecta), lo que llevaría que la prueba haya rechazado la hipótesis de independencia entre los atributos o factores.","code":"\n# Analisis de correspondencias simple de eolicas\n# Disculpen por la falta de tildes!\n\nrm(list = ls())\n\n# DATOS\n\n  # Importando datos\n\n    library (readxl)\n    eolicas <- read_excel(\"eolica_contingencia.xlsx\", sheet =\"Datos\")\n    eolicas <- data.frame(eolicas, row.names = 1)\n    summary (eolicas)##      MARGEN            SOLVENCIA          COM           \n##  Min.   :-1159.297   Min.   :-66.95   Length:51         \n##  1st Qu.:    5.677   1st Qu.: 16.32   Class :character  \n##  Median :   34.252   Median : 42.98   Mode  :character  \n##  Mean   :   47.643   Mean   : 45.76                     \n##  3rd Qu.:   53.388   3rd Qu.: 80.31                     \n##  Max.   : 1790.123   Max.   :100.00                     \n##  NA's   :9                                              \n## \n##      FJUR                ING                NCOMP      \n##  Length:51          Min.   :2.940e-01   Min.   :    0  \n##  Class :character   1st Qu.:9.759e+01   1st Qu.:    1  \n##  Mode  :character   Median :1.241e+03   Median :   31  \n##                     Mean   :1.071e+04   Mean   : 1655  \n##                     3rd Qu.:6.766e+03   3rd Qu.:  170  \n##                     Max.   :2.559e+05   Max.   :72434  \n##                     NA's   :9                          \n## \n##       RES                ACTIVO              FPIOS          \n##  Min.   :-3274.323   Min.   :2.990e+00   Min.   : -1919.42  \n##  1st Qu.:    0.298   1st Qu.:1.605e+02   1st Qu.:    71.57  \n##  Median :   57.760   Median :2.420e+03   Median :   888.78  \n##  Mean   : 2203.986   Mean   :6.790e+04   Mean   :  8991.25  \n##  3rd Qu.:  622.075   3rd Qu.:1.638e+04   3rd Qu.:  6413.52  \n##  Max.   :78290.000   Max.   :2.429e+06   Max.   :148251.00  \n##  NA's   :5                                                  \n## \n##      RENECO             RENFIN           LIQUIDEZ      \n##  Min.   :-103.156   Min.   :-596.79   Min.   :  0.006  \n##  1st Qu.:   0.000   1st Qu.:   0.00   1st Qu.:  0.667  \n##  Median :   3.165   Median :  12.94   Median :  1.833  \n##  Mean   :   6.559   Mean   :  10.94   Mean   : 17.853  \n##  3rd Qu.:  13.329   3rd Qu.:  37.99   3rd Qu.:  6.010  \n##  Max.   :  95.013   Max.   : 144.04   Max.   :541.752  \n##                                       NA's   :2        \n## \n##     APALANCA          AUTOFIN        DIMENSION        \n##  Min.   :-312.62   Min.   :-0.401   Length:51         \n##  1st Qu.:   0.00   1st Qu.: 0.243   Class :character  \n##  Median :  18.73   Median : 0.508   Mode  :character  \n##  Mean   : 333.41   Mean   : 3.468                     \n##  3rd Qu.: 266.68   3rd Qu.: 4.093                     \n##  Max.   :6197.54   Max.   :42.156                     \n##                    NA's   :20                         \n## \n##    AUTOFINA          VALORACION       \n##  Length:51          Length:51         \n##  Class :character   Class :character  \n##  Mode  :character   Mode  :character\n  # Seleccionando factores/atributos para el analisis\n\n    library(dplyr)\n    originales<-select(eolicas, DIMENSION, VALORACION)\n    summary (originales)##   DIMENSION          VALORACION       \n##  Length:51          Length:51         \n##  Class :character   Class :character  \n##  Mode  :character   Mode  :character\n  # Identificando missing values.\n\n    library(visdat)\n    vis_miss(originales)\n    originales %>% filter(is.na(DIMENSION) | is.na(VALORACION)) %>%\n      select(DIMENSION, VALORACION)  ## [1] DIMENSION  VALORACION\n## <0 rows> (o 0- extensión row.names)\n    originales <- originales %>%\n      filter(! is.na(DIMENSION) & ! is.na(VALORACION))  \n# TABLA DE CONTINGENCIA\n\n  tab.originales <- table(originales)\n\n library(knitr)\n library(kableExtra)\n knitr.table.format = \"html\"\n\n addmargins(tab.originales) %>%\n  kable(format = knitr.table.format,\n        caption=\"Empresas eólicas\") %>%\n  kable_styling(full_width = F,\n                bootstrap_options = \"striped\", \"bordered\", \"condensed\",\n                position = \"center\",\n                font_size = 12) %>%\n  add_header_above(c(DIMENSION = 1, VALORACION = 3, \" \" = 1),\n                   bold=T,\n                   line=T) %>%\n  row_spec(0, bold= T, align = \"c\") %>%\n  column_spec(1, bold = T)\n    library (vcd)\n    mosaic(tab.originales,\n           main = \"Eólicas: Dimensión Matriz y Valoración Expertos.\",\n           shade = T,\n           gp = shading_Marimekko(tab.originales),\n           main_gp = gpar(fontsize = 14),\n           sub_gp = gpar(fontsize = 12))\n  # Representando frecuencias de categorias en factores\n\n    library (ggplot2)\n    library (patchwork)\n\n    g1 <- ggplot(originales, mapping= aes(x= DIMENSION, fill = VALORACION)) +\n          geom_bar() +\n          ggtitle(\"Tamaño de la matriz.\", subtitle = \"Empresas eólicas\") + \n          ylab(\"Frecuencias\") +\n          xlab(\"Dimensión\")\n\n    g2 <- ggplot(originales, mapping= aes(x= VALORACION, fill = DIMENSION)) +\n          geom_bar() +\n          ggtitle(\"Valoración Expertos\", subtitle = \"Empresas eólicas\") + \n          ylab(\"Frecuencias\") +\n          xlab(\"Valoración\")\n\n    (g1 + g2) + plot_annotation(title = \"Frecuencias Marginales.\",\n                  theme = theme(plot.title = element_text(size = 14)))\n# INDEPENDENCIA / ASOCIACION\n  \n  # Test de asociación de Pearson (Ji-Cuadrado)\n    \n  Prueba_asoc_Pearson <- chisq.test(tab.originales)\n  Prueba_asoc_Pearson## \n##  Pearson's Chi-squared test\n## \n## data:  tab.originales\n## X-squared = 11.494, df = 4, p-value = 0.02154\n  Residuos_std <- Prueba_asoc_Pearson$stdres\n\n  # Definir una función de sombreado personalizada\n  custom_shading <- function(residuals, cutoff = 1.96) {\n    # Crear una matriz de colores basada en los residuos estandarizados\n    colors <- ifelse(abs(residuals) > cutoff, \"orange\", \"lightgray\")\n    return(colors)\n  }\n  \n  # Aplicar la función de sombreado en el gráfico mosaico\n  \n  assoc(tab.originales,\n        main = \"Asociación: Dimensión Matriz y Valoración Expertos.\",\n        sub = \"Residuos de Pearson Tipificados. Naranja: significativos con sig. = 0,05\",\n        compress = FALSE,\n        gp = gpar(fill = custom_shading(Residuos_std)),\n        main_gp = gpar(fontsize = 14),\n        sub_gp = gpar(fontsize = 12))"},{"path":"análisis-de-datos-cualitativos..html","id":"análisis-de-correspondencias.","chapter":"10 Análisis de Datos Cualitativos.","heading":"10.3 Análisis de correspondencias.","text":"El análisis de correspondencias es una técnica destinada representar visualmente una tabla de contingencia, en un gráfico bidimensional. Puede utilizarse para representar una tabla de dos atributos o factores (análisis de correspondencias simple) o de más de dos (análisis de correspondencias múltiple). Cuando en el gráfico bidimensional se representan más de dos atributos o factores, este análisis de convierte, además, en una técnica de reducción de la dimensión de la información. De hecho, se puede afirmar que el análisis de correspondencias es una suerte de análisis de componentes principales aplicado variables categóricas.De acuerdo lo anterior, se pueden establecer los siguientes paralelismos:Las componentes en el análisis de componentes principales equivalen los ejes o dimensiones del análisis de correspondencias.Las componentes en el análisis de componentes principales equivalen los ejes o dimensiones del análisis de correspondencias.La varianza total o comunalidad de las variables originales métricas del análisis de componentes principales pasa ser, en el análisis de correspondencias, la inercia total.La varianza total o comunalidad de las variables originales métricas del análisis de componentes principales pasa ser, en el análisis de correspondencias, la inercia total.La varianza que es capaz de asumir cada componente en el análisis de componentes principales ahora se denomina inercia principal (del eje o dimensión en cuestión).La varianza que es capaz de asumir cada componente en el análisis de componentes principales ahora se denomina inercia principal (del eje o dimensión en cuestión).Las puntuaciones de las componentes para un caso son, ahora, las coordenadas de una categoría de uno de los atributos o factores.Las puntuaciones de las componentes para un caso son, ahora, las coordenadas de una categoría de uno de los atributos o factores.El papel de las cargas de las variables en las componentes principales lo asumen las contribuciones de las categorías o niveles de los factores o atributos en los ejes o dimensiones.El papel de las cargas de las variables en las componentes principales lo asumen las contribuciones de las categorías o niveles de los factores o atributos en los ejes o dimensiones.Como principal resultado del análisis se obtendrá un gráfico de dos ejes (dimensiones) en el cuál se situarán más próximas las categorías de uno y otro factor que mantengan entre sí cierta tendencia asociarse.Existen varios paquetes de R que permiten desarrollar un análisis de correspondencias simple (de dos atributos o factores), como es nuestro ejemplo. Hemos optado, en el ejemplo, por utilizar el paquete FactoMineR. Este paquete dispone de la función CA(), que es la encargada de realizar los cálculos del análisis.En el siguiente código, el análisis de correspondencias se aplica la tabla de contingencia “tab.originales”, almacenándose la solución en la lista “aceolicas”. Luego, se crea la lista “SolucionCA” para almacenar las tres tablas que recogerán los resultados. Después se crea un data frame, “EigenCA”, con el elemento “eig” de la solución del análisis. Ese elemento es un data frame de dos columnas (dimensiones o ejes del análisis), para cada una de las cuales se reúnen tres informaciones: la inercia principal o varianza recogida por la dimensión o eje, el porcentaje que supone respecto la inercia total puesta en juego, y el porcentaje acumulado. El data frame se pasa formato “tabla” de kable() con el nombre “TableEigenCA”, que se guarda en la lista “SolucionCA” como su primer elemento.Después se generan dos tablas con los mismos elementos, para cada uno de los atributos o factores del análisis. La primera corresponde al atributo DIMENSION. Esta tabla cuenta con una fila por cada una de las categorías de DIMENSION, esto es, “GRANDE”, “MEDIA” y “REDUCIDA”. La segunda corresponde al atributo VALORACION, y cuenta con una fila para sus categorías, “OPTIMA”, “NORMAL”, “PESIMA”. En ambas tablas, las columnas son las siguientes:Inercia: La inercia es una medida de la varianza explicada por cada fila o columna (categoría de alguno de los atributos) en el análisis de correspondencias. Valores más altos indican que la fila o columna contribuye más la varianza total del análisis (Inercia Total).Inercia: La inercia es una medida de la varianza explicada por cada fila o columna (categoría de alguno de los atributos) en el análisis de correspondencias. Valores más altos indican que la fila o columna contribuye más la varianza total del análisis (Inercia Total).Coordenadas Dim. 1 y Dim. 2: Coordenadas de las filas o columnas en las dos dimensiones del espacio de correspondencias. Indican la posición de cada fila o columna en el espacio bidimensional.Coordenadas Dim. 1 y Dim. 2: Coordenadas de las filas o columnas en las dos dimensiones del espacio de correspondencias. Indican la posición de cada fila o columna en el espacio bidimensional.cos2 Dim. 1 y Dim. 2 (Coseno Cuadrado): El coseno cuadrado de los ángulos entre las filas o columnas y las dimensiones. Mide la calidad de la representación de las filas o columnas en las dimensiones. Valores cercanos 1 indican que la fila o columna está bien representada en esa dimensión. Valores bajos indican una mala representación.cos2 Dim. 1 y Dim. 2 (Coseno Cuadrado): El coseno cuadrado de los ángulos entre las filas o columnas y las dimensiones. Mide la calidad de la representación de las filas o columnas en las dimensiones. Valores cercanos 1 indican que la fila o columna está bien representada en esa dimensión. Valores bajos indican una mala representación.\nTable 10.3: Table 10.4: Análisis de correspondencias: Dimensión Matrix vs Valoración.\n\nTable 10.3: Table 10.3: Análisis de correspondencias: DIMENSION.\n\nTable 10.3: Análisis de correspondencias: VALORACION.\nEn nuestro ejemplo, la primera tabla nos informa, esencialmente, de que la primera dimensión o eje recoge el 77,5% de la inercia total (varianza o comportamiento) de las categorías de los dos atributos o factores, mientras que la segunda dimensión o eje asume algo menos del 22,5%. Recordemos que esta interpretación es similar la que se hace cuando se determinan los porcentajes de “comunalidad” que recogen las componentes calculadas, en el análisis de componentes principales. Recordemos también que, en el caso del análisis de correspondencias simple (dos atributos o factores), las dos dimensiones recogen el 100% de la inercia total (puesto que se “descarta” ninguna dimensión de las calculadas).Es habitual que los porcentajes de inercia principal asumidos por ambas dimensiones o ejes puedan estar bastante desequilibrados favor del primer eje; sobre todo cuando hay una fuerte asociación entre los atributos o factores. Algunas características que pueden ayudar un mayor equilibrio son:Distribución de los Datos: Una distribución equilibrada de las frecuencias en la tabla de contingencia puede ayudar que la inercia total se distribuya de manera más uniforme.Distribución de los Datos: Una distribución equilibrada de las frecuencias en la tabla de contingencia puede ayudar que la inercia total se distribuya de manera más uniforme.Número de Categorías: Tener un número similar de categorías en ambos atributos puede favorecer una distribución más equitativa de la inercia entre los ejes.Número de Categorías: Tener un número similar de categorías en ambos atributos puede favorecer una distribución más equitativa de la inercia entre los ejes.Relaciones Simétricas: Que las relaciones entre las categorías de los atributos sean simétricas y estén dominadas por unas pocas asociaciones fuertes (ninguna valoración domina de un modo extraordinario en alguna dimensión de matriz).Relaciones Simétricas: Que las relaciones entre las categorías de los atributos sean simétricas y estén dominadas por unas pocas asociaciones fuertes (ninguna valoración domina de un modo extraordinario en alguna dimensión de matriz).Tamaño de la Muestra: Un tamaño de muestra grande.Tamaño de la Muestra: Un tamaño de muestra grande.Homogeneidad de las Categorías: Que las categorías dentro de cada atributo sean relativamente homogéneas respecto sus asociaciones con las categorías del otro atributo (la estructura de valoraciones es similar entre dimensiones).Homogeneidad de las Categorías: Que las categorías dentro de cada atributo sean relativamente homogéneas respecto sus asociaciones con las categorías del otro atributo (la estructura de valoraciones es similar entre dimensiones).En cuanto la segunda tabla, destinada al análisis de las categorías del atributo o factor DIMENSION, podemos concluir lo siguiente: la mayor varianza (inercia) de las tres categorías de dimensión de la compañía matriz de pertenencia corresponde “GRANDE”, seguida de “MEDIA” y “REDUCIDA”. En cuanto la calidad de la representación de estas categorías en las dos dimensiones o ejes; las categorías “GRANDE” y “MEDIA” están representadas principalmente por la dimensión o eje 1; solo la categoría “REDUCIDA” viene mejor representada (levemente) por la dimensión o eje 2.Por último, la tercera tabla recoge la representación de las categorías del atributo o factor VALORACION: “OPTIMA”, “NORMAL” y “PESIMA”. La mayor inercia o varianza es la de “NORMAL”, seguida de “OPTIMA” y, por último, “PESIMA”. En cuanto la calidad de la representación en las dimensiones o ejes, “NORMAL” y “OPTIMA” vienen representadas, casi exclusivamente, por la dimensión o eje 1; mientras que ocurre lo contrario con la categoría “PESIMA”.El principal output del análisis de correspondencias es el gráfico bidimensional donde se representan las categorías de los atributos o factores. En general, cuanto más cerca se localicen determinada categoría de un factor y determinada categoría del otro, mayor será la relación estadística (asociación) entre ambas categorías, lo que implica su vez mayor asociación entre los atributos o factores.Antes de presentar el gráfico bidimensional, vamos construir un gráfico de barras con los porcentajes de la inercia total que asumen cada una de las dos dimenciones o ejes del gráfico. Esto es importante, ya que si la inercia recogida por la segunda dimensión o eje es muy pequeña, como veces ocurre, habría que tener en cuenta, sobre todo, la localización de las categorías en la primera dimensión o eje, la hora de establecer conclusiones en cuanto la asociación o entre categorías. También es necesario tener en cuenta la calidad con que cada eje representa cada categoría de cada variable o factor, medida con el coseno cuadrado (cos2), como ya se ha comentado.Para crear de un modo sencillo el gráfico de contribuciones de los ejes o dimensiones la inercia total, puede recurrirse la función fviz_screeplot() del paquete factoextra:Puede observarse, como ya se recogió en la primera table de la solución, que la primera dimensión o eje asume el 77,5% de la inercia tital (varianza o comportamiento de las categorías), mientras que la segunda dimensión o eje asume el 22,5% restante.En cuanto al gráfico bidimensional, puede obtenerse igualmente mediante la función fviz_ca_biplot() de factoextra. Este es el papel que juegan los diferentes argumentos de la función:map = \"symmetric\": Representa tanto las filas como las columnas en el mismo espacio, utilizando la misma escala.map = \"symmetric\": Representa tanto las filas como las columnas en el mismo espacio, utilizando la misma escala.axes = c(1, 2): Selecciona las dos primeras dimensiones para el gráfico.axes = c(1, 2): Selecciona las dos primeras dimensiones para el gráfico.label = \"\": Muestra las etiquetas de todas las categorías.label = \"\": Muestra las etiquetas de todas las categorías.repel = TRUE: Evita la superposición de etiquetas.repel = TRUE: Evita la superposición de etiquetas.col.col y col.row: Colores para las columnas y filas, respectivamente.col.col y col.row: Colores para las columnas y filas, respectivamente.labs(): Añade títulos y subtítulos al gráfico.labs(): Añade títulos y subtítulos al gráfico.theme(): Ajusta el tamaño del texto en el gráfico.theme(): Ajusta el tamaño del texto en el gráfico.Es de destacar que el argumento map= controla cómo se representan las filas y columnas en el gráfico bidimensional del análisis de correspondencias. Este argumento tiene varias opciones que determinan la escala y la simetría de la representación. Para representar las posibles asociaciones entre las categorías de los dos factores implicados, la opción más apropiada es “symmetric”. Esta opción permite visualizar las filas y las columnas en el mismo espacio, facilitando la interpretación de las asociaciones entre las categorías de ambas variables.En nuestro ejemplo, se aprecia cómo hay una intensa asociación entre la categoría de VALORACIÓN “OPTIMA”, y la categoria de DIMENSION (de la empresa matriz correspondiente) “REDUCIDA”. También se aprecia cierta asociación entre las categorías de DIMENSION “GRANDE” y VALORACION “NORMAL”. Por último la categoría de VALORACION “PESIMA”, y la categoría del factor o atributo DIMENSION “MEDIA” están bastante alejados de otras categorías, lo que implica que parece estar muy asociadas con otras categorías específicas.Finalmente, para una presentación más compacta de los dos últimos gráficos, puede recurrirse al paquete {patchwork}:","code":"\n# ANALISIS DE CORRESPONDENCIAS SIMPLE\n\n  library (FactoMineR)\n  aceolicas <- CA(X = tab.originales, graph = F)\n\n  SolucionCA <- list()\n  \n  EigenCA <- as.data.frame(t(aceolicas$eig))\n  EigenCA$elemento <- c(\"Inercia Total\", \"% Inercia Principal\", \"Acumulada\")\n  EigenCA <- data.frame(EigenCA, row.names = 3)\n  \n  \n  TableEigenCA <- EigenCA %>%\n    kable(format = knitr.table.format,\n          caption=\"Análisis de correspondencias: Dimensión Matrix vs Valoración.\",\n          col.names = c(\"Dimensión/Eje 1\", \"Dimensión/Eje 2\"),\n          digits = 3,\n          align= c(\"c\", \"c\")) %>%\n    kable_styling(full_width = F,\n                  bootstrap_options = \"striped\", \"bordered\", \"condensed\",\n                  position = \"center\",\n                  font_size = 12) %>%\n    row_spec(0, bold= T, align = \"c\")\n  \n    SolucionCA[[1]] <-TableEigenCA\n  \n  # Extraer la información de las filas\n    rows_data <- aceolicas$row$coord\n    rows_cos2 <- aceolicas$row$cos2\n\n  # Crear un DataFrame para las filas\n    rows_df <- data.frame(\n    Iner_1000 = aceolicas$row$inertia,\n    Dim_1 = rows_data[, 1],\n    cos2_1 = rows_cos2[, 1],\n    Dim_2 = rows_data[, 2],\n    cos2_2 = rows_cos2[, 2]\n    )\n    rownames(rows_df) <- rownames(rows_data)\n    \n    TableRows <- rows_df %>%\n      kable(format = knitr.table.format,\n          caption= (paste0(\"Análisis de correspondencias: \", colnames(originales)[1], \".\")),\n          col.names = c(\"Inercia\", \"Coordenadas Dim. 1\", \"Cos2 Dim. 1\",\n                        \"Coordenadas Dim. 2\", \"Cos2 Dim. 2\"),\n          digits = 3,\n          align= c(\"c\", \"c\", \"c\", \"c\", \"c\")) %>%\n    kable_styling(full_width = F,\n                  bootstrap_options = \"striped\", \"bordered\", \"condensed\",\n                  position = \"center\",\n                  font_size = 12) %>%\n    row_spec(0, bold= T, align = \"c\")\n    \n    SolucionCA[[2]] <-TableRows\n\n  # Extraer la información de las columnas\n    columns_data <- aceolicas$col$coord\n    columns_cos2 <- aceolicas$col$cos2\n\n  # Crear un DataFrame para las columnas\n    columns_df <- data.frame(\n    Inercia = aceolicas$col$inertia,\n    Dim_1 = columns_data[, 1],\n    cos2_1 = columns_cos2[, 1],\n    Dim_2 = columns_data[, 2],\n    cos2_2 = columns_cos2[, 2]\n    )\n    rownames(columns_df) <- rownames(columns_data)\n\n    TableCols <- columns_df %>%\n      kable(format = knitr.table.format,\n          caption= (paste0(\"Análisis de correspondencias: \", colnames(originales)[2], \".\")),\n          col.names = c(\"Inercia\", \"Coordenadas Dim. 1\", \"Cos2 Dim. 1\",\n                        \"Coordenadas Dim. 2\", \"Cos2 Dim. 2\"),\n          digits = 3,\n          align= c(\"c\", \"c\", \"c\", \"c\", \"c\")) %>%\n    kable_styling(full_width = F,\n                  bootstrap_options = \"striped\", \"bordered\", \"condensed\",\n                  position = \"center\",\n                  font_size = 12) %>%\n    row_spec(0, bold= T, align = \"c\")\n    \n   SolucionCA[[3]] <-TableCols\n   \n  SolucionCA[[1]]\n  SolucionCA[[2]]\n  SolucionCA[[3]]\n  # Gráfico de contribuciones de las dimensiones  a la Inercia Total\n\n    library(factoextra)\n\n    gcontrib <- fviz_screeplot(aceolicas,\n                               addlabels= TRUE,\n                               barcolor= \"darkblue\",\n                               barfill= \"orange\",\n                               linecolor= \"red\") +\n                labs(title= \"Contribución de los ejes a la Inercia Total.\",\n                     subtitle = \"Dimensión Matriz y Valoración Expertos.\") +\n                ylab(\"Porcentaje de Inercia Total\") +\n                xlab(\"Eje\") +\n                theme(text = element_text(size = 12))\n\ngcontrib\n    gbiplot <- fviz_ca_biplot (aceolicas,\n                               axes= c(1,2),\n                               label= \"all\",\n                               repel = T,\n                               col.col= \"orange\",\n                               col.row= \"darkblue\",\n                               map= \"symmetric\") +\n    labs(title= \"Gráfico de dispersión de categorías.\",\n         subtitle = \"Eólicas: Matriz y Valoración Expertos.\") +\n    theme(text = element_text(size = 12))\n\n    gbiplot\n    gcombinado <- gcontrib / gbiplot\n    gcombinado <- gcombinado +\n      plot_annotation(title = \"DIMENSIÓN MATRIZ vs VALORACIÓN EXPERTOS.\",\n      subtitle = \"Empresas eólicas.\",\n      caption = \"Análisis de Correspondencias Simple.\",\n      theme = theme(plot.title = element_text(size = 16, face = \"bold\"),\n                    plot.subtitle = element_text(size = 14),\n                    plot.caption = element_text(size = 12))\n    )\n\n    gcombinado"},{"path":"análisis-de-datos-cualitativos..html","id":"modelos-logaritmico-lineales-aplicados-a-tablas-de-contingencia.","chapter":"10 Análisis de Datos Cualitativos.","heading":"10.4 Modelos logaritmico-lineales aplicados a tablas de contingencia.","text":"Volviendo la verificación de la existencia de asociación entre factores, hemos de tener en cuenta la posibilidad de que los factores en estudio sean más de dos. En este caso multifactorial, son aplicable las técnicas exploradas anteriormente, pensadas para el caso bifactorial (simple).Una posibilidad que se nos ofrece es la aplicación de modelos logarítmico-lineales (log-lineales) tablas de contingencia multifactoriales, que es lo que se tratará en este apartado.Los modelos log-lineales aplicados tablas de contingencia parten de la condición de independencia entre factores. Supongamos el caso simple, con una tabla de contingencia de dos factores o atributos, y B. Bajo la hipótesis de independencia (absoluta o teórica) entre los factores, tendremos que cada frecuencia absoluta conjunta de la tabla se obtiene como:\\[\nn_{ij} = \\frac{R_i \\cdot C_j}{N} = N \\cdot \\frac{R_i}{N} \\cdot \\frac{C_j}{N}\n\\] con:( \\(n_{ij}\\) ) es la frecuencia conjunta para la categoría o fila ( ) del atributo o factor , y la categoría o columna ( j ) del atributo o factor B).( \\(R_i\\) ) es el total de la fila ( ) del atributo o factor .( \\(C_j\\) ) es el total de la columna ( j ) del atributo o factor B.Tomando logaritmos:\\[\n\\ln{n_{ij}} = ln{N} \\ + ln{\\frac{R_i}{N}} + ln{\\frac{C_j}{N}}\n\\]Y renombrando los términos:\\[\n\\ln{n_{ij}} = \\lambda + \\lambda^A_i + \\lambda^B_j\n\\]Si existe independencia entre ambas variables o factores, tendremos:\\[\n\\ln{n_{ij}} = \\lambda + \\lambda^A_i + \\lambda^B_j + \\lambda^{AB}_{ij}\n\\]Los términos \\(\\lambda^A_i\\) y \\(\\lambda^B_j\\) se denominan efectos directos o principales, mientras que el término \\(\\lambda^{AB}_{ij}\\) es el efecto conjunto o interacción entre los dos atributos o factores. Bajo la hipótesis de independencia entre los dos factores, ese efecto tomaría valor 0.Si el modelo planteado solo tiene en la especificación los efectos directos, se dirá que es el modelo de independencia. Si se plantean todas las interacciones posibles entre los factores, se hablará del modelo saturado. El modelo saturado otorga un ajuste perfecto; pero es poco útil la hora de extraer conclusiones relevantes. Se requiere un modelo que, aunque ajuste al 100% las frecuencias, recoja solo los efectos más importantes.Si algunos de los efectos más importantes (es decir, significativos en términos estadísticos) son conjuntos (interacciones), podremos concluir que existe asociación entre los factores del modelo (al menos, entre los que existan interacciones importantes, en el caso de más de dos factores). Si es así, y el modelo que mejor representa la realidad es el de independencia, diremos que los factores son independientes unos de otros, y que existe asociación (significativa) entre ellos.Los modelos han de respetar siempre la regla de la jerarquía en su especificación: solo se podrá plantear en el modelo una interacción entre los atributos y B si se han especificado los efectos directos de y de B. O se podrá plantear una interacción conjunta entre los factores , B y C si se han planteado también las interacciones entre y B, B y C, y y C.Los modelos log-lineales se estiman por el método de máxima-verosimilitud.La estrategia seguir para estudiar la asociación entre factores será plantear diferentes especificaciones, y comprobar si son aptas para representar bien la realidad (tabla de contingencia). Eso se consigue mediante pruebas que evalúan la magnitud de los residuos, entendidos como la diferencia entre las frecuencias conjuntas realmente observadas, y las frecuencias estimadas por el modelo estimado. Las dos pruebas más comunes son la del ratio de verosimilitud, y la de Pearson.SI existen varios modelos que, desde el punto de vista de las pruebas anteriores, son aptos para representarrazonablemente la realidad, se eligirá el mejor de ellos mediante algún criterio específico, como puede ser aquel que minimice el Criterio de Información de Akaike, que penaliza, para una capacidad de explicación semejante, al modelo con una estructura más compleja (más términos).Los parámetros estimados finalmente, correspondientes la mejor especificación, informarán si los efectos directos e interacciones o efectos conjuntos entre los distintos factores del modelo final tienen una influencia positiva o negativa sobre el valor de las diferentes frecuencias conjuntas de la tabla de contingencia, lo que informará solo de si existe asociación entre factores o ; sino también de, en caso de existir, de cómo se materializa tal asociación.Para ejemplificar la especificación, estimación e interpretación de los modelos log-lineales aplicados tablas de contingencia, vamos plantear un caso en el que entran en juego 3 factores o atributos, que caracterizan un conjunto o muestra de 474 empresas eólicas. Estos factores o atributos son:DIMENSION: tamaño del grupo empresarial al que pertenece la empresa en cuestión. Tiene tres niveles: grande, media y reducida.DIMENSION: tamaño del grupo empresarial al que pertenece la empresa en cuestión. Tiene tres niveles: grande, media y reducida.AUTOFINA: capacidad de autofinanciación de la empresa medio y largo plazo. Tiene tres niveles: alta, positiva y negativa.AUTOFINA: capacidad de autofinanciación de la empresa medio y largo plazo. Tiene tres niveles: alta, positiva y negativa.FJUR: forma jurídica. Tiene dos posibles categorías: Sociedad anónima o Sociedad limitada.FJUR: forma jurídica. Tiene dos posibles categorías: Sociedad anónima o Sociedad limitada.Los datos se encuentran alojados en la hoja “Datos” del archivo de Microsoft® Excel® “eolica_contingencia2.xlsx”. El código desarrollar está disponible en el script “loglineal_eolica.R”. Puede desarrollarse el ejemplo creando para ello un proyecto de RStudio, por ejemplo, el proyecto “loglineal”.Comenzando ejecutar el código presente en el script, la primera parte se dedica, como es habitual, la gestión de los datos: borrado previo de memoria, importación de los datos alojados en el archivo de Excel y volcado un data frame, corrección del nombre de las filas de este, y selección de las variables categóricas del estudio:El data frame “originales2” albergará los tres factores o atributos del análisis:Posteriormente se desarrolla la localización y tratamiento de missing data, ya que para realizar el análisis es necesario que todos los casos posean dato en todas las variables. Para tener una idea general, se puede recurrir la función vis_miss() del paquete visdat, que localizará gráficamente los missing values de las diferentes variables, y calculará el porcentaje de casos que supone, con respecto al total de observaciones. En el ejemplo, los casos con missing data se concentran en el atributo o factor AUTOFINA, con 133 casos. El nombre concreto de los casos afectados se visualiza con un filtro construido partir de la función filter() del paquete dplyr:Se decide eliminar los casos que carecen de valor en el atributo AUTOFINA, lo que se realiza mediante otro filtro:El siguiente bloque se ocupa de crear el objeto “table” que recoge la tabla de contingencia formada al cruzar la distribución de las frecuencias o casos entre las diferentes categorías de los distintos factores. La función para convertir el data frame en una estructura de almacenamiento de datos especial llamada table (que es la tabla de contingencia) es precisamente table(). La tabla de contingencia construida, de nombre “tab.originales2”, es posteriormente presentada como una tabla diseñada partir de la función kable() del paquete knitr, y otras funciones incluidas en el paquete kableExtra:\nTable 10.5: Table 10.6: Empresas eólicas\nUn modo visual de obtener una primera idea de las relaciones que se incluyen en la tabla es construir un gráfico de mosaico, con la función mosaic() del paquete {vcd}, en la que el área de los diferentes rectángulos sea proporcional la frecuencia conjunta correspondiente:En cuanto las frecuencias marginales de cada nivel o categoría de los tres atributos o factores, pueden representarse estas mediante gráficos de barras, generados mediante las funciones del paquete ggplot2, y reunidos en una sola imagen mediante el paquete patchwork. El código es el siguiente:Los modelos log-lineales se especifican y estiman mediante la función loglm() de la librería MASS (el paquete MASS lo hemos activado junto la propia función loglm() ya que, de hacerlo con la función library(), se crea un conflicto con la función select() del paquete dplyr).Cuando se estiman los modelos, los diferentes resultados se almacenan en una lista. Entre estos elementos se encuentran los parámetros estimados, que, dentro de la lista de resultados, se almacenan, su vez, en una estructura que, según la especificación que se elija del modelo, puede llegar ser compleja. para facilitar el trabajo de extracción de los parámetros y almacenamiento en un data frame, se ha desarrollado una función denominada extraer_coeficientes(). Debido la complejidad del código, y antes de proceder estimar los modelos, exponemos el código de la función, que recibe como input o argumento el nombre de un modelo log-lineal estimado, y devuelve como output un data frame con los coeficientes o parámetros estimados:Del mismo modo, se ha creado otra función para facilitar la pesentación de la información más importante que se genera con la estimación del modelo loglineal. Es la función generar_solucion(). Esta función recibe como argumento o intput el nombre del modelo estimado, y devuelve tres elementos: dos tablas diseñadas con kable(), que se almacenan en una lista, y un gráfico de mosaico que recoge los residuos del modelo (diferencias entre las frecuencias conjuntas reales u observadas de la tabla de contingencia, y sus estimaciones por parte del modelo). La primera de las tablas recoge las pruebas de validez de los modelos del ratio de verosimilitud (deviance), y de Pearson: nombre de la prueba, estadístico del contraste, grados de libertad y p-valor. La segunda tabla recoge los coeficientes o parámetros estimados, para lo cual la función llama, su vez, la función extraer_coeficientes(), toma el data frame que construya tal función, y la usa como base para diseñar la segunda tabla.El código de la función es el siguiente:Con el par de funciones anteriores, es fácil estimar un modelo log-lineal aplicado tablas de contingencia y recopilar la información más relevante.Comenzaremos por el modelo de independencia, que plantea que solo existen efectos directos en la determinación de las frecuencias conjuntas de la tabla (-asociación entre factores). La especificación y estimación del modelo es la siguiente:Puede comprobarse que se ha creado la lista “solucion_modelo_indep”, que guarda dos elementos ($Informacion y $Coeficientes), y se ha generado el gráfico de mosaico de los residuos. Cuanto más intensos son los colores, mayores serán los residuos (en valor absoluto) y, por lo tanto, peor será el ajuste obtenido, lo que se deberá que se ha considerado (erróneamente) que hay interacción entre los factores (asociación o independencia). En cuanto los tonos de color:Rectángulos azulados: Indican que la frecuencia observada es mayor que la frecuencia estimada por el modelo. Es decir, el modelo subestima la frecuencia observada.Rectángulos azulados: Indican que la frecuencia observada es mayor que la frecuencia estimada por el modelo. Es decir, el modelo subestima la frecuencia observada.Rectángulos rojizos: Indican que la frecuencia observada es menor que la frecuencia estimada por el modelo. Es decir, el modelo sobreestima la frecuencia observada.Rectángulos rojizos: Indican que la frecuencia observada es menor que la frecuencia estimada por el modelo. Es decir, el modelo sobreestima la frecuencia observada.En nuestro caso, existen frecuencias en los que los residuos toman colores bastante intensos, lo que hace pensar en que se ha producido un buen ajuste. Como este modelo planteaba un escenario en el que hay asociación entre los atributos; el gráfico parece apoyar la hipótesis de que sí existe asociación, al menos entre algunos de los factores o atributos.Vamos interpretar ahora el primer elemento de la lista “solucion_modelo_indep”, que es la tabla donde se disponen las pruebas de validez del modelo. Para obtener la tabla ejecutaremos:\nTable 10.7: Validación del modelo\nEn ambas pruebas (ratio de verosimilitud y Pearson) se obtiene un p-valor de 0. Teniendo en cuenta que la hipótesis nula de ambas pruebas es que existe un buen ajuste; la conclusión es que, según los resultados, el modelo de independencia es capaz de representar la realidad con suficiente precisión (de ahí la magnitud de los residuos). Esto se puede interpretar, su vez, como que se rechaza la hipótesis de independencia entre los factores y se admite que existe asociación entre, al menos, algunos de ellos.Por último, vamos mostrar el segundo elemento de la lista “solucion_modelo_indep”, que es la tabla donde se disponen los parámetros estimados del modelo, que en este caso corresponden solo efectos directos o principales. Para obtener la tabla ejecutaremos:\nTable 10.7: Coeficientes del modelo\nssage=FALSE, warning=FALSE}Puede destacarse el hecho de que, que la capacidad de autofinanciación sea negativa, tiende provocar una reducción del número de casos en las frecuencias conjuntas implicadas (signo negativo) También el hecho de que la forma jurídica adoptada sea la de Sociedad Anónima. En el extremo opuesto, destacan los signos positivos de la forma jurídica de Sociedad Limitada y la capacidad de autofinanciación alta, lo que implica que estas categorías tienden acumular más frecuencias.Pasamos ahora al modelo saturado, en el cuál se especifican todos los efectos directos o principales de los factores, y todas las interacciones posibles entre dichos factores (interacciones dos dos, y la interacción entre los tres factores de modo simultáneo). Este modelo, en la práctica, es relevante porque, aunque explica al 100% las frecuencias observadas, indica cuál de los niveles de los factores o atributos y sus interacciones son los más relevantes (modelo redundante). Para su estimación, simplemente se sustituyen los signos “+” del modelo anterior por los signos “*”. El modelo se denominará “modelo_sat”:El gráfico de mosaico muestra que, obviamente, todos los residuos son 0, ya que coinciden las frecuencias observadas y las estimadas por el modelo. En cuanto la tabla de validación:\nTable 10.8: Validación del modelo\nEn ambas pruebas el p-valor es 1, dado que se rechaza la hipótesis de que el modelo representa adecuadamente la realidad (de hecho, la representa perfectamente). Pero, como hemos dicho, desde el punto de vista del análisis estructural el modelo saturado es muy útil, ya que distingue entre los efectos e interacciones relevantes y los que son poco importantes. Por último, mostraremos la tabla con los coeficientes estimados, correspondientes tanto los efectos principales o directos como las interacciones:\nTable 10.8: Coeficientes del modelo\nLa estimación del modelo saturado permite aplicar algún algoritmo para la obtención de la especificación de un modelo que, sin llegar ser el saturado, asegure una representación valida de la realidad obviando los efectos e interacciones que sean estadísticamente relevantes. Por ejemplo, un método es el step / backward, que, en función del Criterio de Información de Akaike (AIC), irá probando estimar especificaciones más simples que disminuyan el AIC (lo que implica una mejor especificación). En nuestro caso, se aplicará con el código:En el código anterior, se genera el modelo óptimo siguiendo el proceso de eliminar los términos del modelo que más contribuyan la reducción del valor de AIC, mediante la función step(). El modelo así obtenido se guarda como “modelo_def”.En nuestro ejemplo, la especificación del modelo definitivo incluye los tres efectos directos o principales de los factores o atributos, y todas las interacciones entre pares de factores; es decir, se diferencia del modelo saturado en la inclusión de la interacción entre los tres factores o atributos de modo simultáneo.Luego, se pasa el modelo la función generar_solución() para obtener los resultados (el gráfico de mosaico y las dos tablas). El gráfico obtenido es:Los tonos grisáceos y pardos indican que existen leves diferencias entre las frecuencias conjuntas observadas, y las estimadas por el modelo. En cuanto la tabla de validación:\nTable 10.9: Validación del modelo\nLas dos pruebas muestran un p-valor superior 0,05, lo que implica, para ese nivel de significación, el rechazo de la hipótesis nula de validez o idoneidad del modelo para representar la realidad adecuadamente. En cuanto los coeficientes estimados del modelo:\nTable 10.9: Coeficientes del modelo\nEn la tabla anterior detaca, dentro de los efectos directos o principales, el coeficiente negativo de la capacidad de autofinanciación negativa y el de forma jurídica de Sociedad Anónima como principales categorías que influyen en la disminución de las frecuencias conjuntas implicadas. En el extremo opuesto se encuentra el coeficiente correspondiente la forma jurídica de Sociedad Limitada. En cuanto las interacciones entre factores, se comprueba que la simultaneidad entre una dimensión de la matriz grande y una capacidad de autofinanciación negativa parece influir en una reducción significativa de las frecuencias conjuntas implicadas. Lo mismo ocurre con la simultaneidad entre una dimensión de la empresa matriz media y una capacidad de autofinanciación alta, y entre una dimensión de la empresa matriz reducida y la forma jurídica de Sociedad Anónima. En el extremo opuesto, hay ciertas interacciones que parecen inluir en que las frecuencias conjuntas implicadas aumenten, como por ejemplo una dimensión de la matriz reducida y una capacidad de autofinanciación negativa, o de nuevo una dimensión de la matriz empresarial reducida con una forma jurídica de Sociedad Limitada.","code":"\n# Analisis de Asociacion y modelos log-lineales de eolicas\n# Disculpen por la falta de tildes!\n\nrm(list = ls())\n\n# DATOS\n\n  # Importando datos\n\n    library (readxl)\n    eolicas <- read_excel(\"eolica_contingencia2.xlsx\", sheet =\"Datos\")\n    eolicas <- data.frame(eolicas, row.names = 1)\n    summary (eolicas)\n\n  # Seleccionando factores/atributos para el analisis\n\n    library(dplyr)\n    originales2 <- eolicas %>%\n    select(DIMENSION, AUTOFINA, FJUR)\n    summary (originales2)\n# Analisis de Asociacion y modelos log-lineales de eolicas\n# Disculpen por la falta de tildes!\n\nrm(list = ls())\n\n# DATOS\n\n  # Importando datos\n\n    library (readxl)\n    eolicas <- read_excel(\"eolica_contingencia2.xlsx\", sheet =\"Datos\")\n    eolicas <- data.frame(eolicas, row.names = 1)\n    summary (eolicas)\n\n  # Seleccionando factores/atributos para el analisis\n\n    library(dplyr)\n    originales2 <- eolicas %>%\n    select(DIMENSION, AUTOFINA, FJUR)\n    summary (originales2)##      MARGEN            SOLVENCIA            COM           \n##  Min.   : -4124.22   Min.   :-1162.21   Length:474        \n##  1st Qu.:    14.09   1st Qu.:   13.95   Class :character  \n##  Median :    42.64   Median :   39.36   Mode  :character  \n##  Mean   :   794.48   Mean   :   38.19                     \n##  3rd Qu.:    64.82   3rd Qu.:   72.69                     \n##  Max.   :298700.00   Max.   :  100.00                     \n##  NA's   :87                                               \n## \n##      FJUR                ING                NCOMP      \n##  Length:474         Min.   :     0.11   Min.   :    0  \n##  Class :character   1st Qu.:   392.92   1st Qu.:    3  \n##  Mode  :character   Median :  3221.00   Median :   78  \n##                     Mean   :  8128.59   Mean   : 1193  \n##                     3rd Qu.:  7709.18   3rd Qu.:  392  \n##                     Max.   :364989.00   Max.   :72434  \n##                     NA's   :87          NA's   :1      \n## \n##       RES                ACTIVO              FPIOS           \n##  Min.   :-16107.21   Min.   :9.600e-01   Min.   : -51817.36  \n##  1st Qu.:     3.09   1st Qu.:4.702e+02   1st Qu.:     67.13  \n##  Median :   382.71   Median :8.103e+03   Median :   1578.00  \n##  Mean   :  2164.98   Mean   :3.899e+04   Mean   :  15932.61  \n##  3rd Qu.:  2413.00   3rd Qu.:3.115e+04   3rd Qu.:   8503.93  \n##  Max.   : 78290.00   Max.   :2.429e+06   Max.   :1382020.00  \n##  NA's   :52                                                  \n## \n##      RENECO              RENFIN            LIQUIDEZ       \n##  Min.   : -269.046   Min.   : -687.42   Min.   :   0.000  \n##  1st Qu.:    0.000   1st Qu.:    0.00   1st Qu.:   0.621  \n##  Median :    6.005   Median :   16.29   Median :   1.768  \n##  Mean   :  151.080   Mean   :  194.65   Mean   :  23.474  \n##  3rd Qu.:   17.506   3rd Qu.:   46.66   3rd Qu.:   3.947  \n##  Max.   :66538.096   Max.   :67943.49   Max.   :1622.359  \n##                                         NA's   :14        \n## \n##     APALANCA          DIMENSION           AUTOFINA        \n##  Min.   : -7016.77   Length:474         Length:474        \n##  1st Qu.:     0.00   Class :character   Class :character  \n##  Median :    22.64   Mode  :character   Mode  :character  \n##  Mean   :   769.68                                        \n##  3rd Qu.:   201.63                                        \n##  Max.   :177381.90                                        \n## \n##    Length     Class      Mode \n##       474 character character##   DIMENSION           AUTOFINA             FJUR          \n##  Length:474         Length:474         Length:474        \n##  Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character\n  # Identificando missing values.\n\n    library(visdat)\n    vis_miss(originales2)\n    originales2 %>% filter(is.na(DIMENSION) |\n                           is.na(AUTOFINA)|\n                           is.na(FJUR)) %>%\n    select(DIMENSION, AUTOFINA, FJUR)  ##                                                               DIMENSION AUTOFINA              FJUR\n## Sierra de Selva SL                                               GRANDE     <NA> Sociedad limitada\n## Eolica de Rubio SL                                               GRANDE     <NA> Sociedad limitada\n## Fuerzas Energeticas DEL SUR de Europa II SL.                     GRANDE     <NA> Sociedad limitada\n## Parque Eolico de LA Bobia Y SAN Isidro Sociedad Limitada       REDUCIDA     <NA> Sociedad limitada\n## Engasa Eolica SA                                                  MEDIA     <NA>  Sociedad anonima\n## Parque Eolico de Ameixenda-Filgueira SL                        REDUCIDA     <NA> Sociedad limitada\n## Compañia Eolica Granadina SA                                   REDUCIDA     <NA>  Sociedad anonima\n## Suresa Retama S.L.                                               GRANDE     <NA> Sociedad limitada\n## Parque Eolico de Adraño SL                                     REDUCIDA     <NA> Sociedad limitada\n## M Torres Desarrollos Energeticos SL                            REDUCIDA     <NA> Sociedad limitada\n## Corporacion Eolica de Valdivia SL                                GRANDE     <NA> Sociedad limitada\n## Parque Eolico de A Ruña SL                                     REDUCIDA     <NA> Sociedad limitada\n## Molinos DEL Moncayo SL.                                        REDUCIDA     <NA> Sociedad limitada\n## Eolico Alijar SA                                                 GRANDE     <NA>  Sociedad anonima\n## Parque Eolico de Vicedo SL                                     REDUCIDA     <NA> Sociedad limitada\n## Eolica de Villanueva SL                                          GRANDE     <NA> Sociedad limitada\n## Parque Eolico de Virxe DO Monte SL                             REDUCIDA     <NA> Sociedad limitada\n## Estructuras Y Revestimiento de Galicia SL                      REDUCIDA     <NA> Sociedad limitada\n## Energias Renovables EL Abra SL                                   GRANDE     <NA> Sociedad limitada\n## Proyectos Eolicos Aragoneses SL                                REDUCIDA     <NA> Sociedad limitada\n## Soslaires Canarias SL                                          REDUCIDA     <NA> Sociedad limitada\n## Eolica Cantabria SA                                            REDUCIDA     <NA>  Sociedad anonima\n## Señorio de Bariain SA                                          REDUCIDA     <NA>  Sociedad anonima\n## Energetica DEL Montalt SL                                      REDUCIDA     <NA> Sociedad limitada\n## Eolica Lodosa SL                                               REDUCIDA     <NA> Sociedad limitada\n## Parque Eolico LA Union S.L.                                    REDUCIDA     <NA> Sociedad limitada\n## Eolica Pueyo SL                                                REDUCIDA     <NA> Sociedad limitada\n## Parsona Corporacion SL.                                        REDUCIDA     <NA> Sociedad limitada\n## Infraestructuras Electricas LA Mudarra SL.                       GRANDE     <NA> Sociedad limitada\n## Eolica Unzue SL                                                REDUCIDA     <NA> Sociedad limitada\n## Technical Services Wind SL.                                    REDUCIDA     <NA> Sociedad limitada\n## Saltos DEL Mundo SL                                            REDUCIDA     <NA> Sociedad limitada\n## Sistemas Energeticos LA Plana SA                                 GRANDE     <NA>  Sociedad anonima\n## Renovalia Reserve SL.                                             MEDIA     <NA> Sociedad limitada\n## Eolpop SL.                                                     REDUCIDA     <NA> Sociedad limitada\n## Emprendimientos Y Desarrollo de Iniciativas Energeticas SL     REDUCIDA     <NA> Sociedad limitada\n## Megaturbinas Arinaga SA                                        REDUCIDA     <NA>  Sociedad anonima\n## Sunterra XXI Sociedad Limitada.                                REDUCIDA     <NA> Sociedad limitada\n## Aizdegi SL                                                     REDUCIDA     <NA> Sociedad limitada\n## Parque Eolico LA Sargilla Sociedad Anonima.                    REDUCIDA     <NA>  Sociedad anonima\n## Intercon SA                                                    REDUCIDA     <NA>  Sociedad anonima\n## Altosalvo SL                                                   REDUCIDA     <NA> Sociedad limitada\n## Bluefloat Energy International SL.                             REDUCIDA     <NA> Sociedad limitada\n## Corolla Power 1 SL                                             REDUCIDA     <NA> Sociedad limitada\n## Corolla Power 3 SL                                             REDUCIDA     <NA> Sociedad limitada\n## Parque Energetico de G C SL                                    REDUCIDA     <NA> Sociedad limitada\n## Dapasa Servicios E Inversiones SL                              REDUCIDA     <NA> Sociedad limitada\n## EL Guijorral SL                                                REDUCIDA     <NA> Sociedad limitada\n## Minicentrales Bouza Vella SL                                   REDUCIDA     <NA> Sociedad limitada\n## Locus Mentis SL                                                REDUCIDA     <NA> Sociedad limitada\n## Vento Laracha SL.                                                 MEDIA     <NA> Sociedad limitada\n## Huerto Solar EL Tronco SL                                      REDUCIDA     <NA> Sociedad limitada\n## Fotovoltaica LA Solana SL                                      REDUCIDA     <NA> Sociedad limitada\n## Fargo MAS 3 SL                                                 REDUCIDA     <NA> Sociedad limitada\n## Naduele SL                                                     REDUCIDA     <NA> Sociedad limitada\n## Maririas Energy SL                                             REDUCIDA     <NA> Sociedad limitada\n## Helios Almaden Sociedad Limitada.                              REDUCIDA     <NA> Sociedad limitada\n## Explotaciones MI Cobijo SL.                                    REDUCIDA     <NA> Sociedad limitada\n## Catral Renovables SL                                           REDUCIDA     <NA> Sociedad limitada\n## Energia Solar Turolense SL                                     REDUCIDA     <NA> Sociedad limitada\n## AV Serra de Liñares SL.                                           MEDIA     <NA> Sociedad limitada\n## Parque Eolico Donado SL                                          GRANDE     <NA> Sociedad limitada\n## AV Paxareiras SL.                                                 MEDIA     <NA> Sociedad limitada\n## Terranova Energy Corporation SA                                  GRANDE     <NA>  Sociedad anonima\n## AV Serra DO Farelo SL.                                            MEDIA     <NA> Sociedad limitada\n## Eolica de Cordales BIS SL.                                        MEDIA     <NA> Sociedad limitada\n## AV Cernego SL.                                                    MEDIA     <NA> Sociedad limitada\n## AV Outeiro Rubio SL.                                              MEDIA     <NA> Sociedad limitada\n## Airosa Vento SL                                                   MEDIA     <NA> Sociedad limitada\n## Eolica de Cordales SL.                                            MEDIA     <NA> Sociedad limitada\n## Enerfin Renovables IV SL.                                        GRANDE     <NA> Sociedad limitada\n## Sistemas Energeticos Jupiter Sociedad Limitada.                  GRANDE     <NA> Sociedad limitada\n## Sistemas Energeticos Oberon Sociedad Limitada.                   GRANDE     <NA> Sociedad limitada\n## Sistemas Energeticos Pluton Sociedad Limitada.                   GRANDE     <NA> Sociedad limitada\n## Sistemas Energeticos Saturno Sociedad Limitada.                  GRANDE     <NA> Sociedad limitada\n## Sistemas Energeticos Titan Sociedad Limitada.                    GRANDE     <NA> Sociedad limitada\n## Sistemas Energeticos Urano Sociedad Limitada.                    GRANDE     <NA> Sociedad limitada\n## Sistemas Energeticos Venus Sociedad Limitada.                    GRANDE     <NA> Sociedad limitada\n## Puerto Rosario Solar 3 Sociedad Limitada.                        GRANDE     <NA> Sociedad limitada\n## IM2 Energia Solar Proyecto 24 SL.                                 MEDIA     <NA> Sociedad limitada\n## Parque Eolico Punta Langosteira SL.                            REDUCIDA     <NA> Sociedad limitada\n## Desarrollos Fotovoltaicos Fuentes SL.                            GRANDE     <NA> Sociedad limitada\n## Guadalaviar Consorcio Eolico SA.                                 GRANDE     <NA>  Sociedad anonima\n## Desarrollo Eolico LAS Majas Xxxi SL.                             GRANDE     <NA> Sociedad limitada\n## Puerto Rosario Solar 2 Sociedad Limitada.                        GRANDE     <NA> Sociedad limitada\n## Energias Renovables de Hidra SL.                                 GRANDE     <NA> Sociedad limitada\n## Energias Renovables de Cilene SL.                                GRANDE     <NA> Sociedad limitada\n## Desarrollo Eolico LAS Majas XV SL.                               GRANDE     <NA> Sociedad limitada\n## Renovacyl SA                                                     GRANDE     <NA>  Sociedad anonima\n## Sistemas Energeticos DEL SUR SA                                  GRANDE     <NA>  Sociedad anonima\n## Eolica Santa Teresa Sociedad Limitada.                         REDUCIDA     <NA> Sociedad limitada\n## Lan2030 Toroña S.L.                                               MEDIA     <NA> Sociedad limitada\n## Gerr Grupo Energetico XXI SA                                     GRANDE     <NA>  Sociedad anonima\n## Belidia Energy SL.                                                MEDIA     <NA> Sociedad limitada\n## Parc Tramuntana SL.                                            REDUCIDA     <NA> Sociedad limitada\n## Aerogeneracion Galicia SL                                      REDUCIDA     <NA> Sociedad limitada\n## Greenalia Wind Power A Marabilla, S.L.                           GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Alto DO Rodicio II, S.L.                    GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power AS Lagoas, S.L.                             GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Campos Vellos, S.L.                         GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Cardon, S.L.                                GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Cedeira, S.L.                               GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Cervo, S.L.                                 GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Cordobelas, S.L.                            GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Coto DOS Chaos, S.L.                        GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Dunas, S.L.                                 GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Esteiro, S.L.                               GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Guanche, S.L.                               GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Huracan, S.L.                               GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Lamas II, S.L.                              GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Mojo, S.L.                                  GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Montoxo, S.L.                               GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power O Barral, S.L.                              GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Piñeiro, S.L.                               GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Punta Candieira, S.L.                       GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Regoa, S.L.                                 GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power SAN Isidro, S.L.                            GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power SAN Roman, S.L.                             GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Suime, S.L.                                 GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Teixido, S.L.                               GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Tormenta, S.L.                              GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Vaqueira, S.L.                              GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Vilas, S.L.                                 GRANDE     <NA> Sociedad limitada\n## Greenalia Wind Power Xesteiron, S.L.                             GRANDE     <NA> Sociedad limitada\n## Infraestructuras Para EL Desarrollo de Energias Renovables SL     MEDIA     <NA> Sociedad limitada\n## Renovables DEL Cantabrico Sociedad Limitada.                   REDUCIDA     <NA> Sociedad limitada\n## Sistemas Energeticos Erbania 1, Sociedad Limitada.               GRANDE     <NA> Sociedad limitada\n## Sistemas Energeticos Erbania 2, Sociedad Limitada.               GRANDE     <NA> Sociedad limitada\n## Sistemas Energeticos Marte Sociedad Limitada.                    GRANDE     <NA> Sociedad limitada\n## Sistemas Energeticos Mercurio Sociedad Limitada.                 GRANDE     <NA> Sociedad limitada\n## Sistemas Energeticos Neptuno Sociedad Limitada.                  GRANDE     <NA> Sociedad limitada\n## Wind Premier Monte Redondo, S.L.                                 GRANDE     <NA> Sociedad limitada\n## Wind Premier Serra Pequena, S.L.                                 GRANDE     <NA> Sociedad limitada\n    originales2 <- originales2 %>%\n  filter(! is.na(DIMENSION) &\n           ! is.na(AUTOFINA) &\n           ! is.na(FJUR)) \n# TABLA DE CONTINGENCIA\n\n  tab.originales2 <- table(originales2)\n\n  library(knitr)\n  library(kableExtra)\n  knitr.table.format = \"html\"\n\n  tab.originales2 %>%\n    kable(format = knitr.table.format,\n         caption=\"Empresas eólicas\") %>%\n    kable_styling(full_width = F,\n                  bootstrap_options = \"striped\", \"bordered\", \"condensed\",\n                  position = \"center\",\n                  font_size = 12) %>%\n    row_spec(0, bold= T, align = \"c\")\n  # Representación gráfica de la tabla con mosaico\n\n    library (vcd)\n   mosaic(tab.originales2,\n          main = \"Eólicas: Dimensión Matriz y Valoración Expertos.\",\n          shade = T,\n          gp = shading_Marimekko(tab.originales2),\n          main_gp = gpar(fontsize = 14),\n          sub_gp = gpar(fontsize = 12),\n          labeling_args = list(gp_labels = gpar(fontsize = 7)))\n  # Representando frecuencias de categorias en factores\n\n    library (ggplot2)\n    library (patchwork)\n\n    g1b <- ggplot(originales2, map= aes(x= DIMENSION,\n                                        fill = DIMENSION)) +\n    geom_bar() +\n    ggtitle(\"Dimensión del grupo empresarial\",\n            subtitle = \"Empresas eólicas\") + \n    ylab(\"Frecuencias\") +\n    xlab(\"Dimensión\") \n\n    g2b <- ggplot(originales2, map= aes(x= AUTOFINA,\n                                        fill = AUTOFINA)) +\n    geom_bar() +\n    ggtitle(\"Valoración de expertos\",\n            subtitle = \"Empresas eólicas\") + \n    ylab(\"Frecuencias\") +\n    xlab(\"Valoración\") \n\n    g3b <- ggplot(originales2, map= aes(x= FJUR,\n                                        fill = FJUR)) +\n    geom_bar() +\n    ggtitle(\"Forma Jurídica\",\n            subtitle = \"Empresas eólicas\") + \n    ylab(\"Frecuencias\") +\n    xlab(\"Forma Jurídica\") \n\n    (g1b / g2b / g3b) + plot_annotation(title = \"Frecuencias Marginales.\",\n                      theme = theme(plot.title = element_text(size = 12)))\n# MODELOS LOG-LINEALES\n\n# Función para extraer coeficientes y sus nombres de un modelo ##########\nextraer_coeficientes <- function(modelo) {\n  # Extraer los parámetros del modelo\n  parametros <- modelo$param\n  \n  # Crear listas para almacenar nombres y valores de coeficientes\n  coef_names <- c()\n  coef_values <- c()\n  \n  # Función para generar nombres de coeficientes\n  generate_coef_name <- function(levels) {\n    return(paste(levels, collapse = \":\"))\n  }\n  \n  # Recorrer los coeficientes y extraer los nombres y valores\n  for (term in names(parametros)) {\n    if (term == \"(Intercept)\") {\n      coef_names <- c(coef_names, \"T. Independiente\")\n      coef_values <- c(coef_values, parametros[[term]])\n    } else if (is.matrix(parametros[[term]])) {\n      # Si es una matriz, recorrer filas y columnas\n      for (i in 1:nrow(parametros[[term]])) {\n        for (j in 1:ncol(parametros[[term]])) {\n          coef_names <- c(coef_names,\n                          paste(rownames(parametros[[term]])[i],\n                                colnames(parametros[[term]])[j],\n                                sep = \":\"))\n          coef_values <- c(coef_values, parametros[[term]][i, j])\n        }\n      }\n    } else if (is.array(parametros[[term]])) {\n      # Si es un array de más de dos dimensiones\n      dims <- dim(parametros[[term]])\n      dimnames_list <- dimnames(parametros[[term]])\n      for (i in seq_len(dims[1])) {\n        for (j in seq_len(dims[2])) {\n          for (k in seq_len(dims[3])) {\n            coef_name <- paste(dimnames_list[[1]][i],\n                               dimnames_list[[2]][j],\n                               dimnames_list[[3]][k],\n                               sep = \":\")\n            coef_names <- c(coef_names, coef_name)\n            coef_values <- c(coef_values,\n                             parametros[[term]][i, j, k])\n          }\n        }\n      }\n    } else {\n      levels <- names(parametros[[term]])\n      for (level in levels) {\n        coef_names <- c(coef_names,\n                        generate_coef_name(c(term, level)))\n        coef_values <- c(coef_values,\n                         parametros[[term]][[level]])\n      }\n    }\n  }\n  \n  # Verificar la longitud de los vectores antes de crear el data frame\n  if (length(coef_names) == length(coef_values)) {\n    tabla_coeficientes <- data.frame(Coefficient = coef_names,\n                                     Value = coef_values,\n                                     stringsAsFactors = FALSE)\n    return(tabla_coeficientes)\n  } else {\n    stop(\"Error: Las longitudes de coef_names y coef_values no coinciden.\")\n  }\n}\n############################################################################   \n############################################################################\n# Función generar tablas y gráfico a partir de modelo log-lineal\n\ngenerar_solucion <- function(modelo) {\n  # Extraer la información del modelo\n  summary_modelo <- summary(modelo)\n  \n  # Crear una tabla con la información relevante de las pruebas de validez\n  tabla_informacion <- data.frame(\n    Statistic = c(\"Likelihood Ratio\", \"Pearson\"),\n    X2 = c(summary_modelo$tests[1, \"X^2\"], summary_modelo$tests[2, \"X^2\"]),\n    df = c(summary_modelo$tests[1, \"df\"], summary_modelo$tests[2, \"df\"]),\n    P_value = c(summary_modelo$tests[1, \"P(> X^2)\"], summary_modelo$tests[2, \"P(> X^2)\"]),\n    stringsAsFactors = FALSE\n  )\n  \n  # Formatear la tabla usando kable\n  independencia_valida_tab <- tabla_informacion %>%\n    kable(caption =\"Validación del modelo\",\n          format = \"html\",\n          col.names = c(\"Prueba\", \"Estadístico\", \"Grados Libertad\", \"P-valor\")) %>%\n    kable_styling(full_width = F,\n                  bootstrap_options = c(\"striped\", \"bordered\", \"condensed\"),\n                  position = \"center\",\n                  font_size = 12) %>%\n    row_spec(0, bold= T, align = \"c\")\n  \n  # Extraer los coeficientes del modelo\n  independencia_df <- extraer_coeficientes(modelo)\n  \n  # Formatear la tabla de coeficientes usando kable\n  independencia_coef_tab <- independencia_df %>%\n    kable(format = \"html\",\n          caption =\"Coeficientes del modelo\",\n          digits = 3) %>%\n    kable_styling(full_width = F,\n                  bootstrap_options = c(\"striped\", \"bordered\", \"condensed\"),\n                  position = \"center\",\n                  font_size = 12) %>%\n    row_spec(0, bold= T, align = \"c\")\n  \n  # Crear el gráfico de mosaico con los residuos y mostrarlo en pantalla directamente\n  plot(modelo, panel = mosaic,\n       main=\"Residuos del modelo\",\n       residuals_type = c(\"deviance\"),\n       gp = shading_hcl,\n       gp_args = list(interpolate = c(0, 1)),\n       main_gp = gpar(fontsize = 14),\n       sub_gp = gpar(fontsize = 9),\n       labeling_args = list(gp_labels = gpar(fontsize = 7)))\n  \n  # Guardar las tablas en una lista\n  solucion_nombre <- paste0(\"solucion_\", deparse(substitute(modelo)))\n  solucion_lista <- list(\n    Informacion = independencia_valida_tab,\n    Coeficientes = independencia_coef_tab\n  )\n  \n  assign(solucion_nombre, solucion_lista, envir = .GlobalEnv)\n}\n##########################################################################\n  # Modelo Independencia.\n\n    modelo_indep <- MASS::loglm(~ DIMENSION + AUTOFINA + FJUR,\n                                data= tab.originales2)\n\n    generar_solucion(modelo_indep)\n    solucion_modelo_indep$Informacion\n    solucion_modelo_indep$Coeficientes\n  # Modelo Saturado.\n\n    modelo_sat <- MASS::loglm(~ DIMENSION * AUTOFINA * FJUR,\n                              data= tab.originales2)\n\n    generar_solucion(modelo_sat)\n    solucion_modelo_sat$Informacion\n    solucion_modelo_sat$Coeficientes\n  # Elección del modelo final.\n\n    modelo_def <- step(modelo_sat, scale = 0,\n                       direction = c(\"backward\"),\n                       trace = 1, steps = 1000)## Start:  AIC=36\n## ~DIMENSION * AUTOFINA * FJUR\n## \n##                           Df    AIC\n## - DIMENSION:AUTOFINA:FJUR  4 33.077\n## <none>                       36.000\n## \n## Step:  AIC=33.08\n## ~DIMENSION + AUTOFINA + FJUR + DIMENSION:AUTOFINA + DIMENSION:FJUR + \n##     AUTOFINA:FJUR\n## \n##                      Df    AIC\n## <none>                  33.077\n## - AUTOFINA:FJUR       2 36.975\n## - DIMENSION:AUTOFINA  4 47.839\n## - DIMENSION:FJUR      2 51.252\n    generar_solucion(modelo_def)\n    solucion_modelo_def$Informacion\n    solucion_modelo_def$Coeficientes"},{"path":"análisis-de-datos-cualitativos..html","id":"materiales-para-realizar-las-prácticas-del-capítulo.-8","chapter":"10 Análisis de Datos Cualitativos.","heading":"10.5 Materiales para realizar las prácticas del capítulo.","text":"En esta sección se muestran los links de acceso los diferentes materiales (scripts, datos…) necesarios para llevar cabo los contenidos prácticos del capítulo.Datos (en formato Microsoft (R) Excel (R)):eolica_contingencia.xlsx (obtener aquí)eolica_contingencia.xlsx (obtener aquí)eolica_contingencia2.xlsx (obtener aquí)eolica_contingencia2.xlsx (obtener aquí)Scripts:Scripts:correspondencias_eolica.R (obtener aquí)correspondencias_eolica.R (obtener aquí)loglineal_eolica.R (obtener aquí)loglineal_eolica.R (obtener aquí)","code":""},{"path":"bibliografía.html","id":"bibliografía","chapter":"Bibliografía","heading":"Bibliografía","text":"","code":""}]

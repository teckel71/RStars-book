# Componentes principales.

## Introducción.

A veces, menos es más. Esta es la filosofía que subyace a las **técnicas de reducción de la dimensión** de la información.

Imaginemos una serie de casos (por ejemplo, las empresas de un sector económico) caracterizados por múltiples variables. Puede ocurrir que, paradójicamente, el contar con tantas variables haga difícil la caracterización de los casos. Esto ocurre cuando algunas de las variables aportan una información muy parecida sobre los mismos. Por ejemplo, es más difícil hacerse una idea del comportamiento global en el ámbito económico o financiero de un grupo de empresas si tenemos que atender a los valores que toman en un conjunto de 10 variables, que si solo tenemos que atender a un par de indicadores. Las técnicas de reducción de la dimensión de la información tratan, precisamente, de disminuir el número de variables necesarias para caracterizar un grupo de casos aprovechando la posibilidad de que las (múltiples) variables originales compartan información sobre los mismos. Es decir, la idea subyacente es pasar de un planteamiento basado en manejar muchas variables con información compartida o redundante (variables que "dicen lo mismo" sobre el comportamiento de los casos) a un planteamiento en el que hay **menos variables**, pero que **no comparten información** (variables que "dicen" cosas diferentes sobre el comportamiento de los casos). En este proceso es importante que la pérdida de información sea mínima, y que solo se pierda la información redundante o repetida.

La principal técnica de reducción de la dimensión de la información es la de **componentes principales,** y es la que se expondrá y ejemplificará en el resto del capítulo. Pero antes, es preciso concretar la relación entre dos conceptos muy presentes en esta técnica: información y varianza.

## Información y varianza.

En el apartado anterior hemos hablado de la posibilidad de que algunas variables compartan "información" sobre el comportamiento de los casos que constituyen nuestra muestra u objeto de estudio. Pero, ¿qué es, en este contexto, la "información"?

La ***información*** que una variable contiene sobre un conjunto de casos puede entenderse como su **capacidad para diferenciar a unos casos de otros**.

Observemos este ejemplo, en el que se representan los valores que toman un grupo de 20 empresas en 3 variables.

![Información y Varianza](figuras/varianza.png)

En la variable 1, todas las empresas toman el mismo valor. Por tanto, la capacidad que tiene la variable para distinguir a los casos (empresas), unos de otros, es nula. Eso es debido a que, en definitiva, esta variable no contiene información sobre el grupo de 20 empresas.

En la variable 2, existe cierta dispersión, aunque reducida, en los valores que adoptan los casos. Esto permite distinguir a unos de otros, aunque a veces con cierta dificultad. Por ejemplo, la empresa 17 se distingue del resto por ser la que tiene un valor (un poco) mayor. Aun así, como la dispersión es reducida, no se distinguen algunos casos de otros demasiado bien. En definitiva, la variable 2 contiene cierta cantidad de información sobre el conjunto de empresas de la muestra, aunque no demasiado grande.

Por último, la variable 3 muestra una dispersión considerablemente mayor que las otras dos variables. Existe un amplio abanico de valores que toman los diferentes casos (empresas). Esto hace que puedan diferenciarse con facilidad, en general, unos de otros. Esta variable posee, por tanto, una cantidad de información superior respecto a las empresas, ya que observando los valores que toman en la variable pueden diferenciarse con facilidad unas de otras.

Como conclusión, podemos establecer que cuanto mayor dispersión muestra una variable para un grupo de casos, mayor cantidad de información contiene sobre ellos, en el sentido de disfrutar de un mayor "poder" de diferenciación de unos casos respecto a otros.

Una medida de la dispersión de una variable usualmente utilizada es la **varianza**. Por tanto, en cierta manera, la varianza sirve para medir la cantidad de información que contiene la variable: a mayor varianza, mayor dispersión. Y a mayor dispersión, mayor cantidad de información.

En el ejemplo, puede observarse cómo la variable 3 es la que mayor varianza tiene, luego la variable 2, y la variable 1 tiene una varianza de 0 (y no posee información sobre las 20 empresas). Esta comparación de varianzas es válida siempre y cuando las tres variables estén expresadas en las mismas unidades, ya que la varianza es una medida de dispersión absoluta. Por ello, para poder comparar, hemos añadido también en el ejemplo una medida de dispersión relativa: el coeficiente de variación. Podemos comprobar cómo el mayor coeficiente de variación pertenece a la variable 3 (que es la que tiene una mayor cantidad de información), luego la variable 2 (que cuenta con menor cantidad de información), y por último la variable 1, con un coeficiente de 0 (no contiene información sobre las empresas).

## Cálculo de componentes.

Vamos a considerar una serie de variables (en escala métrica) que caracterizan a la población de empresas de producción eléctrica mediante tecnología eólica en lo referente a la idea de "solidez del negocio". Hemos seleccionado una muestra de 60 empresas. Nuestro objetivo es obtener una combinación lineal de estas variables (“componente principal”) que recoja la mayor parte de la suma de varianzas de las variables originales (“comunalidad”), de manera que pueda usarse como un indicador que resume o sintetiza el grado de solidez del negocio de cada empresa, con una pérdida mínima de información.

Las variables a partir de las cuales construiremos el indicador ("componente principal") son:

-   **RES:** Resultado del ejercicio.

-   **FPIOS:** Fondos propios.

-   **MARGEN:** Margen de beneficio.

-   **SOLVENCIA:** Coeficiente de solvencia.

Vamos a suponer que trabajamos dentro de un **proyecto** que hemos creado previamente, de nombre "componentes". Dentro de la carpeta del proyecto guardaremos el *script* llamado "componentes_eolica.R", y el archivo de Microsoft® Excel® llamado "eolica_60.xlsx". Si abrimos este último archivo, comprobaremos que se compone de tres hojas. La primera muestra un mensaje sobre el uso de los datos, la segunda recoge la descripción de las variables consideradas, y la tercera (hoja "Datos") almacena los datos que debemos importar. Estos datos se corresponden con diferentes variables económico-financieras de 60 empresas productoras de electricidad mediante generación eólica.

Es muy importante observar que existen variables con datos faltantes (*missing values*). En concreto, podemos identificar estas faltas de dato por la existencia de celdas en blanco; pero también por la existencia de celdas con el texto "n.d." (no dato). Así, tendremos que aplicar código adicional en el comando de importación de R para que estos casos queden correctamente recogidos como *NAs* (*not available*).

Cerraremos el archivo de Microsoft® Excel®, "[eolica_60.xlsx](https://docs.google.com/spreadsheets/d/1G7yk22TShY1bYS1i9v4PfVi_HCkOG2ZO/edit?usp=sharing&ouid=115375878280465826079&rtpof=true&sd=true)" y volveremos a RStudio. Después, abriremos nuestro *script* "[componentes_eolica.R](https://drive.google.com/file/d/1d0ss6sh-jIAcrrlWSHHQI_CfRtiDqANX/view?usp=sharing)". Este *script* contiene el programa que vamos a ejecutar en lel ejemplo.

La primera línea / instrucción en el *script* es:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
rm(list = ls())
```

La instrucción tiene como objeto limpiar el *Environment* (memoria) de objetos de anteriores sesiones de trabajo. Para importar los datos almacenados en la hoja "Datos" del archivo "eolica_60.xlsx", ejecutaremos el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# DATOS

library(readxl)
datos <- read_excel("eolica_60.xlsx", sheet = "Datos", na = "n.d.")
```

Podemos observar cómo, en el *Environment,* ya aparece un objeto. Este objeto es una estructura de datos tipo *data frame*, se llama “datos” y contiene 12 columnas, una por cada una de las variables almacenadas en el archivo de Microsoft® Excel®. De estas variables, tres son de tipo cualitativo (atributos o factores), formadas por cadenas de caracteres: el nombre de la empresa (NOMBRE), el nombre de la sociedad matriz (grupo empresarial) a la que pertenece (MATRIZ), y el tamaño de dicho grupo de empresas (DIMENSION).

R ha considerado la primera columna de la hoja de Excel (NOMBRE) como una variable de tipo cualitativo o atributo. En realidad no es una variable; sino el nombre de los casos (empresas). Para evitar esto, podemos redefinir nuestro *data frame* diciéndole que esa primera columna contiene los nombres de los casos (filas):

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# DATOS
datos <- data.frame(datos, row.names = 1)
```

En la línea anterior hemos asignado al *data frame* “datos” los propios datos de “datos”; pero indicando que la primera columna no es una variable; sino el nombre de los casos o filas (empresas). Advertimos que ya no aparece NOMBRE como variable, y que en el *Environment* ya aparece el *data frame* “datos” con 60 observaciones, pero con 11 variables (una menos).

Como en el ejemplo se plantea construir un indicador basado en las 4 variables antes indicadas (RES, FPIOS, MARGEN y SOLVENCIA), crearemos un *data frame* con solo esas variables. Lo llamaremos, por ejemplo, "muestra":

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Seleccionando variables metricas para el analisis.

library(dplyr)
muestra <- datos %>% select(RES, FPIOS, MARGEN, SOLVENCIA)
summary (muestra)
```

El siguiente paso será localizar los posibles ***missing values***, ya que para obtener componentes principales es necesario que todos los casos posean dato para todas las variables del análisis. Para tener una idea general, se puede utilizar la función `vis_miss()` del paquete `{visdat}`, que localizará gráficamente los *missing values* de las diferentes variables, y calculará el porcentaje de casos que supone, con respecto al total de observaciones:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
library(visdat)
vis_miss(muestra)
```

Del gráfico anterior se desprende que existen 5 *missing values* repartidos en las 4 variables del estudio. Para localizarlos, podemos filtrar nuestro *data frame* con las herramientas de `{dplyr}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
muestra %>%
  filter(is.na(RES) | is.na(FPIOS) | is.na(MARGEN) | is.na(SOLVENCIA)) %>%
               select(RES, FPIOS, MARGEN, SOLVENCIA)  
```

Ante la existencia de *missing values*, se puede actuar de varios modos. Por ejemplo, **se puede intentar obtener por otro canal de información el conjunto de valores** que no están disponibles, **o recurrir a alguna estimación**. En caso de que esto sea difícil, se puede optar, simplemente, por **eliminar** estos casos, en especial cuando representan un porcentaje muy reducido respecto al total de casos. En nuestro ejemplo, supondremos que hemos optado por esta última vía, y eliminaremos estos casos con el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
muestra <- muestra %>%
  filter(! is.na(RES) & ! is.na(FPIOS) & ! is.na(MARGEN) & ! is.na(SOLVENCIA))  

```

Verificamos en el *Environment* que el *data frame* “muestra” ha pasado a tener 53 casos.

Por otro lado, la técnica de componentes principales **es muy sensible a la existencia de *outliers***. En consecuencia, deberán ser identificados y, en su caso, eliminados. Para realizar este proceso, y dado que en nuestro análisis contamos con 4 variables, primero “resumiremos” el valor que toman dichas variables para cada observación (empresa), mediante el cálculo de la *distancia de Mahalanobis*. De hecho, las distancias de los diferentes casos se almacenarán en una nueva columna o variable de nuestro *data frame,* a la que llamaremos MAHALANOBIS:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
muestra <- muestra %>%
  mutate (MAHALANOBIS = mahalanobis(cbind(RES, FPIOS, MARGEN, SOLVENCIA),
            center = colMeans(select(., RES, FPIOS, MARGEN, SOLVENCIA)),
            cov = cov(select(., RES, FPIOS, MARGEN, SOLVENCIA))))
```

Dentro de las funciones `select()` hay unos puntos. Recordemos que estos puntos deben ser añadidos cuando una función no es la primera del operador "*pipe*" (`%>%`), para indicar que las variables de los paréntesis hacen referencia al *data frame* "muestra" (o, en general, el objeto que fluye a través del "*pipe*").

A continuación, construiremos un diagrama de caja de la variable MAHALANOBIS, como si fuera cualquier otra variable, a partir de la función `ggplot()` del paquete `{ggplot2}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
library (ggplot2)
ggplot(data = muestra, map = (aes(y = MAHALANOBIS))) +
  geom_boxplot(fill = "orange") +
  ggtitle("DISTANCIA DE MAHALANOBIS", subtitle = "Empresas eólicas") +
  ylab("MAHALANOBIS")

```

En el gráfico se observa que existen, por encima de la caja, varios *outliers*. Para identificarlos de modo concreto, hemos de calcular los cuartiles primero y tercero de la variable MAHALANOBIS y pasar el correspondiente filtro:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
Q1M <- quantile (muestra$MAHALANOBIS, c(0.25))
Q3M <- quantile (muestra$MAHALANOBIS, c(0.75))
muestra %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
         MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
         select(MAHALANOBIS)
```

Si, tras el estudio de los valores que toman las variables originales en estos casos, se decide eliminarlos, el código será:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
muestra_so <- muestra %>%
  filter(MAHALANOBIS <= Q3M + 1.5*IQR(MAHALANOBIS) &
         MAHALANOBIS >= Q1M - 1.5*IQR(MAHALANOBIS))
muestra_so <- muestra_so %>% select(-MAHALANOBIS)

```

Se ha creado un nuevo *data frame* llamado “muestra_so” con los casos que **no** son *outliers* (y que no contienen *missing values*), y se ha eliminado la variable MAHALANOBIS, puesto que su única utilidad era la de localizar y filtrar los *outliers*. Con este *data frame* “muestra_so” es con el que se procederá al cálculo de las componentes.

La condición previa para el cálculo de componentes es que las variables originales del análisis contengan información redundante, es decir, que en buena medida aporten una "misma información" sobre los casos (empresas). Esto se verifica con la existencia de altas correlaciones entre las variables (al menos, entre algunas de ellas). Por tanto, hemos de calcular la matriz de correlaciones correspondiente. Un modo gráfico visualmente efectivo es utilizar las posibilidades que nos ofrece el paquete `{GGally}` mediante la función `ggpairs()`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Correlaciones.

library (GGally)

corr_plot_so <- ggpairs(muestra_so, 
                        lower = list(continuous = wrap("cor",
                                                       size = 4.5,
                                                       method = "pearson",
                                                       stars = TRUE)),
                        title = "Matriz de Correlación sin outliers")
corr_plot_so
```

Puede apreciarse cómo existen altas correlaciones (en valor absoluto) entre todas las variables. Por tanto, tiene sentido hacer un análisis de componentes principales, ya que hay variables que parecen **compartir información**.

La obtención de las componentes se va a realizar mediante la función `prcomp()`. Es conveniente que activemos el argumento `scale =` con “T” (*true*) para que las variables originales sean consideradas en sus **versiones tipificadas**. Vamos a asignar los resultados a un objeto de nombre, por ejemplo, “componentes”. Por último, guardaremos el `summary()` o resumen de los resultados con un nombre provisional, por ejemplo, "temporal". El código es el siguiente:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Obtencion de componentes.

componentes <- prcomp (muestra_so, scale=T)
temporal <- summary (componentes)
temporal
```

La “*Standard deviation*” es la raíz cuadrada de los autovalores asociados a cada componente. “*Proportion of Variance*” nos dice la proporción de la suma de varianzas de las variables originales (*comunalidad*) recogida por cada componente, proporción que se acumula en “*Cumulative Proportion*”. Nótese que las componentes aparecen ordenadas de más a menos importantes en función de la cantidad de varianza que capturan. En este caso, la primera componente acumula más del 71% de la varianza (comportamiento) de las variables originales. Por tanto, esta primera componente resume bastante bien la información que las 4 variables originales contienen sobre los casos (empresas).

Si el elemento "importance" del `summary()` o resumen "temporal" lo convertimos en un *data frame*, por ejemplo "summary_df", podremos presentar los resultados por medio de una tabla estéticamente más atractiva, a partir de la función `kable()` del paquete `{knitr}`, y las funciones complementarias del paquete `{kableExtra}`:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Convertir el resumen en un data frame

summary_df <- as.data.frame(temporal$importance)
summary_df <- t(summary_df)  # Transponer para mejor visualización
rm (temporal)

# Crear la tabla con kable y personalizarla con kableExtra

library (knitr)
library (kableExtra)
knitr.table.format = "html"

summary_df %>%
kable(caption = "Resumen de Componentes",
      col.names = c("Desviación típica","Proporción de varianza (comunalidad)",
                    "Proporción de varianza (comunalidad) acumulada"),
      format.args = list(decimal.mark = ".", digits = 4)) %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "condensed"),
                full_width = F, 
                position = "center") %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(summary_df)), bold= F, align = "c") %>%
  column_spec(1, bold = TRUE, extra_css = "text-align: left;")

```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# Convertir el resumen en un data frame

summary_df <- as.data.frame(temporal$importance)
summary_df <- t(summary_df)  # Transponer para mejor visualización
rm (temporal)

# Crear la tabla con kable y personalizarla con kableExtra

library (knitr)
library (kableExtra)
tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
    knitr.table.format = "html"
summary_df %>%
kable(caption = "Resumen de Componentes",
      col.names = c("Desviación típica","Proporción de varianza (comunalidad)",
                    "Proporción de varianza (comunalidad) acumulada"),
      format.args = list(decimal.mark = ".", digits = 4)) %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "condensed"),
                full_width = F, 
                position = "center") %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(summary_df)), bold= F, align = "c") %>%
  column_spec(1, bold = TRUE, extra_css = "text-align: left;")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
    summary_df %>%
kable(caption = "Resumen de Componentes",
      col.names = c("Desviación típica","Proporción de varianza (comunalidad)",
                    "Proporción de varianza (comunalidad) acumulada"),
      format.args = list(decimal.mark = ".", digits = 4))  
}  
```

Los coeficientes o **cargas** de cada componente se obtienen pidiendo a nuestro objeto “componentes” el elemento “rotation”. Estas cargas las vamos a guardar en un nuevo objeto que llamaremos, por ejemplo, “cargas”, que presentaremos mediante una pequeña tabla diseñada con la función `kable()` del paquete `{knitr}` y otras funciones del paquete `{kableExtra}`:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Cargas de cada componente.

cargas <- componentes$rotation
cargas %>%
  kable(caption = "Cargas de las componentes obtenidas",
        format.args = list(decimal.mark = ".", digits = 4))  %>%
  kable_styling(full_width = F, bootstrap_options = "striped",
                "bordered", "condensed",
                position = "center") %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(cargas)), bold= F, align = "c") %>%
  column_spec(1, bold = TRUE, extra_css = "text-align: left;")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# Cargas de cada componente.

cargas <- componentes$rotation
tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
    knitr.table.format = "html"
cargas %>%
  kable(caption = "Cargas de las componentes obtenidas",
        format.args = list(decimal.mark = ".", digits = 4))  %>%
  kable_styling(full_width = F, bootstrap_options = "striped",
                "bordered", "condensed",
                position = "center") %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(cargas)), bold= F, align = "c") %>%
  column_spec(1, bold = TRUE, extra_css = "text-align: left;")
} else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
  cargas %>%
  kable(caption = "Cargas de las componentes obtenidas",
        format.args = list(decimal.mark = ".", digits = 4))
}
```

A partir de las cargas se pueden explicitar las ecuaciones correspondientes a cada componente. Por ejemplo, para la primera componente, la ecuación será:

$$
\text{CP}_{i1} = 0.4915 \cdot \text{RES}_{i1} + 0.5491 \cdot \text{FPIOS}_{i1} + 0.4803 \cdot \text{MARGEN}_{i1} + 0.4758 \cdot \text{SOLVENCIA}_{i1}
$$ Puede apreciarse que, en cuanto a la primera componente, que es la que especialmente nos interesa como "indicador" de la "solidez del negocio" en el caso de las empresas eólicas seleccionadas, las 4 cargas tienen signo positivo, lo que implica que, cuanto mayores sean los valores de una empresa en las varibles RES (resultado), FPIOS (fondos propios), MARGEN (margen de beneficio) y SOLVENCIA (coeficiente de solvencia), mayor será el valor ndel indicador y, por tanto, la solidez del negocio. Además, como las variables fueron tipificadas, el valor de las cargas son comparables. De este modo, vemos cómo, dentro de la primera componente, que es la que adoptamos como "indicador de solidez", la mayor importancia la tiene el valor de los fondos propios de la empresa, seguido del resultado del ejercicio, el margen y la solvencia.

## Retención de componentes principales.

La etapa de retención de componentes principales consiste en decidir cuántas de las componentes generadas (recordemos que, en un principio, se calculan tantas componentes como variable originales) consideramos que resumen de un modo aceptable la información contenida en las variables originales. Estas **componentes "retenidas" se convertirán en las componentes principales**.

La primera componente siempre es retenida y, por tanto, es una "componente principal". El resto, que van capturando proporciones cada vez más pequeñas de la varianza común de las variables originales (comunalidad), podrán o no retenerse; aunque, siempre, la retención de una componente implica que se han retenido todas las anteriores.

Hay varios procedimientos o criterios para tomar la decisión de cuántas componentes retener. Uno de ellos, comúnmente aplicado, es el de **retener aquellas componentes cuyo autovalor es mayor que 1** (suponiendo que se ha trabajado con las variables en sus versiones tipificadas).

Los autovalores, como ya vimos, son el cuadrado del elemento “Standard deviation” (sdev) del objeto “componentes” que hemos generado a partir de la función `prcomp()`. Hemos creado un *data frame* con estos autovalores calculados (y su orden, al que hemos llamado variable o columna “orden”, y que es un vector de números enteros consecutivos que va desde uno hasta número de variables originales o de componentes) y los hemos dispuesto en un gráfico de barras:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Determinacion Componentes a retener.

# Criterio del Autovalor mayor que 1.

orden <- c(1:ncol(muestra_so))
autovalor <- componentes$sdev^2
autovalores <- data.frame(orden, autovalor)

autograph <- ggplot(data = autovalores,
                    map = (aes(x = orden, y = autovalor))) +
             geom_bar(stat = "identity",
                      colour = "red",
                      fill = "orange",
                      alpha = 0.7) +
             scale_x_continuous(breaks=c(1:nrow(autovalores)))+
             geom_hline(yintercept = 1, colour = "dark blue") +
             geom_text(aes(label = round(autovalor,2)),
                       vjust = 1, colour = "dark blue", size = 3) +
             ggtitle("AUTOVALORES DE LAS COMPONENTES",
                     subtitle = "Empresas eólicas") +
             xlab ("Número de componente") +
             ylab("Autovalor")

autograph
```

Respecto al gráfico, conviene recordar que, al ser un gráfico de barras, si no se quieren representar las frecuencias sino los valores que toma una variable (en este caso, “autovalor”) para cada valor de la otra variable (en este caso, “orden”); en el `geom_bar()` habrá que añadir el argumento `stat =` con el valor “identity”. Además, se utiliza el elemento `scale_x_continuous()` para pesonalizar la escala del eje x, y que se divida dicho eje en tantos tramos como componentes hay.

En el gráfico obtenido se advierte que solo el primer autovalor es mayor que 1, por lo que solo se retendrá la primera componente, que será la única **componente principal.** Nuestro objetivo era, de todos modos, obtener un único indicador (de "solidez del negocio") que resuma la información contenida en las cuatro variables RES, FPIOS, MARGEN y SOLVENCIA, por lo que, aunque hubiera habido más componentes con autovalor mayor que uno, solo hubiéramos seleccionado la primera. No obstante, este resultado es positivo para nuestros intereses, ya que reafirma la idea de que únicamente con la primera componente se recoge una gran proporción de la *comunalidad* o varianza conjunta (comportamiento) de las 4 variables, luego es un buen resumen global de las mismas. Esto ya se vio anteriormente al hacer `summary (componentes)`, aunque se puede representar gráficamente con el siguiente código:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Determinar si cada autovalor es mayor o igual a 1

autovalores <- autovalores %>%
  mutate(variacum = 100*(cumsum((autovalor/nrow(autovalores)))))
checkcp <- ifelse(autovalores$autovalor >= 1, "CP", "NCP")
checkcp
             
vacumgraph <- ggplot(data = autovalores, map = (aes(x = orden,
                                                    y = variacum,
                                                    fill = checkcp))) +
              geom_bar(stat = "identity", colour = "red", alpha = 0.7) +
              scale_x_continuous(breaks=c(1:nrow(autovalores)))+
              geom_text(aes(label = round(variacum,2)), vjust = 1,
                        colour = "dark blue", size = 3) +
              ggtitle("COMUNALIDAD ACUMULADA POR COMPONENTES",
                      subtitle = "Empresas eólicas") +
              xlab ("Número de componente") +
              ylab("Varianza acumulada")
vacumgraph

```

Para obtener el gráfico anterior, se comienza añadiendo al *data frame* “autovalores” una columna o variable que es la s*uma acumulada del porcentaje de* *comunalidad* recogido por las sucesivas componentes, que están ordenadas de mayor a menor autovalor. Para calcular el porcentaje, se usa la función `cumsum()`, y se tiene en cuenta que, como las variables fueron tipificadas para calcular las componentes, la *comunalidad*, que coincide con la suma de las varianzas de las componentes (autovalores), es igual al número de variables o componentes (valor que toma la función `nrow()`).

Después, se ha creado un vector que contiene tantos elementos como variables o componentes hay en el análisis (vector “checkcp”). Con la función condicional `ifelse()` se consigue que los elementos de "checkcp" sean "CP" o "NCP" según los correspondientes autovalores sean mayores o no que 1. Finalmente, según sea el valor de cada elemento de "checkcp", las barras del gráfico se colorearán de uno u otro modo.

Posteriormente, mediante el paquete `{patchwork}`, se han unido los dos gráficos creados en esta fase, poniendo uno debajo del otro:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
library (patchwork)
autograph / vacumgraph
```

## Puntuaciones de los casos (scores)

Para obtener las puntuaciones de cada caso (empresa) en el indicador de "solidez del negocio" (y que es nuestra componente principal, que a su vez coincide con la primera componente), simplemente debemos tener en cuenta que tales puntuaciones están guardadas en la matriz “x” del objeto `prcomp()` creado. Vamos a renombrar a las primera columna (componente) de esta matriz como “scores” y vamos a ver las puntuaciones de las empresas, que volcaremos en una tabla, junto al valor que toman en las variables originales, recolocando las filas (empresas) de mayor a menor valor de la puntuación (lo que se consigue mediante la función `arrange()` del paquete `{dplyr}`:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Scores.

scores <- componentes$x[,1] #tantas columnas como componentes retenidas
scores_df <- as.data.frame(scores)
scores_df <- cbind(scores_df,muestra_so)
scores_df %>%
  arrange(desc(scores)) %>%
  kable(caption = "Puntuaciones de las componentes obtenidas",
        col.names = c("Empresa", "Puntuación", "Resultado", "F. Propios",
                      "Margen", "Solvencia"),
        format.args = list(decimal.mark = ".", digits = 4))  %>%
  kable_styling(full_width = F, bootstrap_options = "striped",
                "bordered", "condensed",
                position = "center", font_size = 12) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(scores_df)), bold= F, align = "c") %>%
  column_spec(1, bold = TRUE, extra_css = "text-align: left;")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# Cargas de cada componente.

scores <- componentes$x[,1] #tantas columnas como componentes retenidas
scores_df <- as.data.frame(scores)
scores_df <- cbind(scores_df,muestra_so)

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
    knitr.table.format = "html"
scores_df %>%
  arrange(desc(scores)) %>%
kable(caption = "Puntuaciones de las componentes obtenidas",
        col.names = c("Empresa", "Puntuación", "Resultado", "F. Propios",
                      "Margen", "Solvencia"),
        format.args = list(decimal.mark = ".", digits = 4))  %>%
  kable_styling(full_width = F, bootstrap_options = "striped",
                "bordered", "condensed",
                position = "center", font_size = 12) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(scores_df)), bold= F, align = "c") %>%
  column_spec(1, bold = TRUE, extra_css = "text-align: left;")
} else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
scores_df %>%
  arrange(desc(scores)) %>%kable(caption = "Puntuaciones de las componentes obtenidas",
        col.names = c("Empresa", "Puntuación", "Resultado", "F. Propios",
                      "Margen", "Solvencia"),
        format.args = list(decimal.mark = ".", digits = 4))
}
```

Las puntuaciones de los casos en nuestro indicador (componente principal) pueden integrarse en un *data frame* y ser utilizadas como cualquier otra variable en un análisis multivariante, sabiendo que este indicador asume gran parte de la información que, como caracterización de las distintas empresas, contenían las 4 variables originales.

## Materiales para realizar las prácticas del capítulo.

En esta sección se muestran los links de acceso a los diferentes materiales (*scripts*, datos...) necesarios para llevar a cabo los contenidos prácticos del capítulo.

**Datos (en formato Microsoft (R) Excel (R)):**

-   eolica_60.xlsx ([obtener aquí](https://docs.google.com/spreadsheets/d/1G7yk22TShY1bYS1i9v4PfVi_HCkOG2ZO/edit?usp=sharing&ouid=115375878280465826079&rtpof=true&sd=true))

**Scripts:**

-   componentes_eolica.R ([obtener aquí](https://drive.google.com/file/d/1d0ss6sh-jIAcrrlWSHHQI_CfRtiDqANX/view?usp=sharing))

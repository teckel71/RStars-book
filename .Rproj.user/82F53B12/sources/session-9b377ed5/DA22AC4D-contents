# Análisis Clúster.

![[Grupos de cargueros en travesía.]{.smallcaps}](figuras/07%20naves%20en%20cluster.jpg){width="100%"}

## ![](figuras/book.svg){.hicon} Introducción.

El análisis de conglomerados —o análisis de clústeres (AC)— agrupa casos con características similares en función de un conjunto de variables clasificadoras. El objetivo es que:

- Los casos de un mismo clúster sean lo más homogéneos posible entre sí.
- Los clústeres entre sí sean lo más heterogéneos posible, de acuerdo con las variables consideradas.


En general, el proceso de determinación de los grupos, conglomerados o clústeres de casos es el siguiente:

-   Se parte de un conjunto de **n** casos, y para cada uno de ellos se cuenta con el valor de **m** variables clasificadoras.
-   Se establece una **medida de distancia** que cuantifica lo que dos casos se parecen, **considerando en conjunto** los valores que poseen para las variables clasificadoras.
-   Se crean los grupos, conglomerados o clústeres con los casos que poseen entre sí una **menor distancia**. Existen dos **enfoques** principales a la hora de crear los grupos de casos a partir de las distancias observadas entre los casos: los *métodos jerárquicos* y los *métodos no-jerárquicos*.
-   Finalmente, se **caracterizan** los grupos, conglomerados o clústeres obtenidos, y se comparan unos con otros para extraer conclusiones.

En lo que respecta a la medida de **distancia** entre los casos, la medida más habitual es la **distancia euclídea**. Así, la distancia euclídea entre dos caso, i e i', para las m variables clasificadoras x, será:

$$
d(i, i') = \sqrt{\sum_{j=1}^{m} (x_{ij} - x_{i'j})^2}
$$ Esta distancia es muy sensible a la escala de las variables clasificadoras. Para evitar este inconveniente, se trabaja con las variables previamente **tipificadas**.

## ![](figuras/book.svg){.hicon} ![](figuras/pie-chart.svg){.hicon} Métodos de agrupación jerárquicos.

Como se acaba de comentar, existen dos enfoques fundamentales de realizar el análisis clúster, dependiendo de cómo son los métodos de agrupación de los casos (y grupos de casos): el enfoque de los métodos jerárquicos, y el enfoque que reúne a los métodos no-jerárquicos.

Ambos enfoques tienen sus ventajas e inconvenientes, y pueden adaptarse mejor a cada problema concreto. Es importante seleccionar un buen método de agrupación, puesto que pueden proporcionar soluciones muy diferentes entre sí.

En los **métodos jerárquicos,** se van formando sucesivamente grupos como agrupación de otros grupos precedentes, hasta llegar a un único grupo que recoge a todos los individuos; tomando el proceso una **estructura piramidal** (también existen métodos jerárquicos descendientes, que parten de un único grupo que contiene a todos los casos, para acabar el n grupos de un solo caso, aunque son menos frecuentes).

Estos métodos suelen aplicarse cuando hay un número reducido de casos. También, cuando nuestro objetivo pasa por crear **grupos que recojan a todos los casos**, más que definir simplemente tipologías más o menos homogéneas de casos (lo que se obtiene caracterizando los grupos obtenidos). Es decir, cuando se incluyen en el análisis a todos los individuos, incluidos los *outliers*. De hecho, estos métodos pueden emplearse, de por sí, como técnicas de localización de *outliers*. Por último, también se suelen emplearse cuando se desconoce a priori el número de grupos, conglomerados o clústeres a formar.

Entre los métodos jerárquicos de agrupación más extendidos, figuran los siguientes:

-   **Método del vecino más cercano (single linkage):** la distancia que se considera entre grupos es la distancia entre sus elementos más próximos.

-   **Método del vecino más lejano (complete linkage):** la distancia que se considera entre grupos es la distancia entre sus elementos más lejanos.

-   **Método de Ward (Ward method):** se unen los grupos que dan lugar a otro grupo cuyos casos tienen una menor suma de los cuadrados de sus distancias respecto al centro de dicho grupo (menor varianza intra-clúster).

-   **Otros métodos:** vinculación intergrupos (average linkage between groups), vinculación intragrupos (whithin-group)...

De entre ellos, ¿cuál elegir?

La cuestión no es fácil de resolver, y no tiene por qué tener una única respuesta. Por otro lado, cada método proporciona soluciones que pueden variar mucho entre sí. Una estrategia puede pasar por probar con varios métodos y se seleccionar la solución que parezca más coherente desde el punto de vista teórico, y estable desde el punto de vista empírico.

En la práctica, uno de los métodos más utilizados es el **método de Ward**, porque proporciona grupos muy homogéneos, ya que se basa en la minimización de la varianza o dispersión de los elementos que componen cada grupo con respecto a su centro de gravedad o **centroide.** Precisamente, este método será aplicado en el ejemplo práctico que desarrollaremos en R a continuación.

### ![](figuras/star.svg){.hicon} El Informe *Bluebird*.

![[Agencia Interplanetaria del Transporte de Mercancías.]{.smallcaps}](figuras/aitm.jpg){width="100%"}

La *Agencia Interplanetaria de Transporte de Mercancías* es un organismo dedicado a estudiar el funcionamiento del sector. Entre sus actividades, hay una consistente en la selección de un grupo de empresas para su segmentación en términos de fidelidad de los clientes (**IFIDE**), diversificación del negocio (**IDIVERSE**), y digitalización de la compañía (**IDIG**). En esta ocasión, se ha elegido un panel de 25 empresas o compañías. La investigación corre a cargo de una de las investigadoras de la agencia, la doctora *Xelia Bluebird* (en la imagen), por lo que al informe que contiene los resultados de la segmentación se le denomina *Informe Bluebird*.

![[Xelia Bluebird, investigadora de la AITM.]{.smallcaps}](figuras/Bluebird.jpg){.d-block .mx-auto width="400"}

En esta edición del informe, y tras la reciente compra de la compañía *Home One Cargo* por parte del magnate *Arg-us Korp*, la doctora *Bluebird* está especialmente interesada en cómo queda encuadrada dicha empresa.

### ![](figuras/pie-chart.svg) Preparación de los datos.

Dado que son pocos los casos (empresas) a segmentar, vamos a utilizar un método jerárquico de agrupación de casos. En concreto, utilizaremos el **método de Ward**.

Vamos a suponer que trabajamos dentro de un **proyecto** que hemos creado previamente, de nombre **"cluster"**. Dentro de la carpeta del proyecto guardaremos estos dos elementos:

-   El *script* llamado "cluster_rstars.R".

-   El archivo de Microsoft® Excel® llamado "interestelar_25.xlsx". Si abrimos este último archivo, comprobaremos que se compone de tres hojas. La primera muestra un mensaje sobre el uso de los datos, la segunda recoge la descripción de las variables consideradas, y la tercera (hoja "Datos") almacena los datos que debemos importar. Estos datos se corresponden con diferentes variables económicas y financieras de 25 empresas dedicadas a los servicios de transporte interestelar de mercancías.

Comenzaremos a ejecutar el código del *script*. En primer lugar, el código se ocupa de limpiar el *Global Environment* y cargar los paquetes necesarios:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
### CLUSTER jerárquico 25 empresas TMI.###

# Limpiando el Global Environment
rm(list = ls())

# Cargando paquetes
library(readxl)
library(dplyr)
library(ggplot2)
library(gtExtras)
library(visdat)
library (factoextra)
library (knitr)
library (kableExtra)
library (patchwork)
```

La siguiente sección de código es algo especial. Se trata de una **función**, en la que no se entrará en detalle, dada la complejidad de su código. Basta decir que el *input* de la función es una **lista** de gráficos generados con el paquete `{ggplot2}`. El *output* será una serie de **composiciones** de dimensión 4X4 realizada con los gráficos de la lista de modo automático, dejando los huecos en blanco necesarios en caso de que el número de gráficos no sea múltiplo de 4. La función se denomina `create_patchwork()`.

A lo largo del script se llamará dos veces a esta función, lo que ahorrará una buena cantidad de código. Se ha ubicado al comienzo del script para asegurar su ejecución previa a sus llamadas, aunque, en realidad, lo más adecuado sería integrar la función en un paquete, instalarlo y activarlo, y así evitar alargar el script con su código (se deja esta opción para usuarios avanzados).

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
##### Función para crear composiciones de gráficos con patchwork ###############
create_patchwork <- function(plot_list) {
  n <- length(plot_list)
  if (n == 0) return(NULL)
  full_rows <- n %/% 4
  remaining <- n %% 4
  patchworks <- list()
  
  if (full_rows > 0) {
    for (i in seq(1, full_rows * 4, by = 4)) {
      patchworks <- c(patchworks, list((plot_list[[i]] + plot_list[[i+1]]) / 
                                         (plot_list[[i+2]] + plot_list[[i+3]])))
    }
  }
  
  if (remaining > 0) {
   last_plots <- plot_list[(full_rows * 4 + 1):n]
   empty_plots <- lapply(1:(4 - remaining), function(x) ggplot() + theme_void())
   last_patchwork <- do.call(patchwork::wrap_plots, c(last_plots, empty_plots))
   patchworks <- c(patchworks, list(last_patchwork))
  }
  return(patchworks)
}
################################################################################
```

Posteriormente importaremos los datos del archivo de Excel®, y trataremos los casos con datos faltantes o *missing values*:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
## DATOS

# Importando datos desde Excel
interestelar_25 <- read_excel("interestelar_25.xlsx",
                               sheet = "Datos",
                               na = c("n.d."))
interestelar_25 <- data.frame(interestelar_25, row.names = 1)

# Seleccionando variables metricas para el analisis.
seleccion <- interestelar_25 %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion_df_graph <- gt_plt_summary(seleccion)
seleccion_df_graph

# Localizando missing values.
seleccion %>%
  vis_miss() +
  labs(title = "Indicadores: Diversificación, Fidelidad, Digitalización",
       subtitle = "Transporte de mercancías interestelar",
       y = "Observación",
       fill = NULL) +
  scale_fill_manual(
    values = c("TRUE" = "red", "FALSE" = "grey"),
    labels = c("TRUE" = "NA", "FALSE" = "Presente")) +
  theme(
    plot.title = element_text(face = "bold", size = 14))

seleccion %>% filter(is.na(IDIVERSE) |
                       is.na(IFIDE) |
                       is.na(IDIG)) %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion <- seleccion %>%
  filter(! is.na(IDIVERSE) &
           ! is.na(IFIDE) &
           ! is.na(IDIG)) 
```

No se ha localizado ninguna observación con *missing values*, luego no se ha de realizar ningún tipo de tratamiento.

El siguiente paso es la **identificación de *outliers*.** Para realizar este proceso, y dado que en nuestro análisis contamos con 3 variables, primero “resumiremos” el valor que toman dichas variables para cada caso, mediante el cálculo de la *distancia de Mahalanobis*. De hecho, las distancias de los diferentes casos se almacenarán en una nueva variable, a la que llamaremos MAHALANOBIS, que se incorporará al *data frame* "originales" por medio de la función `mutate()` de `{dplyr}`, y la función `mahalanobis()`. Recordemos que, en los diferentes argumentos de esta función, el punto "." hace referencia al *data frame* que está delante del operador pipe (%\>%).

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Identificando y descartando outliers con distancia de Mahalanobis.
seleccion <- seleccion %>%
  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),
                                   center = colMeans(.),
                                   cov    = cov(.)))
```

Una vez creada la variable MAHALANOBIS, se estudia la existencia en sus valores de *outliers* mediante la construcción de un diagrama de caja o *boxplot*:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Identificando outliers con distancia de Mahalanobis.
seleccion <- seleccion %>%
  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),
                                   center = colMeans(.),
                                   cov    = cov(.)))

ggplot(data = seleccion, map = (aes(y = MAHALANOBIS))) +
  geom_boxplot(fill = "orange") +
  ggtitle("DISTANCIA DE MAHALANOBIS",
          subtitle = "IDIVERSE, IFIDE, IDIG. Empresas TMI.") +
  ylab("MAHALANOBIS")
```

En el gráfico se observa que existen, por encima de la caja, 4 *outliers*. Para identificarlos de modo concreto, hemos de calcular los cuartiles primero y tercero de la variable MAHALANOBIS y pasar el correspondiente filtro:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
Q1M <- quantile (seleccion$MAHALANOBIS, c(0.25))
Q3M <- quantile (seleccion$MAHALANOBIS, c(0.75))

seleccion %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(MAHALANOBIS, IDIVERSE, IFIDE, IDIG)
```

De la tabla anterior se desprende que los outliers identificados son las empresas *Chakotay Cargo Systems*, *Home One Cargo*, y de modo más moderado, *Tannhäuser Freight* e *Hyperion Star Haulage*. Estos son, en definitiva, los 4 casos en los que nos fijaremos específicamente más adelante, al analizar los resultados del análisis clúster.

Estas empresas cuentan con un valor atípico en la variable MAHALANOBIS, lo que, a su vez, implica que muestren valores atípicos en una o varias de las variables originales (IDIVERSE, IFIDE, IDIG). En el desarrollo de otras técnicas, en este punto localizaríamos y eliminaríamos los *outliers*. En este caso **no** lo vamos a hacer, ya que queremos agrupar **todos los casos** que tenemos en el análisis. Precisamente, si hay algún caso que permanece aislado, sin agruparse con otros en el proceso de agrupación hasta las últimas etapas, quizá se trate de un candidato a *outlier*, por lo que el análisis clúster **también puede considerarse una técnica de localización de casos atípicos**.

Por último, borramos la variable MAHALANOBIS del *data frame* "seleccion", puesto que ya ha cumplido la función de localizar los casos atípicos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Eliminando variable MAHALANOBIS del df
seleccion    <- seleccion    %>% select(-MAHALANOBIS)
```

La siguiente etapa se refiere a la aplicación propia del análisis clúster al grupo de 25 compañías que toman valores para las tres variables incluidas en el análisis.

### ![](figuras/pie-chart.svg) Aplicación del método de *Ward*.

Los métodos de agrupación usualmente se basan en la **distancia euclídea**. Como la distancia euclídea es sensible a las unidades de medida de las diferentes variables clasificadoras, es preciso trabajar con las **variables tipificadas**, lo que lograremos creando, por ejemplo, un *data frame* “zseleccion” con la función `scale()`. Luego, aplicaremos el método elegido a este *data frame,* en lugar de al *data frame* que contiene los datos originales sin tipificar:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# CLUSTER JERARQUICO CON VARIABLES ORIGINALES.

# Tipificando variables
zseleccion <- data.frame(scale(seleccion))
summary(zseleccion)
```

Este nuevo *data frame* contiene las mismas variables del análisis; pero tipificadas (obsérvese, en el `summary()`, las medias de las variables).

Previamente a aplicar un método de agrupación concreto, es necesario calcular la **matriz de distancias** entre los casos, a la que llamaremos, por ejemplo, **“d”**. Esta matriz se calcula con la función `dist()`. Para visualizarla, una opción es representarla mediante el *gráfico de temperatura* que ofrece la función `fviz_dist()` del paquete `{factoextra}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Matriz de distancias
d <- dist(zseleccion)
fviz_dist(d, lab_size = 8)  # Del paquete {factoextra}
```

Los casos con intersecciones en tonos anaranjados tenderán a agruparse con mayor facilidad (o a agruparse antes); mientras que los casos cuyas intersecciones están en tonos azulados tenderán a pertenecer a grupos diferentes (o a agruparse más tarde). Cabe destacar los colores azulados asociados a las empresas *Chakotay Cargo Systems*, *Home One Cargo*, que precisamente eran las dos compañías identificadas más claramente como *outliers*.

Vamos a realizar el análisis clúster jerárquico mediante uno de los métodos más habituales, el de ***Ward***, como es común en las aplicaciones prácticas, ya que este método proporciona grupos muy homogéneos (mínima varianza). La función a utilizar es `hclust()`. La solución la guardaremos en el objeto (lista) que hemos llamado, por ejemplo, “cluster_j”. Luego se visualizará el ***dendograma*** construido con la función `fviz_dend()` del paquete `{factoextra}`, que permite personalizar el gráfico con una gramática similar a la utilizada con los gráficos del paquete `{ggplot2}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Método de Ward.
cluster_j<-hclust(d, method="ward.D2")

fviz_dend(cluster_j,
          cex = 0.6,
          rect = FALSE,
          labels_track_height = 5.5) +
  labs(title = "Empresas TMI.",
       subtitle = "Método de Ward. Variables originales tipificadas.") +
  theme_grey()
```

En el código anterior:

-   **`cluster_j`**: Es el objeto que contiene el dendrograma que se desea visualizar.

-   **`cex = 0.6`**: Este argumento ajusta el tamaño del texto de las etiquetas en el dendrograma. Un valor de **`0.6`** significa que el texto será más pequeño que el tamaño predeterminado.

-   **`rect = FALSE`**: Este argumento indica si se deben dibujar rectángulos alrededor de los clústeres en el dendrograma. **`FALSE`** significa que no se dibujarán rectángulos.

-   **`labels_track_height = 5.5`**: Este argumento ajusta la altura de la pista de etiquetas, que es el espacio reservado para las etiquetas de los objetos en el dendrograma. Un valor de **`5.5`** proporciona más espacio para las etiquetas.

Además, el código incluye funciones adicionales para mejorar la visualización:

-   **`labs(title = "Empresas TMI.", subtitle = "Método de Ward. Variables originales tipificadas.")`**: Esta función añade un título y un subtítulo al gráfico.

-   **`theme_grey()`**: Esta función aplica un tema gris al gráfico, que es el tema predeterminado en `{ggplot2}`, proporcionando un fondo gris claro y un estilo de texto específico.

El eje vertical del *dendograma* recoge las distancias (o disimilitud) entre los casos y/o grupos previos que se van agrupando sucesivamente. La escala depende de cada método empleado. En el caso del método de *Ward*, la escala refleja la suma de cuadrados de la distancia de los casos dentro del clúster.

Por otro lado, en este ejemplo, es interesante observar que las empresas *Chakotay Cargo Systems*, *Home One Cargo*, las *outliers* comentadas anteriormente, permanecen sin agruparse hasta una zona muy avanzada del proceso de agrupación, en coherencia con el gráfico de temperatura de la matriz de distancias euclídeas. En cambio, compañías como *Betazoid Transport* y *Skywalker Freight Co.* se han unido en el mismo grupo casi inmediatamente, lo que cuadra con el tono anaranjado de su intersección en la matriz de distancias.

Una cuestión importante consiste en determinar con **cuántos grupos** hemos de quedarnos. Aunque existen algoritmos y paquetes de R que aconsejan un número (por ejemplo, la función `NbClust()` del paquete `{NbClust}`); a veces puede ser preferible que el propio investigador decida el número de grupos a crear, mediante la observación del dendograma, y de acuerdo a los objetivos de su propia investigación.

Dentro de los métodos *objetivos*, uno muy extendido es el del *método de la anchura media de silueta*, muy utilizado en este tipo de análisis.

Para cada observación $i$, la **anchura de la silueta** $s_i$ se calcula como:

$$
s_i = \frac{b_i - a_i}{\max(a_i, b_i)}
$$

Donde:

-   $a_i$ es la distancia promedio de la observación $i$ a todas las demás observaciones dentro de su propio clúster.
-   $b_i$ es la distancia promedio de la observación $i$ al clúster más cercano.

La anchura de la silueta varía entre **-1 y 1.** Un valor cercano a 1 indica que la observación está bien agrupada dentro de su clúster; un valor cercano a 0 quiere decir que la observación está en el límite entre dos clústeres; y un valor negativo implica que la observación podría estar mal clasificada en su clúster actual. De este modo, para evaluar cuál es el mejor número de conglomerados a retener:

1.  Se prueban diferentes valores de $k$ (número de clústeres).

2.  Se calcula el promedio de los valores de silueta $k$ .

3.  Se selecciona el $k$ que maximiza el **promedio de la silueta**, lo que indica que la partición es más adecuada.

De todos modos, es necesario insistir en que estos métodos no llevan a una conclusión única ni irrevocable; por lo que puede ser preferible que el **propio investigador** decida el número de grupos a crear, mediante la observación del dendograma, y de acuerdo a los objetivos de su propia investigación, tomando los métodos cuantitativos solo como **orientación**.

En nuestro caso, el gráfico del *método de la silueta* se obtiene a partir de la función `fviz_nbclust` del paquete `{factoextra}`, con el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Método de obtención de número de grupos (k) del Ancho de Silueta.
p <- fviz_nbclust(
  x = zseleccion, 
  FUNcluster = hcut, 
  method = "silhouette",
  hc_method = "ward.D2",
  k.max = 15
)
p
```

El método aconseja una división de los casos en 8 grupos o conglomerados diferentes. Sin embargo, parece un número excesivo si se quieren caracterizar posteriormente los grupos e incidir en sus diferencias. Un segundo número de grupos apropiado según el método es 2; pero podría ocurrir lo contrario que el caso anterior: sería un valor demasiado bajo, lo que provocaría que tan solo se distinguiese un grupo con los dos outliers de otro con los 23 casos restantes. Así, el investigador puede decidir qué número de grupos parece equilibrado y se ajusta a sus intereses. En este ejemplo, **un número de grupos razonable podría ser 5**, que contaría con el aval de mantener individualizadas a 2 de las empresas etiquetadas como *outliers*.

Si se acepta esta opción, se podrá visualizar de nuevo el dendograma coloreando los grupos formados, con el código siguiente (debe modificarse el código para igualar el argumento `k =` al número de grupos seleccionado):

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
ngrupos = 5   # números de conglomerados decidido! #############################
fviz_dend(cluster_j,
          cex = 0.6,
          k = ngrupos, # número de conglomerados que se ha decidido formar!
          k_colors = "black",
          labels_track_height = 5.5,
          rect = TRUE,
          rect_border = "npg",
          rect_fill = TRUE) +
  labs(title = "Empresas TMI.",
       subtitle = "Método de Ward. Variables originales tipificadas.") +
  theme_grey()
```

En el código anterior:

-   **`k = 5`**: Especifica el número de grupos o clústeres que se desea formar en el dendrograma. En este caso, se han decidido formar 5 grupos.

-   **`k_colors = "black"`**: Define el color de las etiquetas de los clústeres. Aquí, se ha elegido el color negro para las etiquetas.

-   **`rect = TRUE`**:Significa que se dibujarán rectángulos delimitando los grupos formados.

-   **`rect_border = "npg"`**: Define el color del borde de los rectángulos que rodean los clústeres. **`"npg"`** es un conjunto de colores predefinidos (el de las publicaciones del *Nature Publishing Group*) en el paquete **`{ggsci}`**, que proporciona paletas de colores científicas.

-   **`rect_fill = TRUE`**: Indica si los rectángulos que rodean los clústeres deben estar rellenos. **`TRUE`** significa que los rectángulos estarán rellenos con el color especificado.

Al observar el gráfico, pueden destacarse varios conglomerados:

- Primer grupo (rojo, izquierda):

Integrado por *Hyperion Star Haulage* y *Event Horizon Haulage*. Estas dos empresas están muy próximas entre sí, indicando un alto grado de similitud en sus características. Se separan tempranamente del resto, lo que sugiere que poseen un perfil muy diferenciado respecto al resto de compañías (quizá por especialización extrema o tamaño atípico).

- Segundo grupo (azul, central):

Es el grupo más numeroso (14), e incluye empresas como *Bib Fortuna Haulage*, *Razor Freight Lines*, *Kashyyyk Logistics* o *Tatooine Movers*, entre otras. Aglutina compañías con comportamientos relativamente homogéneos, posiblemente representando el segmento “medio” del sector. Dentro de este grupo se distinguen subgrupos internos, lo que refleja cierta diversidad en estrategias o mercados específicos.

- Tercer grupo (verde, derecha):

Formado por empresas (7) como *Terminator Freight*, *Blizzard Transport* y *Rebel Alliance Transport*. Su distancia con respecto al grupo azul indica una diferenciación moderada, quizá por operar en mercados más especializados o disponer de tecnologías más avanzadas.

- Por último, aparecen a la derecha las compañías *Chakotay Cargo Systems* y *Home One Cargo*, que son nuestros *outliers* más destacados en el análisis previo y en el estudio de la matriz de distancias. Su aislamiento sugiere perfiles muy particulares.

A continuación vamos a **identificar con mayor detalle los casos** que integran cada uno de los grupos, así como a **caracterizar tales grupos** en función de los valores medios de las variables originales. Para ello, crearemos el vector de valores enteros que indica el grupo al que pertenece cada caso (empresa). A este vector se le llamará, por ejemplo, “whatcluster_j”, y se construirá mediante la función `cutree()`, donde el primer argumento es el nombre del objeto que guarda la solución del análisis clúster (“cluster_j”), y el segundo argumento es el número de grupos que hemos decidido crear (k = 5).

Lamentablemente, el "número de grupo" que `cutree()` asigna a cada caso no tiene por qué coincidir con el orden de grupos del dendograma (es decir, el segundo grupo del dendograma puede ser denominado por `cutree()`, por ejemplo, grupo "4"). Para que los grupos se llamen según el orden en que se disponen en el dendograma, hay que hacer varios ajustes adicionales, como se muestra en el código. La idea general es re-etiquetar los grupos de cutree() para que sus ids (1, 2, …, k) sigan el orden visual de izquierda→derecha del dendrograma (el que se ve con fviz_dend). cutree() no garantiza ese orden: puede llamar “1” a un grupo que está en el centro o a la derecha. Este código crea un mapa de ids que arregla eso.

Además, conviene convertir esta variable "whatcluster_j" en un factor con la función `as.factor()`, para que deje de ser variable métrica, a efectos de incorporar una leyenda en gráficos posteriores, . Finalmente, ese factor se incorporará al *data frame* “seleccion” (importante: **no a ”zseleccion”**; sino al *data frame* que contiene a las variables no tipificadas):

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
## CARACTERIZACIÓN Y COMPOSICIÓN DE GRUPOS.

# cortando árbol en grupos
cl <- cutree(cluster_j, k = ngrupos)

# etiquetas en el orden exacto del dendrograma (izq→der)
ord_labels <- cluster_j$labels[cluster_j$order]

# posición (1..n) de cada caso según ese orden
pos <- match(names(cl), ord_labels)

# mapa de id antiguo -> id nuevo (1..ngrupos) según el bloque más a la izquierda
mins       <- tapply(pos, cl, min)             # posición más a la izq por clúster
old_order  <- names(sort(mins))                # ids antiguos ordenados izq→der
map        <- setNames(seq_along(old_order), old_order)

# Formar etiqueta de grupos (factor "whatcluster")
whatcluster_j <- factor(map[as.character(cl)], levels = 1:ngrupos)
seleccion$whatcluster_j <- whatcluster_j
```

En el código anterior:

- **`cl <- cutree(cluster_j, k = ngrupos)`**: Asigna a cada observación un número de clúster del 1 a k (pero ese 1..k es arbitrario respecto al dibujo). Resultado: *cl* es un vector nombrado:

  - Nombres = labels/casos.
  - Valores = nombre o id de clúster (arbitrario).

- **`ord_labels <- cluster_j$labels[cluster_j$order]`**: `ord_labels` es un vector de nombres de casos en el mismo orden visual que se ve en `fviz_dend()`. En objetos de clustering jerárquico (p.ej., de `hclust()`),

  - `cluster_j$labels` son las etiquetas de las hojas (casos).
  - `cluster_j$order` da el índice que ordena esas hojas como aparecen en el dendrograma de izquierda a derecha.

- **`pos <- match(names(cl), ord_labels)`**: `names(cl)` son los casos en el orden “normal” (no visual). `match(…, ord_labels)` devuelve, para cada caso, en qué posición (1..n) cae dentro del orden del dendrograma. Como resultado, `pos[i]` es la posición (izquierda→derecha) del caso `names(cl)[i]`.

- **`mins <- tapply(pos, cl, min)`**: la idea clave es que si se observa un clúster en el dendrograma, su “anclaje” más a la izquierda es la menor posición de sus casos. Lo que hace esta instrucción, pues, agrupa las posiciones pos por el identificador o nombre en `cl` (el nombre “viejo”). Luego toma el mínimo de cada grupo (“la primera hoja” de ese clúster al recorrer de izquierda a derecha), y de este modo `mins` queda nombrado por los nombres de clúster originales (p.ej., "1", "2", …) y con valores numéricos (la mínima posición).

- **`sort(mins)`** ordena los clústeres por esa primera aparición, de izquierda a derecha.

- **`names(sort(mins))`** produce `old_order`, que es el orden de los nombres de clúster (ids) originales según se avanza de izquierda a derecha.

- **`setNames(seq_along(old_order), old_order)`** construye `map`:

  - Las claves son los nombres de clúster o ids originales (tipo carácter).
  - Los Valores son los nombres de clúster o ids nuevos en 1..k siguiendo el recorrido de izquierda a derecha.
  - Por ejemplo, si el clúster “3” aparece primero a la izquierda, `map["3"] = 1`; si el “1” aparece segundo, `map["1"] = 2`, etc.
  
- En cuanto a **`whatcluster_j <- factor(map[as.character(cl)], levels = 1:ngrupos)`**:

  - `as.character(cl)` convierte los nombres de clúster o ids antiguos (numéricos) a texto para que casen con las claves de `map` (que son caracteres).
  - `map[as.character(cl)]` reemplaza, caso a caso, y de izquierda a derecha, el nombre o id viejo por el nuevo.
  - Se envuelve en `factor(..., levels = 1:ngrupos)` para asegurar que los niveles estén completos y ordenados (1, 2, …, k), aunque algún grupo no apareciera en un subconjunto.

- Se guarda en el *data frame* "seleccion" como variable `whatcluster_j`

### ![](figuras/pie-chart.svg) Caracterización de los grupos o conglomerados.

Una vez incorporado el grupo de pertenencia de cada empresa al *data frame* "seleccion", se podrán calcular y almacenar las medias de cada grupo de las distintas variables originales, usando las funciones `by_group()` y `summarise()` de `{dplyr}`. Toda la información se asigna al *data frame* “tablamedias” para poder representarla en una tabla mediante las facilidades que ofrecen los paquetes `{knitr}` y `{kableExtra}`:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# Tabla con centroides de grupos.
tablamedias <- seleccion %>%
               group_by(whatcluster_j) %>%
               summarise(obs = length(whatcluster_j),
                                      Idiverse = mean(IDIVERSE),
                                      Ifide = mean(IFIDE),
                                      Idig = mean(IDIG))

tablamedias %>%
  kable(caption = "Método de Ward. 5 grupos. Medias de variables",
        col.names = c("Clúster",
                      "Observaciones",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(0, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                                    "bordered",
                                    "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T,
           align = "c") %>%
  row_spec(1:nrow(tablamedias),
           bold= F,
           align = "c")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# Tabla con centroides de grupos.

# Tabla con centroides de grupos.
tablamedias <- seleccion %>%
               group_by(whatcluster_j) %>%
               summarise(obs = length(whatcluster_j),
                                      Idiverse = mean(IDIVERSE),
                                      Ifide = mean(IFIDE),
                                      Idig = mean(IDIG))

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
tablamedias %>%
  kable(caption = "Método de Ward. 5 grupos. Medias de variables",
        col.names = c("Clúster",
                      "Observaciones",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(0, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                                    "bordered",
                                    "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T,
           align = "c") %>%
  row_spec(1:nrow(tablamedias),
           bold= F,
           align = "c")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
tablamedias %>%
  kable(caption = "Método de Ward. 5 grupos. Medias de variables",
        col.names = c("Clúster",
                      "Observaciones",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(0, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE))
}  
```

Obviamente, también se podrían comparar las medias de los grupos, para cada variable, con un simple gráfico de barras. A fin de crear un método que valga para cualquier número de variables, realizaremos la tarea con un bucle. El código es el siguiente:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Gráficos de centroides

  # Vector de nombre de variables excluyendo la variable no deseada
    variables <- setdiff(names(tablamedias), c("whatcluster_j", "obs"))

  # Lista para almacenar los gráficos
    graficos.centroides <- list()

  # Bucle para crear y almacenar los gráficos
    for (i in seq_along(variables)) {
    var1 <- variables[[i]]
    grafico <- ggplot(data= tablamedias,
                      map = (aes_string(y = var1, x = "whatcluster_j"))) +
               geom_bar(stat = "identity",
                        colour = "red",
                        fill = "orange",
                        alpha = 0.7) +
               ggtitle(paste0(var1, ". Media por grupos."),
                       subtitle = "Empresas eólicas")+
               xlab ("Grupo") +
               ylab(var1)
    graficos.centroides[[paste0("grafico_", var1)]] <- grafico
}  
```

En el código anterior, `setdiff()` crea un vector "variables" que contiene todos los nombres de las columnas del *data frame* "tablamedias", excepto `whatcluster_j` y `obs`, que no son las variables originales. Luego se crea una lista vacía "graficos.centroides" para almacenar los gráficos generados. Con `for (i in seq_along(variables))` comienza el bucle, que recorre cada elemento del vector "variables". En el código de gráfico, "var1" toma el nombre, en cada iteración, de la variable a representar. En el "mapeo", es importante utilizar `aes_string()`, que requiere que los nombres de las variables se pasen como cadenas de texto (entre comillas), lo que es útil cuando los nombres de las variables se generan dinámicamente o se pasan como argumentos de función, y cuando se necesitan construir *mapeos estéticos* de manera programática. Finalmente, con `graficos.centroides[[paste0("grafico_", var1)]] <- grafico` se guarda el gráfico en la lista "graficos.centroides" con un nombre basado en "var1".

Los gráficos guardados en la lista "gráficos.centroides" se pueden agrupar en composiciones, de, por ejemplo, 2x2, utilizando el paquete `{patchwork}`, empleando la función `create.patchwork()`que ya incorporamos al comienzo del script. Con esta función, crearemos la lista de composiciones de gráficos denominada, por ejemplo, "grupos.graficos.centroides":

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Aplicar función de composiciones a gráficos de centroides.
  grupos.graficos.centroides <- create_patchwork(graficos.centroides)
  
# Presentar las composiciones
  for (n in 1:length(grupos.graficos.centroides)){
       print(grupos.graficos.centroides[[n]])
  }
```

Como solo hay 3 variables originales en el análisis; "grupos.graficos.centroides" solo cuenta con un elemento o composición, que contiene tres gráficos (valores medios de IDIVERSE, IFIDE e IDIG, por conglomerado).

Hablando siempre en términos de la media (centroides), puede comprobarse cómo el grupo 4 (compañía *Chakotay Cargo Systems*) destaca por su alto valor en el índice de diversificación y en el Fidelización, quedando en segunda posición en el índice de digitalización. El otro grupo de una única compañía, el grupo 5, que contiene a la empresa *Home One Cargo*, destca por poseer el mayor índice de digitalización. En adición, ocupa la segunda posición en el índice de fidelización. Por último, en cuanto al índice de diversificación, ocupa una posición media. En cuanto al grupo 3, em promedio es un conglomerado formado por empresas con índices de fidelización y digitalización medios, mientras que en cuando a la diversificación ocupa una destacada segunda posición, solo por detrás del grupo 4. Por su parte, las empresas del grupo 2, en promedio, presentan un índice de fidelización medio, mientras que los índices de diversificación y digitalización son más bien bajos. Por último, el grupo 1, compuesto por las compañías *Hyperion Star Haulage* y *Event Horizon Haulage*, se caracteriza por mostrar los menores valores medios en los tres indicadores.

Por otro lado, se pueden presentar en diferentes **tablas** las informaciones de **cada grupo**. Vamos a automatizar de nuevo el proceso de generación de las tablas mediante el empleo de un *bucle.* Las diferentes tablas se irán guardando en una lista de nombre, por ejemplo, "tablascompo":

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# Tablas con composiciones de grupos

  # Número de tablas y lista para guardarlas

  numclusters <- nlevels(seleccion$whatcluster_j)
  tablascompo <- list()

  # Bucle para generar las tablas

  for (n in 1:numclusters){
      tabla <- seleccion %>%
      filter(whatcluster_j == as.character(n)) %>%
      select(IDIVERSE, IFIDE, IDIG) %>%
      kable(caption = paste("Método de Ward. Grupo ", n, "."),
            col.names = c("I. Diversificación",
                          "I. Fidelización",
                          "I. Digitalización"),
            digits = c(0, 3, 3, 3),
            format.args = list(decimal.mark = ".",
                               scientific = FALSE)) %>%
      kable_styling(full_width = FALSE, 
                    bootstrap_options = c("striped",
                                          "bordered",
                                          "condensed"),
                    position = "center",
                    font_size = 12) %>%
      row_spec(0, bold = TRUE, align = "c")
      tablascompo[[n]] <- tabla
  }

  # Presentar las tablas
  for (n in 1:numclusters){
    print(tablascompo[[n]])
  }
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# Tablas con composiciones de grupos

  # Número de tablas y lista para guardarlas

  numclusters <- nlevels(seleccion$whatcluster_j)
  tablascompo <- list()

# Bucle para generar las tablas

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
for (n in 1:numclusters){
      tabla <- seleccion %>%
      filter(whatcluster_j == as.character(n)) %>%
      select(IDIVERSE, IFIDE, IDIG) %>%
      kable(caption = paste("Método de Ward. Grupo ", n, "."),
            col.names = c("I. Diversificación",
                          "I. Fidelización",
                          "I. Digitalización"),
            digits = c(0, 3, 3, 3),
            format.args = list(decimal.mark = ".",
                               scientific = FALSE)) %>%
      kable_styling(full_width = FALSE, 
                    bootstrap_options = c("striped",
                                          "bordered",
                                          "condensed"),
                    position = "center",
                    font_size = 12) %>%
      row_spec(0, bold = TRUE, align = "c")
      tablascompo[[n]] <- tabla
   }
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
for (n in 1:numclusters){
      tabla <- seleccion %>%
      filter(whatcluster_j == as.character(n)) %>%
      select(IDIVERSE, IFIDE, IDIG) %>%
      kable(caption = paste("Método de Ward. Grupo ", n, "."),
            col.names = c("I. Diversificación",
                          "I. Fidelización",
                          "I. Digitalización"),
            digits = c(0, 3, 3, 3),
            format.args = list(decimal.mark = ".",
                               scientific = FALSE))
  tablascompo[[n]] <- tabla
  }
}
```

Las tablas obtenidas serán:

```{r, eval=TRUE, echo=FALSE, message=FALSE, results='asis', warning=FALSE}
  # Presentar las tablas
  for (n in 1:numclusters){
    tablascompo[[n]]
  }
```

Para representar los grupos gráficamente, tenemos la dificultad de contar con más de dos variables clasificadoras. Una idea es **generar todas las combinaciones de variables posibles** y los correspondientes gráficos de dispersión con los casos coloreados de modo diferente según el grupo de pertenencia. Finalmente, los gráficos de modo compacto utilizando la función `create_patchwork()` que se incluyó anteriormente en el *script* para hacer mediante `{patchwork}`.composiciones de 2x2 gráficos.

Vamos a realizar la tarea de generar todos los gráficos de modo automatizado. Primero generaremos un vector con el nombre de todas las variables (excluyendo al *factor* whatcluster_j) mediante la función `setdiff()`. Luego, crearemos una lista para ir almacenando los gráficos (lista "graficos"). Por último, calcularemos todas las combinaciones de nombres de variables posibles, con la función `combn()`, y las almacenaremos en la lista "combinaciones". En esta función, el argumento **`simplify = FALSE`** le dice a la función que no vuelque el resultado a una matriz. En su lugar, devuelve una lista donde cada elemento de la misma es una combinación de los elementos del vector original.

```{r, eval=TRUE, echo=TRUE, message=FALSE, results='asis', warning=FALSE}
# Gráficos Variable vs Variable

  # Lista de variables excluyendo la variable no deseada
    variables <- setdiff(names(seleccion), "whatcluster_j")

  # Lista para almacenar los gráficos
    graficos <- list()

  # Generar todas las combinaciones posibles de pares de variables
    combinaciones <- combn(variables, 2, simplify = FALSE)
```

El siguiente paso consiste en utilizar un bucle para generar los gráficos de dispersión de acuerdo a las combinaciones de variables obtenidas y guardarlos en la lista "graficos". El bucle itera tantas veces como elementos guarda la lista "combinaciones" (argumento/función `seq_along()`):

```{r, eval=TRUE, echo=TRUE, message=FALSE, results='asis', warning=FALSE}
  # Bucle para crear y almacenar los gráficos
    for (i in seq_along(combinaciones)) {
      var1 <- combinaciones[[i]][1]
      var2 <- combinaciones[[i]][2]
      grafico <- ggplot(seleccion,
                        map = aes_string(x = var1,
                                         y = var2,
                                         color = "whatcluster_j")) +
                 geom_point() +
                 labs(title = paste("GRÁFICO", var1, "-", var2),
                      subtitle = "Empresas TMI") +
                 xlab (var1) +
                 ylab (var2) +
                 scale_color_brewer(palette = "Set1") 
      graficos[[paste0("grafico_", var1, "_", var2)]] <- grafico
}
```

Una vez generados los gráficos de todas las combinaciones de variables (6 gráficos en el ejemplo), y almacenados en la lista "graficos", se podrán reagrupar y presentar en composiciones de 2x2 mediante el empleo de la función `create_patchwork()`:

```{r, eval=TRUE, echo=TRUE, message=FALSE, results='asis', warning=FALSE}
  # Hacer agrupaciones con la función de patchworks creada anteriormente
    gruposgraficos <- create_patchwork(graficos)

  # Presentar las composiciones
    for (n in 1:length(gruposgraficos)){
      print(gruposgraficos[[n]])
    }
```

Pueden extraerse algunas conclusiones, a partir de estos gráficos, sobre cada uno de los grupos, que confirman las ideas anteriores:

**1. Grupos 1 y 2:** empresas tradicionales de baja diversificación.

Los clústeres 1 y 2 se sitúan en la parte baja del eje de diversificación (IDIVERSE \< 30).

El clúster 2, el más numeroso, agrupa empresas con niveles medios de fidelidad (IFIDE ≈ 37-42) y baja digitalización (IDIG \< 20). Estas compañías parecen representar un modelo tradicional y estable, con una clientela relativamente fiel pero escasa inversión tecnológica. Son negocios consolidados, pero con riesgo de perder competitividad en entornos cada vez más digitalizados.

El clúster 1, aunque pequeño, muestra menor fidelidad y ligeramente más digitalización, sugiriendo empresas que están experimentando cambios estructurales o en transición, probablemente buscando nuevos nichos o adaptándose a la digitalización con dificultades.

**2. Grupo 3:** empresas equilibradas y diversificadas.

El clúster 3 reúne compañías con diversificación y fidelidad moderadamente altas (IDIVERSE e IFIDE entre 35 y 45) y digitalización intermedia (IDIG entre 20 y 35). Estas firmas parecen situarse en una posición estratégica óptima, combinando una cartera de servicios variada con clientelas estables y cierto grado de modernización. Representan el núcleo competitivo del sector, con potencial para liderar la transición tecnológica sin perder estabilidad comercial.

**3. Grupo 4:** empresa líder altamente diversificada y digitalizada

El clúster 4, compuesto por una única empresa (*Chakotay Cargo Systems*), se sitúa en los valores máximos de los tres indicadores: diversificación, fidelidad y digitalización. Esta empresa se distingue como referente del sector, con una clara orientación a la innovación y una base de clientes sólida. Su posición sugiere una estrategia de diferenciación tecnológica y de servicio, que le otorga ventajas competitivas sostenibles.

**4. Grupo 5:** empresa digital pero poco diversificada

Finalmente, el clúster 5 (*Home One Cargo*) presenta muy alta digitalización (IDIG \> 60) y fidelidad elevada, pero una diversificación reducida. Se trataría de una compañía especializada en un nicho muy digitalizado, posiblemente centrado en transporte premium o servicios automatizados. Su modelo es eficiente y tecnológicamente avanzado, aunque dependiente de un ámbito de negocio limitado.

En cuanto a las tres variables consideradas, se pueden inferir algunos patrones de relación también interesantes:

**1. Relación entre IDIVERSE e IFIDE (diversificación y fidelidad):**

En el gráfico IDIVERSE–IFIDE, los puntos muestran una ligera pendiente ascendente, aunque no muy marcada. Esto sugiere una correlación positiva débil a moderada: las empresas con una mayor diversificación tienden a mantener una clientela algo más fiel.

En el clúster 2 (el grupo mayoritario y más tradicional), la nube de puntos es bastante compacta y casi horizontal, indicando poca relación entre ambas variables. En este segmento, la fidelidad parece más determinada por la trayectoria o reputación que por la diversificación.

En cambio, el clúster 3 (empresas más diversificadas y equilibradas) sí muestra una relación más consistente: conforme aumenta la diversificación, también se mantiene o eleva la fidelidad.

El clúster 4 (chakotay cargo systems) refuerza esa tendencia, al situarse en los valores más altos de ambas variables — es decir, una empresa muy diversificada y con gran fidelidad, un ejemplo de sinergia entre ambas dimensiones.

EN conclusión, existe una correlación positiva leve, que se hace más fuerte en las empresas modernas y diversificadas.

**2. Relación entre IDIVERSE e IDIG (diversificación y digitalización):**

En el gráfico IDIVERSE–IDIG la relación parece algo más heterogénea. En general, no hay una correlación lineal clara entre ambas variables para el conjunto total, pero sí aparecen patrones diferenciados por clúster.

En los clústeres 1 y 2, la digitalización es baja y relativamente independiente de la diversificación: empresas poco diversificadas pueden tener tanto baja como media digitalización.

En el clúster 3, se aprecia una correlación positiva más nítida: las empresas con mayor diversificación presentan también mayores niveles de digitalización. Esto sugiere que estas firmas han apostado por diversificar apoyándose en la tecnología.

Los casos extremos (clústeres 4 y 5) marcan los límites de esa relación:

Clúster 4: alta diversificación + alta digitalización → perfil de liderazgo.

Clúster 5: baja diversificación + digitalización muy alta → modelo de especialización tecnológica.

EN definitiva, la relación entre diversificación y digitalización es positiva en los grupos más avanzados, pero inexistente o débil en los tradicionales.

**3. Relación entre IFIDE e IDIG (fidelidad y digitalización):**

El gráfico IFIDE–IDIG muestra una dispersión considerable, aunque con algunas señales interesantes. En la mayor parte de los casos (clústeres 1 y 2), a mayor digitalización no necesariamente corresponde mayor fidelidad; las empresas digitalizadas no parecen haber consolidado aún esa ventaja comercial.

Sin embargo, en el clúster 3 la nube de puntos tiende ligeramente al alza: las empresas con niveles medios-altos de digitalización también presentan fidelidad superior, lo que podría indicar una mejor experiencia de cliente o una oferta más personalizada gracias a la tecnología.

Los casos atípicos refuerzan la idea de estrategias distintas:

Clúster 4 (*Chakotay Cargo Systems*) combina altos niveles de ambas, señal de madurez digital y consolidación de la relación con clientes.

Clúster 5 (*Home One Cargo*) muestra muy alta digitalización pero fidelidad algo menor, típico de modelos disruptivos o muy especializados donde la clientela puede fluctuar.

Se puede concluir que la correlación global entre fidelidad y digitalización es débil, pero se vuelve positiva en empresas más equilibradas o maduras tecnológicamente.

## ![](figuras/book.svg){.hicon} ![](figuras/pie-chart.svg){.hicon} Métodos de agrupación no-jerárquicos.

Dentro del análisis clúster, los métodos de agrupación no-jerárquicos se utilizan en situaciones en las que hay un elevado número de casos que clasificar. Son especialmente útiles cuando nuestro objetivo pasa por crear grupos que definan **una tipología de casos o individuos**, más que clasificar casos o individuos concretos. En definitiva, identificar patrones de subpoblaciones a partir de una muestra. Por eso es conveniente, previamente, detectar los *outliers* y, en su caso, eliminarlos; ya que podrían distorsionar las características de los grupos debido a la sensibilidad de los algoritmos a la presencia de casos atípicos.

Una diferencia clave con respecto a los métodos jerárquicos es que **es necesario decidir *a priori* el número de conclomerados** o grupos de casos a formar. Por otro lado, son métodos más eficientes, y permiten el traslado de casos de unos grupos a otros.

Aunque existen otros métodos, la técnica no-jerárquica más común es la de **k-medias**. Este es un método iterativo. Se establece un *centroide* inicial (“semilla”) para cada uno de los k grupos que se quieren crear, y se van asignando a cada grupo los casos que se sitúen más cerca de su centro. Una vez asignados los casos, se recalculan los *centroides* de los grupos, y se repite el proceso en una nueva iteración. El procedimiento termina cuando el algoritmo encuentra la solución convergente (estable). Precisamente, la elección de las "semillas" iniciales es otro de las debilidades que presenta el método, ya que de ello puede depender la obtención de soluciones diferentes.

¿Cómo fijar el **número de grupos o conglomerados a formar**? Hay ocasiones en que las que el investigador establecerá un número que le sea manejable o útil según los objetivos que persiga. Si esto no es así, y no se tiene claro el número de grupos a construir, se podrá optar por probar con varios números, y evaluar las soluciones obtenidas. También puede ayudar el realizar previamente un análisis jerárquico para estudiar el dendograma. Además, existen métodos como el del *Ancho de Silueta*; o algoritmos, como *NbClust* en R, que sugieren un número de grupos en función de una batería de pruebas presentes en la literatura.

La segunda cuestión clave es **cómo determinar las "semillas"** o centroides iniciales. Una opción es generar las semillas de modo aleatorio, aunque no es un método muy conveniente. De hecho, cada vez que se aplicara el algoritmo de *k-medias*, podría obtenerse una solución diferente. Otra alternativa es la fijación de las "semillas" por parte del investigador. Una idea, en este sentido, es hacer un *clúster jerárquico* previo, y tomar los *centroides* de la solución final como “semillas” de k-medias. Otra posibilidad interesante es aplicar el método del *centroide más lejano*: se fija el primer centroide al azar, pero luego el 2º centroide coincidirá con el punto de datos más alejado de él. En general, el jº centroide coincidirá con el punto cuya distancia mínima a los centroides precedentes sea mayor. Se pretende que los centroides estén bien separados unos de otros. Una versión mejorada de este procedimiento es el método *k-medias++*.

Para nuestro ejemplo práctico, vamos a volver a utilizar como variables clasificadoras las del ejemplo desarrollado para los métodos jerárquicos; pero esta vez para una muestra de 300 empresas de transporte de mercancías interestelar. Estas variables son, de nuevo, los índices de diversificación (IDIVERSE), fidelización de clientes (IFIDE) y de digitalización (IDIG). Con base en ellos, queremos **establecer una serie de perfiles o tipologías** de las empresas que componen el sector.

Trabajaremos, como en el ejemplo de clúster jerarquizado, en el proyecto llamado "cluster". Vamos a ir a la carpeta del proyecto y vamos a guardar en ella los dos archivos de esta práctica: un archivo de Microsoft® Excel® llamado “interestelar_300.xlsx" y un *script* denominado "kmedias_rstars.R". En la última sección del capítulo dispones de los enlaces a dicho material.

Si abrimos el archivo de Microsoft® Excel®, "interestelar_300.xlsx", comprobaremos que tiene la misma estructura y "hojas" que nos encontramos en "interestelar_25.xlsx"; pero extendido a 300 empresas o compañías.

### ![](figuras/pie-chart.svg) Preparación de los datos.

La primera parte del script es semejante a la del *script* de la parte de "clúster jerárquico", *cluster_rstars.R*. Solo cambia en los siguientes aspectos:

-   No utiliza el paquete `{factoextra}`, y sí utiliza los paquetes `{cluster}`y `{ClusterR}`.
-   El archivo a importar es *interestelar_300.xlsx*, y los datos se almacenan en el *data frame* "interestelar_300".
-   Los *outliers*, en esta ocasión, sí son eliminados, con lo que se trabajará con un *data frame* sin casos atípicos, denominado "seleccion_so".

Dicho esto, pasamos a transcribir el código correspondiente a la limpieza del *Global Environment*, carga de paquetes, función para crear composiciones de 2X2 gráficos con `{patchwork}`, importación de datos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
### CLUSTER de k-medias empresas TMI. ###

# Limpiando el Global Environment
rm(list = ls())

# Cargando paquetes
library(readxl)
library(dplyr)
library(ggplot2)
library(gtExtras)
library(visdat)
library (cluster)
library (ClusterR)
library (knitr)
library (kableExtra)
library (patchwork)
library (pgirmess)

##### Función para crear composiciones de gráficos con patchwork ###############
create_patchwork <- function(plot_list) {
  n <- length(plot_list)
  if (n == 0) return(NULL)
  full_rows <- n %/% 4
  remaining <- n %% 4
  patchworks <- list()
  
  if (full_rows > 0) {
    for (i in seq(1, full_rows * 4, by = 4)) {
      patchworks <- c(patchworks, list((plot_list[[i]] + plot_list[[i+1]]) / 
                                         (plot_list[[i+2]] + plot_list[[i+3]])))
    }
  }
  
  if (remaining > 0) {
    last_plots <- plot_list[(full_rows * 4 + 1):n]
    empty_plots <- lapply(1:(4 - remaining), function(x) ggplot() + theme_void())
    last_patchwork <- do.call(patchwork::wrap_plots, c(last_plots, empty_plots))
    patchworks <- c(patchworks, list(last_patchwork))
  }
  return(patchworks)
}
################################################################################

## DATOS

# Importando datos desde Excel
interestelar_300 <- read_excel("interestelar_300.xlsx",
                              sheet = "Datos",
                              na = c("n.d."))
interestelar_300 <- data.frame(interestelar_300, row.names = 1)

# Seleccionando variables metricas para el analisis.
seleccion <- interestelar_300 %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion_df_graph <- gt_plt_summary(seleccion)
seleccion_df_graph

# Localizando missing values.
seleccion %>%
  vis_miss() +
  labs(title = "Indicadores: Diversificación, Fidelidad, Digitalización",
       subtitle = "Transporte de mercancías interestelar",
       y = "Observación",
       fill = NULL) +
  scale_fill_manual(
    values = c("TRUE" = "red", "FALSE" = "grey"),
    labels = c("TRUE" = "NA", "FALSE" = "Presente")) +
  theme(
    plot.title = element_text(face = "bold", size = 14))

seleccion %>% filter(is.na(IDIVERSE) |
                       is.na(IFIDE) |
                       is.na(IDIG)) %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion <- seleccion %>%
  filter(! is.na(IDIVERSE) &
           ! is.na(IFIDE) &
           ! is.na(IDIG)) 
```

Aunque se ha incluido el código para identificar y eliminar los casos con ***missing values***, el gráfico generado con la función `vis_miss()` ya informó de que no había ningún dato ausente en las tres variables de interés, almacenadas en el *data frame* "seleccion", luego se siguen manteniendo los 300 casos.

Para la detección de ***outliers***, se ha utilizado también el mismo código que en el análisis clúster jerárquico: obtención de las distancias de *Mahalanobis*, y aplicación de diagrama de caja o *boxplot*: 

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Identificando outliers con distancia de Mahalanobis.
seleccion <- seleccion %>%
  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),
                                   center = colMeans(.),
                                   cov    = cov(.)))

ggplot(data = seleccion, map = (aes(y = MAHALANOBIS))) +
  geom_boxplot(fill = "orange") +
  ggtitle("DISTANCIA DE MAHALANOBIS",
          subtitle = "IDIVERSE, IFIDE, IDIG. Empresas TMI.") +
  ylab("MAHALANOBIS")
```

Una vez se constata que existen casos *outliers*, se pueden identificar con el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
Q1M <- quantile (seleccion$MAHALANOBIS, c(0.25))
Q3M <- quantile (seleccion$MAHALANOBIS, c(0.75))

seleccion %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
         MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(MAHALANOBIS, IDIVERSE, IFIDE, IDIG)
```

Puede comprobarse que son 26 casos. En esta ocasión, y a diferencia del caso del análisis jerárquico, vamos a eliminar estas observaciones, ya que lo que deseamos no es agrupar casos concretos, sino identificar patrones que caractericen a la mayor parte de las empresas. Para eliminar los *outliers*, ejecutaremos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Eliminando outliers.
seleccion_so <-seleccion %>%
  filter(MAHALANOBIS <= Q3M + 1.5*IQR(MAHALANOBIS) &
         MAHALANOBIS >= Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(IDIVERSE, IFIDE, IDIG)
```

Hemos creado un *data frame* llamado "seleccion_so" con las variables de nuestro análisis; pero sin los casos considerados atípicos. Por último, eliminaremos la variable MAHALANOBIS, ya que no será necesaria para el resto del análisis:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Eliminando variable MAHALANOBIS del df con outliers
seleccion <- seleccion %>% select(-MAHALANOBIS)
```

### ![](figuras/pie-chart.svg) Determinación del número de grupos o clústeres a formar.

Una vez preparados los datos, vamos a proceder a aplicar la técnica de formación de conglomerados no-jerárquica de **k-medias**. Primero, y puesto que se basa en el cálculo de las distancias euclídeas entre casos, procederemos a tipificar los valores de las variables, utilizando la función `scale()`, y creando el data frame "zseleccion_so". Después, calcularemos la matriz de distancias **d**:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
## CLUSTER K-MEDIAS CON VARIABLES ORIGINALES

# Tipificando variables
zseleccion_so <- data.frame(scale(seleccion_so))
summary(zseleccion_so)

d <- dist(zseleccion_so)
```

Una cuestión clave es la determinación previa del número de grupos a formar. Si no se tiene decidido un número de conglomerados *a priori* como consecuencia del interés o de los objetivos de la propia investigación, se puede recurrir como orientación a algún método o algoritmo "objetivo". De nuevo, puede recurrirse al **método de la *anchura media de silueta**, proponiendo diferentes números de grupos (parámetro *k*).

La función `KMeans_rcpp()` del paquete `{ClusterR}` permite calcular la *anchura media de Silueta* para cada número de grupos propuesto, estimando los grupos mediante el método de **k-medias++**. Aplicaremos el método para cada número de grupos *k* propuesto, mediante una función que guarda el ancho medio de silueta en un vector:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# --- Calcular el ancho medio de silueta para distintos k ---
set.seed(123)
res_sil <- sapply(2:10, 
                  function(k) {
                    km <- KMeans_rcpp(zseleccion_so,
                    clusters = k,
                    num_init = 10,
                    max_iters = 100,
                    initializer = "kmeans++")
  mean(silhouette(km$clusters, d)[, "sil_width"])
}
)
```

El código anterior ejecuta la siguiente secuencia:

1. **`set.seed(123)`** fija la “semilla” aleatoria. *k-medias* (incluso con *kmeans++*) usa un poco de azar al empezar; fijar la semilla hace que siempre se obtengan los mismos resultados al repetir el código (reproducibilidad).

2.  **`sapply(2:10, function(k) { ... })`**:

  - 2:10 es la secuencia de valores de *k* que vamos a probar (de 2 a 10 clústeres).

  - `sapply(..., function(k) { ... })` aplica la función a cada *k* y devuelve el vector numérico *res_sil* con los resultados.
  
3.  Dentro de la función anónima **`function(k) { ... }`**:

  - `KMeans_rcpp()` (del paquete `{ClusterR}`) ejecuta *k-medias* sobre los datos del *data frame* "zseleccion_so".

  - `clusters = k`: número de grupos que queremos formar.
  
  - `num_init = 10`: ejecuta el algoritmo 10 veces con diferentes inicios y se queda con la mejor solución (esto reduce la mala suerte de una mala inicialización).
  
  - `max_iters = 100`: tope de iteraciones por ejecución.
  
  - `initializer = "kmeans++"`: estrategia de inicio que suele dar mejores resultados que un inicio totalmente aleatorio.
  
El objeto `km` contiene, entre otras cosas, `km$clusters`, que es el vector de asignaciones: dice a qué cluster pertenece (1, 2, 3, …, k) cada caso.

`silhouette(...)` (del paquete cluster) calcula, para cada observación, su ancho de silueta. Necesita:

  - `km$clusters`: las etiquetas de clúster.

  - `d`: matriz de distancias entre observaciones.

El resultado de silhouette es una matriz con columnas; una de ellas es `"sil_width" =`, que es el ancho de silueta de cada punto.

  - `[...] [, "sil_width"]` extrae esa columna.

  - `mean(...)` hace la media: se obtiene el ancho medio de la silueta para ese k.

El resultado final es el vector "res_sil", con 9 valores (para k = 2, 3, …, 10). Cada valor es el ancho medio de silueta correspondiente. Servirá para crear, junto a cada valor de "k", un pequeño *data frame* de nombre "df_sil" que será la base, a su vez, para representar los *anchos de silueta* gráficamente con `{ggplot2}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# --- Crear data frame para graficar ---
df_sil <- data.frame(
  k = 2:10,
  Silhouette = res_sil
)

# --- Gráfico con ggplot2 ---
ggplot(df_sil, aes(x = k, y = Silhouette)) +
  geom_line(color = "steelblue", linewidth = 1) +
  geom_point(size = 3, color = "darkred") +
  labs(title = "Ancho medio de Silueta para distintos k (k-means++)",
       x = "Número de clústeres (k)",
       y = "Silueta media") +
  theme_minimal(base_size = 13)
```

El número de clústeres aconsejado por el gráfico es k=2 (primer punto, el más alto). No obstante, dos grupos puede aportar poco detalle en cuanto a la identificación de patrones de empresa diferentes. La segunda opción, según el gráfico, es la de **7 grupos**. Es la que adoptaremos.

### ![](figuras/pie-chart.svg) Aplicación del método de k-medias con determinación de semillas *kmeans++*.

Una vez decidido el número de grupos (en el ejemplo, 7), se aplica (de nuevo) el método de ***k-medias***, en la versión de la función `KMeans_rcpp()` del paquete `{ClusterR}`, que permite obtener las "semillas" (centroides iniciales) mediante el algoritmo *kmeans++*, que ofrece buenos resultados frente a otras posibilidades de obtención de "semillas", como la generación puramente aleatoria. La solución final se guarda en el objeto “cluster_k”:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Aplicación definitiva de k-medias.
  k <- 7  # poner aquí número de grupos decidido!!!!

  # Aplicando k-means con inicializacion kmeans++

  cluster_k <-KMeans_rcpp (zseleccion_so,
                           clusters = k,
                           num_init = 10,
                           max_iters = 100,
                           initializer = "kmeans++")
```

Como se acaba de explicar, en la función `KMeans_rcpp()`, el primer argumento es el *data frame* con las variables clasificadoras (en sus versiones tipificadas). El segundo es el número de clústeres o grupos a obtener (que lo hemos asignado anteriormente al parámetro “**k**”). El tercero, `num_init =`, es el número de veces que se repite el procedimiento a fin de retener la mejor solución. `Max_iters =` fija el máximo de iteraciones del procedimiento de *k-medias* hasta obtener una solución estable. `Initializer =` define el método de obtención de las semillas (en nuestro caso, *k-means++*). La solución final se guarda en el objeto “cluster_k”, como se puede apreciar en el *Environment*.

Dentro de la solución, el vector con el grupo de pertenencia de cada empresa se obtiene con el elemento “\$clusters”. Conviene guardar ese vector como factor. En concreto, se ha añadido como el factor **`whatcluster_k`**, integrado dentro del *data frame* "seleccion_so".

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  seleccion_so$whatcluster_k <- as.factor(cluster_k$clusters)
```

### ![](figuras/pie-chart.svg) Caracterización de los grupos o conglomerados.

A continuación vamos a **caracterizar los grupos** formados en función de las medias de las variables originales (coordenadas de los *centroides*). Así, se podrán mostrar en pantalla las medias de cada grupo de las distintas variables originales, usando las funciones `by_group()` y `summarise()` de `{dplyr}`. Toda la información se asigna al *data frame* “tablamedias”; para poder, posteriormente, representarla en una tabla mediante las facilidades que ofrecen los paquetes `{knitr}` y `{kableExtra}`:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}
# CARACTERIZANDO GRUPOS FORMADOS
  
  # Tabla con centroides de grupos.
  tablamedias <- seleccion_so %>%
    group_by(whatcluster_k) %>%
    summarise(obs = length(whatcluster_k),
              Idiverse = mean(IDIVERSE),
              Ifide = mean(IFIDE),
              Idig = mean(IDIG))
  
  tablamedias %>%
    kable(caption = "Método de k-medias. 7 grupos. Medias de variables",
          col.names = c("Clúster",
                        "Observaciones",
                        "I. Diversif.",
                        "I. Fidelizac.",
                        "I. Digitalizac."),
          digits = c(NA, 0, 3, 3, 3),
          format.args = list(decimal.mark = ".",
                             scientific = FALSE)) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped",
                  "bordered",
                  "condensed",
                  position = "center",
                  font_size = 11) %>%
    row_spec(0, bold= T,
             align = "c") %>%
    row_spec(1:nrow(tablamedias),
             bold= F,
             align = "c")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
# CARACTERIZANDO GRUPOS FORMADOS
  
  # Tabla con centroides de grupos.
  tablamedias <- seleccion_so %>%
    group_by(whatcluster_k) %>%
    summarise(obs = length(whatcluster_k),
              Idiverse = mean(IDIVERSE),
              Ifide = mean(IFIDE),
              Idig = mean(IDIG))

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

    if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {

    tablamedias %>%
    kable(caption = "Método de k-medias. 7 grupos. Medias de variables",
          col.names = c("Clúster",
                        "Observaciones",
                        "I. Diversif.",
                        "I. Fidelizac.",
                        "I. Digitalizac."),
          digits = c(NA, 0, 3, 3, 3),
          format.args = list(decimal.mark = ".",
                             scientific = FALSE)) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped",
                  "bordered",
                  "condensed",
                  position = "center",
                  font_size = 11) %>%
    row_spec(0, bold= T,
             align = "c") %>%
    row_spec(1:nrow(tablamedias),
             bold= F,
             align = "c")
    }else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {      
tablamedias %>%
    kable(caption = "Método de k-medias. 7 grupos. Medias de variables",
          col.names = c("Clúster",
                        "Observaciones",
                        "I. Diversif.",
                        "I. Fidelizac.",
                        "I. Digitalizac."),
          digits = c(NA, 0, 3, 3, 3),
          format.args = list(decimal.mark = ".",
                             scientific = FALSE))
    }
```

Obviamente, también se podrían comparar las medias de los grupos, para cada variable, con un gráfico de barras. El código es el siguiente:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Gráficos de centroides
  
  # Vector de nombre de variables excluyendo la variable no deseada
  variables <- setdiff(names(tablamedias), c("whatcluster_k", "obs"))
  
  # Lista para almacenar los gráficos
  graficos.centroides <- list()
  
  # Bucle para crear y almacenar los gráficos
  for (i in seq_along(variables)) {
    var1 <- variables[[i]]
    grafico <- ggplot(data= tablamedias,
                      map = (aes_string(y = var1, x = "whatcluster_k"))) +
      geom_bar(stat = "identity",
               colour = "red",
               fill = "orange",
               alpha = 0.7) +
      ggtitle(paste0(var1, ". Media por grupos."),
              subtitle = "Empresas TMI.")+
      xlab ("Grupo") +
      ylab(var1)
    graficos.centroides[[paste0("grafico_", var1)]] <- grafico
  }          
```

Para hacer composiciones de 4 gráficos (que en este caso será solo una, dado que hemos almacenado en la lista "graficos.centroides" 4 elementos correspondientes a las 4 variables clasificadoras), volveremos a utilizar la función `create_patchwork()`, que ya se mostró en el ejemplo de clúster jerárquico. De nuevo se incluye su código y se aplica a la lista de gráficos "graficos.centroides":

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Aplicar función de composiciones a gráficos de centroides.
  grupos.graficos.centroides <- create_patchwork(graficos.centroides)
  
  # Presentar las composiciones
  for (n in 1:length(grupos.graficos.centroides)){
    print(grupos.graficos.centroides[[n]])
  }
```

La lista "grupos.graficos.centroides" almacena las composiciones de 4 (o menos) gráficos generadas. En este ejemplo, solo tiene un elemento, que es mostrado al ejecutar el bucle de presentación de composiciones.

Por medio de la tabla y los gráficos anteriores se pueden trazar algunas conclusiones, comparando las medias de las variables para cada grupo de casos formado (coordenadas de los centroides). El análisis revela una clara heterogeneidad estructural entre las empresas de transporte de mercancías interestelar. Las tres dimensiones —diversificación, fidelización y digitalización— se combinan de manera diferente en cada grupo, reflejando distintos modelos de negocio y grados de madurez estratégica. Así, podemos distinguir tres grandes perfiles:

- Empresas avanzadas e innovadoras (altos niveles en las tres dimensiones).

- Empresas especializadas tradicionales, con baja digitalización y diversificación.

- Empresas en transición o de nicho, con fortalezas parciales (por ejemplo, alta fidelidad pero baja digitalización).

Analizando cada grupo de modo individualizado:

- **Grupo 1** (57 empresas). Alta diversificación (35.8), alta fidelidad (40.2), digitalización media (22.2). En general, el grupo se compone de empresas sólidas, con una base de clientes fiel y estrategias diversificadas que les permiten operar en distintos segmentos o rutas. Aunque su digitalización es moderada, su estructura sugiere estabilidad y resiliencia. Perspectiva futura: Si invierten más en tecnología, podrían convertirse en líderes del sector, con ventajas competitivas sostenibles a largo plazo.

- **Grupo 2** (17 empresas). Diversificación media-baja (17.9), muy alta fidelidad (43.1), alta digitalización (26.0). Se compone de empresas tecnológicamente dinámicas y con gran fidelidad, pero con un negocio poco diversificado. Probablemente dominan un nicho específico del transporte interestelar.
Perspectiva futura: Su foco estratégico y digitalización pueden asegurarles crecimiento, aunque deberían explorar una mayor diversificación para reducir riesgos sectoriales.

- **Grupo 3** (17 empresas). Baja diversificación (15.8), fidelidad media (34.8), digitalización media (21.8). Las empresas que conforman este conglomerado se encuentran, en general, en fase de consolidación, con cierto desarrollo digital pero sin un posicionamiento claro ni fidelización destacable. Perspectiva futura: Si no fortalecen su propuesta de valor o amplían su mercado, podrían quedar rezagadas frente a competidores más innovadores.

- **Grupo 4** (30 empresas). Muy baja diversificación (7.5), fidelidad baja (31.6), muy baja digitalización (7.8). Este grupo reúne las empresas más rezagadas, con escasa adaptación tecnológica y un modelo de negocio poco flexible.Perspectiva futura: Riesgo elevado de obsolescencia o pérdida de cuota de mercado. Necesitan transformarse urgentemente mediante digitalización y estrategias de retención de clientes.

- **Grupo 5** (74 empresas, grupo más numeroso). Diversificación baja (11.1), fidelidad media-alta (38.1), muy baja digitalización (6.8). Grupo formado, en término medio, por empresas tradicionales, que conservan buena relación con sus clientes pero no han avanzado en digitalización. Representan el núcleo clásico del sector TMI. Perspectiva futura: Son vulnerables a la disrupción tecnológica. Sin inversión en digitalización y diversificación, podrían perder relevancia en mercados más competitivos.

- **Grupo 6** (18 empresas). Alta diversificación (35.9), muy alta fidelidad (43.7), muy alta digitalización (33.7). Grupo constituido por las empresas punteras del sector, con máximos en las tres dimensiones. Combinan diversificación de servicios, lealtad de clientes y transformación digital. Perspectiva futura: Tienen el liderazgo asegurado y marcan el estándar competitivo. Probablemente sean grandes corporaciones interplanetarias con capacidad de expansión internacional.

- **Grupo 7** (61 empresas). Diversificación media (24.6), fidelidad alta (39.3), digitalización baja-media (14.8). Grupo formado por empresas en transición digital, con buen posicionamiento comercial y cierta diversificación. Perspectiva futura: Si continúan su proceso de digitalización, pueden convertirse en empresas de alto rendimiento similares al grupo 1 o incluso al 6.

Conclusión **general**: El Grupo 6 es el referente: altamente competitivo, innovador y sostenible. El Grupo 4 y parte del Grupo 5 enfrentan los mayores desafíos estructurales. Los Grupos 1 y 7 son prometedores si logran acelerar su digitalización. Los Grupos 2 y 3 podrían especializarse o asociarse para sobrevivir en un mercado cada vez más concentrado y tecnológico. En conjunto, el análisis sugiere que la digitalización se convierte en el factor diferenciador clave para la supervivencia y crecimiento en el transporte de mercancías interestelar, actuando como catalizador de la fidelidad y la diversificación.

El análisis gráfico anterior se puede complementar de un modo más formal, a fin de verificar si las diferencias observadas entre los valores medios de los grupos, para cada variable, son significativas. Para ello, y teniendo en cuenta que los grupos se pueden considerar submuestras que representan a subpoblaciones, se puede aplicar alguna prueba de comparaciones múltiples de las medias de los grupos formados, de modo que se pueda confirmar, para cierta significación estadística (usualmente 0,05), **si las diferencias en las medias de los grupos para cada variable son estadísticamente relevantes** (significativas) o no.

Una prueba clásica para llevar a cabo esta tarea es aplicar el *test de comparaciones múltiples de Tuckey*. No obstante, y dado que esta prueba requiere del cumplimiento de ciertos requisitos previos (normalidad de los grupos, varianzas homogéneas); hemos optado por la ***prueba robusta de Kruskal-Wallis*****.** Para cada una de las variable originales, realizaremos un gráfico múltiple de diagramas de caja, y procederemos a mostrar los resultados de la prueba.

En primer lugar, vamos a definir el vector con el nombre de las variables que entran en el análisis (vector "variables"), e inicializaremos las listas para guardar los gráficos y los resultados de la prueba para cada variable ("graficos_kw" y "tablas_kw", respectivamente): 

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # ¿Centroides estadísticamente significativos?

    # Vector de nombre de variables excluyendo la variable no deseada
      variables <- setdiff(names(seleccion_so), "whatcluster_k")

    # Inicializar listas para almacenar gráficos y tablas
      graficos_kw <- list()
      tablas_kw <- list()
```

Luego, haremos un bucle para que se realice el gráfico de caja para cada una de las variables. Los gráficos se almacenan en la lista "graficos_kw", que luego pasan a la función `create_patchwork()`para agruparse en elementos de la lista "gruposgraficos_kw" (que solo tendrá un elemento, puesto que únicamente hay tres gráficos que agrupar en composiciones de 4 o menos. 

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # ¿Diferencias entre centroides estadísticamente significativas?

   # Vector de nombre de variables excluyendo la variable no deseada
      variables <- setdiff(names(seleccion_so), "whatcluster_k")

   # Bucle para generar gráficos
     for (i in seq_along(variables)) {
          variable <- variables[i]
  
    # Crear el gráfico
      p <- ggplot(data = seleccion_so,
                  aes_string(x = "whatcluster_k",
                             y = variable,
                             fill = "whatcluster_k")) +
           geom_boxplot(outlier.shape = NA) +
           stat_summary(fun = "mean",
                        geom = "point",
                        size = 3,
                        col = "red") +
           stat_summary(fun = "mean",
                        geom = "line",
                        col = "red",
                        aes(group = TRUE)) +   
           geom_jitter(width = 0.1,
                       size = 1,
                       col = "red",
                       alpha = 0.40) +
           ggtitle(paste(variable, ". Comparación de grupos."),
                   subtitle = "Empresas TMI.") +
           ylab("Valor")
  
    # Almacenar el gráfico en la lista
      graficos_kw[[i]] <- p
    }

    # Crear composiciones de gráficos.
            gruposgraficos_kw <- create_patchwork(graficos_kw)    

            for (i in seq_along(gruposgraficos_kw)) {
              print(gruposgraficos_kw[[i]])
    }   
```

En cuanto a la prueba de *comparaciones múltiples*, la función que se ocupa de llevarla a cabo es `kruskalmc()`, del paquete `{pgirmess}`. la solución se guarda en un objeto, por ejemplo, "datos_kmc", y los argumentos son, por un lado, la variable analizada y el factor que se utiliza para denominar los grupos ("whatcluster_k"), unidos por el símbolo "\~". La función se aplica dentro del bucle que permite aplicar la prueba a cada una de las tres variables. Además, los resultados se pasan en tablas que son almacenadas en la lista "tablas_kw":

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
    # Realizar el análisis de Kruskal-Wallis  y llevar resultados a tablas

      tablas_kw <- list()
      for (i in seq_along(variables)) {
         variable <- variables[i]
         datos_kmc <- kruskalmc(as.formula(paste(variable, "~ whatcluster_k")),
                                 data = seleccion_so)
              
         tabla <- datos_kmc$dif.com %>%
         kable(caption = paste("k-medias. Diferencias de Centroides", variable),
               col.names = c("Diferencias centros",
                             "Diferencias críticas",
                             "Significación"),
               digits = c(3, 3, NA),
               format.args = list(decimal.mark = ".",
                                  scientific = FALSE)) %>%
         kable_styling(full_width = F,
                bootstrap_options = c("striped",
                                      "bordered",
                                      "condensed"),
                position = "center",
                font_size = 11) %>%
         row_spec(0, bold = T, align = "c") %>%
         row_spec(1:nrow(datos_kmc$dif.com), bold = F, align = "c")

    # Almacenar la tabla en la lista
      
      tablas_kw[[i]] <- tabla
      }
  
    # Mostrar tablas almacenadas

      for (i in seq_along(tablas_kw)) {
           print(tablas_kw[[i]])
      }
```

A partir del código anterior se obtienen los siguientes resultados, concernientes al análisis de comparaciones múltiples (de medias) de *Kruskal-Wallis*. Cada fila compara dos grupos. Las columnas indican:

- Diferencias centros: diferencia absoluta entre los rangos medios de esos dos grupos.

- Diferencias críticas: umbral mínimo que esa diferencia debe superar para considerarse significativa.

- Significación (TRUE/FALSE): indica si la diferencia es estadísticamente significativa (TRUE) o no (FALSE).

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

    # Realizar el análisis de Kruskal-Wallis  y llevar resultados a tablas

            tablas_kw <- list()
            for (i in seq_along(variables)) {
              variable <- variables[i]
              datos_kmc <- kruskalmc(as.formula(paste(variable, "~ whatcluster_k")),
                                     data = seleccion_so)
if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
              tabla <- datos_kmc$dif.com %>%
                kable(caption = paste("k-medias. Diferencias de Centroides", variable),
                      col.names = c("Diferencias centros",
                                    "Diferencias críticas",
                                    "Significación"),
              digits = c(3, 3, NA),
              format.args = list(decimal.mark = ".",
                                 scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = c("striped",
                                      "bordered",
                                      "condensed"),
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold = T, align = "c") %>%
  row_spec(1:nrow(datos_kmc$dif.com), bold = F, align = "c")
  }else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
tabla <- datos_kmc$dif.com %>%
                kable(caption = paste("k-medias. Diferencias de Centroides", variable),
                      col.names = c("Diferencias centros",
                                    "Diferencias críticas",
                                    "Significación"),
              digits = c(3, 3, NA),
              format.args = list(decimal.mark = ".",
                                 scientific = FALSE))
  }
   # Almacenar la tabla en la lista
     tablas_kw[[i]] <- tabla
  }
  
    # Mostrar tablas almacenadas

      for (i in seq_along(tablas_kw)) {
           print(tablas_kw[[i]])
      }  
```

En el caso de la variable **IDIVERSE** (grado de diversificación del negocio), se aprecia que los grupos 1 y 6, con los mayores valores medios de diversificación (~35 puntos), no difieren entre sí, pero sí del resto. Son empresas altamente diversificadas, posiblemente grandes corporaciones con presencia en múltiples rutas galácticas y segmentos de carga. Por otro lado, los grupos 4 y 5, con valores muy bajos (≈7–11), se sitúan en el extremo opuesto, significativamente diferentes de casi todos los demás. Representan compañías especializadas o locales, con operaciones limitadas y modelos menos flexibles. Los grupos 2, 3 y 7 ocupan posiciones intermedias, sin diferencias significativas entre ellos. Podrían estar en una fase de transición o exploración de nuevos mercados, con estrategias aún en consolidación.

En cuanto a la variable **IFIDE** (grado de fidelización de los clientes del negocio), se puede deducir que los grupos 2 y 6 son los “campeones de fidelización”: no muestran diferencias significativas entre sí, pero sí con casi todos los demás grupos. Representan empresas muy consolidadas comercialmente, con redes de clientes leales. Económicamente, disfrutan de ingresos estables y bajos costes de adquisición de clientes, lo que les confiere ventaja competitiva sostenida. Por otro lado, los grupos 1 y 7 exhiben una “alta fidelidad consolidada”, similar entre ambos, sin diferencias significativas con los campeones (2 y 6) en algunos casos. Estas compañías probablemente combinan buen servicio y reputación con cierto margen para mejorar la digitalización o la personalización de su oferta. Adicionalmente, el grupo 5 se caracteriza por una fidelidad media, es decir, alcanzan un nivel aceptable, pero significativamente inferior al de los grupos líderes. Finalmente, los grupos 3 y 4 adolecen de una fidelización débil. No difieren entre sí, pero sí del resto. Son, en general grupos formados por empresas con poca orientación al cliente o escaso conocimiento del mismo, probablemente con modelos operativos más básicos y precios competitivos. Desde una perspectiva de futuro, son las más vulnerables a la pérdida de clientes y a la competencia.

PO último, en relación con la variable **IDIG** (grado de digitalización de cada empresa: integración de sistemas inteligentes, automatización logística, uso de IA en gestión de flotas, plataformas de clientes, etc.), podemos distinguir cuatro niveles tecnológicos:

- Grupo 6 (media 33.7). Difere significativamente de casi todos los grupos, menos 1, 2 y 3. Representa a las empresas más avanzadas tecnológicamente, probablemente grandes corporaciones con sistemas integrados de navegación, inteligencia artificial en mantenimiento predictivo y plataformas de comercio galáctico.

- Grupos 1 (22.2), 2 (26.0), 3 (21.8). No hay diferencias significativas entre ellos, lo que indica un bloque intermedio tecnológicamente.Han avanzado en digitalización (probablemente con sistemas de trazabilidad o automatización parcial), pero aún no alcanzan la madurez total del grupo 6.

- Grupo 7 (14.8). Difiere significativamente de casi todos los grupos más avanzados, pero no de 3. Empresas con digitalización parcial o básica (gestión digital de clientes o control de flota limitado).

- Grupos 4 (7.8) y 5 (6.8). No difieren entre sí, pero sí de todos los demás.Son los más rezagados tecnológicamente: probablemente dependientes de procesos manuales, escasa automatización y baja conectividad. 

Finalmente, es interesante mostrar cómo los elementos de cada grupo se disponen en los diagramas de dispersión producto de cruzar las variables clasificadoras entre sí. Para generar estos gráficos de un modo automatizado, primero generaremos un vector con el nombre de todas las variables (excluyendo al *factor* whatcluster_k mediante la función `setdiff()`. Luego, crearemos una lista para ir almacenando los gráficos (lista "graficos").a continuación, calcularemos todas las combinaciones de nombres de variables posibles, con la función `combn()`, y las almacenaremos en la lista "combinaciones". En esta función, el argumento **`simplify = FALSE`** le dice a la función que no vuelque el resultado a una matriz:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# GRÁFICOS Variable vs Variable

  # Lista de variables excluyendo la variable no deseada
    variables <- setdiff(names(seleccion_so), "whatcluster_k")

  # Lista para almacenar los gráficos
    graficos <- list()

  # Generar todas las combinaciones posibles de pares de variables
    combinaciones <- combn(variables, 2, simplify = FALSE)
```

Una vez almacenados los elementos anteriores, procederemos a crear los gráficos de dispersión mediante un bucle. Estos gráficos se guardarán en la lista "graficos", que se pasará por la función `create_patchwork()` para que se sinteticen en composiciones de 4 (o menos) gráficos. Estas composiciones se almacenan en la lista "gruposgraficos", y se presentan mediante un nuevo bucle:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Bucle para crear y almacenar los gráficos
    for (i in seq_along(combinaciones)) {
         var1 <- combinaciones[[i]][1]
         var2 <- combinaciones[[i]][2]
         grafico <- ggplot(seleccion_so,
                           map = aes_string(x = var1,
                                            y = var2,
                                            color = "whatcluster_k")) +
         geom_point() +
         labs(title = paste("GRÁFICO", var1, "-", var2),
              subtitle = "Empresas eólicas") +
         xlab (var1) +
         ylab (var2) +
         scale_color_brewer(palette = "Set1") 
         graficos[[paste0("grafico_", var1, "_", var2)]] <- grafico
    }

  # Aplicar función de composiciones de patchwork

    gruposgraficos <- create_patchwork(graficos)

  # Presentar las composiciones
    for (n in 1:length(gruposgraficos)){
         print(gruposgraficos[[n]])
    }
```

Los gráficos de dispersión muestran con claridad la estructura espacial de los siete clústeres y su correspondencia con los niveles de **diversificación**, **fidelización** y **digitalización** de las empresas. Los grupos 4 y 5 se concentran en la zona inferior izquierda de los diagramas, caracterizados por los valores más bajos en todas las variables —empresas con escasa diversificación y digitalización, y menor fidelidad de clientes—, mientras que el grupo 6 se sitúa de forma destacada en el extremo superior derecho, evidenciando su liderazgo tecnológico y su alta diversificación. Los grupos 1 y 7 se ubican en posiciones intermedias, aunque algo más desplazados hacia niveles medios-altos de fidelidad y diversificación, lo que los perfila como empresas con potencial de crecimiento. Finalmente, los grupos 2 y 3, más concentrados en la parte superior central, presentan altos niveles de fidelización y moderada digitalización, pero con menor dispersión en diversificación, lo que sugiere estrategias más focalizadas.

En conjunto, los diagramas reflejan relaciones positivas entre las tres variables, especialmente entre diversificación y digitalización (IDIVERSE–IDIG), donde se observa un patrón casi lineal: las empresas más diversificadas tienden también a estar más digitalizadas. La relación entre fidelización y digitalización (IFIDE–IDIG) es más dispersa, aunque se vislumbra una pendiente positiva: la digitalización parece acompañarse de cierta mejora en la fidelidad de los clientes. En cambio, la asociación fidelidad–diversificación (IFIDE–IDIVERSE) resulta más compleja, con un grupo principal en torno a valores medios-altos de fidelización independientemente de la diversificación, lo que podría indicar que la fidelidad no depende directamente de la amplitud del negocio, sino de otros factores como la calidad del servicio o la relación cliente–empresa.

## ![](figuras/book.svg){.hicon} ![](figuras/pie-chart.svg){.hicon} Clustering basado en densidad (DBSCAN).

El método **DBSCAN** —acrónimo de *Density-Based Spatial Clustering of Applications with Noise*— pertenece a la familia de algoritmos de agrupación basados en densidad.

A diferencia de los métodos jerárquicos o de partición, que buscan minimizar distancias o varianzas internas, DBSCAN identifica regiones del espacio donde los puntos están suficientemente “apiñados” y las considera clústeres naturales, mientras que los puntos aislados son clasificados como ruido o outliers.

La idea central es muy intuitiva. Imaginemos que recorremos el espacio de datos con un círculo (en tres dimensiones, una esfera) de radio eps. Si ese círculo abarca al menos minPts observaciones, el punto central se considera parte de una zona densa. Si la densidad se propaga a los vecinos, se forma un clúster; si no, el punto se etiqueta como ruido.

Así, el algoritmo forma clústeres a partir de regiones densas y deja fuera los puntos dispersos.

En este método, cada punto o caso se clasifica según su *vecindad*:

- **Punto núcleo (core point):** tiene al menos `minPts` vecinos a una distancia menor que `eps`.  
- **Punto borde (border point):** no cumple esa condición, pero pertenece a la vecindad de un punto núcleo.  
- **Ruido:** punto que no pertenece a ningún clúster.

El proceso se repite hasta que todos los puntos han sido etiquetados.
De este modo, DBSCAN no necesita especificar el número de clústeres a priori: el número total emerge de los patrones de densidad del propio conjunto de datos.

Los dos **hiperparámetros clave** de la técnica son:

| Parámetro | Significado | Efecto principal |
|------------|--------------|------------------|
| `eps` | Radio máximo para considerar vecindad | Si es muy grande, fusiona clústeres distintos. Si es muy pequeño, genera exceso de ruido. |
| `minPts` | Número mínimo de puntos para formar un clúster denso | Valores típicos entre 4 y 10 según el tamaño de muestra. |

Como **ventajas** frente a los métodos *jerárquicos* y de *k-medias* podemos destacar:

- **No requiere fijar k**: a diferencia de *k-medias*, donde el número de grupos se define manualmente, *DBSCAN* detecta automáticamente cuántos clústeres existen.

- **Identifica ruido y *outliers* **: los métodos clásicos fuerzan la asignación de todos los casos a algún grupo; *DBSCAN* puede dejarlos fuera, lo que resulta muy útil cuando hay observaciones anómalas.

- **Detecta clústeres de forma irregular**: *k-medias* tiende a producir grupos esféricos; *DBSCAN* permite clústeres de formas arbitrarias y de tamaños distintos.

- **Escala bien con datos medianos o grandes**: es eficiente para conjuntos de varios miles de observaciones.

En cuanto a sus **limitaciones**, y las **precauciones que se han de tener en cuenta**, hemos de advertir:

- **Sensibilidad a los parámetros *eps* y *minPts* **: su resultado depende fuertemente de estos valores. Si *eps* es demasiado pequeño, surgen muchos puntos de ruido; si es demasiado grande, los clústeres se fusionan.

- **Dificultad en densidades heterogéneas**: cuando existen regiones con densidades muy distintas, un único eps puede ser inadecuado para todas (en esos casos, el algoritmo *HDBSCAN*, su versión jerárquica, ofrece mejores resultados).

- **Escalado y distancia**: requiere que las variables estén en la misma escala y que la métrica de distancia sea representativa del problema.

Una de las mayores virtudes de DBSCAN es su **robustez ante outliers**. En los métodos jerárquicos o de partición (*k-medias*, *Ward*), un punto extremo puede distorsionar el cálculo de distancias o medias y alterar significativamente la estructura de los grupos. En cambio, *DBSCAN* identifica explícitamente los puntos atípicos como ruido, y no los utiliza para formar ni expandir clústeres. Dicho de otro modo: los *outliers* no influyen en los límites de los clústeres; y el algoritmo no intenta “forzarlos” a pertenecer a algún grupo, sino que los deja fuera de la estructura principal. Esto convierte a *DBSCAN* en una técnica intrínsecamente más robusta frente a observaciones anómalas, especialmente cuando los datos han sido previamente tipificados.

En el ejemplo práctico volveremos a utilizar la misma muestra de 300 empresas de transporte de mercancías interestelar empleada en el apartado de *k-medias*. Las variables clasificadoras son, de nuevo, los tres índices ya conocidos: diversificación (IDIVERSE), fidelización de clientes (IFIDE) y digitalización (IDIG).

Trabajaremos dentro del proyecto denominado "cluster", donde guardaremos los dos archivos necesarios para esta práctica: el archivo de datos en formato Microsoft® Excel® (interestelar_300.xlsx) y el *script* de R denominado *dbscan_rstars.R*. En la última sección del capítulo se facilitan los enlaces a ambos materiales.

### ![](figuras/pie-chart.svg) Preparación de los datos.

La primera parte del script reproduce exactamente el mismo código que vimos en el ejemplo de k-medias: limpieza de *Global Environment*, activación de paquetes, código de la función `create_patchwork()`, importación de los datos, selección y presentación de las variables del análisis, tratamiento de *missing values* e identificación de *outliers*:

```{r, eval=FALSE, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
### CLUSTER por algoritmo DBScan empresas TMI. ###

# Limpiando el Global Environment
rm(list = ls())

# Cargando paquetes
library(readxl)
library(dplyr)
library(ggplot2)
library(gtExtras)
library(visdat)
library(dbscan)
library (knitr)
library (kableExtra)
library (patchwork)

##### Función para crear composiciones de gráficos con patchwork ###############
create_patchwork <- function(plot_list) {
  n <- length(plot_list)
  if (n == 0) return(NULL)
  full_rows <- n %/% 4
  remaining <- n %% 4
  patchworks <- list()
  
  if (full_rows > 0) {
    for (i in seq(1, full_rows * 4, by = 4)) {
      patchworks <- c(patchworks, list((plot_list[[i]] + plot_list[[i+1]]) / 
                                         (plot_list[[i+2]] + plot_list[[i+3]])))
    }
  }
  
  if (remaining > 0) {
    last_plots <- plot_list[(full_rows * 4 + 1):n]
    empty_plots <- lapply(1:(4 - remaining), function(x) ggplot() + theme_void())
    last_patchwork <- do.call(patchwork::wrap_plots, c(last_plots, empty_plots))
    patchworks <- c(patchworks, list(last_patchwork))
  }
  return(patchworks)
}
################################################################################

## DATOS

# Importando datos desde Excel
interestelar_300 <- read_excel("interestelar_300.xlsx",
                               sheet = "Datos",
                               na = c("n.d."))
interestelar_300 <- data.frame(interestelar_300, row.names = 1)

# Seleccionando variables metricas para el analisis.
seleccion <- interestelar_300 %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion_df_graph <- gt_plt_summary(seleccion)
seleccion_df_graph

# Localizando missing values.
seleccion %>%
  vis_miss() +
  labs(title = "Indicadores: Diversificación, Fidelidad, Digitalización",
       subtitle = "Transporte de mercancías interestelar",
       y = "Observación",
       fill = NULL) +
  scale_fill_manual(
    values = c("TRUE" = "red", "FALSE" = "grey"),
    labels = c("TRUE" = "NA", "FALSE" = "Presente")) +
  theme(
    plot.title = element_text(face = "bold", size = 14))

seleccion %>% filter(is.na(IDIVERSE) |
                       is.na(IFIDE) |
                       is.na(IDIG)) %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion <- seleccion %>%
  filter(! is.na(IDIVERSE) &
           ! is.na(IFIDE) &
           ! is.na(IDIG)) 

# Identificando outliers con distancia de Mahalanobis.
seleccion <- seleccion %>%
  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),
                                   center = colMeans(.),
                                   cov    = cov(.)))

ggplot(data = seleccion, map = (aes(y = MAHALANOBIS))) +
  geom_boxplot(fill = "orange") +
  ggtitle("DISTANCIA DE MAHALANOBIS",
          subtitle = "IDIVERSE, IFIDE, IDIG. Empresas TMI.") +
  ylab("MAHALANOBIS")

Q1M <- quantile (seleccion$MAHALANOBIS, c(0.25))
Q3M <- quantile (seleccion$MAHALANOBIS, c(0.75))

seleccion %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(MAHALANOBIS, IDIVERSE, IFIDE, IDIG)

# Eliminando variable MAHALANOBIS del df con outliers
seleccion <- seleccion %>% select(-MAHALANOBIS)
```

Aunque el *script* incluye el código para detectar y eliminar posibles *missing values*, el gráfico generado por `vis_miss()` confirma que no existen datos ausentes en las tres variables de interés. Por tanto, el *data frame* "seleccion" mantiene los 300 casos originales:

```{r, eval=TRUE, echo=FALSE, results='hide', fig.show='hide', message=FALSE, warning=FALSE}
### CLUSTER por algoritmo DBScan empresas TMI. ###

# Limpiando el Global Environment
rm(list = ls())

# Cargando paquetes
library(readxl)
library(dplyr)
library(ggplot2)
library(gtExtras)
library(visdat)
library(dbscan)
library (knitr)
library (kableExtra)
library (patchwork)

##### Función para crear composiciones de gráficos con patchwork ###############
create_patchwork <- function(plot_list) {
  n <- length(plot_list)
  if (n == 0) return(NULL)
  full_rows <- n %/% 4
  remaining <- n %% 4
  patchworks <- list()
  
  if (full_rows > 0) {
    for (i in seq(1, full_rows * 4, by = 4)) {
      patchworks <- c(patchworks, list((plot_list[[i]] + plot_list[[i+1]]) / 
                                         (plot_list[[i+2]] + plot_list[[i+3]])))
    }
  }
  
  if (remaining > 0) {
    last_plots <- plot_list[(full_rows * 4 + 1):n]
    empty_plots <- lapply(1:(4 - remaining), function(x) ggplot() + theme_void())
    last_patchwork <- do.call(patchwork::wrap_plots, c(last_plots, empty_plots))
    patchworks <- c(patchworks, list(last_patchwork))
  }
  return(patchworks)
}
################################################################################

## DATOS

# Importando datos desde Excel
interestelar_300 <- read_excel("interestelar_300.xlsx",
                               sheet = "Datos",
                               na = c("n.d."))
interestelar_300 <- data.frame(interestelar_300, row.names = 1)

# Seleccionando variables metricas para el analisis.
seleccion <- interestelar_300 %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion_df_graph <- gt_plt_summary(seleccion)
seleccion_df_graph
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Localizando missing values.
seleccion %>%
  vis_miss() +
  labs(title = "Indicadores: Diversificación, Fidelidad, Digitalización",
       subtitle = "Transporte de mercancías interestelar",
       y = "Observación",
       fill = NULL) +
  scale_fill_manual(
    values = c("TRUE" = "red", "FALSE" = "grey"),
    labels = c("TRUE" = "NA", "FALSE" = "Presente")) +
  theme(
    plot.title = element_text(face = "bold", size = 14))
```

En la detección de *outliers*, se calcularon las *distancias de Mahalanobis* y se representaron mediante un *boxplot*. El gráfico evidenció la presencia de casos atípicos, que fueron identificados mediante el filtro habitual (26 observaciones):

```{r, eval=TRUE, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
seleccion %>% filter(is.na(IDIVERSE) |
                       is.na(IFIDE) |
                       is.na(IDIG)) %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion <- seleccion %>%
  filter(! is.na(IDIVERSE) &
           ! is.na(IFIDE) &
           ! is.na(IDIG)) 

# Identificando outliers con distancia de Mahalanobis.
seleccion <- seleccion %>%
  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),
                                   center = colMeans(.),
                                   cov    = cov(.)))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data = seleccion, map = (aes(y = MAHALANOBIS))) +
  geom_boxplot(fill = "orange") +
  ggtitle("DISTANCIA DE MAHALANOBIS",
          subtitle = "IDIVERSE, IFIDE, IDIG. Empresas TMI.") +
  ylab("MAHALANOBIS")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
Q1M <- quantile (seleccion$MAHALANOBIS, c(0.25))
Q3M <- quantile (seleccion$MAHALANOBIS, c(0.75))

seleccion %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(MAHALANOBIS, IDIVERSE, IFIDE, IDIG)
```

```{r, eval=TRUE, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# Eliminando outliers.
seleccion_so <-seleccion %>%
  filter(MAHALANOBIS <= Q3M + 1.5*IQR(MAHALANOBIS) &
           MAHALANOBIS >= Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(IDIVERSE, IFIDE, IDIG)

# Eliminando variable MAHALANOBIS del df con outliers
seleccion <- seleccion %>% select(-MAHALANOBIS)
```

A partir de aquí iniciaremos un procedimiento de análisis diferente. *DBSCAN* es una técnica robusta frente a los *outliers*, ya que estos no afectan a la formación de los clústeres, sino que el propio algoritmo los identifica y clasifica como “ruido”. Por tanto, mantendremos las 300 observaciones en el análisis.

### ![](figuras/pie-chart.svg) Aplicación de la técnica.

Antes de calcular distancias, es imprescindible que todas las variables estén en la **misma escala**. Si no lo hacemos, aquellas con un rango numérico más amplio dominarán el cálculo de distancias y distorsionarán la formación de los grupos. Para evitarlo, aplicamos una **tipificación estándar (z-score)** a cada variable: restamos su media y dividimos entre su desviación típica.

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
## APLICACION DE DBSCAN

# 1) Tipificación (z-scores)
zseleccion <- scale(seleccion)
zseleccion <- as.matrix(zseleccion)  # dbscan espera matriz/numérico
```

Como ya se precisó, *DBSCAN* depende de dos parámetros principales que determinan cómo se definen las regiones densas del espacio de datos:

- **minPts**: número mínimo de observaciones que deben encontrarse dentro de un vecindario para que se considere suficientemente denso como para formar parte de un clúster.

- **eps**: radio del vecindario, es decir, la distancia máxima que define si dos puntos son vecinos.

En la práctica, una regla empírica útil es **minPts = 2 × p**, donde *p* es el número de variables métricas. Este valor garantiza que el algoritmo considere suficiente densidad en función de la dimensionalidad del problema. Como en nuestro caso p = 3, usaremos minPts = 6.

Además, definiremos una **proporción objetivo de ruido**, que representa el porcentaje aproximado de observaciones que esperamos que el algoritmo clasifique como *outliers* o no pertenecientes a ningún clúster. A partir de este nivel deseado de ruido, el procedimiento buscará automáticamente el valor de *eps* que lo produzca.

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# 2) Determinación de minPts y eps
minPts0 <- 6  # regla general: 2*p (p=3)
ruido_obj <- 0.20 # proporción objetivo de ruido (p. ej., 0.10, 0.20, 0.30)
tol_ruido <- 0.02 # tolerancia (±2 p.p.)
max_iter  <- 30   # tope de iteraciones
```

En este bloque de código, los parámetros cumplen las siguientes funciones:

- **`minPts0`**: define la **densidad mínima** requerida para formar un clúster (en este caso, seis puntos).

- **`ruido_obj`**: fija la **proporción** deseada de observaciones que se considerarán **ruido** o casos atípicos.

- **`tol_ruido`**: margen de **tolerancia** aceptado respecto al valor objetivo de ruido (±2 puntos porcentuales).

- **`max_iter`**: número *máximo de iteraciones* permitidas para ajustar *eps* y alcanzar la proporción de ruido deseada.

Para comenzar la búsqueda de un valor adecuado para el radio *eps*, analizamos la distribución de las **distancias al (minPts − 1)-ésimo vecino más cercano** de cada observación. Este gráfico, conocido como *curva kNN*, suele presentar una primera zona plana (puntos en regiones densas) y, a partir de cierto valor, un “codo” o cambio brusco de pendiente que marca la transición hacia las zonas menos densas o de ruido. Ese punto de inflexión nos orienta sobre el **rango de valores razonables** para *eps*.

A partir de esta información inicial, el siguiente paso consistirá en acotar automáticamente la búsqueda dentro de un intervalo que incluya tanto la zona densa como la de transición.

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# 3) Rango inicial inteligente para eps usando kNN (k = minPts0-1)
knn_d <- kNNdist(zseleccion, k = minPts0 - 1)
d_sorted <- sort(as.numeric(knn_d))

eps_lo <- max(min(d_sorted) * 0.9, 1e-6)      # límite inferior
eps_hi <- max(d_sorted) * 1.5                  # límite superior amplio
```

El código anterior calcula un rango inicial de búsqueda para el parámetro *eps* a partir de las distancias entre observaciones más cercanas. La función `kNNdist()` (del paquete `{dbscan}`) obtiene, para cada punto del conjunto tipificado "zseleccion", la distancia a su (minPts − 1)-ésimo vecino más próximo. Estas distancias reflejan cuán denso es el entorno de cada observación: los puntos en zonas compactas tienen distancias pequeñas, mientras que los puntos aislados presentan distancias más grandes.

A continuación:

- **`cknn_d`** guarda todas esas distancias.

- **`d_sorted`** las ordena de menor a mayor para facilitar su análisis.

- **`eps_lo`** y **`eps_hi`** definen, respectivamente, los límites inferior y superior del rango dentro del cual se buscará el valor óptimo de *eps*.

El límite inferior (`eps_lo`) se fija ligeramente por debajo de la distancia mínima observada. El límite superior (`eps_hi`) se amplía un 50 % por encima del valor máximo, para garantizar que el rango incluya tanto las zonas densas como las menos densas del espacio de datos.

En conjunto, este paso permite acotar de forma inteligente la búsqueda de *eps*, evitando probar valores arbitrarios y asegurando que el algoritmo explore un **intervalo realista** basado en la estructura de los propios datos.

En el paso o código siguiente se define una función auxiliar denominada `noise_rate()`, que servirá para **evaluar la proporción de ruido** generada por el modelo *DBSCAN* para distintos valores del parámetro *eps*. Su objetivo es permitirnos ajustar automáticamente el radio óptimo que produce el nivel de ruido deseado:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# 4) Función auxiliar: calcula proporción de ruido para un eps
noise_rate <- function(eps) {
  fit <- dbscan::dbscan(zseleccion, eps = eps, minPts = minPts0)
  mean(fit$cluster == 0)
}
```

La lógica del código anterior es sencilla. Cada vez que se ejecute `noise_rate()` con un valor concreto de *eps*, la función aplicará internamente el algoritmo `dbscan()` sobre el conjunto de datos tipificados "zseleccion". Se mantienen constantes el parámetro minPts (almacenado en minPts0) y el número de variables.

El argumento *eps* se actualiza en cada llamada, de modo que se prueban distintos radios de vecindad. La función calcula el porcentaje de observaciones clasificadas como ruido, es decir, aquellas que el modelo identifica con la etiqueta *cluster = 0*.

La expresión `mean(fit$cluster == 0)` devuelve el **proporción de puntos fuera de los clústeres**, ya que cuenta cuántos casos cumplen esa condición y los divide entre el total.

En resumen, este paso crea una herramienta compacta que nos permitirá, más adelante, probar de forma sistemática diferentes valores de *eps* y comprobar qué proporción de ruido generan. Gracias a ello, podremos automatizar la elección de eps en lugar de depender exclusivamente de la inspección visual del gráfico *kNN*.

En le siguiente paso pretendemos encontrar de forma automática el valor de *eps* que produzca una proporción de ruido cercana al objetivo fijado (`ruido_obj`). Para ello, este bloque de código implementa una búsqueda iterativa que ajusta progresivamente el valor de eps hasta aproximarse al nivel deseado de ruido. El procedimiento se basa en una estrategia conocida como **búsqueda binaria** *(binary search)*, un método eficiente que va dividiendo el intervalo de búsqueda por la mitad en cada paso, descartando la zona que no cumple el criterio.

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# 5) Búsqueda binaria para acercarnos a ruido_obj
tested <- data.frame(eps = numeric(0), ruido = numeric(0))

for (i in seq_len(max_iter)) {
  eps_mid <- (eps_lo + eps_hi) / 2
  r_mid   <- noise_rate(eps_mid)
  tested  <- rbind(tested, data.frame(eps = eps_mid, ruido = r_mid))
  
  if (abs(r_mid - ruido_obj) <= tol_ruido) break
  if (r_mid > ruido_obj) {
    # demasiado ruido -> aumentar eps para unir vecindarios y reducir ruido
    eps_lo <- eps_mid
  } else {
    # poco ruido -> disminuir eps para ser más estricto
    eps_hi <- eps_mid
  }
}
```

En este código:

- Se crea primero el *data frame* "tested", que almacenará los pares de valores *eps* y la proporción de ruido (ruido) obtenidos en cada iteración.

- En cada paso del bucle for:

  1.  Se calcula un **valor intermedio del radio**, `eps_mid`, como el punto medio entre los límites actuales del rango (`eps_lo` y `eps_hi`).

  2.  Se ejecuta la función `noise_rate()` para ese valor de *eps*, obteniendo la proporción de ruido `r_mid`.

  3.  El resultado se guarda en el *data frame* "tested" para poder consultarlo o graficarlo posteriormente.

- A continuación, el código compara el ruido obtenido `r_mid` con el ruido objetivo `ruido_obj`:

  - Si la diferencia está dentro de la tolerancia admitida (`tol_ruido`), el bucle se detiene porque se ha encontrado un *eps* adecuado.

  - Si el ruido actual es demasiado alto, significa que los clústeres son demasiado pequeños; para corregirlo, se aumenta *eps*, lo que amplía los vecindarios y une más puntos.

  - Si el ruido es demasiado bajo, se reduce *eps*, haciendo los clústeres más estrictos y separando más puntos como atípicos.

Este proceso se repite hasta alcanzar la proporción deseada de ruido o hasta completar el número máximo de iteraciones definido en `max_iter`.

El resultado final es un conjunto de valores *eps* probados con sus correspondientes niveles de ruido, entre los cuales se encontrará el radio más adecuado para aplicar el modelo *DBSCAN* con los parámetros óptimos.

Una vez completadas las iteraciones del paso anterior, disponemos de varios valores de *eps* junto con las proporciones de ruido que generaron. El siguiente paso consiste en seleccionar, pues, el **valor óptimo** de eps, es decir, aquel que produce un **nivel de ruido más próximo al objetivo fijado** (`ruido_obj`).

Este bloque de código realiza esa selección de forma automática y guarda los resultados finales para ser utilizados en el modelo definitivo:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# 6) Elegimos el eps con ruido más cercano a la diana
ix_best   <- which.min(abs(tested$ruido - ruido_obj))
eps_final <- tested$eps[ix_best]
ruido_est <- tested$ruido[ix_best]
```

En este fragmento de código:

- "tested" es el *data frame* generado en la etapa anterior, que contiene todas las combinaciones de *eps* evaluadas junto con su proporción de ruido.

- La instrucción `which.min(abs(tested$ruido - ruido_obj))` identifica el índice (`ix_best`) del valor de *eps* cuya proporción de ruido está más cerca del objetivo deseado. A partir de ese índice:

  - `eps_final` guarda el valor de **radio óptimo** que se empleará finalmente en el algoritmo *DBSCAN*.

  - `ruido_est` almacena la **proporción real de ruido** que produce ese valor óptimo.

Este paso resume todo el proceso de búsqueda en un resultado claro y cuantitativo: el radio `eps_final`, ajustado para lograr la densidad de agrupación coherente con el nivel de ruido que se considera adecuado para el análisis.

Con el valor óptimo de *eps* ya determinado, podemos ajustar el modelo *DBSCAN* definitivo. En este paso se aplica el algoritmo utilizando los parámetros seleccionados (`eps_final` y `minPts0`) sobre el conjunto de datos tipificados "zseleccion". El objetivo es obtener la estructura final de clústeres, junto con la asignación de cada observación a su correspondiente grupo o al conjunto de ruido.

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# 7) Modelo final
modelo_db <- dbscan::dbscan(zseleccion, eps = eps_final, minPts = minPts0)
modelo_db
```

En este bloque:

- La función `dbscan()` ejecuta el algoritmo con los parámetros óptimos:

  - `eps = eps_final`, el radio de vecindad que determina qué puntos se consideran cercanos.

  - `minPts = minPts0`, el número mínimo de observaciones necesarias en un vecindario para formar un clúster.

El resultado se guarda en el objeto "modelo_db", que contiene toda la información del agrupamiento: número de clústeres detectados, tamaño de cada grupo y proporción de ruido. AL mostrarlo en pantalla, R presenta un resumen con el conteo de puntos por clúster y el porcentaje de observaciones no asignadas (ruido).

Finalmente, se muestra un resumen sintético de los resultados obtenidos por el modelo *DBSCAN* y se asocian las etiquetas de clúster a las observaciones originales. Este paso permite comprobar de un vistazo la coherencia entre los parámetros elegidos y los resultados alcanzados:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# 8) Resumen rápido
cat("\n---\n",
    "eps_final =", round(eps_final, 3), 
    "| ruido_obj =", scales::percent(ruido_obj),
    "| ruido_logrado =", scales::percent(ruido_est), "\n")

table(Cluster = modelo_db$cluster)

seleccion$whatcluster_dbs <- as.factor(modelo_db$cluster)
```

En este bloque:

- La función `cat()` imprime un resumen limpio con los valores finales:

- `eps_final`, el radio óptimo de vecindad.

- `ruido_obj`, la proporción de ruido objetivo definida por el usuario.

- `ruido_est`, el porcentaje real de observaciones que el modelo ha clasificado como ruido.

Este breve informe permite verificar si el algoritmo alcanzó el nivel de ruido esperado o si se desvió ligeramente dentro del margen de tolerancia.

La función `table()` genera un recuento del número de observaciones asignadas a cada clúster, incluido el *clúster 0*, que representa las observaciones no agrupadas o de baja densidad.

Finalmente, la instrucción `seleccion$whatcluster_dbs <- as.factor(modelo_db$cluster)` incorpora al *data frame* original una nueva columna denominada **whatcluster_dbs**, que almacena la etiqueta de clúster asignada por *DBSCAN* a cada empresa. Esta variable será muy útil en la siguiente fase del análisis, cuando visualicemos los clústeres y analicemos sus características internas.

### ![](figuras/pie-chart.svg) Caracterización de los clústeres o grupos formados.

Una vez obtenidos los clústeres, y asignados los elementos (teniendo en cuenta que el *grupo 0*
no es realmente un clúster; sino que reúne a los casos considerados atípicos, *ouliers* o *ruido*), puede aplicarse el mismo código que se empleó en el caso de *k-medias* a la hora de caracterizar tales grupos (omitimos por no extender el código la prueba de comparaciones múltiples de *Kruskal-Wallis*):

```{r, eval=FALSE, echo=TRUE, results='hide', fig.show='hide', message=FALSE, warning=FALSE}
# CARACTERIZANDO GRUPOS FORMADOS

# Tabla con centroides de grupos.
tablamedias <- seleccion %>%
  group_by(whatcluster_dbs) %>%
  summarise(obs = length(whatcluster_dbs),
            Idiverse = mean(IDIVERSE),
            Ifide = mean(IFIDE),
            Idig = mean(IDIG))

tablamedias %>%
  kable(caption = "Método DBSCAN. Medias de variables (Grupo 0 = Ruido)",
        col.names = c("Clúster",
                      "Observaciones",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(NA, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                "bordered",
                "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T,
           align = "c") %>%
  row_spec(1:nrow(tablamedias),
           bold= F,
           align = "c")

# Gráficos de centroides

# Vector de nombre de variables excluyendo la variable no deseada
variables <- setdiff(names(tablamedias), c("whatcluster_dbs", "obs"))

# Lista para almacenar los gráficos
graficos.centroides <- list()

# Bucle para crear y almacenar los gráficos
for (i in seq_along(variables)) {
  var1 <- variables[[i]]
  grafico <- ggplot(data= tablamedias,
                    map = (aes_string(y = var1, x = "whatcluster_dbs"))) +
    geom_bar(stat = "identity",
             colour = "red",
             fill = "orange",
             alpha = 0.7) +
    ggtitle(paste0(var1, ". Media por grupos."),
            subtitle = "Empresas TMI.")+
    xlab ("Grupo") +
    ylab(var1)
  graficos.centroides[[paste0("grafico_", var1)]] <- grafico
}                 

# Aplicar función de composiciones a gráficos de centroides.
grupos.graficos.centroides <- create_patchwork(graficos.centroides)

# Presentar las composiciones
for (n in 1:length(grupos.graficos.centroides)){
  print(grupos.graficos.centroides[[n]])
}

# GRÁFICOS Variable vs Variable

# Lista de variables excluyendo la variable no deseada
variables <- setdiff(names(seleccion), "whatcluster_dbs")

# Lista para almacenar los gráficos
graficos <- list()

# Generar todas las combinaciones posibles de pares de variables
combinaciones <- combn(variables, 2, simplify = FALSE)

# Bucle para crear y almacenar los gráficos
for (i in seq_along(combinaciones)) {
  var1 <- combinaciones[[i]][1]
  var2 <- combinaciones[[i]][2]
  grafico <- ggplot(seleccion,
                    map = aes_string(x = var1,
                                     y = var2,
                                     color = "whatcluster_dbs")) +
    geom_point() +
    labs(title = paste("GRÁFICO", var1, "-", var2),
         subtitle = "Empresas TMI.") +
    xlab (var1) +
    ylab (var2) +
    scale_color_brewer(palette = "Set1") 
  graficos[[paste0("grafico_", var1, "_", var2)]] <- grafico
}

# Aplicar función de composiciones de patchwork

gruposgraficos <- create_patchwork(graficos)

# Presentar las composiciones
for (n in 1:length(gruposgraficos)){
  print(gruposgraficos[[n]])
}
```

Con el código anterior, por ejemplo, podemos construir la tabla de número de elementos y medias de cada clúster:

```{r, eval=TRUE, echo=FALSE, fig.show='hide', message=FALSE, warning=FALSE}
# Tabla con centroides de grupos.
tablamedias <- seleccion %>%
               group_by(whatcluster_dbs) %>%
               summarise(obs = length(whatcluster_dbs),
                                      Idiverse = mean(IDIVERSE),
                                      Ifide = mean(IFIDE),
                                      Idig = mean(IDIG))

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
tablamedias %>%
  kable(caption = "Método DBSCAN. Medias de variables (Grupo 0 = Ruido)",
        col.names = c("Clúster",
                      "Observaciones",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(0, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                                    "bordered",
                                    "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T,
           align = "c") %>%
  row_spec(1:nrow(tablamedias),
           bold= F,
           align = "c")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
tablamedias %>%
  kable(caption = "Método DBSCAN. Medias de variables (Grupo 0 = Ruido)",
        col.names = c("Clúster",
                      "Observaciones",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(0, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE))
}  
```


Además, obtenemos los gráficos de las medias de los clústeres y de dispersión de las variables 2 a 2:

```{r, eval=TRUE, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# Gráficos de centroides

# Vector de nombre de variables excluyendo la variable no deseada
variables <- setdiff(names(tablamedias), c("whatcluster_dbs", "obs"))

# Lista para almacenar los gráficos
graficos.centroides <- list()

# Bucle para crear y almacenar los gráficos
for (i in seq_along(variables)) {
  var1 <- variables[[i]]
  grafico <- ggplot(data= tablamedias,
                    map = (aes_string(y = var1, x = "whatcluster_dbs"))) +
    geom_bar(stat = "identity",
             colour = "red",
             fill = "orange",
             alpha = 0.7) +
    ggtitle(paste0(var1, ". Media por grupos."),
            subtitle = "Empresas TMI.")+
    xlab ("Grupo") +
    ylab(var1)
  graficos.centroides[[paste0("grafico_", var1)]] <- grafico
}                 

# Aplicar función de composiciones a gráficos de centroides.
grupos.graficos.centroides <- create_patchwork(graficos.centroides)

# Presentar las composiciones
for (n in 1:length(grupos.graficos.centroides)){
  print(grupos.graficos.centroides[[n]])
}

# GRÁFICOS Variable vs Variable

# Lista de variables excluyendo la variable no deseada
variables <- setdiff(names(seleccion), "whatcluster_dbs")

# Lista para almacenar los gráficos
graficos <- list()

# Generar todas las combinaciones posibles de pares de variables
combinaciones <- combn(variables, 2, simplify = FALSE)

# Bucle para crear y almacenar los gráficos
for (i in seq_along(combinaciones)) {
  var1 <- combinaciones[[i]][1]
  var2 <- combinaciones[[i]][2]
  grafico <- ggplot(seleccion,
                    map = aes_string(x = var1,
                                     y = var2,
                                     color = "whatcluster_dbs")) +
    geom_point() +
    labs(title = paste("GRÁFICO", var1, "-", var2),
         subtitle = "Empresas TMI.") +
    xlab (var1) +
    ylab (var2) +
    scale_color_brewer(palette = "Set1") 
  graficos[[paste0("grafico_", var1, "_", var2)]] <- grafico
}

# Aplicar función de composiciones de patchwork

gruposgraficos <- create_patchwork(graficos)

# Presentar las composiciones
for (n in 1:length(gruposgraficos)){
  print(gruposgraficos[[n]])
}
```

Se deja al lector, como ejercicio práctico, la interpretación de estos gráficos a fin de caracterizar los 4 clústeres identificados (ha de tenerse en cuenta que el grupo 0 no es un clúster, sino que recoge los elementos *ruido*, y por tanto, su interpretación carece de sentido, al menos en térmisnos de valores medios de las variables).

Para concluir, conviene hacerse una pregunta relevante: **¿Cómo ha clasificado *DBSCAN* a los casos 26 que fueron identificados como *outliers*?** ¿Los ha etiquetado como "ruido", como sería lo coherente?

La siguiente tabla nos muestra la respuesta:

```{r, eval=FALSE, echo=TRUE, fig.show='hide', message=FALSE, warning=FALSE}
# ¿Los outliers son ruido?

seleccion <- seleccion %>%
  mutate(MAHALANOBIS = mahalanobis(select(., IDIVERSE, IFIDE, IDIG),
                                   colMeans(select(., IDIVERSE, IFIDE, IDIG)),
                                   cov(select(., IDIVERSE, IFIDE, IDIG))))
Q1M <- quantile (seleccion$MAHALANOBIS, c(0.25))
Q3M <- quantile (seleccion$MAHALANOBIS, c(0.75))

seleccion_out <- seleccion %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(whatcluster_dbs, MAHALANOBIS, IDIVERSE, IFIDE, IDIG)

seleccion_out %>%
  kable(caption = "¿Outliers son ruido? (Grupo 0 = Ruido)",
        col.names = c("Caso",
                      "Observaciones",
                      "Grupo (0=ruido)",
                      "D. Mahalanobis",
                      "I. Diversificación",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(NA, 3, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                "bordered",
                "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T,
           align = "c") %>%
  row_spec(1:nrow(seleccion_out),
           bold= F,
           align = "c")
```

```{r, eval=TRUE, echo=FALSE, fig.show='hide', message=FALSE, warning=FALSE, results='asis'}
# ¿Los outliers son ruido?

seleccion <- seleccion %>%
  mutate(MAHALANOBIS = mahalanobis(select(., IDIVERSE, IFIDE, IDIG),
                                   colMeans(select(., IDIVERSE, IFIDE, IDIG)),
                                   cov(select(., IDIVERSE, IFIDE, IDIG))))
Q1M <- quantile (seleccion$MAHALANOBIS, c(0.25))
Q3M <- quantile (seleccion$MAHALANOBIS, c(0.75))

seleccion_out <- seleccion %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(whatcluster_dbs, MAHALANOBIS, IDIVERSE, IFIDE, IDIG)

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
seleccion_out %>%
  kable(caption = "¿Outliers son ruido? (Grupo 0 = Ruido)",
        col.names = c("Caso",
                      "Observaciones",
                      "Grupo (0=ruido)",
                      "D. Mahalanobis",
                      "I. Diversificación",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(NA, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                "bordered",
                "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T,
           align = "c") %>%
  row_spec(1:nrow(seleccion_out),
           bold= F,
           align = "c")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
  seleccion_out %>%
  kable(caption = "¿Outliers son ruido? (Grupo 0 = Ruido)",
        col.names = c("Caso",
                      "Observaciones",
                      "Grupo (0=ruido)",
                      "D. Mahalanobis",
                      "I. Diversificación",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(NA, 3, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE))
}
```

En efecto, todas las compañías que fueron identificadas como *outliers* han sido clasificadas como *ruido* (es decir, no pertenecen a ninguno de los clústeres identificados). De hecho, *DBSCAN* ha sido más exigente, y ha clasificado como *ruido* otras empresas que no fueron localizadas, por el método tradicional, como *outliers*.

## Materiales para realizar las prácticas del capítulo.

En esta sección se muestran los links de acceso a los diferentes materiales (*scripts*, datos...) necesarios para llevar a cabo los contenidos prácticos del capítulo.

**Datos (en formato Microsoft (R) Excel (R)):**

-   interestelar_25.xlsx ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/interestelar_25.xlsx))
-   interestelar_300.xlsx ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/interestelar_300.xlsx))

**Scripts:**

-   cluster_rstars.R ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/cluster_rstars.R))
-   kmedias_rstars.R ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/kmedias_rstars.R))
-   dbscan_rstars.R ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/dbscan_rstars.R))

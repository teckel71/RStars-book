---
title: "R-Stars: La Guía"
author: 'Miguel-Ángel Tarancón'
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography:
- lola.bib
- packages.bib
biblio-style: apalike
link-citations: true
url: https://teckel71.github.io/RStars-book/
cover-image: figuras/Portada_v3-2.jpg
description: |
  Manual de R y técnicas multivariantes.
always_allow_html: true
---

#  {.unnumbered}

![](figuras/Portada_v3-2.jpg){.portada-img width="100%"}

**Autor:**

| Miguel Ángel Tarancón Morán.

| Catedrático de Economía Aplicada. Universidad de Castilla - La Mancha.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

```{r include=FALSE}
if (knitr:::is_html_output()) '# Bibliografía {-}'
```

<!--chapter:end:index.Rmd-->

# Prefacio {.unnumbered}
  
![[Estrellas... y R.]{.smallcaps}](figuras/00%20Prefacio.jpg){width="100%"}
  
Este libro recoge diversas prácticas que se han ido desarrollando a lo largo de multitud de cursos en varias asignaturas de grado y máster relacionadas con el **análisis de datos**, especialmente de tipo económico, en la Facultad de Derecho y Ciencias Sociales de Ciudad Real, tomando como motor de análisis el lenguaje **R**, y el IDE (Interfaz) R-Studio.

Podría extenderme en por qué elegir R en lugar de otros lenguajes o software. En algún momento del capítulo introductorio hablo de la cuestión brevemente. Aquí, diré que R cuenta con una serie de ventajas: es un recurso de uso libre y gratuito (igual que R-Studio), orientado especialmente para el análisis estadístico (frente a otros lenguajes realmente potentes, como Python), y cuenta con una comunidad muy activa que genera continuamente una gran variedad de algoritmos y código para el análisis estadístico de datos.

Como desventaja, hay que reconocer que, al principio, la curva de aprendizaje puede tener una pendiente relativamente elevada. Pero, una vez familiarizados con el funcionamiento de R y su entorno, las posibilidades son casi infitas.

Para hacer más llevadera esta etapa inicial de aprendizaje, o al menos más divertida, los ejemplos prácticos que se ofrecen se han extraído del proyecto **R-Stars**. Este proyecto consiste en la generación, mediante herramientas de inteligencia artificial, de una base de datos con múltiples variables, especialmente de tipo económico, que engloban a 300 empresas ficticias. Pese a ser todo una gran simulación, los datos mantienen las normas de coherencia económico-contables básicas, y cierta dosis de realismo (dentro de lo que cabe).

Este sector es el del transporte interestelar de mercancías.

```{r, echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("figuras/Gemini_Generated_azul_fondo_blanco.png")
```

Poco a poco, este sector imaginario ha ido rodeándose de un contexto que, espero, sea divertido. El nombre de las empresas, planetas, etc. son un constante homenaje al cine de ciencia ficción.

Por tanto, ruego que disculpéis por anticipado cierta orientación "friki" de los datos utilizados para aprender las diferentes técnicas. Pero bueno, esta afición por el cine fantástico tiene la culpa. Disfrutemos de ello, con imaginación y cierta (muchas) dosis de humor.

Adicionalmente, hay secciones en algunos capítulos en que se os da acceso a la realización de algunas de prácticas propuestas haciendo uso de varios paquetes creados ad-hoc (sistema MAT) para aquellos de vosotros que, a cambio de una menor personalización y flexibilidad en el código, queráis realizar algunas acciones habituales y análisis de un modo más sencillo, con menos código (funciones creadas expresamente).

Las secciones en las que se organizan los diferentes capítulos pueden ser de diversos tipos. Cada uno de ellos se identifica por medio de un pequeño icono:

| Icono | Tipo de sección |
|:----------------------------------:|:----------------------------------:|
| ![](figuras/book-open.svg) | Sección teórica (alternativa usando paquetes de sistema MAT) |
| ![](figuras/pie-chart.svg) | Sección práctica (manos a la obra) |
| ![](figuras/save.svg) | Sección práctica (alternativa usando paquetes de sistema MAT) |
| ![](figuras/arrow-down-circle.svg) | Descarga de materiales del capítulo |
| ![](figuras/book.svg) | Bibliografía |
| ![](figuras/key.svg) | Sección básica (recomendable leer) |
| ![](figuras/paperclip.svg) | Ampliación (recomendable leer si se quiere profundizar algo más) |
| ![](figuras/star.svg) | Sección con datos y curiosidades del universo R-Stars |
: Tipo de Secciones.

También, por supuesto, pido disculpas por los errores que inevitablemente encontraréis en el libro, que poco a poco irán puliéndose (esperemos) tras constantes revisiones y mejoras. En este sentido, os agradeceré infinitamente que cualquier error que encontréis, ortográfico, de redacción, o de contenido o programación, me lo comuniquéis.

Para finalizar, quiero dar las gracias a tantas y tantas personas y compañeros/as que hacen posible la construcción y mejora constante del libro.

Y gracias a vosotros también, por usar (si lo hacéis) esta peculiar guía de R enfocada al análisis de datos económicos mediante herramientas estadísticas.

Y ahora, vamos a comenzar este viaje galáctico hasta adentrarnos en el corazón del propio lenguaje R.

```{r, echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("figuras/ChatGPT Image 11 sept 2025, 19_52_43.png")
```

<!--chapter:end:00-prefacio.Rmd-->

# Introducción.

![[La distribución Normal...]{.smallcaps}](figuras/01%20Intro.jpg){width="100%"}

## ![](figuras/book.svg){.hicon} Llámalo Estadística.

Todo el mundo habla de las estadísticas. Constantemente se les hace referencia en los medios de comunicación: todo está medido y estructurado por estos entes que convierten la realidad en una amalgama de números. Y más aún en el campo del comportamiento humano, es decir, en lo que conocemos como *Ciencias Sociales*. A diario nos llegan las estadísticas sobre la intención de voto cuando hay unas elecciones, del crecimiento de la economía en términos del *PIB*, del comportamiento de los precios medido mediante el concepto de *inflación*...

El secreto de la relevancia que les damos a las estadísticas subyace a que, de partida, suponen una forma sintética y objetiva de representar la realidad que nos rodea, de manera que podamos abarcar el conocimiento de tal realidad de un modo más o menos plausible. Y esta representación de la realidad es a priori *objetiva* porque las estadísticas se elaboran siguiendo unas metodologías que se apoyan en un lenguaje universal: las matemáticas.

Sí. El lenguaje matemático es un lenguaje que pueden entender todas las personas, tengan la procedencia que tengan, y sean de la condición que sean. Si necesitas comunicarte con *casi* cualquier persona del mundo, habla en inglés. Si necesitas comunicarte con cualquier persona del mundo, hazlo mediante las matemáticas, aunque sean matemáticas más o menos elementales.

Por ello, las estadísticas se expresan en lenguaje matemático.

Pero lo que comúnmente entendemos como *estadísticas* no son más que unos resultados, unos outputs de la Estadística. La Estadística en realidad es algo mucho más complejo. Es una ciencia. Las estadísticas son construidas usando el método estadístico.

Pero la Estadística se utiliza para muchas más cosas que para crear y publicar estadísticas.

### ![](figuras/key.svg){.hicon} Concepto de Estadística.

El término "Estadística" proviene de la palabra latina *status*, "el Estado", y fue acuñado por Achenwall a mediados del siglo XVIII con el significado de "recogida, procesamiento y utilización de datos por parte del Estado".

Sin embargo, tal y como se entiende hoy en día, es decir, en el sentido de ***Ciencia Estadística***, surgió como resultado de la integración de dos disciplinas: la *Aritmética Política*, en ese sentido de la cuantificación del Estado; y del *Cálculo de Probabilidades*, que nace en el siglo XVII como Teoría Matemática de los juegos de azar y que podríamos asociar al sentido de *Estadística Matemática*.

Ciñéndonos pues a este último sentido, a lo largo de la Historia se han dado múltiples definiciones de Estadística. Fisher propone una definición quizá demasiado generalista al decir que la Ciencia Estadística es esencialmente una rama de las matemáticas aplicada a los datos observados. Una reflexión que puede ayudar a delimitar la definición de la Ciencia Estadística es la que realiza [@Peña1983] cuando realiza la siguiente reflexión:

> *"La Estadística como disciplina científica ocupa un lugar muy singular en el conjunto de las ciencias. La Física, la Medicina o la Sociología tienen un área sustantiva de conocimiento y cuando utilizan modelos matemáticos, los subordinan al objeto principal de hacer avanzar el conocimiento en su parcela de estudio de la realidad. El objetivo de la Matemática, en contraposición, es ampliar la concepción y generalidad de sus propias herramientas analíticas, con absoluta independencia de la posible relación entre los entes matemáticos abstractos y los fenómenos reales. La Estadística participa de esos dos objetivos, aunque con rasgos muy peculiares. Su campo de estudio son los fenómenos aleatorios que están presentes, en mayor o menor medida, en toda actividad humana de adquisición de conocimiento empírico."*

En este mismo sentido, [@MartinPliego2004] apunta:

> *"La Estadística, por tanto, se configura como la tecnología del método científico que proporciona instrumentos para la toma de decisiones cuando éstas se adoptan en ambiente de incertidumbre, siempre que esa incertidumbre pueda ser medida en términos de probabilidad. Por ello, la Estadística se preocupa de los métodos de recogida y descripción de datos, así como de generar técnicas para el análisis de esta información."*

En definitiva, la Estadística reúne tanto la concepción derivada de la *Aritmética Política*, entendida como recopilación sistemática de datos cara a la descripción de la realidad ("hacer" *estadísticas*); como la concepción *probabilística*, entendida como la modelización de dicha realidad cuando está inscrita en un ambiente de incertidumbre, con el objeto de acotar dicha incertidumbre y servir de ayuda en la toma decisiones (*representar matemáticamente el comportamiento de fenómenos sujetos a incertidumbre*, cuando contamos con **datos** que caracterizan a esos fenómenos).

### ![](figuras/key.svg){.hicon} El método estadístico.

En cuanto al método seguido por la Ciencia Estadística, prima el **razonamiento *inductivo****:* las hipótesis que se plantean en la investigación implican propiedades observables en un conjunto de casos, cuyo análisis lleva a formular hipótesis más generales, aplicables ya a un conjunto mayor de casos. El método estadístico consiste, en definitiva, en sistematizar y organizar este procedimiento de aprendizaje que parte de lo particular para llegar a lo general.

En la aplicación del método estadístico podemos diferenciar una serie de **etapas básicas** que se exponen a continuación:

*a)* **Planteamiento del problema.** Consiste en definir el objeto de la investigación, (¿qué quiero obtener? ¿a dónde quiero llegar?), para lo cual debemos precisar la **población** de referencia y determinar las características que debemos observar y cómo serán recogidas. El resultado de esta fase es un sistema de **características** de interés observadas en un subconjunto de la población representativo de esta, al que llamamos **muestra**. Estas características se llamarán variables si están en escala métrica; o atributos, variables cualitativas o factores si están en escala no-métrica (nominal u ordinal). Las variables toman valores para cada elemento o caso de la muestra. Los atributos adoptan una categoría o nivel para cada uno de los casos que integran la muestra. Según los objetivos planteados en la investigación, el tamaño de la muestra, tipo de características, etc., se podrá hacer una primera selección de los posibles tipos de técnicas y modelos estadíticos a aplicar.

*b)* **Recogida y preparación de la información muestral**. Los datos, que son los valores (en caso de trabajar con variables) o categorías o niveles (en el caso de trabajar con atributos o factores) que adoptan los distintos casos que constituyen la muestra en relación con las características de interés de la población; han de ser obtenidos de las fuentes disponibles. Estas fuentes pueden ser primarias, cuando somos los propios investigadores los que generamos los datos (a través de la observación o la realización de encuestas), o secundarias, cuando estos datos ya han sido generados y/o recopilados por otros investigadores o instituciones. En cualquier caso, la muestra debe ser lo suficientemente amplia como para extraer conclusiones válidas para toda la población, y los datos deben ser de calidad, pues son la materia prima con la que trabajamos. Para ello, un requisito importante es que las fuentes de datos sean fiables.

*c)* **Depuración de los datos**. Antes de utilizar los datos muestrales conviene aplicar un análisis descriptivo que permitirá detectar posibles inconsistencias en los datos identificando los valores anómalos, posibles errores, etc. En esta fase es clave tanto identificar las carencias de datos existentes (datos faltantes o *missing data*), como identificar aquellos elementos de la muestra que no representan bien a la población, puesto que presentan comportamientos extraños en alguna o algunas de las variables o atributos en estudio (casos atípicos u *outliers*).

*d)* **Aplicación de técnicas o modelos estadísticos** para obtener resultados generalizables al conjunto de la población. Una vez se tienen claros los objetivos de la investigación y las características de la información muestral de la que se dispone (datos), y se han depurado convenientemente los datos, será el momento de plantear qué técnica o modelo estadístico aplicar. Aquí podemos distinguir, a su vez, distintas subetapas.

-   Por un lado, la aplicación correcta de ciertas técnicas o modelos de naturaleza inferencial, requiere del **cumplimiento por parte de los datos de ciertos patrones de comportamiento** (por ejemplo, el cumplimiento por parte de las variables de un comportamiento acorde con una Ley Normal). Así, deberán aplicarse una serie de pruebas para comprobar hasta qué punto los datos de partida cumplen con estos patrones.

-   Tras superar el punto anterior, podrá aplicarse la técnica o modelo a los datos para obtener los resultados que contribuyan a cubrir los objetivos de la investigación (usualmente, esta etapa se corresponde con la de **estimación** del modelo estadístico aplicado).

-   Por último, los resultados deben ser sometidos a una subetapa de **validación y contraste**, en la que se valora hasta qué punto los resultados representan el comportamiento real de los casos estudiados (estudio de la bondad del modelo), y el grado de aptitud técnica del modelo, en el sentido de si el modelo estimado cumple con los requisitos que garantizan la calidad de los resultados (por ejemplo, si se cumplen ciertas hipótesis básicas que garanticen que los coeficientes estimados del modelo gozan de las mejores propiedades estadísticas, como insesgadez, eficiencia y consistencia).

-   En esta etapa, además, se intentará simplificar el modelo, es decir, conseguir un modelo tan sencillo como sea posible, sin más parámetros de los necesariosy, que represente la realidad sin mucha pérdida de calidad con respecto a otro modelo más complejo, o sea, ciñéndose al ***principio de parsimonia*** de la modelización.

*e)* **Crítica y diagnosis del modelo**. Si una vez culminada la fase anterior se considera que el modelo es válido y técnicamente correcto, podrá ser adoptado para ayudar a la toma de decisiones, mediante análisis estructural, realización de previsiones o planteamiento de simulaciones. En caso contrario, si el modelo no se considera válido y/o correcto, deberemos reformular dicho modelo repitiendo las etapas anteriores hasta obtener un modelo que represente la realidad en estudio más adecuado.

En definitiva, el método estadístico sigue el método científico en cuanto a que tiene unas etapas bien delimitadas en las que se trata el **conocimiento *a priori*** (teoría) para obtener un **conocimiento *a posteriori*,** lo que pasa a engrosar el cuerpo de la Ciencia.

Es relevante destacar cómo, a su vez, el método científico, al ser aplicado al resto de ciencias, y a la propia Ciencia Estadística, recurre al método estadístico en su ejecución. Así, por ejemplo, en la etapa de recogida de evidencias observables (datos), a fin de verificar las consecuencias o hipótesis que se desprenden de una teoría previa, la Estadística interviene tanto a partir de la *Teoría de Muestras* como del *Diseño de Experimentos* para garantizar la validez y coherencia de los datos. En una fase posterior del método científico, se pasaría a verificar la nueva teoría que se desprende de las hipótesis articuladas a partir de la teoría preexistente. Nuevamente aquí interviene la Estadística como herramienta auxiliar, mediante la *modelización inferencial*. Además, en todo el proceso, que abarca tanto la observación de la realidad como a la generalización de los resultados como modo de confirmar una nueva teoría, aparece la incertidumbre en mediciones y resultados, por lo que el papel de la Estadística como procedimiento para la medición de dicha incertidumbre es indispensable.

De lo dicho se desprende una característica que hace de la Estadística una ciencia singular: su carácter de ***ciencia instrumental*** que auxilia al resto de ciencias en el desarrollo de sus cuerpos de conocimiento. De ahí que la Estadística es aplicada en la totalidad de las ciencias, bien sean naturales, jurídicas o sociales, y en todos los campos del saber, desde las áreas más técnicas hasta en las propias humanidades. Es decir, la Estadística es una herramienta fundamental en todo el proceso de adquisición de conocimientos a través de datos empíricos y, desde este punto de vista, podemos referirnos a la afirmación de [@Mood&Graybill1963]:

> *"La Estadística es la tecnología del método científico".*

Esta extensión de la Ciencia Estadística como ciencia auxiliar de otras ciencias, junto con su crecimiento y madurez metodológica, ha permitido el nacimiento de áreas con un cuerpo de conocimiento específico que pueden ser consideradas, a su vez, como entidades con la categoría de ciencia, como pueden ser la Psicometría, la Estadística Económica y la Econometría[[1]](#_ftn1). Así, a continuación, nos centraremos en la Estadística Económica, rama que ha ocupado un papel primordial en el desarrollo de la propia Ciencia Estadística desde el principio de sus orígenes.

[[1]](#_ftnref1) En nuestra opinión, no existe una delimitación clara entre *Estadística Económica* y *Econometría*, siendo la diferencia en todo caso un matiz dependiente de las técnicas y el enfoque empleado al enfrentarse a un determinado estudio. Quizá ambas disciplinas pudieran englobarse en otra disciplina más general que podría ser llamada 'Economía Cuantitativa'. Véase en relación con este respecto [@Hernandez2000].

### ![](figuras/paperclip.svg){.hicon} Economía y Estadística.

La aplicación del método estadístico a la Economía puede entenderse como el proceso de representación de los sistemas económicos, constituidos por los distintos agentes que operan en las economías, y las relaciones que los ligan. La Economía suele especificar dichas relaciones dándoles forma de teorías económicas. No obstante, las teorías económicas con frecuencia son demasiado imprecisas a la hora de plantear modelos económicos verificables. Como Paul Samuelson apunta ([@Samuelson&Nordhaus2006]):

> "*Solo en una muy pequeña parte de las obras de Economía teóricas o aplicadas se ha tratado la derivación de los teoremas significativos operacionalmente. En parte, por lo menos, tal situación se debe a los malos preconceptos metodológicos, según los cuales, las leyes económicas deducidas de los supuestos a priori poseen rigor y validez, independientemente de cualquier conducta humana real... De hecho, las obras de economía rebosan de malas generalizaciones."*

La aplicación de los instrumentos estadísticos, y en concreto del Método Estadístico, permite dotar a la Teoría Económica del grado de concreción necesario para verificar en los sistemas reales el cumplimiento y la validez de dichas teorías. Este proceso de representación de sistemas reales puede llegar a tal grado de especificación que se puedan cuantificar las consecuencias en los cambios provocados en los elementos y relaciones del sistema ([@Intriligatoretal1996], capítulo II). Sin embargo, por muy alto que sea el nivel de especificación del modelo que representa la realidad económica, este deberá llevar implícito cierta carga de abstracción de la realidad a la que representa, para poder ser abarcable. La realidad económica, el sistema económico, supera necesariamente en complejidad a cualquier modelo propuesto por la Teoría Económica, ya que el sistema económico depende, en última instancia, de fenómenos inmersos en cierto grado de incertidumbre; lo que es atribuible, a su vez, a su vinculación con el comportamiento humano. De este hecho se deduce la necesidad de incluir en la modelización de la realidad económica elementos estocásticos, lo que origina una visión no determinista, sino probabilista de la realidad económica.

Como señala [@MartinPliego2004], parte del conjunto de técnicas estadísticas aplicadas a la investigación económica es común a otras ciencias, mientras que otra parte es específica de este tipo de investigación, fruto de una evolución de la aplicación de la disciplina en el tratamiento de los temas económicos. Entre estas metodologías específicas se encuentran el estudio de las series temporales económicas, de la distribución de la renta, la construcción y análisis de números índices, la modelización regional, el análisis input-output e intersectorial, las técnicas demográficas e incluso, en nuestra opinión, la propia Econometría.

## ![](figuras/key.svg){.hicon}![](images/clipboard-2401404524.png) ¿Qué es R y cómo nos ayuda a analizar datos desde el punto de vista estadístico?

En los apartados anteriores hemos partido del concepto de Estadística como ciencia instrumental hasta llegar a la Estadística Económica, como aquel cuerpo de la Ciencia Económica que se sirve de las herramientas que ofrece la Estadística para profundizar en el conocimiento de la realidad económica.

Pero claro, lo interesante de esto es llevarlo a la práctica. Se necesita un *soporte de hardware y software* para poder aplicar las técnicas estadísticas a los datos económicos, con el objetivo de crear conocimiento a partir de dichos datos. Este conocimeinto se traducirá en una reducción de la incertidumbre que inevitablemente viene aparejada a los fenómenos económicos, lo que redundará en una mejor toma de decisiones.

En los últimos tiempos se ha producido una evolución de hardware sin precedentes, lo que ha dado soporte al desarrollo de un potente software dedicado al análisis de datos (todo tipo de datos, no solamente económicos). Este software permite a cualquier investigador aplicar las últimas técnicas de análisis estadístico a cualquier masa de datos, lo que ha supuesto una verdadera revolución. A su vez, esta realidad se ha retroalimentado, de modo que se ha producido un constante avance en el desarrollo de técnicas y tecnologías de análisis de datos cada vez más complejas. Así, podemos hablar de técnicas de aprendizaje automático o *machine learning* (supervisado, no-supervisado o reforzado) o, más recientemente, de modelos de análisis basados en la *inteligencia artificial*.

En este caldo de cultivo, en el que se dispone de grandes masas de datos, de hardware capaz de procesarlas, y de técnicas capaces de extraer información de las mismas, se ha desarrollado un software cada vez más potente que une todos estos elementos para modelizar la realidad. Este software se concreta en aplicaciones y plataformas diversas: SPSS® Stata, SAS®... Y también lenguajes de programación orientados al análisis estadístico y matemático, como pueden ser Python, Matlab, Julia o... R.

Sí. R no es solo una aplicación al uso. Es todo un lenguaje de programación, orientado principalmente a la analítica de datos, sobre todo desde una perspectiva estadística. R es un proyecto de GNU, por lo que los usuarios son libres de modificarlo y extenderlo. R se distribuye como software libre bajo la licencia GNU y es multiplataforma, lo que ha facilitado su difusión y la existencia de una comunidad muy activa de ususarios y desarrolladores.

## ![](images/clipboard-1383707725.png)![](figuras/pie-chart.svg){.hicon} Instalación de R y R-Studio.

Como ya se ha mencionado, R es un software o lenguaje de uso y difusión gratuitos, bajo licencia GNU. El modo de instalar R es sencillo: basta con ir a la web [*CRAN*](https://cran.r-project.org/) (*Comprehensive R Archive Network*) y descargar la última versión disponible en el sistema operativo del que se sea usuario (en este manual, Microsoft® Windows®). Se ejecutará el archivo descargado, y se completará la instalación.

Una limitación de R es la interfaz o IDE (entorno de desarrollo integrado) que incorpora. Es decir, el "software" con el que se interactúa con el lenguaje R. Esta IDE es muy poco amigable. Para superar esta limitación, existen IDEs alternativas, entre las que destaca RStudio, desarrollada por Posit® Software. Esta IDE es gratuita. De nuevo, simplemente tendremos que ir a la web de[RStudio](https://posit.co/download/rstudio-desktop/) y descargar e instalar la versión gratuita.

## ![](images/clipboard-1383707725.png)![](figuras/pie-chart.svg){.hicon} R y RStudio. Comienzo: Proyectos.

Tras instalar R y su IDE RStudio, podremos comenzar a trabajar. Para ello, abriremos RStudio pulsando en el icono correspondiente. Aparecerá la siguiente ventana:

![[IDE de RStudio.]{.smallcaps}](figuras/Imagen1_01.png){.d-block .mx-auto width="100%"}

La parte izquierda de la ventana es la **consola**. La consola es la sección de RStudio donde podemos manejar R mediante la introducción de código. Por ejemplo, podemos escribir `2+2` después del cursor (signo "\>"), y pulsar Enter. La propia consola nos devolverá el valor 4:

```{r,eval=TRUE, echo=TRUE}
2+2
```

De todos modos, la forma más eficiente de trabajar es mediante "proyectos" y "scripts".

Un **proyecto** básicamente viene asociado a la carpeta donde R trabajará, buscando los datos que sean sus "inputs", y, en su caso, enviando sus resultados u "outputs". Dicho de otro modo, es una carpeta más de nuestro sistema de carpetas o directorios; pero a la que dotamos de una característica especial: ser un proyecto de R. Si abrimos desde RStudio el proyecto, estaremos diciendo a R que, por defecto, preferentemente busque todos los archivos e inputs (datos, etc.) que necesite en esa carpeta de proyecto; y que, en su caso, guarde en tal carpeta los outputs que genere.

Para crear un nuevo proyecto, seguiremos la instrucción `File → New Project`, luego se nos preguntará si se crea el proyecto en una nueva carpeta o en una ya existente. Vamos a crearlo, por ejemplo, en el disco extraíble D, carpeta R, subcarpeta "explora", que ya está creada. Nos saldrá una ventana para buscar la carpeta y, cuando la encontremos, pulsaremos `Open` y `Create Project`. Ya tendremos creado nuestro proyecto. Si nos vamos al explorador de Windows®, y buscamos la carpeta "explora", encontraremos que en tal carpeta aparece un archivo de nombre "explora", con un icono de un cubo con una "R". Ese archivo lo que está haciendo es actuar como un "faro" que le dice a R que, cuando trabajemos en el proyecto "explora", todos los archivos de datos necesarios estarán en esa carpeta (también llamada "explora", porque el proyecto adopta el nombre de la carpeta donde lo localizamos). Y que, si nuestro trabajo aporta algún fichero de "output", también se depositará en esa carpeta del proyecto.

En futuras sesiones, si queremos trabajar en el mismo proyecto, en lugar de seguir la ruta `File → New Project`, tendremos que hacer `File → Open Project`.

## ![](images/clipboard-1383707725.png)![](figuras/pie-chart.svg){.hicon} Scripts.

En cuanto a los **scripts**, son programas o rutinas donde varias instrucciones se ejecutan secuencialmente. Para crear un script, se seguirá la ruta Fi`le → New File → R Script`. Y si el script lo guardamos, ¿dónde lo hará? Pues en la carpeta "explora", que es la del proyecto en el que estamos trabajando.

Informáticamente, un script es simplemente un archivo de texto plano. Se puede modificar con cualquier editor de texto. Afortunadamente, para no estar entrando y saliendo de R-Studio, esta interfaz incorpora un **editor** de scripts, lo cual es muy cómodo.

Vemos cómo ahora, a la izquierda de RStudio, ha aparecido, en la parte superior, una nueva ventana, pasando la consola a ocupar la parte inferior. Es la ventana del "editor":

![[El editor de Scripts de RStudio.]{.smallcaps}](figuras/Imagen1_02.png){.d-block .mx-auto width="100%"}

Igual que con los proyectos, podemos crear desde RStudio un script nuevo, o abrir uno preexistente; y modificarlo, ejecutarlo, o volverlo a guardar.

Vamos a comenzar a escribir nuestro script. Si queremos hacer un comentario que no ejecute ninguna instrucción, éste irá precedido del símbolo almohadilla o *hashtag* "\#". Luego, vamos a ordenar a R que haga la operación de suma: 2+2. Escribimos, por tanto, en el editor:

```{r, eval=FALSE, echo=TRUE}
#Ejemplo de Script
2+2  #este script hace una simple suma.
```

Si pulsamos `Control + Mayúsculas + ENTER` o al desplegable de `Source → Source with Echo`, se ejecutará el script (para ejecutar solo la línea donde está el cursor, pulsaremos `Control + ENTER` o el botón de `Run`; y para ejecutar varias líneas, hemos de sombrearlas y pulsar `Control + ENTER` o el botón de `Run`). En la consola aparecerá:

```{r, eval=TRUE, echo=FALSE}
#Ejemplo de Script
2+2  #este script hace una simple suma.
```

Podemos guardar el script con `File → Save As…` ¿Dónde se guardará por defecto? Pues en la carpeta "explora", que es la de nuestro proyecto. Una vez nuestro script ya tiene nombre, podemos ir guardándolo de vez en cuando pulsando simplemente en el botón del "disquete" del editor. Vamos a llamarlo, por ejemplo, "explorando". Si vamos, en el explorador de Windows®, a nuestra carpeta de proyecto, veremos que hay un archivo de texto llamado "explorando" con extensión ".R" (explorando.R). Este script lo podremos ejecutar cuantas veces queramos sin tener que escribir nada, o reescribirlo si vemos que no funciona o que necesitamos hacer modificaciones. Esa es la ventaja de trabajar con scripts.

Para recuperar un script en una nueva sesión de trabajo simplemente tenemos que seguir las instrucciones `File → Open File…` y seleccionarlo.

## ![](images/clipboard-1383707725.png)![](figuras/pie-chart.svg){.hicon} Funciones.

R trabaja con datos y funciones, principalmente. Pero, ¿qué es una **función**?

Una función es un conjunto o **sistema de instrucciones** que convierten unos datos de entrada o **inputs** en otros datos de salida, resultados, u **outputs**. Una función puede ser muy sencilla o ser verdaderamente compleja. Por otro lado, no todas las funciones están integradas en "paquetes"; sino que el usuario puede crear sus propias funciones (por ejemplo, escribiéndolas en un script) y ejecutarlas.

Las partes básicas de una función son:

-   **Entradas, inputs o argumentos:** son las diversas informaciones necesarias para realizar el procedimiento de la función. Los argumentos pueden ser introducidos por el usuario, o pueden venir dados por defecto, lo que quiere decir que, si el usuario no dota de valor a un argumento, este tomará automáticamente un valor prestablecido.

-   **Cuerpo:** está formado por un conjunto de instrucciones que transforman los *inputs* o entradas en los *outputs* o salidas. Si el cuerpo de la función está formado por varias instrucciones, éstas deben escribirse entre llaves `{ }`.

-   **Salidas:** son los resultados u *output* de la función. Si una función ofrece como salida varios tipos de *objetos,* estos objetos suelen ser almacenados en una estructura de almacenaje de *lista*.

Como ejemplo, vamos a integrar en nuestro script una función, llamada "suma". Esta función requerirá de dos entradas o argumentos (dos números cualesquiera), y ofrecerá, como resultado, salida u output; la suma de tales entradas. El código es:

```{r, eval=TRUE, echo=TRUE}
suma <- function(x, y) {
  resultado <- x + y
  return(resultado)
}
```

Ahora, una vez ejecutado el código anterior; si queremos sumar, por ejemplo, los números 12 y 16, solo tendremos que teclear en la consola, o escribir en el script y hacer run, a la línea:

```{r, eval=TRUE, echo=TRUE}
suma(x=12, y=16)
```

## ![](images/clipboard-1383707725.png)![](figuras/pie-chart.svg){.hicon} Paquetes (packages).

R es un lenguaje de programación en torno al cual se ha desarrollado una cantidad casi inimaginable de recursos: funciones, bases de datos, utilidades... Tal es la cantidad de recursos, que no sería operativo abrir R (directamente, o a través de una IDE, como RStudio) y tener inmediatamente todos esos recursos activos y preparados para ser utilizados. Además, R debería ser actualizado de un modo casi constante.

Por todo ello, todos los recursos disponibles están organizados mediante "paquetes" ("packages" en inglés). Un **paquete** es una colección de funciones y/o un conjunto de datos desarrollados por la comunidad de R. Estos incrementan el potencial de R ampliando sus capacidades básicas, o añadiendo otras nuevas.

De hecho, cuando abrimos R, algunos de estos paquetes, que se han instalado junto al propio lenguaje, se activan. Pero solo algunos. Un ejemplo es el paquete `{base}` o el paquete `{stats}` [@R-base].

La mayor parte de los paquetes disponibles no forman parte, por "defecto", en la misma instalación de R. Se encuentran en diversos servidores llamados repositorios. El más importante, es [CRAN](https://cran.r-project.org/), que es el "repositorio oficial" y que alberga más de 10.000 paquetes. Pero existen otros repositorios, a destacar, por ejemplo, GitHub.

Para **instalar** un paquete en nuestra máquina que esté albergado en CRAN, un modo sencillo es, dentro de R-Studio, pulsar en la ventana inferior / izquierda sobre la pestaña "Packages", y sobre el botón "Install". Emergerá entonces una ventana donde hay un campo para escribir el nombre del paquete (al comenzar a escribirlo, el propio R-Studio te sugerirá los paquetes disponibles). Esto equivale a usar (bien directamente en la consola, o bien como línea de código insertada en un script) la instrucción `install.packages()`, con el nombre del paquete entre comillas (si son varios, pues irán separados por comas.

Una vez se tiene instalado el paquete, ya no habrá que volver a instalarlo para utilizarlo; sino **activarlo**. De hecho, todos los paquetes que no se encuentran por defecto en la propia instalación de R, deben ser activados para poder usar sus funcionalidades y/o datos. Para hacerlo, se debe utilizar la instrucción `library()`, y el nombre del paquete dentro del paréntesis.

Del nombre de esta instrucción surge la confusión común de tomar como sinónimos las palabras "paquete" y "librería" en el entorno de R. Si nos referimos a estas colecciones de funcionalidades y/o datos; lo correcto es "paquete", ya que "librería" tiene más que ver con la organización informática de un software.

## ![](images/clipboard-1383707725.png)![](figuras/pie-chart.svg){.hicon} Help! (sistema de ayuda).

A veces podemos albergar dudas sobre la correcta utilización de las funcionalidades y herramientas que nos proporciona un paquete. Hay varias fuentes de ayuda para intentar encontrar respuesta a las cuestiones que se nos plantean.

Una opción, para obtener información general sobre un paquete, es utilizar la función `help()`, con el argumento "package". Por ejemplo:

```{r, eval=TRUE, echo=TRUE}
help(package="base")
```

Observaremos como en la ventana inferior / izquierda de R-Studio nos saldrá la información correspondiente. De hecho, en tal ventana existe una pestaña **"Help"** para obtener la ayuda sin teclear código.

Además, cada función puede ser consultada individualmente mediante `help("nombre de la función")` o `help(function, package = "package")` si el paquete no ha sido cargado. Estas instrucciones nos mostrarán la descripción de la función y sus argumentos acompañados de ejemplos de utilización. Por ejemplo:

```{r, eval=FALSE, echo=TRUE}
help("rm", package="base")
```

La instrucción anterior nos aporta la documentación sobre la función `rm()` del paquete `{base}` de R (nota: este paquete se activa por defecto al abrir R o R-Studio; por lo que el segundo argumento, con el nombre del paquete que contiene la instrucción no es necesario).

Otra opción para mostrar información de ayuda es la exploración de las "viñetas" (vignettes). Las **viñetas** son documentos que muestran de un modo más detallado las funcionalidades de un paquete. La información de las viñetas de un paquete están disponibles en el archivo "documentation". Puede obtenerse una lista de las viñetas de nuestros paquetes instalados con la función `browseVignettes()`. Si solo queremos consultar las viñetas de un paquete concreto pasaremos como argumento a la función el nombre del mismo: `browseVignettes(package = "packagename")`. En ambos casos, una ventana del navegador se abrirá para que podamos fácilmente explorar el documento.

Si optamos por permanecer en la consola, la instrucción `vignette()` nos mostrará una lista de viñetas, `vignette(package = "packagename")` las viñetas incluidas en el paquete, y una vez identificada la viñeta de interés podremos consultarla mediante `vignette("vignettename")`.

## ![](figuras/star.svg){.hicon} Origen y Evolución del Transporte Interestelar de Mercancías: Una Historia del Sector (2210).

El transporte interestelar de mercancías nació oficialmente a mediados del siglo XXI, en un contexto marcado por la exploración espacial comercial y la necesidad de recursos más allá de la Tierra. El primer hito significativo fue la explotación minera de asteroides cercanos, liderada por corporaciones como *SpaceX* y *Blue Origin*, que sentaron las bases de un incipiente comercio más allá del Sistema Solar. Sin embargo, fue en 2084 cuando el sector dio un salto cuántico: el descubrimiento del *plasma warp* permitió velocidades cercanas a la de la luz, abriendo rutas hacia sistemas estelares vecinos y revolucionando la logística galáctica.

### ![](figuras/paperclip.svg){.hicon} La Expansión del Sector.

A principios del siglo XXII, la colonización de planetas como *Pandora* (Vía Láctea) y *Tatooine* (Andrómeda) impulsó la creación de bases interestelares. Este desarrollo no solo conectó galaxias distantes, sino que también fomentó el surgimiento de empresas especializadas en transporte de mercancías, como **Arrakis Freight** y **Vader & Company.** Estas compañías diseñaron naves capaces de transportar cargamentos masivos, como minerales raros de *Arrakis* o componentes biotecnológicos de *Fhloston Paradise*, a distancias inimaginables.

En paralelo, el **Tratado de Libre Comercio Interestelar** (TLCI) de 2135 formalizó las reglas para el intercambio entre galaxias. Este acuerdo facilitó el crecimiento exponencial del sector al garantizar rutas seguras entre la Vía Láctea, Andrómeda y más allá, permitiendo a empresas como **Nova Haulers** y **ExoCargo Alliance** expandir sus operaciones.

### ![](figuras/paperclip.svg){.hicon} Cifras globales del sector en 2210.

| Magnitud / Variable             | Valor                    |
|---------------------------------|--------------------------|
| Empresas                        | 300                      |
| Activo Total                    | 72 421.97 miles de PAVOs |
| Ingresos totales                | 65 515.85 miles de PAVOs |
| Número Total de Rutas           | 109 674                  |
| Rentabilidad Media del Sector   | 60.16 %                  |
| Solvencia Media del Sector      | 185.72 %                 |
| Distancia Media de las Rutas    | 133.69 años luz          |
| Número de Galaxias donde Operan | 5                        |

: Cifras Globales del Sector.

Estos datos muestran un sector altamente rentable y con una fuerte estabilidad financiera, con márgenes elevados y una solvencia promedio sólida. La distancia media de las rutas operadas confirma la expansión del comercio interestelar a través de vastas regiones del universo conocido.

### ![](figuras/paperclip.svg){.hicon} **Empresas Líderes en el Transporte Interestelar de Mercancías en 2210.**

En el competitivo sector del transporte interestelar de mercancías, destacan varias empresas que, por su solidez financiera, innovación tecnológica y alcance operativo, dominan el mercado. A continuación, analizamos algunas de las más relevantes basándonos en los datos más recientes de la industria.

**1. Arrakis Freight (Gran Nube de Magallanes).**

Arrakis Freight es una de las empresas más rentables y con mayor solidez del sector. Su alta solvencia le permite operar con una gran estabilidad financiera, minimizando riesgos. Además, su margen del 51.42% la convierte en una compañía altamente eficiente en costos operativos. Gracias a su origen en *Arrakis*, una fuente clave de especias y minerales raros, ha consolidado su posición como un actor clave en la Gran Nube de Magallanes.

![[Romulus Steiner, CEO de Arrakis Freight.]{.smallcaps}](figuras/Romulus%20Steiner%20G.jpg){.d-block .mx-auto width="500"}

**2. Hyperdrive Express (Gran Nube de Magallanes).**

Hyperdrive Express se distingue por su altísimo margen de rentabilidad, reflejando su eficiencia operativa y control de costos. Con sede en *Miranda*, su especialización en el transporte de bienes tecnológicos la ha convertido en una empresa de referencia dentro de la Gran Nube de Magallanes. Su fuerte inversión en innovación y digitalización la mantiene en la vanguardia del sector.

**3. Excalibur Freight Systems (Gran Nube de Magallanes).**

Excalibur Freight Systems es una de las empresas más rentables de toda la industria. Con un margen del 77.25%, logra obtener grandes beneficios por cada unidad transportada. Su índice de diversificación es alto, lo que le permite operar en múltiples sectores, desde el comercio de biotecnología hasta el transporte de bienes de lujo. Su alto beneficio medio por año luz es una muestra de su extrema eficiencia operativa.

**4. Kaiju Haulage Co. (Galaxia de Andrómeda).**

Desde su base en *Mustafar*, Kaiju Haulage Co. se ha consolidado como una de las mayores transportistas en la Galaxia de Andrómeda. Su enfoque en rutas de larga distancia y comercio de materiales pesados la hace indispensable en las cadenas de suministro intergalácticas. Su solvencia extremadamente alta la posiciona como una de las empresas más estables financieramente en el sector.

### ![](figuras/paperclip.svg){.hicon} **Análisis DAFO del sector.**

**Fortalezas del Sector.**

-   **Tecnología avanzada:** El uso de reactores *warp* y escudos de energía ha reducido significativamente los tiempos y riesgos de transporte.

-   **Diversidad de mercados:** La expansión a múltiples galaxias ha diversificado los bienes transportados, desde minerales raros hasta biotecnología avanzada.

-   **Colaboración intergaláctica:** Acuerdos como el TLCI aseguran rutas seguras y relaciones comerciales estables.

**Debilidades del Sector.**

-   **Dependencia tecnológica:** Las interrupciones en la producción de *plasma warp* podrían paralizar el comercio interestelar.

-   **Costos operativos elevados:** El mantenimiento de naves y estaciones en galaxias remotas sigue siendo prohibitivo para nuevas empresas.

**Oportunidades.**

-   **Nuevas rutas comerciales:** La exploración de la *Galaxia de Cetus* y la *Nube Circungaláctica* abre oportunidades para expandir las operaciones.

-   **Innovación en biotecnología:** Los avances en bioingeniería permiten transportar organismos vivos de forma segura, ampliando mercados.

**Riesgos.**

-   **Piratería espacial:** Sistemas como *Klendathu* y *Miller* son zonas rojas donde el contrabando y el saqueo afectan el comercio legítimo.

-   **Inestabilidad política:** Conflictos en planetas estratégicos como *Mustafar* y *Nueva Caprica* ponen en riesgo las rutas clave.

-   **Cambio climático galáctico:** Fenómenos como tormentas de radiación en el *Cinturón de Magallanes* afectan la navegación segura.

**Grandes Retos Pendientes.**

1.  **Estabilidad energética:** Encontrar fuentes de energía alternativas al *plasma warp* para reducir costos y mitigar riesgos.

2.  **Regulación interestelar:** Crear una autoridad galáctica que estandarice leyes de comercio, impuestos y responsabilidad.

3.  **Sostenibilidad ambiental:** Minimizar el impacto de la actividad industrial en planetas habitados y proteger ecosistemas únicos como los de *Naboo* y *Fhloston Paradise.*

El sector del transporte interestelar de mercancías, a pesar de sus desafíos, sigue siendo el motor económico de la civilización galáctica en 2210. Su capacidad para conectar mundos y galaxias garantiza su relevancia en los siglos venideros, siempre que pueda adaptarse a un universo en constante cambio.

<!--chapter:end:01-intro.Rmd-->

# Almacenando y manipulando datos.

![[¡Datos, datos!]{.smallcaps}](figuras/02%20robots.jpg){width="100%"}

## ![](figuras/book.svg){.hicon} Objetos. Datos.

Como vimos en el capítulo 1, tras ejecutar un sencillo script (o al escribir instrucciones directamente desde la consola), R es interactivo: responde a las entradas que recibe. Las entradas o **expresiones** pueden ser, básicamente:

-   Expresiones aritméticas.

-   Expresiones lógicas.

-   Llamadas a funciones.

-   Asignaciones.

Las expresiones realizan acciones sobre **objetos** de R. Los objetos en R son entes que tienen ciertas características, *metadatos*, llamados atributos. No todos los objetos tienen los mismos atributos y, ni tan siquiera, todos los objetos tienen atributos que los caractericen.

Los objetos más importantes en R son ciertas estructuras o *contenedores* diseñados para almacenar elementos:

-   Vectores.

-   Matrices.

-   Listas.

-   Data frames.

-   Factores.

Los elementos almacenados en los objetos se dividen en *clases*. Entre las diferentes clases, destacan las clases referidas a **datos**, que pueden ser de diferentes *modos*: logical (verdadero/falso), numeric (números) o character (cadena de texto). El modo numeric puede ser, a la vez, de tipo integer (número entero) o double (número real). En el caso de logical y carácter, modo y tipo coinciden.

Vamos a profundizar un poco en algunas de estos contenedores de datos. Vamos a suponer que trabajamos en el proyecto que creamos en el capítulo anterior (proyecto "explora"), y que vamos a editar el script que también creamos en tal capítulo (script "explorando.R", que se encontrará ubicado en la carpeta del proyecto "explora").

### ![](figuras/key.svg){.hicon} Vectores.

Los **vectores**, son conjuntos de elementos **de la misma clase**. Vamos a definir por ejemplo el vector x = (1,3,5,8). Para ello, vamos a escribir en nuestro script:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
x <- c(1,3,5,8)
```

Ejecutamos la línea (situando el cursor en algún lugar de ella, dentro del script; y pulsando a la vez las teclas `Control + Enter` o pinchando con el ratón en el botón `Run` del editor). Ya tenemos nuestro primer objeto de tipo *vector* en memoria. Por cierto, lo que hemos hecho es una **asignación**, que se escribe con una flecha creada mediante los signos "\<" y "-". Hemos asignado a un vector llamado "x" los elementos 1, 3, 5 y 8.

Para ver el vector simplemente escribimos en la consola (o en el script) el nombre del vector, "x". El resultado será:

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
x
```

Además, si miramos en la ventana superior-derecha de R-Studio, veremos que en el ***Global Environment*** se muestra nuestro vector y que, además, se nos informa de que tiene *modo* *numérico*. El *Global Environment* nos informa de los objetos que R tiene en memoria:

![Nuestro vector en memoria.](figuras/Imagen2_01.png){.d-block .mx-auto width="100%"}

Si queremos obtener un vector de números consecutivos del 2 al 6, basta con ejecutar en la "consola" (o escribir y ejecutar en el script):

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
y <- c(2:6)
```

Al escribir el nombre del vector "y" en la "consola" obtendremos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
y
```

Si queremos saber la longitud de un vector, usaremos la *función* `length()`. Por ejemplo, `length(y)` nos devolverá el valor 5. Escribamos en el script y ejecutemos:

```{r, eval=TRUE, echo=TRUE, message= FALSE, warning=FALSE}
length(y)
```

Un vector puede incluir, además de números, caracteres o grupos de caracteres alfanuméricos; siempre entrecomillados (lo fundamental es que sean elementos de la misma clase). Por ejemplo, el vector "genero" (¡no pongamos tildes o podemos tener problemas!). Así, si ejecutamos estas dos líneas de código:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
genero<-c("Mujer","Hombre")
genero
```

Se habrá creado el vector "genero":

```{r,eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
genero<-c("Mujer","Hombre")
genero
```

Podemos obtener la clase de los elementos almacenados en nuestro vector con la función `class()`:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
class(genero)
```

Si falta un dato en un vector, habrá que escribir "NA" (not available). Por ejemplo, si falta el tercer dato de este vector "z", este vector se escribirá como:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
Z <- c(1,2,NA,2,8)
```

Para **seleccionar un elemento** concreto de un vector, indicaremos entre corchetes la posición en la que se encuentra. Por ejemplo, refiriéndonos al vector "x", para obtener el valor de su tercer elemento, haremos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
x[3]
```

Si queremos que se nos muestren los elementos del vector x del 2º al 4º:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
x[2:4]
```

Por último, si queremos sacar en pantalla los elementos 1º y 4º, tendremos que incluir una "c" seguida de un paréntesis que recoja el orden de los elementos que queremos seleccionar:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
x[c(1,4)]
```

### ![](figuras/key.svg){.hicon} Matrices.

Las **matrices**, internamente en R, son vectores; pero con dos atributos adicionales: número de filas y número de columnas. Se definen mediante la función matrix(). Por ejemplo, para definir la matriz "a":

$$
\begin{pmatrix}
    1 & 4 & 7 \\
    2 & 5 & 8 \\
    3 & 6 & 9
\end{pmatrix}
$$ Tendremos que escribir:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
a <- matrix(c(1,2,3,4,5,6,7,8,9),nrow=3)
a
```

El número de filas de la matriz (y por tanto, el número de columnas) se fija con el argumento `nrow =` . También podríamos fijar el número de columnas, con `ncol =` .

Como vemos, por defecto, R va "cortando" el vector por columnas (si lo preferimos, lo puede hacer también por filas, añadiendo a la función `matrix()` el argumento `by row = true`; pero, en nuestro ejemplo, obtendríamos la matriz traspuesta a la que queremos almacenar).

Las dimensiones (número de filas y de columnas) de la matriz pueden obtenerse mediante la función `dim()`:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
dim(a)
```

3 filas y 3 columnas.

Si queremos seleccionar elementos concretos de una matriz, lo haremos utilizando corchetes para indicar filas y columnas. Hemos de tener en cuenta que, trabajando con matrices, siempre tenemos $$rango de filas, rango de columnas$$ Si se deja en blanco el espacio entre el corchete inicial y la coma, esto querrá decir que consideramos todas las filas. Y si no insertamos nada entre la coma y el corchete de cierre, esto significará que consideramos todas las columnas. A continuación tenemos varios ejemplos de código, con el resultado obtenido en la consola:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
a[2,3]
a[1:2,2:3] 
a[,c(1,3)]
a[c(1,3),]
```

Tanto para vectores como para matrices, funcionan las operaciones suma y diferencia sin más complicaciones. En el caso del producto, sin embargo, hay que tener en cuenta que, por ejemplo, a\*a devuelve la multiplicación elemento a elemento, es decir:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
a*a
```

Devuelve la multiplicación elemento a elemento (en este caso, el cuadrado de cada número, al multiplicar la matriz a por sí misma):

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
a*a
```

Para hacer el verdadero producto matricial deberá introducirse:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
a%*%a
```

### ![](figuras/key.svg){.hicon} Data frames.

Un ***data frame*** es un objeto que almacena datos organizados mediante la clase `data.frame`. Esta organización consiste en que, por filas, se disponen los diferentes casos o sujetos; mientras que por columnas se posicionan las variables. Así:

-   Es similar a una matriz en el sentido de que tiene dos dimensiones. Podemos acceder a sus elementos con corchetes, tenemos nombres de filas y columnas, y podemos operar con ellas.

-   Cada columna tiene un nombre, de manera que podemos acceder a una columna concreta con el símbolo **`$`**. Todas las columnas (variables) son vectores con la misma longitud.

-   Cada columna puede ser un vector numérico, factor, de tipo carácter o lógico.

Por ejemplo, vamos a crear el *data frame* **"datos"**, con tres variables: "peso", "altura", y "color de ojos", llamadas "Peso", "Altura" y "Cl.ojos", respectivamente; para 3 individuos o casos. Una opción es crear primero las tres variables como vectores, y luego crear el *data frame* mediante la función `dataframe()`:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
Peso<-c(68,75,88)
Altura<-c(1.6,1.8,1.9)
Cl.ojos<-c("azules","marrones","marrones")
datos<-data.frame(Peso,Altura,Cl.ojos)
```

Si ahora ejecutamos una línea con el nombre de nuestro *data frame*, lo obtendremos como resultado en la consola:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
datos
```

Para obtener los nombres de las variables (es decir, el nombre de cada columna) teclearemos la función:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
names(datos)
```

Obteniéndose:

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
names(datos)
```

Para obtener solo los datos de la columna (variable) color de ojos teclearemos datos\$Cl.ojos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
datos$Cl.ojos
```

Y para obtener los datos de peso: datos\$Peso:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
datos$Peso
```

Para saber el número de filas y de columnas de una hoja de datos utilizaremos las funciones `nrow()` y `ncol()`:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
nrow(datos)
ncol(datos)
```

Para seleccionar elementos de un data frame, se pueden seguir las mismas reglas que para la selección de elementos de una matriz (con el número de cada fila, que es cada individuo; y el número de cada columna, que es cada variable. Para elegir una variable, no obstante, ya hemos visto que es posible usar su nombre; aunque precedido del nombre del data frame y el signo `$`. Por ejemplo, si ejecutamos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
datos[,2]
datos$Altura
```

Obtenemos el mismo resultado.

### ![](figuras/key.svg){.hicon} Factores.

En R, un factor es un tipo especial de objeto que se usa para representar datos categóricos. Es decir, datos que no son números continuos sino que pertenecen a un conjunto limitado de categorías (también llamadas niveles). Por ejemplo, anteriormente creamos el vector "Cl.ojos". Es un vector tipo carácter; pero, para R, aún no es factor.

Para convertirlo en un objeto de la clase "factor", podemos ejecutar:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
Cl.ojosF <- factor(Cl.ojos)
```

En el Global Environment vemos que se ha creado el objaeto "Cl.ojosF", de tipo factor. Si representamos ambos objetos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
Cl.ojos
Cl.ojosF
```

Vemos como "Cl.ojosF" no es un vector de elementos tipo carácter; sino algo más: es una variable cuyos elementos pueden tomar dos valores categóricos (que pueden estar en el vector una vez, o estar repetidos, o no estar), categorías o niveles, que son "azules" y "marrones".

¿Por qué es necesario, a veces, transformar los datos "categóricos" en "factores"?

-   Ahorro de memoria: internamente, en lugar de guardar “marrones” muchas veces, guarda un número que representa la categoría (ej. 1 = marrones, 2 = azules).

-   Orden y niveles: puedes definir si las categorías del factor están en escala ordinal. Por ejemplo, supongamos que tenemos las siguientes categorías de calificaciones: "Suspenso", "Aprobado", "Notable", "Sobresaliente". Si las guardamos como factor en escala ordinal, R puede saber que Suspenso \< Aprobado \< Notable \< Sobresaliente, lo que te permitirá hacer comparaciones, ordenar, etc.

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
notas <- factor(
  c("Aprobado", "Notable", "Sobresaliente", "Suspenso"),
  levels = c("Suspenso", "Aprobado", "Notable", "Sobresaliente"),
  ordered = TRUE
)
notas
```

-   Modelos estadísticos: muchos métodos de R tratan automáticamente a los factores como variables cualitativas y hacen análisis adecuados (tablas de contingencia, regresiones categóricas, etc.).

En síntesis, Un factor en R es como una etiqueta inteligente para categorías, que permite trabajar con datos cualitativos de forma más organizada y útil para análisis.

### ![](figuras/key.svg){.hicon} Listas.

Por último, una lista es un tipo de objeto que puede contener cualquier cosa dentro:

-   números

-   textos

-   vectores

-   matrices

-   *data frames*

-   otras listas

Es decir, es como una caja organizadora con muchos compartimentos, y en cada compartimento puedes guardar un objeto distinto, tengan la naturaleza que tengan. Por ejemplo, vamos a crear la lista "mi_lista":

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
mi_lista <- list(
  nombre = "María",
  edad = 21,
  notas = c(8.5, 9.0, 7.2),
  aprobado = TRUE
)

mi_lista
```

Esta lista está almacenada en el Global Environment. Si queremos visualizar solo el tercer elemento podemos hacerlo invocando a su posición (con corchetes dobles), o bien a su nombre:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
mi_lista$notas
mi_lista[[3]]

```

Las listas son las estructuras de almacenamiento más flexibles de R, y son muy utilizadas para organizar y empaquetar resultados complejos.

## ![](figuras/pie-chart.svg){.hicon} Importando datos.

Lo más frecuente es que no tecleemos los datos, como hemos hecho hasta ahora; sino que los importemos a R desde algún contenedor externo (archivo de texto, hoja de cálculo, base de datos...). Nosotros vamos a importar nuestros datos desde Microsoft® Excel®. Vamos a cerrar el script que hemos estado construyendo en los apartados anteriores (para conservarlo hay que guardarlo antes), aunque vamos a seguir trabajando en el mismo proyecto (que habíamos llamado "explora"). Iremos a la carpeta del proyecto y guardaremos en ella los dos archivos de esta práctica (obtén el enlace a los archivos en la sección final del capítulo):

-   Un archivo de Microsoft® Excel® llamado "interestelar_100.xlsx"

-   Un script con las instrucciones que vamos a mostrar a continuación, y que se llama "explora_rstars.R"

Si abrimos el archivo de Microsoft® Excel® "interestelar_100.xlsx", comprobaremos que se compone de tres hojas. La primera muestra un aviso sobre el uso exclusivo que se debe dar a los datos incorporados; la segunda recoge la descripción de las variables consideradas; y la tercera (hoja "Datos") guarda los datos que debemos **importar** desde R-Studio. Estos datos se corresponden con diferentes variables económico-financieras y de diverso índole de una muestra de empresas que se dedican al transporte de mercancías interestelar.

Vamos a abrir nuestro script "explora_rstars.R" con `File → Open File…` (o haciendo click en el archivo correspondiente en la ventana inferior derecha de RStudio, pestaña "Files"). Este script contiene el programa que vamos a ir ejecutando en la práctica.

La primera línea / instrucción en los scripts suele ser:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
## Importando datos de las empresas R-Stars

# Limpiando el Global Environment
rm(list = ls())
```

La instrucción tiene como objeto **limpiar el *Global Environment*** (memoria) de objetos de anteriores sesiones de trabajo.

Luego, pueden **cargarse los paquetes que harán falta para ejecutar el código**, si bien se puden cargar en cualquier parte del script (aunque siempre antes de ejecutar una línea que requiera de algún elemento incluido en uno de estos paquetes).

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Cargando paquetes
library(readxl)
library(gtExtras)
```

Obviamente, para cargar o activar un paquete, previamente debe de haber sido instalado en la máquina donde estamos trabajando. Anteriormente hemos visto como instalar un paquete del repositorio *CRAN*. De nuevo, si no hemos instalado antes los paquetes `{readxl}` y `{gtExtras}`, tendremos que instalarlos, o nos dará error.

Para instalar, por ejemplo,el paquete `{readxl}`, (que contiene el código necesario para importar datos de un archivo de Microsoft® Excel®), iremos a la ventana inferior-derecha y pulsaremos la pestaña `Packages`, pulsaremos en `Install`, y emergerá una ventana donde dejaremos el "repositorio" que viene por defecto. En el campo "Packages", escribiremos el nombre del "paquete" que contiene la librería que nos hace falta (normalmente coincide con el nombre de la propia librería, en nuestro caso `{readxl}`. Una vez descargado el "paquete", podremos ejecutar el código anterior sin problemas. Si hemos de instalar el paquete `{gtExtras}`, procederemos de idéntico modo.

Para importar los datos localizados en el archivo de Microsoft® Excel® "interestelar_100.xlsx" el código que podemos usar es:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# DATOS

interestelar_100 <- read_excel("interestelar_100.xlsx", sheet = "Datos", na = c("n.d."))
```

La función encargada de importar los datos es `read_excel()`, del paquete `{readxl}`.

Una cuestión importante a tener en cuenta es que, si hay valores perdidos que, en la hoja de cálculo vienen indicados en las celdas mediante algún tipo de anotación, como por ejemplo, "n.d." (no disponible); deberá incluirse el argumento `na =` para informar de las anotaciones en las celdas que deben traducirse como valores faltantes. Si las celdas sin dato aparecieran en la hoja de cálculo siempre en blanco, este argumento no haría falta.

Volviendo a nuestro ejemplo, podemos observar cómo en el *Environment* ya aparece un objeto. Este objeto es una estructura de datos tipo *data frame*, se llama "interestelar_100" y contiene 104 filas (una por empresa) y 29 columnas, una por cada una de las variables almacenadas en el archivo de Microsoft® Excel®. De estas variables, cinco son de tipo cualitativo, formadas por cadenas de caracteres.

Puede explorarse el contenido del *data frame* y los principales estadísticos con la función `summary()`:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
summary (interestelar_100)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}

library (dplyr)
df <- select(interestelar_100, everything())
# Número de variables por bloque
variables_por_bloque <- 4

# Dividir las variables en bloques
for (i in seq(1, ncol(interestelar_100), variables_por_bloque)) {
  # Comentario: No mostramos el nombre del bloque
  print(summary(interestelar_100[, i:min(i + variables_por_bloque - 1, ncol(interestelar_100))]))
  cat("\n")
}
rm (i)
rm (variables_por_bloque)
rm (df)
```

Veremos cómo aparecen 29 variables con algunos estadísticos básicos.

R ha considerado a la primera columna como una variable de tipo cualitativo (atributo). En realidad no es una variable, sino el nombre de los individuos o casos. Para evitar que R tome los nombres de los casos como una variable, podemos redefinir nuestro *data frame* diciéndole que considere esa primera columna como los *nombres de los individuos o filas*:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
interestelar_100 <- data.frame(interestelar_100, row.names = 1)
```

En la línea anterior hemos asignado al data frame "interestelar_100" los propios datos de "interestelar_100"; pero indicando que la primera columna de datos no es una variable; sino el nombre de los casos. Si hacemos ahora el `summary()`:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
summary (interestelar_100)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
library (dplyr)
df <- select(interestelar_100, everything())
# Número de variables por bloque
variables_por_bloque <- 4

# Dividir las variables en bloques
for (i in seq(1, ncol(interestelar_100), variables_por_bloque)) {
  # Comentario: No mostramos el nombre del bloque
  print(summary(interestelar_100[, i:min(i + variables_por_bloque - 1, ncol(interestelar_100))]))
  cat("\n")
}
rm (i)
rm (variables_por_bloque)
rm (df)
```

Vemos que ya no aparece "NOMBRE" como variable, y en el *Environment* ya aparece el data frame "interestelar_100" con 100 observaciones (casos), pero con 28 variables (una menos).

Un modo visualmente más elegante de explorar el contenido del data frame es la utilización de la función `gt_plt_summary()` del paquete `{gtExtras}`:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# visualizando el data frame de modo elegante con {gtExtras}
datos_df_graph <- gt_plt_summary(interestelar_100)
```

Este código asigna al nombre "datos_df_graph" (o al que queramos) un gráfico/tabla con las variables que contiene el data frame (en este caso, "datos", añadiendo características y medidas básicas de las diferentes variables (según sea su tipología). Podremos ver este gráfico/tabla evocando a su nombre:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
datos_df_graph
```

Y el resultado será:

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# visualizando el data frame de modo elegante con {gtExtras}
datos_df_graph <- gt_plt_summary(interestelar_100)
datos_df_graph
```

Antes de seguir con la manipulación de nuestros datos, es preciso decir que existen otros muchos formatos de datos que pueden ser importados. Por ejemplo, con el paquete `{readr}` se pueden importar datos de archivos de texto de tipo tabular (por ejemplo, archivos \*.csv). Con el paquete `{haven}` se pueden capturar los datos almacenados en archivos de SPSS® (.sav), Stata® (.dta), SAS® (.sas7bdat), etc. Finamente, se pueden capturar datos almacenados en páginas web (archivos en formato JSON o XML, o en tablas HTML)) o en bases de datos gestionadas mediante diversos sistemas (SQLite, MySQL, MariaDB, PostgreSQL, Oracle®).

## ![](figuras/pie-chart.svg){.hicon} {dplyr}.

### ![](figuras/key.svg){.hicon} El Tidyverse. Cargando {dplyr}.

El ***Tidyverse*** es un conjunto de paquetes / librerías con una filosofía común, como es el uso de ciertas estructuras gramaticales, que facilitan muchas de las tareas y análisis que podrían hacerse con el lenguaje R estándar. Una buena obra para profundizar en el Tidyverse es @Wickham2017R.

Uno de esos paquetes es `{dplyr}`, que proporciona una gramática más sencilla que la del lenguaje R convencional para **manipular** los objetos de estructuras de datos conocidos como ***data frames***.

Los data frames, como ya sabemos, son estructuras en las que se almacenan datos de modo que, por columnas, se disponen las variables del análisis; y por filas los casos que conforman la muestra / población.

Vamos a suponer que trabajamos dentro del proyecto que hemos creado previamente, de nombre "explora" (ver capítulo 1), y que seguimos con el script "explora_rstars.R", co el que importamos los datos del archivo de Microsoft® Excel® "interestelar_100.xlsx", que contení información sobre 104 empresas de transporte de mercancías a escala interestelar.

Suponemos que hemos ejecutado la primera parte del script (importación de datos con corrección de la columna de nombres de las filas, al *data frame* interestelar_100, que está almacenado en el *Global Environment.*

Vamos a continuar desde este punto.

A continuación, cargaremos el paquete `{dplyr}`. Si nunca antes se ha utilizado este paquete, cuando lo intentemos activar con la función `library()` nos dará un error o nos dirá que previamente hay que importarlo. En ese caso, iremos a la ventana inferior-derecha y pulsaremos la pestaña "Packages", pulsaremos en `Install`, y emergerá una ventana donde dejaremos el "repositorio" que viene por defecto y, en el campo `Packages`, escribiremos el nombre del "paquete" (en nuestro caso `{dplyr}`). Una vez descargado el "paquete", podremos ejecutar el código sin problemas:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
## dplyr
# Cargando dplyr
library (dplyr)

```

Para entender mejor la **sintaxis** que siguen las funciones o instrucciones a las que da acceso `{dplyr}`, hay que tener en cuenta lo siguiente:

-   El primer argumento que tiene una función de `{dplyr}` es el *data frame* con el que se va a trabajar.

-   Los otros argumentos describen qué hay que hacer con el *data frame* especificado en el primer argumento. Es posible referirse a las columnas (variables) del *data frame* con su nombre, **sin utilizar el operador \$**.

-   El valor de retorno es un **nuevo** *data frame*.

En los siguientes subapartados practicaremos con algunas de las principales funciones que aporta `{dplyr}`.

### ![](figuras/key.svg){.hicon} Seleccionando columnas de un data frame.

La función clave de `{dplyr}` para seleccionar una o varias columnas (variables) de un *data frame* es la función `select()`.

Así, vamos a imaginar por ejemplo que queremos eliminar de nuestro *data frame* la variable (de tipo "carácter") FJUR (forma jurídica de la empresa). Podremos ejecutar la asignación:

```{r, eval=FALSE, echo=TRUE, message=FALSE}

# Seleccionando variables
interestelar_100 <-select(interestelar_100, -FJUR)
summary (interestelar_100)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}

#Seleccionando variables
interestelar_100 <-select(interestelar_100, -FJUR)

library (dplyr)
df <- select(interestelar_100, everything())
# Número de variables por bloque
variables_por_bloque <- 4

# Dividir las variables en bloques
for (i in seq(1, ncol(interestelar_100), variables_por_bloque)) {
  # Comentario: No mostramos el nombre del bloque
  print(summary(interestelar_100[, i:min(i + variables_por_bloque - 1, ncol(interestelar_100))]))
  cat("\n")
}
rm (i)
rm (variables_por_bloque)
rm (df)
```

Podemos verificar que, en el *Environment,* el *data frame* ha pasado a tener una variable menos (27), ya que hemos eliminado la variable FJUR. Es decir, con el guión "-" se pueden eliminar directamente variables de un *data frame*.

Ahora, suponemos que queremos visualizar las variables del *data frame* "interestelar_100": ACTIVO, FPIOS, LIQUIDEZ. Para ello, ejecutaremos el código:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
select(interestelar_100, ACTIVO, FPIOS, LIQUIDEZ)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
df <-select(interestelar_100, ACTIVO, FPIOS, LIQUIDEZ)
df <- dplyr::slice_head(df, n = 10)  # Solo primeras 10 filas

# Número de columnas por bloque
columnas_por_bloque <- 4

# Dividir las columnas en bloques y mostrar los datos en la consola
for (i in seq(1, ncol(df), columnas_por_bloque)) {
  print(df[, i:min(i + columnas_por_bloque - 1, ncol(df))])
  cat("\n")
}
rm(i)
rm(df)
rm(columnas_por_bloque)
```

(**Nota:** solo mostramos aquí los 10 primeros casos del *data frame*).

Como no hemos asignado el resultado de la función a ningún "nombre", R simplemente muestra el resultado en pantalla; pero no guarda ningún objeto en el *Environment*. Si asignamos un `select()` a un "nombre", se creará un *data frame* con ese nombre, y las variables seleccionadas:

```{r, eval=FALSE, echo=TRUE, message=FALSE}

interestelar_100A <-select(interestelar_100, ACTIVO, FPIOS, LIQUIDEZ)
summary (interestelar_100A)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}

interestelar_100A <-select(interestelar_100, ACTIVO, FPIOS, LIQUIDEZ)

library (dplyr)
df <- select(interestelar_100A, everything())
# Número de variables por bloque
variables_por_bloque <- 3

# Dividir las variables en bloques
for (i in seq(1, ncol(interestelar_100A), variables_por_bloque)) {
  # Comentario: No mostramos el nombre del bloque
  print(summary(interestelar_100A[, i:min(i + variables_por_bloque - 1, ncol(interestelar_100A))]))
  cat("\n")
}
rm (i)
rm (variables_por_bloque)
rm (df)
```

Podemos comprobar cómo en el *Global Environment* hay otro objeto *data frame* llamado "interestelar_100A", con 4 variables (y los mismos 104 casos).

Otra posibilidad que tenemos es hacer una copia de un *data frame* rápidamente con el argumento `everything()`. Por ejemplo:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
interestelar_100_replica <-select(interestelar_100, everything())
```

Se ha creado el *date frame* "interestelar_100_replica" que es una copia exacta de "interestelar_100".

### ![](figuras/key.svg){.hicon} Seleccionando casos de un *data frame*.

Además de seleccionar variables, con `{dplyr}` también se pueden seleccionar casos que cumplan ciertas condiciones. La función para realizar este cometido es `filter()`. Por ejemplo, si queremos seleccionar las empresas con un resultado (variable RES) mayor o igual a 500 miles de PAVOs y presentar en pantalla el valor de RES y RENECO, la instrucción será:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Seleccionando casos
select(filter(interestelar_100, RES >= 500), RES, RENECO)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}

#definir df
df<-select(filter(interestelar_100, RES >= 500), RES, RENECO)

# Número de columnas por bloque
columnas_por_bloque <- 2

# Dividir las columnas en bloques y mostrar los datos en la consola
for (i in seq(1, ncol(df), columnas_por_bloque)) {
  print(df[, i:min(i + columnas_por_bloque - 1, ncol(df))])
  cat("\n")
}
rm(df)
rm(columnas_por_bloque)
```

Se pueden incluir varias condiciones en un mismo filtro. Por ejemplo, vamos a construir un nuevo *data frame* llamado "interestelar_100B" con las empresas que posean un resultado mayor o igual a 500 miles de PAVOs y una rentabilidad económica (variable RENECO) inferior al 40%, y que contenga las variables RES, RENECO y ACTIVO:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
interestelar_100B <-select(filter(interestelar_100,
                                  RES >= 500 & RENECO < 40),
                                  RES, RENECO, ACTIVO)
interestelar_100B
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
interestelar_100B <-select(filter(interestelar_100,
                                  RES >= 500 & RENECO < 40),
                                  RES, RENECO, ACTIVO)
df <- select(interestelar_100B, everything())

# Número de columnas por bloque
columnas_por_bloque <- 3

# Dividir las columnas en bloques y mostrar los datos en la consola
for (i in seq(1, ncol(df), columnas_por_bloque)) {
  print(df[, i:min(i + columnas_por_bloque - 1, ncol(df))])
  cat("\n")
  }
rm(df)
rm(columnas_por_bloque)
```

En el *Global* *Environment* aparecerá el *data frame* "interestelar_100B" con solo un caso: la empresa que cumple con ambas condiciones, introducidas mediante el operador lógico relacional "**&**", que es el equivalente a la conjunción "y" o, dicho de otro modo, la intersección. Otro operador lógico relacional muy utilizado es la barra vertical "**\|**", que es el equivalente a la conjunción "o", es decir, la unión.

Los filtros más usuales son \>, \<, \>=, \<=, == (igual, ojo, con dos símbolos de igualdad seguidos) y != (no igual).

### ![](figuras/key.svg){.hicon} Ordenando casos de un *data frame*.

Además de seleccionar determinados casos u observaciones (filas) de un *data frame*, con las funciones de `{dplyr}` también se pueden ordenar estos casos a partir de los valores de ciertas variables (columnas). La función a utilizar es `arrange()`. Esta función, por defecto, ordena los casos de modo **ascendente**. Por ejemplo, creremos el data frame "interestelar_100C" con variables RENECO, EFLO y ACTIVO, con los casos ordenados de modo ascendente según el valor de RENECO:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Ordenando casos
interestelar_100C <- select(interestelar_100, RENECO, EFLO, ACTIVO)
interestelar_100C <- arrange(interestelar_100C, RENECO)
interestelar_100C
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
interestelar_100C <- select(interestelar_100, RENECO, EFLO, ACTIVO)
interestelar_100C <- arrange(interestelar_100C, RENECO)
df<-interestelar_100C
df <- dplyr::slice_head(df, n = 10)  # Solo primeras 10 filas
# Número de columnas por bloque
columnas_por_bloque <- 3

# Dividir las columnas en bloques y mostrar los datos en la consola
for (i in seq(1, ncol(df), columnas_por_bloque)) {
  print(df[, i:min(i + columnas_por_bloque - 1, ncol(df))])
  cat("\n")
  }
rm(df)
rm(columnas_por_bloque)
```

(**Nota:** solo mostramos aquí los 10 primeros casos del *data frame*).

En cambio, para ordenar de modo descendente, hay que utilizar el argumento `desc()`:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
interestelar_100D <- arrange(interestelar_100C, desc(RENECO))
interestelar_100D
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
interestelar_100D <- arrange(interestelar_100C, desc(RENECO))
df<-interestelar_100D
df <- dplyr::slice_head(df, n = 10)  # Solo primeras 10 filas
# Número de columnas por bloque
columnas_por_bloque <- 3

# Dividir las columnas en bloques y mostrar los datos en la consola
for (i in seq(1, ncol(df), columnas_por_bloque)) {
  print(df[, i:min(i + columnas_por_bloque - 1, ncol(df))])
  cat("\n")
  }
rm(df)
rm(columnas_por_bloque)
```

(**Nota:** solo mostramos aquí los 10 primeros casos del *data frame*).

En el supuesto de que, en relación con una variable, hubiera varios casos con el mismo valor, podría añadirse otro criterio de ordenación con otra variable, que afectaría a tales casos para deshacer el "empate". Por ejemplo,vamos a ordenar las empresas según la variable EFLO (categorización de la antigüedad promedio de la flota, con categorías: ANTIGUA, MADURA, RENOVADA). Al ser una variable categórica, los casos se ordenarán por orden alfabético según la categoría a la que pertenenecen. Para ordenar los casos pertenecientes a una misma categoría de EFLO, utilizaremos, de nuevo RENECO (en orden de valor descendente):

```{r, eval=FALSE, echo=TRUE, message=FALSE}
interestelar_100E <- arrange(interestelar_100C, EFLO, desc(RENECO))
interestelar_100E
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
interestelar_100E <- arrange(interestelar_100C, EFLO, desc(RENECO))
df<- interestelar_100E
df <- dplyr::slice_head(df, n = 10)  # Solo primeras 10 filas
# Número de columnas por bloque
columnas_por_bloque <- 3

# Dividir las columnas en bloques y mostrar los datos en la consola
for (i in seq(1, ncol(df), columnas_por_bloque)) {
  print(df[, i:min(i + columnas_por_bloque - 1, ncol(df))])
  cat("\n")
}
rm(df)
rm(columnas_por_bloque)
rm(i)
```

(**Nota:** solo mostramos aquí los 10 primeros casos del *data frame.* Una vez concluidos los casos con categoría de EFLO "ANTIGUA", comenzarían a aparecer los de categoría "MADURA", desde el caso con posición 52, exactamente).

### ![](figuras/key.svg){.hicon} Cambiando el nombre de las variables de un *data frame*.

`{dplyr}` cuenta con una función que cambia fácilmente el nombre de una variable o columna de un *data frame*: la función `rename()`. Por ejemplo, si queremos cambiar el nombre de la variable SOLVENCIA por SOLVE, simplemente ejecutaremos:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Renombrando variables
interestelar_100 <- rename(interestelar_100, SOLVE = SOLVENCIA)
```

Podemos comprobar en el *Global* *Environment*, despegando el *data frame* "interestelar_100", cómo ya no aparece la variable SOLVENCIA; pero sí SOLVE en su lugar (obviamente, con los mismos datos). Es necesario tener en cuenta que en el **lado izquierdo** de la igualdad hay que poner el **nuevo nombre**, y en la derecha el antiguo. Además, en el mismo `rename()` se pueden cambiar los nombres de **varias variables**, separando las igualdades correspondientes con comas.

### ![](figuras/key.svg){.hicon} Añadiendo variables como transformación de otras variables en un *data frame*.

El paquete `{dplyr}` también permite añadir a un *data frame* variables que son el resultado de **someter a otras variables a diversas transformaciones**. La función para realizar este cometido es `mutate()`.

Así, por ejemplo, imaginemos que necesitamos calcular una variable como el cociente entre los resultados obtenidos y el activo. A esta nueva variable la denominaremos RATIO. El código será:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Añadiendo variables como transformacion de otras variables
interestelar_100 <- mutate (interestelar_100, RATIO = RES / ACTIVO)
summary(interestelar_100)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# Añadiendo variables como transformacion de otras variables
interestelar_100 <- mutate (interestelar_100, RATIO = RES / ACTIVO)

library (dplyr)
df <- select(interestelar_100, everything())
# Número de variables por bloque
variables_por_bloque <- 4

# Dividir las variables en bloques
for (i in seq(1, ncol(interestelar_100), variables_por_bloque)) {
  # Comentario: No mostramos el nombre del bloque
  print(summary(interestelar_100[, i:min(i + variables_por_bloque - 1, ncol(interestelar_100))]))
  cat("\n")
}
rm (i)
rm (variables_por_bloque)
rm (df)
```

En la transformación de variables mediante la función `mutate()`, se pueden utilizar **funciones integradas en otros paquetes** de R. Por ejemplo, si queremos calcular la variable ACTIVOS_ACUM como la variable que recoge los activos acumulados de las empresas, comenzando por la empresa con menor activo, podríamos utilizar la función `cumsum()` del paquete `{base}`, y hacer:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
interestelar_100 <- arrange(interestelar_100, ACTIVO)
interestelar_100 <- mutate (interestelar_100, ACTIVOS_ACUM = cumsum(ACTIVO))
select(interestelar_100, ACTIVO, ACTIVOS_ACUM)
```

Podemos verificar cómo se ha integrado en el *data frame* la variable ACTIVOS_ACUM:

```{r, eval=TRUE, echo=FALSE, message=FALSE}
interestelar_100 <- arrange(interestelar_100, ACTIVO)
interestelar_100 <- mutate (interestelar_100, ACTIVOS_ACUM = cumsum(ACTIVO))
df<- select(interestelar_100, ACTIVO, ACTIVOS_ACUM)
df <- dplyr::slice_head(df, n = 10)  # Solo primeras 10 filas
# Número de columnas por bloque
columnas_por_bloque <- 3

# Dividir las columnas en bloques y mostrar los datos en la consola
for (i in seq(1, ncol(df), columnas_por_bloque)) {
  print(df[, i:min(i + columnas_por_bloque - 1, ncol(df))])
  cat("\n")
}
rm(df)
rm(columnas_por_bloque)
rm(i)
```

(**Nota:** solo mostramos aquí los 10 primeros casos del *data frame)*

Un último ejemplo de adición de una variable que es transformación de otras. En este caso, crearemos la variable DIM (dimensión), que es **categórica** (los datos son conjuntos de caracteres). Esta variable tomará valor "ALTA" para las empresas con un valor de la variable ACTIVO mayor o igual que 216 miles de PAVOs, "MEDIA" para las empresas con un ACTIVO menor que 216 miles de PAVOs y mayor o igual que 54 miles de PAVOs, y "REDUCIDA" para las que tengan un valor en la variable ACTIVO menor que 54 miles de PAVOs. Para calcular automáticamente esta nueva variable categórica, utilizaremos la función de `{base}` llamada `cut()`. De este modo, haremos:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
interestelar_100 <- mutate(interestelar_100,
                           DIM = cut(ACTIVO,
                                 breaks = c(-Inf, 54, 216, Inf),
                                 labels = c("REDUCIDA", "MEDIA", "ALTA")))
select(interestelar_100, ACTIVO, DIM)
```

Podemos advertir cómo la función `cut()`, que incluimos dentro de nuestra función de `{dplyr}` `mutate()`, tiene, a su vez, varios argumentos: la variable numérica de referencia (ACTIVO); el argumento `breaks =`, en el que decimos los intervalos en que quedarán divididos los casos (uno, de menos infinito a 54, otro de 54 a 216, y otro de 216 a más infinito), y `labels =`, que es el valor que tomará la variable creada (DIM) según el intervalo en el que se sitúe cada caso de la muestra:

```{r, eval=TRUE, echo=FALSE, message=FALSE}
interestelar_100 <- mutate(interestelar_100,
                           DIM = cut(ACTIVO,
                                 breaks = c(-Inf, 54, 216, Inf),
                                 labels = c("REDUCIDA", "MEDIA", "ALTA")))
df <-select(interestelar_100, ACTIVO, DIM)
df <- dplyr::slice_head(df, n = 10)  # Solo primeras 10 filas
# Número de columnas por bloque
columnas_por_bloque <- 3

# Dividir las columnas en bloques y mostrar los datos en la consola
for (i in seq(1, ncol(df), columnas_por_bloque)) {
  print(df[, i:min(i + columnas_por_bloque - 1, ncol(df))])
  cat("\n")
}
rm(df)
rm(columnas_por_bloque)
rm(i)

```

(**Nota:** solo mostramos aquí los 10 primeros casos del *data frame)*

Cabe destacar que podíamos haber escrito el código para crear la variable DIM de un modo más elegante y cómodo, utilizando **el operador** **"*pipe*"** `%>%`. Este operador permite concatenar una serie de instrucciones:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
interestelar_100 <- interestelar_100 %>%
                              mutate(DIM = cut(ACTIVO,
                                breaks = c(-Inf, 54, 216, Inf),
                                labels = c("REDUCIDA", "MEDIA", "ALTA")))
interestelar_100 %>% select(ACTIVO, DIM)
```

Podríamos interpretar la línea de código así: asigna al *data frame* "interestelar_100" sus propios datos, después (`%>%`) crea la variable DIM con la función `cut()` y añádela a "interestelar_100". En el segundo caso, es más sencillo aún: toma "interestelar_100" y saca en pantalla los valores de las variables ACTIVO y DIM.

### ![](figuras/key.svg){.hicon} Extrayendo y sintetizando información de las variables de un *data frame*.

Otra posibilidad que permite `{dplyr}` es extraer y sintetizar mediante *medidas* la información de las variables contenidas en un *data frame*. Para ello, nos ayudaremos de la función `summarise()`. Como ejemplo, calculemos la *rentabilidad financiera* media de las 20 empresas:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
#Extrayendo información de las variables de un data frame
summarise(interestelar_100, RENFIN_media = mean(RENFIN)) 
```

A veces, es de gran utilidad combinar `summarise()` con `group_by()`, que extrae la información dividiendo el conjunto de casos por grupos definidos por una de las variables. Para ilustrarlo, vamos a utilizar la variable recién creada DIM para hacer tres grupos de empresas, tras lo cual calcularemos la media de las rentabilidades para cada grupo:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
interestelar_100 %>%
  group_by(DIM) %>%
  summarise(RENFIN_media = mean(RENFIN))
```

Hemos utilizado el operador *pipe* `%>%` para concatenar diferentes instrucciones de `{dplyr}`: primero agrupar casos, y luego calcular las medias de cada grupo. Es decir, en este caso se podría "traducir" la línea de código como: "Toma el *data frame*"interestelar_100", divide sus casos en grupos según el valor de la variable DIM, y para cada grupo calcula la media de la variable RENFIN". Es de destacar que hay elementos que tienen como valor categórico en DIM "NA" (es decir, no hay dato). Esto se debe a que, cuando se creo DIM, había una empresa sin valor de ACTIVO (exactamente, "Vega Transport").

## ![](figuras/pie-chart.svg){.hicon}![](figuras/key.svg){.hicon} Exportando datos.

Antes de concluir el capítulo, vamos a tratar brevemente el aspecto de la exportación de datos.

R cuenta con un **formato propio de datos**, que se traduce en archivos de extensión "RData", y que puede incluir cualquier objeto de R. Vamos a exportar el *data frame* "interestelar_100" como el archivo de datos de R "interestelar_100.RData". Posteriormente, borraremos el *data frame* del *Environment* y recuperaremos los datos cargando ese archivo "interestelar_100.RData".

Para exportar el *data frame* "interestelar_100" al archivo de formato R, "interestelar_100.RData", utilizaremos la función `save()`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
## Exportación de datos

# Exportando data frame a formato R (.RData)
save(interestelar_100, file = "interestelar_100.RData")
```

Puede comprobarse cómo se ha generado el archivo correspondiente en la carpeta de proyecto. Para comprobar que la exportación es correcta, vamos a borrar del *Global* *Environment* el *data frame* "interestelar_100" con la función `rm()` (remove). Después, cargaremos el archivo "interestelar_100.RData" con la función `load()`. Como resultado, podremos comprobar que tenemos un nuevo *data frame* "interestelar_100" que es exactamente igual al que teníamos al principio:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Borrando el data frame interestelar_100
rm(interestelar_100)

# Importando el archivo .RData con los mismos datos
load("interestelar_100.RData")
summary (interestelar_100)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# Borrando el data frame interestelar_100
rm(interestelar_100)

# Importando el archivo .RData con los mismos datos
load("interestelar_100.RData")

library (dplyr)
df <- select(interestelar_100, everything())
# Número de variables por bloque
variables_por_bloque <- 4

# Dividir las variables en bloques
for (i in seq(1, ncol(interestelar_100), variables_por_bloque)) {
  # Comentario: No mostramos el nombre del bloque
  print(summary(interestelar_100[, i:min(i + variables_por_bloque - 1, ncol(interestelar_100))]))
  cat("\n")
}
rm (i)
rm (variables_por_bloque)
rm (df)
```

Por supuesto, hay más formatos en los que se pueden exportar datos desde R. Por ejemplo, **a un archivo de Microsoft® Excel®**. Un modo de hacerlo es haciendo uso de la función `write_xlsx()` del paquete {writexl}. Para que en la hoja de cálculo resultante se incluyan los nombres de las filas (empresas eólicas), hemos tenido previamente que crear un vector con el nombre de estas (vector "NOMBRE"), mediante la función `row.names()`, y unir ese vector al *data frame* "interestelar_100", a modo de primera columna, creando un nuevo finalmente un *data frame* llamado "interestelar_100n", para lo que se ha utilizado la función `cbind()`, que permite **pegar columnas de datos** que tengan un mismo número de filas.

Como resultado de todo el código, se ha obtenido el archivo de Microsoft® Excel® "interestelar_100_new.xlsx":

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Exportando el data frame interestelar_100 a Microsoft (R) Excel (R)
library(writexl)
NOMBRE <- row.names(interestelar_100)
interestelar_100n <- cbind(NOMBRE, interestelar_100)
write_xlsx(interestelar_100n, path = "interestelar_100_new.xlsx")

#Fin del script :)
```

## ![](figuras/star.svg){.hicon} Las variables de la base de datos del proyecto R-Stars.

La base de datos completa del proyecto R-Stars contiene datos económicos, financieros y de diversa índole de 300 empresas de trasnporte interestelar de mercancías, recogidos en unas 30 variables.

El sector del transporte interestelar se extiende a lo largo y ancho de 5 galaxias. Son 30 los planetas donde se localizan las sedes de las 300 empresas.

![[Galaxias y principales planetas del transporte de mercancías.]{.smallcaps}](figuras/mapa_galactico_final.jpg){width="100%"}

Las galaxias donde se desarrolla este tipo de actividad son:

-   **Andrómeda (M31), Vía Láctea y Galaxia del Triángulo (M33):** Son galaxias espirales, caracterizadas por un disco con brazos espirales, un bulbo central y un halo estelar. Presentan componentes como discos delgados y gruesos, bulbos y halos, con poblaciones estelares de diferentes edades y metalicidades.

-   **Gran Nube de Magallanes (LMC) y Pequeña Nube de Magallanes (SMC):** Son galaxias irregulares, con estructuras menos definidas, ricas en gas y con intensa formación estelar. Su morfología es más caótica y carecen de un bulbo central prominente.

En cuanto a las variables que componen la base de dato, son las siguientes (**importante:** estos datos se refieren a la base de datos madre, de 300 casos. No se refiere a la muestra de 104 observaciones utilizada para desarrollar los ejemplos de este capítulo):

**ACTCOR.** Activo corriente. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 6.810, 'q1': 17.450, 'median': 24.005, 'mean': 59.787, 'q3': 55.635, 'max': 889.956}.\
Número estimado de outliers: 36.

**ACTIVO.** Valor total de los recursos de la empresa (miles de PAVOs). Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 17.550, 'q1': 50.530, 'median': 156.315, 'mean': 241.407, 'q3': 227.017, 'max': 2966.520}.\
Número estimado de outliers: 45.

**APALANCA.** Nivel de apalancamiento (Pasivo total / Fondos propios). Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 44.380, 'q1': 100.593, 'median': 121.860, 'mean': 132.667, 'q3': 154.550, 'max': 433.550}.\
Número estimado de outliers: 13.

**BMAL.** Beneficio medio por año luz. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 24.135, 'q1': 466.114, 'median': 1491.416, 'mean': 6084.857, 'q3': 4368.461, 'max': 262555.815}.\
Número estimado de outliers: 31.

**CAPEX.** Gastos de capital. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 1.038, 'q1': 6.156, 'median': 11.600, 'mean': 33.748, 'q3': 34.329, 'max': 1087.196}.\
Número estimado de outliers: 29.

**COSTOP.** Costes operativos. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 0.980, 'q1': 15.305, 'median': 31.195, 'mean': 97.803, 'q3': 105.143, 'max': 1422.300}.\
Número estimado de outliers: 28.

**DIST.** Distancia operada globalmente. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 0.151, 'q1': 12.606, 'median': 44.229, 'mean': 133.686, 'q3': 165.292, 'max': 2151.642}.\
Número estimado de outliers: 26.

**EFLO.** Edad media de la flota (categorías). Tipo de datos: carácter/categórico. NAs: 0.\
Distribución/aproximación: {'ANTIGUA': 148, 'MADURA': 90, 'RENOVADA': 62}.

**EFLO_FACTOR.** Factor derivado de la edad media de la flota. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 0.800, 'q1': 0.800, 'median': 1.000, 'mean': 0.943, 'q3': 1.000, 'max': 1.200}.\
Número estimado de outliers: 0.

**EMPLEA.** Número de empleados. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 7.000, 'q1': 22.750, 'median': 30.000, 'mean': 36.287, 'q3': 44.000, 'max': 106.000}.\
Número estimado de outliers: 16.

**FLOTA.** Número de cargueros espaciales. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 3.000, 'q1': 15.000, 'median': 24.000, 'mean': 26.143, 'q3': 38.000, 'max': 50.000}.\
Número estimado de outliers: 0.

**FJUR.** Forma jurídica de la empresa. Tipo de datos: carácter/categórico. NAs: 0.

**FPIOS.** Fondos propios de la empresa. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 8.670, 'q1': 22.530, 'median': 57.430, 'mean': 111.042, 'q3': 108.408, 'max': 1453.450}.\
Número estimado de outliers: 23.

**FROM.** Inspiración temática de la empresa. Tipo de datos: carácter/categórico. NAs: 0.\
Distribución/aproximación: {'Star Wars': 84, '2001: Una Odisea en el Espacio': 84, 'Dune': 52, 'Star Trek': 47, 'Interstellar': 33}.

**GALAXIA.** Galaxia principal en la que opera la empresa. Tipo de datos: carácter/categórico. NAs: 0.

**GALAXY_FACTOR.** Factor de distancia intergaláctica. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 1.000, 'q1': 1.000, 'median': 2.500, 'mean': 2.812, 'q3': 4.500, 'max': 5.000}.\
Número estimado de outliers: 0.

**GMEDRUT.** Distancia media de las rutas (años luz). Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 0.005, 'q1': 0.112, 'median': 0.224, 'mean': 0.319, 'q3': 0.448, 'max': 1.550}.\
Número estimado de outliers: 10.

**IDIG.** Índice de digitalización. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 0.338, 'q1': 8.498, 'median': 15.672, 'mean': 17.234, 'q3': 23.848, 'max': 60.833}.\
Número estimado de outliers: 4.

**IDIVERSE.** Índice de diversificación. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 0.000, 'q1': 11.655, 'median': 21.165, 'mean': 22.210, 'q3': 32.390, 'max': 78.298}.\
Número estimado de outliers: 2.

**IFIDE.** Índice de fidelización. Tipo de datos: numérico (real o entero). NAs: 0. Observaciones corruptas: 0.\
Distribución/aproximación: {'min': 5.261, 'q1': 36.928, 'median': 39.268, 'mean': 39.002, 'q3': 40.432, 'max': 79.154}.\
Número estimado de outliers: 33.

**IMD.** Gasto en I+D. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 0.478, 'q1': 1.409, 'median': 4.700, 'mean': 15.203, 'q3': 18.240, 'max': 202.478}.\
Número estimado de outliers: 40.

**ING.** Ingresos generados por la actividad principal de la empresa. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 19.400, 'q1': 42.005, 'median': 103.470, 'mean': 218.386, 'q3': 240.973, 'max': 2471.540}.\
Número estimado de outliers: 23.

**LIQUIDEZ.** Cociente entre ACTCOR y PASCOR. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 0.225, 'q1': 0.526, 'median': 0.706, 'mean': 0.702, 'q3': 0.836, 'max': 3.118}.\
Número estimado de outliers: 7.

**MARGEN.** Margen operativo como porcentaje sobre los ingresos. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 12.432, 'q1': 48.095, 'median': 57.449, 'mean': 60.160, 'q3': 73.610, 'max': 96.489}.\
Número estimado de outliers: 0.

**NOMBRE.** Nombre de la empresa. Tipo de datos: carácter/categórico. NAs: 0.

**PASCOR.** Pasivo corriente. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 6.660, 'q1': 20.222, 'median': 59.224, 'mean': 97.773, 'q3': 100.558, 'max': 1143.810}.\
Número estimado de outliers: 25.

**RENECO.** Rentabilidad económica. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 24.082, 'q1': 44.110, 'median': 50.323, 'mean': 51.055, 'q3': 57.745, 'max': 93.847}.\
Número estimado de outliers: 4.

**RENFIN.** Rentabilidad financiera. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 51.388, 'q1': 95.620, 'median': 113.048, 'mean': 116.138, 'q3': 131.138, 'max': 262.245}.\
Número estimado de outliers: 6.

**RES.** Resultado del ejercicio (beneficio neto). Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 9.620, 'q1': 25.315, 'median': 59.220, 'mean': 120.583, 'q3': 128.178, 'max': 1561.930}.\
Número estimado de outliers: 28.

**RUTAS.** Número de rutas atendidas. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 36.000, 'q1': 176.000, 'median': 310.000, 'mean': 365.580, 'q3': 436.500, 'max': 3138.000}.\
Número estimado de outliers: 12.

**SEDE.** Planeta principal donde se encuentra la base de operaciones de la empresa. Tipo de datos: carácter/categórico. NAs: 0.

**SOLVENCIA.** Capacidad de la empresa para cubrir sus obligaciones financieras a largo plazo. Tipo de datos: numérico (real o entero). NAs: 0.\
Distribución/aproximación: {'min': 123.070, 'q1': 164.703, 'median': 182.060, 'mean': 185.718, 'q3': 199.415, 'max': 325.320}.\
Número estimado de outliers: 10.

## ![](figuras/arrow-down-circle.svg){.hicon} Materiales para realizar las prácticas del capítulo.

En esta sección se muestran los links de acceso a los diferentes materiales (*scripts*, datos...) necesarios para llevar a cabo los contenidos prácticos del capítulo.

**Datos (en formato Microsoft® Excel®:**

-   interestelar_100.xlsx ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/interestelar_100.xlsx))

**Scripts:**

-   explora_rstars.R ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/explora_rstars.R))

<!--chapter:end:02-datos.Rmd-->

---
title: "R-Lite Cluster"
author: "Miguel-Ángel Tarancón"
date: "`r format(Sys.Date(), '%d de %B de %Y')`"
lang: "es"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    always_allow_html: true
  word_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción.

El análisis de conglomerados o análisis clúster (AC) trata de clasificar individuos o casos asignándolos a grupos homogéneos, de manera que:

-   Cada grupo, conglomerado o clúster contenga a **los casos más parecidos** entre sí, en términos de una serie de variables (**variables clasificadoras**).

-   **Los grupos** contengan casos que, en general, **sean muy diferentes** a los casos del resto de grupos, de acuerdo con las variables consideradas.

En general, el proceso de determinación de los grupos, conglomerados o clústeres de casos es el siguiente:

-   Se parte de un conjunto de **n** casos, y para cada uno de ellos se cuenta con el valor de **m** variables clasificadoras.
-   Se establece una **medida de distancia** que cuantifica lo que dos casos se parecen, **considerando en conjunto** los valores que poseen para las variables clasificadoras.
-   Se crean los grupos, conglomerados o clústeres con los casos que poseen entre sí una **menor distancia**. Existen dos **enfoques** principales a la hora de crear los grupos de casos a partir de las distancias observadas entre los casos: los *métodos jerárquicos* y los *métodos no-jerárquicos*.
-   Finalmente, se **caracterizan** los grupos, conglomerados o clíusteres obtenidos, y se comparan unos con otros para extraer conclusiones.

En lo que respecta a la medida de **distancia** entre los casos, la medida más habitual es la **distancia euclídea**. Así, la distancia euclídea entre dos caso, i e i', para las m variables clasificadoras x, será:

$$
d(i, i') = \sqrt{\sum_{j=1}^{m} (x_{ij} - x_{i'j})^2}
$$ Esta distancia es muy sensible a la escala de las variables clasificadoras. Para evitar este inconveniente, se trabaja con las variables previamente **tipificadas**.

# Métodos de agrupación jerárquicos.

Como se acaba de comentar, existen dos enfoques fundamentales de realizar el análisis clúster, dependiendo de cómo son los métodos de agrupación de los casos (y grupos de casos): el enfoque de los métodos jerárquicos, y el enfoque que reúne a los métodos no-jerárquicos.

Ambos enfoques tienen sus ventajas e inconvenientes, y pueden adaptarse mejor a cada problema concreto. Es importante seleccionar un buen método de agrupación, puesto que pueden proporcionar soluciones muy diferentes entre sí.

En los **métodos jerárquicos,** se van formando sucesivamente grupos como agrupación de otros grupos precedentes, hasta llegar a un único grupo que recoge a todos los individuos; tomando el proceso una **estructura piramidal** (también existen métodos jerárquicos descendientes, que parten de un único grupo que contiene a todos los casos, para acabar el n grupos de un solo caso, aunque son menos frecuentes).

Estos métodos suelen aplicarse cuando hay un número reducido de casos. También, cuando nuestro objetivo pasa por crear **grupos que recojan a todos los casos**, más que definir simplemente tipologías más o menos homogéneas de casos (lo que se obtiene caracterizando los grupos obtenidos). Es decir, cuando se incluyen en el análisis a todos los individuos, incluidos los *outliers*. De hecho, estos métodos pueden emplearse, de por sí, como técnicas de localización de *outliers*. Por último, también se suelen emplearse cuando se desconoce a priori el número de grupos, conglomerados o clústeres a formar.

Entre los métodos jerárquicos de agrupación más extendidos, figuran los siguientes:

-   **Método del vecino más cercano (single linkage):** la distancia que se considera entre grupos es la distancia entre sus elementos más próximos.

-   **Método del vecino más lejano (complete linkage):** la distancia que se considera entre grupos es la distancia entre sus elementos más lejanos.

-   **Método de Ward (Ward method):** se unen los grupos que dan lugar a otro grupo cuyos casos tienen una menor suma de los cuadrados de sus distancias respecto al centro de dicho grupo (menor varianza).

-   **Otros métodos:** vinculación intergrupos (average linkage between groups), vinculación intragrupos (whithin group)...

De entre ellos, ¿cuál elegir?

La cuestión no es fácil de resolver, y no tiene por qué tener una única respuesta. Por otro lado, cada método proporciona soluciones que pueden variar mucho entre sí. Una estrategia puede pasar por probar con varios métodos y se seleccionar la solución que parezca más coherente desde el punto de vista teórico, y estable desde el punto de vista empírico.

En la práctica, uno de los métodos más utilizados es el **método de Ward**, porque proporciona grupos muy homogéneos, ya que se basa en la minimización de la varianza o dispersión de los elementos que componen cada grupo con respecto a su centro de gravedad o **centroide.** Precisamente, este método será aplicado en el ejemplo práctico que desarrollaremos en R a continuación.

# Caso práctico del Método jerárquico de Ward.

Para ilustrar el análisis clúster jerárquico, por el método concreto de *Ward*, vamos a recurrir de nuevo al caso del sector del "Transporte Interestelar".

Contexto: la *Agencia Interplanetaria de Transporte de Mercancías* es un organismo dedicado a estudiar el funcionamiento del sector. Entre sus actividades, hay una consistente en la selección de un grupo de empresas para su segmentación en términos de fidelidad de los clientes (**IFIDE**), diversificación del negocio (**IDIVERSE**), y digitalización de la compañía (**IDIG**). En esta ocasión, se ha elegido un panel de 25 empresas o compañías. El informe corre a cargo de una de las investigadoras de la agencia, la doctora *Xelia Stark* (en la imagen).

![Dra. Xelia Stark](images/Xelia%202.png){width="339"}

Dado que son pocos los casos (empresas) a segmentar, vamos a utilizar un método jerárquico de agrupación de casos. En concreto, utilizaremos el **método de Ward**.

Pasando a la preparación de la práctica, se requiere que se tengan instalados dos paquetes de R provenientes un repositorio de **Git-Hub**. Muy posiblemente, la instalación de estos dos paquetes requerirá, a su vez, la instalación de otros paquetes localizados en el repositorio oficial **CRAN**, que serán instalados de modo automático. Si eventualmente fuera necesario instalar algún paquete adicional, utilizaremos, por ejemplo, las facilidades de la pestaña "Packages" de RStudio, en la ventana inferior-derecha.

Los dos paquetes a instalar provenientes de **Git-Hub** son: `{MATdatatools}` y `{MATmultivar}`. Pueden ser instalados, ambos del siguiente modo. Desde la consola de RStudio:

1.  Asegúrate de tener instalado el paquete `{devtools}`: `install.packages("devtools")`

2.  Actívalo: `library(devtools)`

3.  Descarga e instala el primer paquete desde GitHub: `devtools::install_github("teckel71/R_for_Economics/packages/MATdatatools")`

4.  Descarga e instala el primer paquete desde GitHub: `devtools::install_github("teckel71/R_for_Economics/packages/MATmultivar")`

Vamos a suponer que trabajamos dentro de un **proyecto** que hemos creado previamente, de nombre **"cluster"**. Dentro de la carpeta del proyecto guardaremos estos dos elementos:

-   El *script* llamado "clusterlite.R".

-   El archivo de Microsoft® Excel® llamado "interestelar_25.xlsx". Si abrimos este último archivo, comprobaremos que se compone de tres hojas. La primera muestra un mensaje sobre el uso de los datos, la segunda recoge la descripción de las variables consideradas, y la tercera (hoja "Datos") almacena los datos que debemos importar. Estos datos se corresponden con diferentes variables económicas y financieras de 25 empresas dedicadas a los servicios de transporte interestelar de mercancías.

## Tratamiento previo de datos.

Desde el proyecto "componentes", abriremos en el editor de RStudio el *script* "**clusterlite.R**" (por ejemplo, pinchando en él en la pestaña "Files" de la ventana inferior derecha). De este modo, tendremos acceso a su contenido, que podremos ir ejecutando.

La primera línea / instrucción en el *script* es:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Análisis Clúster jerárquico (Ward)

# Limpiando el Global Environment

rm(list = ls())

# Cargando paquetes

library(MATdatatools)
library(MATmultivar)
library(dplyr)
library(gtExtras)
```

La primera línea tiene como objeto limpiar el *Global Environment* (memoria) de objetos de anteriores sesiones de trabajo. Luego, hemos activado los paquetes que nos harán falta para la práctica con la función `library()`.

Para importar los datos almacenados en la hoja "Datos" del archivo "interestelar_25.xlsx", ejecutaremos el código:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# importando datos

datos <- MATfexcel("interestelar_25.xlsx", "Datos",
                   na_values = c("n.d."))
```

La función `MATfexcel()` del paquete `{MATdatatools}` permite fácilmente importar los datos, asignándolos a un ***data frame*** cuyo nombre podemos elegir (en este caso, "datos"). Para ello, hay que completar los siguientes argumentos: el nombre del archivo de Microsoft® Excel® (que debe estar en la carpeta de proyecto), la hoja donde se hallan los datos y, si es preciso, los caracteres que, en la hoja de Excel, encontraremos en las posiciones o celdas donde no hay dato (NA), si en estos casos la celda no se encuentra totalmente vacía.

Al ejecutar el código, podemos observar cómo en el *Environment* ya aparece un objeto. Este objeto es una estructura de datos tipo *data frame*, se llama "datos" y contiene 28 columnas, una por cada una de las variables almacenadas en el archivo de Microsoft® Excel®. De estas variables, 4 son de tipo cualitativo, formadas por cadenas de caracteres.

Además, en la consola aparecen los nombres de las variables que conforman el *data frame* que hemos creado al importar los datos desde la hoja de Microsoft® Excel®, con algunas medidas básicas (dependiendo del tipo de escala de la variable, en cada caso):

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
library(readxl)
datos <- read_excel("interestelar_25.xlsx", sheet = "Datos",
                         na = c("n.d.", "s.d."))
datos <- data.frame(datos, row.names = 1)
library (dplyr)
df <- select(datos, everything())
# Número de variables por bloque
variables_por_bloque <- 4

# Dividir las variables en bloques
for (i in seq(1, ncol(df), variables_por_bloque)) {
  # Comentario: No mostramos el nombre del bloque
  print(summary(df[, i:min(i + variables_por_bloque - 1, ncol(df))]))
  cat("\n")
}
rm (i)
rm (variables_por_bloque)
rm (df)
```

Evidentemente, no vamos a utilizar todas las variables del *data frame* "datos" para el análisis de componentes principales, sino solo las 3 que dijimos anteriormente: **IDIVERSE**, **IFIDE** e **IDIG**. Por ello, es conveniente crear un *data frame* con las únicas variables que vamos a emplear. Esta tarea es fácil con la función `select()` del paquete `{dplyr}`. Llamando al nuevo *data frame* "seleccion":

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# Seleccionando variables metricas para el analisis.

library(dplyr)
seleccion <- datos %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion_df_graph <- gt_plt_summary(seleccion)
seleccion_df_graph
```

La última línea muestra el contenido de este nuevo *data frame* con la función `gt_plt_summary()` del paquete `{gtExtras}`.

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# Seleccionando variables metricas para el analisis.

library(dplyr)
seleccion <- datos %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion_df_graph <- gt_plt_summary(seleccion)
seleccion_df_graph
```

La primera acción que debe realizarse es comprobar que todos los casos tienen su correspondiente dato o valor para las variables del análisis, es decir, que no existen **valores perdidos o *missing values***. Si existen casos de este tipo, se puede actuar de varios modos. Por ejemplo, se puede intentar obtener los valores faltantes por otro canal de información o recurrir a realizar alguna estimación. En caso de que esto sea difícil, se puede optar, simplemente, por eliminar estos casos, en especial cuando representan un porcentaje muy reducido respecto al total de casos.

Si optamos por esta última vía, podremos identificar y eliminar los casos involucrados facilmente con la función `MATmv()` del paquete `{MATdatatools}`. Para ello, ejecutaremos el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
# Identificando missing values.

MATmv(seleccion)
```

Al ejecutar el código, se obtienen dos objetos que se almacenan en el *Global Environment*. Uno es un nuevo *data frame*, llamado "seleccion_sm", que contiene las mismas columnas o variables que "seleccion"; y el mismo número de casos (25). Esto se debe a que no hay ningún caso al que le falte dato en alguna de las variables del análisis. El otro objeto es una "lista" llamada "seleccion_sm_info". Esta lista guarda dos elementos:

-   Un gráfico llamado "grafico_vismiss", donde se muestran visualmente los valores perdidos de las variables seleccionadas en la función.

-   Una tabla en formato *html*, llamada "tabla_na", donde se informa de los casos que, en su caso, no tenían valor en alguna de las variables del análisis, y que han sido eliminados.

Para visualizar el gráfico, ejecutaremos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
seleccion_sm_info$grafico_vis_miss
```

Y la tabla:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
seleccion_sm_info$tabla_na
```

En ambos elementos, se confirma que no existían casos con *missing values*. Por tanto, los *data frames* "seleccion" y "seleccion_sm" **son iguales, y podemos proseguir el análisis con cualquiera de los dos**.

El siguiente paso será la **identificación de** ***outliers***. Para realizar este proceso, y dado que en nuestro análisis contamos con 3 variables, primero “resumiremos” el valor que toman dichas variables para cada observación (empresa), mediante el cálculo de la ***distancia de Mahalanobis***. Esta distancia se considerará como una variable cuyas puntuaciones serán evaluadas para saber si los casos se comportan respecto a ella como *outliers*.

El tratamiento de los *outliers* depende de la información que se tenga, del número de casos afectados en relación con el total de casos, de la técnica de análisis elegida, y hasta de los propios objetivos perseguidos en el análisis. En este caso **no** vamos a eliminar los *outliers*, ya que son pocos los casos y suponemos que queremos hacer una segmentación de **todos** ellos. Precisamente, si hay algún caso en el análisis clúster que permanece aislado, sin agruparse con otros en el proceso de agrupación hasta las últimas etapas, quizá se trate de un candidato a *outlier*, por lo que el análisis clúster **también puede considerarse una técnica de localización de casos atípicos**.

Para localizar los *outliers*, y así poder observar cómo quedan agrupados en el análisis clúster que realicemos, vamos a utilizar la función `MATout_Mahalanobis()` del paquete `{MATdatatools}` El código es:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Identificación de outliers.

MATout_Mahalanobis(seleccion_sm)
```

Como resultado, se obtienen dos nuevos objetos almacenados en el *Global Environment*. Uno es un nuevo *data frame*, llamado "seleccion_sm_so", que contiene las mismas columnas o variables que "seleccion_sm"; pero con 13 casos menos (89 observaciones). Esos casos son los que se comportaban como *outliers* en alguna, algunas, o todas de las variables del análisis. Hemos de recordar que, en este caso, no queremos eliminar los outliers, solo identificarlos, por lo que no usaremos "seleccion_sm_so" para el análisis clúster posterior. Para evitar confusiones, puede eliminarse este *data frame* con:

```{r, eval=TRUE, echo=TRUE, message=TRUE, warning=TRUE}
remove(seleccion_sm_so)
```

El otro objeto que crea la función `MATout_Mahalanobis()`es una "lista" llamada "seleccion_sm_so_info". Esta lista guarda dos elementos:

-   Un gráfico de caja o *box-plot* llamado "Boxplot", donde se muestran visualmente los casos considerados *outliers* para la *distancia de Mahalanobis* calculada a partir de las variables **IDIVERSE**, **IFIDE** e **IDIG**.

-   Una tabla en formato *html*, llamada "Outliers_table", donde se informa de los casos identificados como *outliers* para la *distancia de Mahalanobis* y que han sido eliminados.

Para visualizar el gráfico de caja ejecutaremos:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
seleccion_sm_so_info$Boxplot
```

En el gráfico se aprecia cómo existen varios puntos por encima del "bigote" superior de la caja, lo que implica que hay varios casos considerados, globalmente, *outliers*.

Para visualizar la tabla:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
seleccion_sm_so_info$Outliers_Table
```

De la tabla anterior se desprende que los outliers identificados son las empresas *Chakotay Cargo Systems*, *Home One Cargo*, y de modo más moderado, *Tannhäuser Freight* e *IO Star Transport*. Estos son, en definitiva, los 4 casos en los que nos fijaremos específicamente más adelante, al analizar los resultados del análisis clúster.

Por otro lado, como hemos anticipado, el *data frame* "seleccion_sm" será el que se utilice para realizar el análisis clúster que segmentará nuestra selección de 25 empresas.

## Determinación del número de grupos, conglomerados o clústeres a formar.

La siguiente etapa y parte del script se refiere a la aplicación propia del análisis clúster al grupo de casos (25 empresas) que toman valores para las 3 variables incluidas en el análisis.

Para este propósito, realizaremos un análisis preliminar con la función `MATclus_Ward()` del paquete `{MATmultivar}`. Esta función realiza, con las variables que indiquemos, un análisis clúster jerárquico por el *método de Ward*, ampliamente utilizado por aportar conglomerados, grupos o clústeres muy homogéneos (mínima varianza interna). La función, además, utiliza como medida de la distancia entre elementos la **distancia euclídea** y, puesto que esta distancia es muy sensible a la escala de los datos, trabaja con las variables clasificadoras **tipificadas**.

Los **argumentos** de la función son:

1.  **`data`**: El *data frame* que contiene los datos a analizar.

2.  **Variables específicas (sin comillas) a utilizar en el clustering**. Si no se especifican, se utilizarán todas las variables numéricas del dataframe.

3.  **`k`** *(por defecto `k = 0`)*: Número de clusters a formar.

-   Si `k = 0`, la función no asigna grupos y solo genera el dendrograma.
-   Si `k > 0`, se generan los grupos y se devuelve información detallada de los mismos.

4.  **`silueta`** *(por defecto `FALSE`)*: Si es `TRUE`, la función calculará el número óptimo de clusters utilizando el *método de la **silueta**.*

Dependiendo de los valores de `k` y `silueta`, la función devuelve una lista con varios elementos:

-   **`hclust`**: Modelo jerárquico ajustado.

-   **`heatmap`**: Mapa de calor de la matriz de distancias euclídeas.

-   **`dendrogram`**: Dendrograma generado con `fviz_dend()`.

-   **`silueta`** *(solo si `silueta = TRUE`)*: Gráfico de la técnica de la Silueta indicando el número óptimo de clústeres.

-   **`data_groups`** *(si `k > 0`)*: *Data frame* con los datos originales y la asignación de grupo de cada observación.

-   **`group_summary`** *(si `k > 0`)*: Tabla resumen con estadísticas de cada grupo.

-   **`group_tables`** *(si `k > 0`)*: Listado de tablas con los casos asignados a cada grupo.

-   **`bar_patchworks`** *(si `k > 0`)*: Gráficos de barras con las medias de cada variable en cada grupo.

-   **`scatter_patchworks`** *(si `k > 0`)*: Gráficos de dispersión de todas las combinaciones posibles de variables, coloreando cada observación según su grupo.

Puesto que a priori no sabemos qué número de grupos es adecuado formar, utilizaremos la función `MATclus_Ward()` con el argumento `k= 0`, para que no forme aún grupos; pero sí nos muestre el dendograma y otras herramientas que nos ayuden a decidir el número de conglomerados:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Modelo tentativo para determinar k

cluster0 <- MATclus_Ward(seleccion_sm, IDIVERSE, IFIDE, IDIG,
                         k=0, silueta = T)
```

En el código anterior, nos encontramos con los siguientes argumentos:

-   **Datos de entrada**: Se aplican los cálculos sobre el dataframe `seleccion_sm` y las variables `IDIVERSE`, `IFIDE`, `IDIG`.

-   **`k = 0`**: La función no forma grupos, solo genera el dendrograma.

-   **`silueta = TRUE`**: Se calcula el número óptimo de clusters utilizando el método de la **Silueta**.

Con ellos se obtiene una lista con el nombre que hemos asignado a la función ("cluster0"). Esta lista contiene los siguientes elementos:

-   **`cluster0$hclust`**: Guarda el modelo jerárquico basado en el método de Ward.
-   **`cluster0$heatmap`**: Gráfico de temperatura de la matriz de distancias euclídeas.
-   **`cluster0$dendrogram`**: Representación gráfica del dendrograma sin particionar en grupos.
-   **`cluster0$silhouette`**: Gráfico del método de la Silueta con la línea roja indicando el número de clusters aconsejado.

Para visualizar los resultados, ejecutaremos las siguientes líneas:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
cluster0$hclust         # modelo guardado
```

Que es el modelo obtenido. Un gráfico visualmente interesante es el gráfico de temperatura de la matriz de distancias euclídeas. Los tonos azulados indican grandes distancias entre casos (lo que implica esos casos que se agruparán en la parte alta del dendograma), mientras que los anaranjados y rojizos indican pequeñas distancias (lo que implica que los casos involucrados se agruparán en la parte inferior del dendograma):

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
cluster0$heatmap        # mapa de calor con distancias euclídeas
```

Cabe destacar los colores azulados asociados a las empresas *Chakotay Cargo Systems*, *Home One Cargo*, que precisamente eran las dos compañías identificadas más claramente como *outliers*.

El output principal en un análisis clúster jerárquico es el dendograma, que es un gráfico en el que se visualiza cómo los distintos casos y grupos de casos se van agrupando entre sí, hasta llegar a un único grupo o conglomerado que incluye a todos los casos. El eje vertical del *dendograma* recoge las distancias (o disimilitud) entre los casos y/o grupos previos que se van agrupando sucesivamente. La escala depende de cada método empleado. En el caso del método de *Ward*, la escala refleja la suma de cuadrados de la distancia de los casos dentro del clúster. Para obtener el dendograma, basta ejecutar:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
cluster0$dendrogram     # dendograma
```

En el dendograma obtenido se puede observar cómo las empresas *Chakotay Cargo Systems*, *Home One Cargo*, las *outliers* comentadas anteriormente, permanecen sin agruparse hasta una zona muy avanzada del proceso de agrupación, en coherencia con el gráfico de temperatura de la matriz de distancias euclídeas. En cambio, compañías como *Betazoid Transport* y *Skywalker Freight Co.* se han unido en el mismo grupo casi inmediatamente, lo que cuadra con el tono anaranjado de su intersección en la matriz de distancias.

Una cuestión importante consiste en determinar con **cuántos grupos** hemos de quedarnos. Existen algoritmos y paquetes de R que aconsejan un número (por ejemplo, la función `NbClust()` del paquete `{NbClust}`. Precisamente, la función `MATclus_Ward()` integra el *método de la silueta*, muy utilizado en este tipo de análisis.

Para cada observación $i$, la **anchura de la silueta** $s_i$ se calcula como:

$$
s_i = \frac{b_i - a_i}{\max(a_i, b_i)}
$$

Donde:

-   $a_i$ es la distancia promedio de la observación $i$ a todas las demás observaciones dentro de su propio clúster.
-   $b_i$ es la distancia promedio de la observación $i$ al clúster más cercano.

La anchura de la silueta varía entre **-1 y 1.** Un valor cercano a 1 indica que la observación está bien agrupada dentro de su clúster; un valor cercano a 0 quiere decir que la observación está en el límite entre dos clústeres; y un valor negativo implica que la observación podría estar mal clasificada en su clúster actual. De este modo, para evaluar cuál es el mejor número de conglomerados a retener:

1.  Se prueban diferentes valores de $k$ (número de clústeres).

2.  Se calcula el promedio de los valores de silueta $k$ .

3.  Se selecciona el $k$ que maximiza el **promedio de la silueta**, lo que indica que la partición es más adecuada.

De todos modos, es necesario insistir en que estos métodos no llevan a una conclusión única ni irrevocable; por lo que puede ser preferible que el **propio investigador** decida el número de grupos a crear, mediante la observación del dendograma, y de acuerdo a los objetivos de su propia investigación, tomando los métodos cuantitativos solo como **orientación**.

En nuestro caso, el gráfico del *método de la silueta* se obtiene con el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
cluster0$silhouette     # k aconsejado por método de la Silueta.
```

El método aconseja una división de los casos en 8 grupos o conglomerados diferentes. SIn embargo, parece un número excesivo si se quieren caracterizar posteriormente los grupos e incidir en sus diferencias. Un segundo número de grupos apropiado según el método es 2; pero podría ocurrir lo contrario que el caso anterior: sería un valor demasiado bajo, lo que provocaría que tan solo se distinguiese un grupo con los dos outliers de otro con los 23 casos restantes. Así, el investigador puede decidir qué número de grupos parece equilibrado y se ajusta a sus intereses. En este ejemplo, un número de grupos razonable podría ser 5, que contaría con el aval de mantener individualizadas a 2 de las empresas etiquetadas como *outliers*.

## Generación de conglomerados y caracterización.

Si se acepta esta opción, se podrá visualizar de nuevo el dendograma coloreando los grupos formados, con el código siguiente (debe modificarse el código para igualar el argumento `k =` al número de grupos seleccionado):

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Selección de k y resultados.

cluster1 <- MATclus_Ward(seleccion_sm, IDIVERSE, IFIDE, IDIG,
                         k=5, silueta = F)
```

En el código anterior:

-   **Datos de entrada**: Se usa el mismo dataframe y las mismas variables (`IDIVERSE`, `IFIDE`, `IDIG`).

-   **`k = 5`**: Se fuerza la segmentación en 5 grupos.

-   **`silueta = FALSE`**: No se realiza el análisis de la Silueta para sugerir el número óptimo de clusters.

Como resultado, se crea una lista llamada "cluster1"con los siguientes elementos:

-   **`cluster1$hclust`**: Modelo jerárquico ajustado.

-   **`cluster1$heatmap`**: Mapa de calor con distancias euclídeas.

-   **`cluster1$dendrogram`**: Representación del dendrograma con los 5 grupos resaltados.

-   **`cluster1$data_groups`**: Dataframe con las variables originales y la asignación de grupo de cada observación.

-   **`cluster1$group_summary`**: Tabla con estadísticas descriptivas de cada grupo.

-   **`cluster1$bar_patchworks`**: Conjunto de gráficos de barras mostrando la media de cada variable en cada grupo.

-   **`cluster1$scatter_patchworks`**: Conjunto de gráficos de dispersión representando todas las combinaciones de variables, coloreadas según su grupo de pertenencia.

De los elementos anteriores, es interesante volver a visualizar el dendograma, que ahora señaliza los 5 grupos generados. Para ello ejecutaremos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
cluster1$dendrogram          # dendograma
```

A partir del dendograma se aprecia que algunas compañías se agrupan rápidamente en pequeños conglomerados, mientras que otras permanecen aisladas hasta el final del proceso. Además, se confirman los *outliers* detectados en el análisis exploratorio previo, con tres compañías que quedan separadas del resto. Si nos detenemos en la dinámica del proceso de agrupación, podemos distinguir diversas fases:

-   **Primera Fase (Niveles más bajos del dendrograma):** Las compañías más similares se fusionan primero. Por ejemplo, *Hyperion Star Haulage* y *Event Horizon Haulage* forman un pequeño grupo inicial (destacado en rojo a la izquierda). Otras compañías como *Borg Freight Lines* y *Razor Star Freight* también se agrupan rápidamente en esta fase. Esto indica que estos pares de compañías tienen valores muy cercanos en fidelidad, diversificación y digitalización.

-   **Segunda Fase (Formación de conglomerados intermedios):** Grupos más pequeños comienzan a unirse en conglomerados más grandes. Por ejemplo, *Radius Logistics*, *Bio Fortuna Haulage* y *Anchor Cargo Lines* se fusionan en un subgrupo, indicando una similitud en sus índices. En el centro del dendrograma (zona azul), observamos un gran conglomerado que fusiona diversas empresas en etapas sucesivas.

-   **Tercera Fase (Unión de clústeres grandes):** Se forman tres grandes grupos centrales:

    -   Un grupo amplio que integra empresas como *Dagobah Deliveries*, *Kasriyya Freightlines* y *Tatooine Movers* (zona azul).

    -   Otro conglomerado compuesto por *Skywalker Freight Co.*, *Icarus Star Transport* y *Rebel Alliance Transport* (zona verde).

    -   Un tercer clúster más pequeño que incluye a *Tannhäuser Freight* y *IO Star Transport*, que en el análisis previo mostraron ciertas tendencias de **outliers moderados**.

-   **Cuarta Fase (Diferenciación de los outliers extremos):** En los niveles más altos del dendrograma (mayor altura), vemos que las compañías *Chakotay Cargo Systems* y *Home One Cargo* se separan completamente del resto. Su fusión ocurre **muy tarde en el proceso**, lo que confirma que son significativamente diferentes del resto de empresas en términos de los índices considerados.

Una vez obtenidos los grupos, conglomerados o clústeres, es preciso **caracterizarlos**, destacando las claves que, en promedio, los diferencian.

Uno de los resultados ofrecidos por `MATcluster_Ward()` es una tabla con el promedio de las variables por grupos. Para visualizarla, basta con ejecutar:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
cluster1$group_summary       # tabla resumen de grupos
```

La tabla presenta los **5 grupos formados** en el análisis clúster jerárquico mediante el método de Ward, mostrando el **número de empresas por grupo** y los **valores promedio** de las tres variables utilizadas en el clustering:

-   **IDIVERSE**: Índice de diversificación del negocio.

-   **IFIDE**: Índice de fidelidad de los clientes.

-   **IDIG**: Índice de digitalización de la compañía.

A continuación, analizamos cada grupo y sus diferencias clave.

### **Grupo 1: Empresas con baja diversificación y digitalización, pero fidelidad media-alta:**

-   **Tamaño**: 2 empresas.

-   **Valores promedio**:

    -   **IDIVERSE = 10.877** → Baja diversificación.

    -   **IFIDE = 28.039** → Nivel medio-bajo de fidelidad de los clientes.

    -   **IDIG = 13.206** → Baja digitalización.

-   **Interpretación**:\
    Este grupo contiene **empresas poco diversificadas**, con **baja adopción de tecnologías digitales** y **fidelidad de clientes por debajo de la media**. Posiblemente sean compañías con un modelo de negocio más tradicional o especializadas en un solo tipo de servicio.

### **Grupo 2: Empresa altamente diversificada y fidelizada:**

-   **Tamaño**: 1 empresa *(outlier identificado previamente)*.

-   **Valores promedio**:

    -   **IDIVERSE = 54.368** → Muy alta diversificación.

    -   **IFIDE = 64.260** → Alta fidelidad de clientes.

    -   **IDIG = 42.720** → Digitalización moderada-alta.

-   **Interpretación**:\
    Esta empresa es **muy distinta al resto** (posible **outlier** en el análisis previo).

    -   Tiene **altos niveles de diversificación** y fidelidad de clientes, lo que sugiere una empresa con presencia en múltiples sectores y una base de clientes consolidada.

    -   Su nivel de **digitalización** es **moderado-alto**, lo que indica que ha adoptado tecnologías, pero no es la más avanzada en este aspecto.

### **3. Grupo 3: Empresas moderadamente diversificadas y digitalizadas:**

-   **Tamaño**: 7 empresas.

-   **Valores promedio**:

    -   **IDIVERSE = 38.788** → Diversificación media-alta.

    -   **IFIDE = 39.712** → Fidelidad media.

    -   **IDIG = 24.083** → Digitalización moderada.

-   **Interpretación**:

    -   Representa **empresas con una diversificación mayor que el promedio**, lo que podría indicar modelos de negocio más versátiles.

    -   La **fidelidad de clientes está en un nivel medio**, lo que sugiere que su éxito en retención de clientes es variable.

    -   **Digitalización moderada**, lo que implica que han adoptado ciertas tecnologías, pero todavía pueden mejorar en este aspecto.

Este grupo podría representar **empresas en proceso de modernización y expansión**.

### **4. Grupo 4: El conglomerado mayoritario con baja diversificación y digitalización:**

-   **Tamaño**: 14 empresas *(el grupo más grande, más de la mitad de la muestra)*.

-   **Valores promedio**:

    -   **IDIVERSE = 17.945** → Diversificación baja.

    -   **IFIDE = 39.253** → Fidelidad media.

    -   **IDIG = 13.301** → Baja digitalización.

-   **Interpretación**:

    -   Este es el **grupo más representativo del mercado**, con **empresas de baja diversificación y digitalización**.

    -   Su **fidelidad de clientes está en el promedio**, lo que sugiere que estas empresas podrían depender de relaciones comerciales estables, pero sin estrategias avanzadas de fidelización.

    -   **Poco uso de tecnología**, lo que indica que son **empresas más tradicionales** y posiblemente menos competitivas en entornos digitales.

Este grupo podría ser el **sector convencional del transporte interestelar**, con negocios que aún dependen de prácticas tradicionales.

### **5. Grupo 5: Empresa altamente digitalizada, pero con diversificación baja-moderada:**

-   **Tamaño**: 1 empresa *(otro outlier previamente identificado)*.

-   **Valores promedio**:

    -   **IDIVERSE = 22.176** → Diversificación baja-moderada.

    -   **IFIDE = 46.776** → Fidelidad de clientes alta.

    -   **IDIG = 60.833** → Extremadamente alta digitalización.

-   **Interpretación**:

    -   Se trata de **una empresa altamente digitalizada**, con el nivel más alto de **adopción tecnológica** entre todas las compañías.

    -   Tiene **una fidelidad de clientes alta**, lo que sugiere que su **digitalización ha sido efectiva** para captar y retener clientes.

    -   **Poca diversificación**, lo que indica que, aunque su modelo de negocio está centrado en un sector específico, ha logrado una fuerte ventaja competitiva a través de la digitalización.

Esta empresa podría representar **una compañía innovadora y especializada**, con un modelo basado en tecnología para ofrecer servicios diferenciados.

En **conclusión**:

-   **Grupo 4** representa la mayoría de empresas, con un modelo tradicional y poco digitalizado.

-   **Grupo 3** se encuentra en una posición intermedia, con compañías en proceso de diversificación y modernización.

-   **Los grupos 2 y 5** son casos extremos (*outliers*), altamente diferenciados:

    -   Grupo 2 es muy diversificado y fidelizado.

    -   Grupo 5 es muy digitalizado pero menos diversificado.

-   **Grupo 1** está compuesto por empresas pequeñas con modelos simples y poca inversión en digitalización.

Otro resultado interesante para el análisis lo constituyen los gráficos de barras de comparación de los promedios por grupo de cada variable. Para visulaizar estos gráficos, ejecutaremos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
cluster1$bar_patchworks      # gráficos de barras por variable
```

Los **gráficos de barras** muestran las medias de cada variable (**IDIVERSE**, **IFIDE**, **IDIG**) en los cinco grupos obtenidos en el análisis de clústeres. Estos gráficos permiten visualizar de manera clara las diferencias entre los clústeres y confirmar las conclusiones obtenidas previamente a partir de la tabla de centroides:

### **1. Diversificación del Negocio (IDIVERSE):**

📊 **Interpretación del gráfico (arriba izquierda):**

-   **Grupo 2** es el más diversificado, con un valor superior a 50, lo que lo confirma como una empresa altamente diversificada.

-   **Grupo 3** también tiene una diversificación considerable, aunque menor que la del Grupo 2.

-   **Grupo 4 y Grupo 5** presentan una baja diversificación, situándose en torno a valores de 20.

-   **Grupo 1** es el menos diversificado, con un valor muy bajo, cercano a 10.

🔍 **Conclusión:**

-   **Grupo 2** se destaca como una empresa **altamente diversificada**, mientras que el Grupo 1 representa empresas con estrategias más especializadas.

-   La mayoría de los grupos (excepto el 2) tienen niveles de diversificación bajos o moderados.

### **2. Fidelidad de los Clientes (IFIDE):**

📊 **Interpretación del gráfico (arriba derecha):**

-   **Grupo 2** es el de mayor fidelidad, con un índice de fidelidad superior a 60.

-   **Grupo 5** también muestra un alto nivel de fidelización, superando el 45.

-   **Grupos** 3 y 4 tienen niveles intermedios, en torno a 39-40.

-   **Grupo 1** tiene la menor fidelidad de clientes, con un valor inferior a 30.

🔍 **Conclusión:**

-   **Grupo 2** confirma su posición dominante, con la mayor fidelización de clientes.

-   **Grupo 5,** a pesar de no estar muy diversificado, ha logrado fidelizar clientes a través de su estrategia altamente digitalizada.

-   **Grupo 1** se encuentra en la peor posición, con poca capacidad de retención de clientes.

### **3. Digitalización (IDIG):**

📊 **Interpretación del gráfico (abajo):**

-   **Grupo 5** es el más digitalizado, con un índice que supera 60, destacándose sobre el resto.

-   **Grupo 2** también tiene una digitalización relativamente alta, con un valor superior a 40.

-   **Grupo 3** tiene un nivel moderado de digitalización (\~24).

-   **Grupo 1 y Grupo 4** son los menos digitalizados, con valores por debajo de 15.

🔍 **Conclusión:**

-   **Grupo 5** es claramente el más avanzado digitalmente, lo que explica su alta fidelización de clientes.

-   **Grupo 2** combina alta digitalización con alta diversificación, consolidando su posición como empresa innovadora.

-   **Grupos 1 y 4** representan modelos tradicionales, con baja digitalización y probablemente estrategias de negocio más convencionales.

📌 **Conclusión General**:\
Los gráficos refuerzan lo analizado en la tabla de centroides. Se observan **estrategias de negocio claramente diferenciadas entre los grupos**, desde empresas altamente diversificadas hasta aquellas que han apostado por la digitalización como ventaja competitiva.

El último elemento de análisis creado es un conjunto de gráficos de dispersión, en los que se establecen la disposición de los casos y centros de cada grupo, diferenciados por colores, para cada combinación de variables. Al tener 3 variables, obtendremos 3 gráficos. Para visualizarlos, ejecutaremos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
cluster1$scatter_patchworks  # gráficos de dispersión variable x variable
```

Los gráficos de dispersión permiten visualizar **la distribución de los clústeres** en función de las variables utilizadas en el análisis (*IDIVERSE*, *IFIDE*, *IDIG*). En cada gráfico:

-   Los puntos representan las empresas.

-   Los colores indican los clústeres.

-   Los puntos de mayor diámetro son los centroides de cada grupo.

### **1. Relación IDIVERSE - IFIDE (Arriba izquierda):**

**Interpretación:**

-   Se observa una **relación positiva débil** entre diversificación y fidelidad de clientes.

-   **Grupo 2 (azul)** es un *outlier* con valores extremadamente altos de *IDIVERSE* y *IFIDE* (muy diversificada y con clientes muy fieles).

-   **Grupo 3 (verde)** tiene un rango medio de diversificación y fidelidad.

-   **Grupo 4 (morado)** está concentrado en niveles bajos de diversificación y con fidelidad de clientes en torno a 40.

-   **Grupo 1 (rojo)** es el menos diversificado y con baja fidelización de clientes.

-   **Grupo 5 (naranja)** es otro *outlier,* con baja diversificación pero alta fidelización.

🔍 **Conclusión**:

Los grupos 2 y 5 representan estrategias opuestas pero exitosas:

-   **Grupo 2** tiene una **diversificación extrema** que le permite fidelizar clientes.

-   **Grupo 5** tiene **baja diversificación** pero ha conseguido fidelizar clientes, probablemente por su alto nivel de digitalización.

### **2. Relación IDIVERSE - IDIG (Arriba derecha):**

**Interpretación:**

-   Existe una **tendencia positiva** entre diversificación y digitalización: **empresas más diversificadas tienden a estar más digitalizadas**.

-   **Grupo 2 (azul)** vuelve a destacar como el más diversificado y digitalizado.

-   **Grupo 5 (naranja)** es un caso excepcional, ya que tiene poca diversificación pero es la más digitalizada.

-   **Grupo 3 (verde)** tiene una diversificación moderada y digitalización media.

-   **Grupo 4 (morado)** agrupa la mayoría de empresas con baja diversificación y baja digitalización.

-   **Grupo 1 (rojo)** se mantiene con los valores más bajos en ambas variables.

🔍 **Conclusión**:

-   **Grupo 2** combina diversificación y digitalización, lo que refuerza su éxito en fidelización de clientes.

-   **Grupo 5** es un caso atípico, ya que su digitalización extrema no va acompañada de diversificación.

-   **Grupos 1 y 4** tienen estrategias tradicionales, con valores bajos en ambas variables.

### **3. Relación IFIDE - IDIG (Abajo):**

**Interpretación:**

-   La relación entre fidelización y digitalización **no es clara** en todos los grupos.

-   **Grupo 5 (naranja)** es el único que destaca por alta fidelización y máxima digitalización.

-   **Grupo 2 (azul)** también tiene alta fidelización y digitalización media-alta.

-   **Grupo 3 (verde)** se mantiene en valores intermedios.

-   **Grupo 4 (morado)** tiene fidelidad media y baja digitalización.

-   **Grupo 1 (rojo)** es el grupo con menos fidelización y baja digitalización.

🔍 **Conclusión**:

-   **Grupo 5** demuestra que la digitalización extrema puede contribuir a una mayor fidelización de clientes.

-   **Grupo 2** confirma que la combinación de diversificación y digitalización mejora la fidelización.

-   **Grupo 1** es el más desfavorecido, ya que tiene bajos valores en todas las variables.

-   **Grupo 4** representa el sector mayoritario tradicional, con fidelización media pero bajo uso de tecnología.

### **Conclusiones Generales:**

📌 **Grupo 2 (azul)** → **Diversificación extrema, fidelización alta y digitalización media-alta**. Estrategia mixta de expansión y tecnología.\
📌 **Grupo 5 (naranja)** → **Alta digitalización con poca diversificación, pero fidelización exitosa**. Modelo basado en tecnología avanzada.\
📌 **Grupo 3 (verde)** → **Posición intermedia** en todas las variables, con empresas diversificadas pero sin extremos.\
📌 **Grupo 4 (morado)** → **Mayoría de empresas tradicionales**, con fidelidad media pero poca diversificación y digitalización.\
📌 **Grupo 1 (rojo)** → **El grupo menos competitivo**, con valores bajos en todos los indicadores.

Los gráficos confirman que **las estrategias de diversificación y digitalización pueden ser clave para aumentar la fidelización de clientes**, pero existen distintos caminos para lograrlo.

## Nota final.

Antes de concluir la práctica, es importante tener en cuenta que, para realizar cualquier análisis estadístico **diferenciando por conglomerado de pertenencia**, se ha creado el *data frame* "data_groups", que está incluido en la lista "cluster1". Este data frame incluye las 3 variables del análisis, más un atributo que indica el grupo de pertenencia (variable **GRUPO**). Para utilizarlo, es aconsejable "extraerlo" de la lista, como, por ejemplo, se muestra en el siguiente código:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
cluster1$data_groups         # dataframe con grupo de pertenencia
seleccion_con_grupos <- cluster1$data_groups %>%
                       select(-NOMBRE)
```

<!--chapter:end:03-analisis_cluster_updated.Rmd-->

# Gráficos.

![[El valor de las imágenes.]{.smallcaps}](figuras/03%20gráficos.jpg){width="100%"}

## ![](figuras/book.svg){.hicon} El valor de las imágenes.

Una imagen vale más que mil palabras.

En el análisis de datos, las representaciones gráficas no son un adorno, sino el **lenguaje común** entre datos, analistas y decisores.

Un buen gráfico puede condensar miles de observaciones en **patrones perceptibles de un vistazo**, revelando tendencias, rupturas, relaciones y valores atípicos que las tablas suelen ocultar.

En la fase exploratoria del análisis de datos, orienta hipótesis y elecciones de modelos; en la validación comprueba supuestos y diagnostica errores; y en la comunicación transforma resultados complejos en evidencia comprensible y accionable.

Su fuerza descansa en cómo aprovechan la percepción humana —posición, longitud, forma, color— para reducir la carga cognitiva y evitar interpretaciones equívocas. Por ello, dominar el diseño con intención, elegir el tipo de gráfico adecuado a cada pregunta y aplicar principios de claridad, comparabilidad y honestidad es tan crucial como cualquier técnica estadística.

Este capítulo recorre ese camino: del boceto exploratorio a la visualización final que informa, persuade y, sobre todo, ayuda a decidir mejor.

## ![](figuras/pie-chart.svg){.hicon} Tidyverse para gráficos: ggplot2.

R, en su instalación básica (paquete `{base}`), cuenta con funciones destinadas a crear gráficos y, de este modo, visualizar nuestros datos a fin de generar información y extraer conclusiones de un modo sencillo.

No obstante, estas funciones, a veces, se quedan "cortas", o requieren de un complejo y/o extenso código. Esta es la razón por la que en el *Tidyverse* se incluyó un paquete específico destinado a la construcción de gráficos de un modo flexible y amigable. Recordemos que el *Tidyverse* es un conjunto de paquetes con una filosofía común, como es el uso de ciertas estructuras gramaticales, que facilitan muchas de las tareas y análisis que podrían hacerse con el lenguaje R estándar.

Este paquete destinado a la producción de gráficos es `{ggplot2}`, que proporciona unas herramientas muy flexibles para visualizar conjuntos de datos. A continuación, se expondrán los fundamentos de la sintaxis de `{ggplot2}` y se indicará cómo construir algunos de los gráficos más habituales.

Para ilustrar la creación de gráficos, vamos a suponer que trabajamos dentro de un proyecto que hemos creado previamente, de nombre "explora". Dentro de la carpeta del proyecto guardaremos el *script* llamado "ggplot2_rstars.R" y el archivo de *Microsoft® Excel®* llamado "interestelar_100.xlsx". Para decargar los ficheros, ve al final de este capítulo y pincha en los enlaces.

Si abrimos "interestelar_100.xlsx", comprobaremos que se compone de tres hojas. La primera muestra un aviso sobre el uso exclusivo que se debe dar a los datos incorporados; la segunda recoge la descripción de las variables consideradas; y la tercera (hoja "Datos") guarda los datos que debemos **importar** desde R-Studio. Estos datos se corresponden con diferentes variables económico-financieras y de diversa índole de una muestra de empresas que se dedican al transporte de mercancías interestelar.

Vamos a abrir nuestro script "ggplot2_rstars.R" con `File → Open File…` (o haciendo click en el archivo correspondiente en la ventana inferior derecha de RStudio, pestaña "Files"). Este script contiene el programa que vamos a ir ejecutando en la práctica.

Tras abrir el *script* "explora_ggplot2.R" en el editor de R-Studio, observaremos que la primera línea / instrucción es:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
## Generando gráficos con {ggplot2}

# Limpiando el Global Environment
rm(list = ls())
```

La instrucción tiene como objeto limpiar el *Global Environment* (memoria) de objetos de anteriores sesiones de trabajo.

Luego, si queremos despreocuparnos de la carga de los paquetes que utilizaremos en el script, podemos activarlos ahora:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Cargando paquetes
library(readxl)
library (ggplot2)
library(gtExtras)
library (ggExtra)
library(ggrepel)
```

Recuerda que, si alguno de los paquetes no está instalado en la máquina, dará error. Entonces, habrá que instalarlo y volver a ejecutar el bloque de código anterior.

Para importar los datos que hay en la hoja "Datos" del archivo de Microsoft® Excel® "interestelar_100.xlsx", ejecutaremos el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Importando datos desde Excel
interestelar_100 <- read_excel("interestelar_100.xlsx",
                               sheet = "Datos",
                               na = c("n.d."))
interestelar_100 <- data.frame(interestelar_100, row.names = 1)
```

Podemos observar cómo en el *Environment* ya aparece un *objeto*data frame\* denominado "interestelar_100", que contiene 104 filas (una por empresa) y 28 variables. Podemos listar las variables de modo visual con la función `gt_plt_summary()` del paquete `{gtExtras}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# visualizando el data frame de modo elegante con {gtExtras}
datos_df_graph <- gt_plt_summary(interestelar_100)
datos_df_graph
```

## ![](figuras/pie-chart.svg){.hicon} Gráficos de una variable: histogramas, gráficos de densidad, gráficos de caja o *boxplots*.

La primera instrucción para crear un gráfico con el paquete `{ggplot2}` es `ggplot()`. A continuación, entre paréntesis, se deberán aportar una serie de argumentos o informaciones. Estas informaciones irán definiendo el gráfico en mayor o menor detalle.

En realidad, lo que se hace es definir el conjunto de datos a representar (que suelen estar contenidos en un *data frame*, o en varios), y a partir de ellos se van añadiendo capas gráficas o ***geoms**,* que son caracterizadas con ciertos atributos estéticos *(aesthetics, o* *aes*).

### ![](figuras/key.svg){.hicon} Histograma.

Uno de los gráficos indispensables para tener una idea de la distribución de frecuencias que siguen los casos (en nuestro ejemplo, las empresas eólicas) con relación a una variable métrica es el **histograma**. Vamos a construir un histograma para la variable LIQUIDEZ, que recoge la ratio de liquidez, calculada como cociente entre el activo corriente y el pasivo corriente, y que mide la capacidad de la empresa para hacer frente a sus deudas a corto plazo con activos a corto plazo. Para algunos sectores se acepta que un valor adecuado de la ratio oscila entre 1,5 y 2 (menos de 1,5 puede indicar tensiones de caja, mientras que más de 2 puede ser un síntoma de capital ocioso).

El código será:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Histograma
ggplot(data = interestelar_100, map = aes(x = LIQUIDEZ)) +
       geom_histogram()
```

Como acabamos de decir, en primer lugar viene el comando `ggplot()`, seguido de unos paréntesis que recogen ciertas informaciones:

-   **"data ="**, seguido de la fuente que almacena los datos a graficar (en nuestro caso, el *data frame* "interestelar_100").

-   **"map ="**, o "mapping =", que define los aspectos del gráfico que dependen del valor de alguna o algunas variables (es decir, se fijan roles o "misiones" a diferentes variables para realizar el gráfico). Siempre que alguna característica del gráfico no sea "fija", sino que dependa de los valores que toma una variable, tal variable deberá ir indicada dentro de un *elemento estético* (**aes**). En el código de ejemplo, el elemento *aes* sirve para indicar que las coordenadas del eje x que toman los casos a representar, dependen de los valores de la variable LIQUIDEZ.

Para indicar que las siguientes líneas continúan con el código del gráfico, se añade al final de esta línea el símbolo "**+**".

En la segunda línea, se establece el tipo de gráfico que se va a realizar, mediante la inclusión de un elemento ***geom***. Para decir que lo que queremos construir es un histograma, el elemento *geom* será `geom_histogram()`.

El resultado del código anterior es el siguiente gráfico:

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# Histograma
ggplot(data = interestelar_100, map = aes(x = LIQUIDEZ)) +
       geom_histogram()
```

**Nota:** se puede prescindir en la línea de la función ggplot() del argumento `map =` y pasar los elementos estéticos `aes =` a las diversas capas o *geoms* que se usen. Por ejemplo, este código da el mismo resultado:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Histograma
ggplot(data = interestelar_100) +
       geom_histogram( aes(x = LIQUIDEZ))
```

Por supuesto, `{ggplot2}` permite personalizar y refinar la apariencia del gráfico. Uno de los aspectos que nos puede interesar modificar es el número de intervalos en los que queda dividido el rango de valores que puede tomar la variable ("grosor" de las barras), o *bins*. Por defecto, el número es 30. Para reducir este número de barras a 20, por ejemplo, añadiremos en la línea del *geom* el argumento `bins =`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
ggplot(data = interestelar_100, map = aes(x = LIQUIDEZ)) +
       geom_histogram(bins = 20)
```

A veces es difícil determinar el número adecuado de intervalos. Puede obtarse, por ejemplo, por el método de *Sturges*, que sugiere que este número sea calculado como el máximo entre 1 y el número k redondeado hacia arriba:

$$
k = \left\lceil \log_{2}(n) + 1 \right\rceil
$$

donde *n* es el tamaño muestral.

Así, utilizando la función `nclass.Sturges()` del paquete `{base}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Nº de bins según Sturges (con NA/Inf fuera)
nbins <- nclass.Sturges(interestelar_100$LIQUIDEZ[is.finite(interestelar_100$LIQUIDEZ)])
ggplot(data = interestelar_100, map = aes(x = LIQUIDEZ)) +
       geom_histogram(bins = nbins)
```

**Nota:** la función `is.finite()` devuelve un vector lógico indicando qué elementos son numéricos finitos:

-   **TRUE** → números reales/enteros (y complejos) finitos.

-   **FALSE** → `Inf`, `-Inf`, `NaN` y `NA`.

*Sturges* propone tan solo 8 intervalos de valores.

A continuación, vamos a modificar el color de las barras. Para el borde de estas, se utiliza el argumento `colour =`; y, para el relleno, `fill =`. Además, vamos a mejorar la presentación del gráfico añadiéndole un título y un subtítulo, y unas etiquetas en los ejes. Hay que prestar atención a los signos **"+"** incluidos para que R entienda que el código de la siguiente línea pertenece al mismo gráfico que estamos diseñando:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
ggplot(data = interestelar_100, map = aes(x = LIQUIDEZ)) +
  geom_histogram(bins = nbins, colour = "red", fill = "orange") +
  ggtitle("RATIO DE LIQUIDEZ", subtitle = "Transporte Interestelar")+
  xlab("Liquidez (ratio)") +
  ylab("Frecuencias")
```

Puede ser que nos interese diferenciar los casos según grupos preestablecidos. Por ejemplo, entre las variables de nuestro *data frame* "interestelar_100", existe una variable categórica denominada EFLO, que clasifica a las empresas, según la antigüedad media de su flota, en "ANTIGUA", "MADURA" o "RENOVADA". Lo que vamos a hacer es crear, en el mismo gráfico, un histograma del ratio de liquidez, pero para cada categoría de DIMENSION. Para ello, habrá que incluir esta variable categórica en las características estéticas del *geom*, en concreto mediante el argumento "fill =". **Es** **necesario hacerlo dentro del elemento *aes***, ya que el resultado (color de grupo de empresas) depende del valor que toma la variable EFLO para cada caso o empresa:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
ggplot(data = interestelar_100, map = aes(x = LIQUIDEZ)) +
  geom_histogram(bins = nbins, colour = "red", aes(fill = EFLO)) +
  scale_fill_brewer(palette = "Oranges") +
  ggtitle("RATIO DE LIQUIDEZ", subtitle = "Transporte Interestelar")+
  xlab("Liquidez (ratio)") +
  ylab("Frecuencias")
```

Como puede verificarse, se superponen los tres histogramas, con tres colores diferentes, dependiendo de la dimensión considerada. Además, aparece, al lado derecho del gráfico, una leyenda que detalla qué color se asocia a cada uno de los grupos de empresas. La función `scale_fill_brewer()` nos permite personalizar la paleta de colores a utilizar (para ver las paletas disponibles, podemos consultar [esta sección](https://ggplot2-book.org/scales-colour) de [@ggplot22021].

### ![](figuras/key.svg){.hicon} Gráfico de densidad.

Un gráfico parecido al histograma es el de **densidad**. Un gráfico de densidad estima la función de densidad de probabilidad empírica de la variable representada. En realidad, podemos considerarlo como un histograma "suavizado". Probemos a ejecutar este código:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
ggplot(data = interestelar_100, map = aes(x = LIQUIDEZ)) +
  geom_density(colour = "red", fill = "orange") +
  ggtitle("RATIO DE LIQUIDEZ", subtitle = "Transporte Interestelar")+
  xlab("Liquidez (ratio)") +
  ylab("Densidad")
```

En el código se observa la utilización del tipo de gráfico `geom_density()`. Además, desaparece el número de intervalos o *bins*, y se puede dotar a la función de densidad estimada de un color en su borde (`colour=`), y de un color de relleno (`fill=`).

Como en el caso del histograma, se puede crear una función de densidad estimada para cada grupo de empresas (según la variable EFLO), incluyendo la característica `colour=` y `fill=` dentro del correspondiente `aes()`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
ggplot(data = interestelar_100, map = aes(x = LIQUIDEZ)) +
  geom_density(aes(fill = EFLO), colour = "red", alpha = 0.70, ) +
  scale_fill_brewer(palette = "Oranges") +
  ggtitle("RATIO DE LIQUIDEZ", subtitle = "Transporte Interestelar") +
  xlab("Liquidez (ratio)") +
  ylab("Densidad")
```

En efecto, el argumento `fill=` ha pasado a integrarse, en el *geom*, dentro de un elemento `aes`, ya que el color de relleno va a variar dependiendo del grupo de pertenencia de la empresa (variable EFLO). Por otro lado, también se ha añadido el argumento `alpha=`. Esta información consiste en un número de 0 a 1 que gradúa el grado de transparencia / opacidad de los rellenos de las figuras (en este caso las funciones de densidad estimadas) incluidas en los gráficos.

### ![](figuras/key.svg){.hicon} Gráfico de caja o *Box-Plot*.

Un tipo muy interesante de gráfico es el de "caja" (***box-plot***), que informa de la dispersión de una variable. Fijémonos en el siguiente código:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
ggplot(data = interestelar_100, map = (aes(x = "", y = LIQUIDEZ))) +
  geom_boxplot(fill= "orange") +
  ggtitle("RATIO DE LIQUIDEZ", subtitle = "Transporte Interestelar") +
  xlab("") +
  ylab("Ratio de liquidez")
```

Puede observarse cómo en el "mapeo" se fija la variable que va a determinar las coordenadas del eje "y". Como es una variable, hay que incluirla en el "mapeo" mediante una característica `aes`. El *geom* o tipo de gráfico es `geom_boxplot()`, y en este caso no le hemos añadido ninguna característica específica. Las últimas líneas configuran los títulos del gráfico y del eje "y". El resultado de ejecutar el código es el siguiente gráfico:

```{r, eval=TRUE, echo=FALSE, message=FALSE}
ggplot(data = interestelar_100, map = (aes(x = "", y = LIQUIDEZ))) +
  geom_boxplot(fill= "orange") +
  ggtitle("RATIO DE LIQUIDEZ", subtitle = "Transporte Interestelar") +
  xlab("") +
  ylab("Ratio de liquidez")
```

**Nota:** Es necesario añadir en el eje "x" una variable "vacía", y en el nombre de dicho eje también un campo vacío, para evitar que en dicho eje salga una escala y el título "x".

El gráfico se caracteriza por una "caja" (rectángulo) central. Esta caja está limitada por el primer y tercer cuartil, luego recoge el 50% de los casos con una ratio de liquidez superior al 25% de los casos con menor ratio, y por debajo del 25% de los casos con la ratio más alta. Así, la altura de la caja es la diferencia entre los cuartiles tercero y primero, que es lo que se denomina "rango intercuartílico" (*IQR* por las siglas en inglés). La caja, a su vez, está dividida en dos zonas por una línea horizontal, que es la **mediana** de la distribución: la ratio de liquidez que divide a la muestra en dos grupos con el mismo número de elementos, uno con los casos de mayor ratio de liquidez, y otro con los casos de menor ratio.

Por encima y por debajo de la caja se disponen dos segmentos (llamados "bigotes"). Estos "bigotes" recogen los casos con valores en la variable inferiores al primer cuartil (comenzando por la base de la caja, hacia abajo), o superiores al tercer cuartil (comenzando por el techo de la caja, hacia arriba); y que están a menos de **1.5 veces** la altura de la caja. Los casos con valores de la ratio de liquidez inferiores al primer cuartil (por abajo) y superiores al tercero (por arriba), que están alejados de la caja en más de 1.5 veces la altura de esta, se indican con puntos, y se corresponden con los casos conocidos como **casos atípicos** o ***outliers***. La identificación de los *outliers* es una fase muy importante a la hora de aplicar algunas técnicas estadísticas.

En esta práctica, comprobamos cómo, en el caso de la ratio de liquidez (LIQUIDEZ), no existen *outliers*, es decir, casos que presentan ratios anormalmente elevadas o bajas.

En cambio, vamos a representar ahora la variable RES (resultado del ejercicio o beneficio neto):

```{r, eval=TRUE, echo=FALSE, message=FALSE}
ggplot(data = interestelar_100, map = (aes(x = "", y = RES))) +
  geom_boxplot(fill= "orange") +
  ggtitle("RESULTADO DEL EJERCICIO", subtitle = "Transporte Interestelar") +
  xlab("") +
  ylab("Resultado (miles de PAVOs)")
```

En esta ocasión apreciamos cómo existen 4 empresas *outliers*, es decir, compañías con resultados atípicamente elevados (superiores al tercer cuartil más 1,5 veces el rango intercuartílico del resultado del beneficio neto. Dos de las empresas superan, como beneficio neto o resultado del ejercicio, el millón de PAVOs.

`{ggplot2}` permite **integrar en el gráfico medidas estadísticas** y otros cálculos. Por ejemplo, en el box-plot se representa el valor de la mediana; pero no el de la media. Si queremos incluir el valor de la media (u otro estadístico), podemos calcularlo e integrarlo con la función `stat_summary()`, algo parecido al `summarise()` de `{dplyr}`. El argumento `fun = "mean"` indica que la medida a calcular y representar es la media aritmética, el argumento `geom = "point"` el tipo de gráfico para representar esa medida (un punto). También hay otros argumentos opcionales:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
ggplot(data = interestelar_100, map = (aes(x = "", y = RES))) +
  geom_boxplot(fill = "orange") +
  stat_summary(fun = "mean",
               geom = "point",
               size = 3,
               col = "darkblue") +
  ggtitle("RESULTADO DEL EJERCICIO", subtitle = "Transporte Interestelar") +
  xlab("") +
  ylab("Resultado (miles de PAVOs)")
```

Se aprecia cómo el valor de la *rentabilidad económica media* se ha insertado como un punto azul oscuro grueso dentro del gráfico de caja (tiene un valor algo superior a la *mediana*).

Vamos a refinar el *box-plot* anterior. Por ejemplo, quizá nos pueda interesar crear un *box-plot* para cada grupo de empresas, según el tamaño del grupo empresarial de pertenencia (atributo DIMENSION). Esto lo conseguiremos con el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
ggplot(data = interestelar_100, map = (aes(x = EFLO, y = RES))) +
  geom_boxplot(aes(fill = EFLO)) +
  stat_summary(fun = "mean",
               geom = "point",
               size = 3,
               col = "darkblue") +
  stat_summary(fun = "mean",
               geom = "line",
               col = "darkblue",
               map = (aes(group = TRUE))) +
  scale_fill_brewer(palette = "Oranges") +
  ggtitle("RESULTADO DEL EJERCICIO", subtitle = "Transporte Interestelar") +
  xlab("") +
  ylab("Resultado (miles de PAVOs)")
```

Para construir una caja por categoría de la variable EFLO, se ha incluido, en el "mapeo" de la primera línea, el eje x con la variable tal variable. Como, además, queremos que cada caja sea de un color diferente, hemos hecho que los colores de estas dependan de la variable EFLO; añadiendo en el `aes()` del `geom_boxplot` respecto a la característica `fill=` (que se refiere al color de relleno de las cajas). También se ha incluido una línea con el `scale_fill_brewer()` para que los colores de las cajas consistan en diferentes tonalidades de naranjas.

En el ejemplo, el primer bloque de `stat_summary()` consigue puntear, para cada grupo de empresas, la media de RES en dicho grupo, en color azul oscuro. Para comparar mejor estas medias, se ha procedido a unir los puntos con unos segmentos o líneas de color azul oscuro, lo que se consigue con el segundo bloque de `stat_summary()`. La última línea de ese bloque, `map = (aes(group = TRUE))`, obliga a que las líneas vayan de una media a otra de los grupos (de punto azul oscuro a punto azul oscuro), es decir, al añadir `aes(group = TRUE)` (equivalente a `aes(group = 1)`), se le pide a `ggplot()` que **ignore el agrupamiento por `EFLO` y conecte todos los promedios con una sola línea**.

Como última extensión, se ha considerado que, a veces, es conveniente tener en cuenta la posición de cada caso individual dentro del gráfico. Una opción es utilizar una capa o bloque `geom_jitter()`. Con este *geom* se dispondrán, para cada grupo, los valores individuales de la variable RES; y para que estos, en su caso, no se solapen, se situarán un poco más a la izquierda o a la derecha, de modo aleatorio. Como los *outliers* son ya casos individuales, para que no se dupliquen con los provenientes del "jitter", se indicará en el `geom_boxplot()` que, en ese bloque gráfico, no se señalen los *outliers*. Esto se conseguirá con el argumento `outlier.shape = NA`. El código, en definitiva, será:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
ggplot(data = interestelar_100, map = (aes(x = EFLO, y = RES))) +
  geom_boxplot(aes(fill = EFLO), outlier.shape = NA) +
  stat_summary(fun = "mean",
               geom = "point",
               size = 3,
               col = "darkblue") +
  stat_summary(fun = "mean",
               geom = "line",
               col = "darkblue",
               map = (aes(group = TRUE))) +
    geom_jitter(width = 0.1,
              size = 1,
              col = "darkred",
              alpha = 0.40) +
  scale_fill_brewer(palette = "Oranges") +
  ggtitle("RESULTADO DEL EJERCICIO", subtitle = "Transporte Interestelar") +
  xlab("") +
  ylab("Resultado (miles de PAVOs)")

```

Como puede observarse, el `geom_jitter()` proporciona, en cada caja, la nube de casos (empresas) individuales, en cuanto a la rentabilidad económica (incluidos los *outliers*). Las características de estos puntos (amplitud del desplazamiento lateral "aleatorio", tamaño, color, opacidad) se controlan con diversos argumentos (`width=`, `size=`, `col=`, `alpha=`).

## ![](figuras/pie-chart.svg){.hicon} Gráficos de dos variables.

### ![](figuras/key.svg){.hicon} Gráfico de dispersión o *scatterplot*.

Pasamos ahora a comentar un tipo de gráfico muy común cuando trabajamos con dos variables métricas: los **gráficos de dispersión** (o ***scatterplots***). En este tipo de gráficos, cada variable ocupa un eje (x o y), y los puntos internos al gráfico representan los diversos casos u observaciones.

Como ejemplo, vamos a crear un gráfico de dispersión que represente las empresas de transporte de mercancías interestelar en función de su ratio de liquidez (LIQUIDEZ) y del resultado del ejercicio (RES). El código es el siguiente:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
ggplot(data = interestelar_100, map = (aes(x = LIQUIDEZ, y = RES))) +
  geom_point(color = "red", size = 2, alpha = 0.7) +
  ggtitle("LIQUIDEZ vs RESULTADO DEL EJERCICIO",
          subtitle = "Transporte Interestelar") +
  xlab("Ratio de Liquidez") +
  ylab("Resultado del ejercicio (miles PAVOs)")
```

El resultado es el siguiente gráfico:

```{r, eval=TRUE, echo=FALSE, message=FALSE}
ggplot(data = interestelar_100, map = (aes(x = LIQUIDEZ, y = RES))) +
  geom_point(color = "red", size = 2, alpha = 0.7) +
  ggtitle("LIQUIDEZ vs RESULTADO DEL EJERCICIO",
          subtitle = "Transporte Interestelar") +
  xlab("Ratio de Liquidez") +
  ylab("Resultado del ejercicio (miles PAVOs)")
```

Vamos a refinar el gráfico algo más. En primer lugar, puede ser interesante **distinguir entre los tipos de empresas**, según la antigüedad de sus flotas ( variable EFLO). Para ello, podemos poner el color de los puntos en función de esa variable:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
ggplot(data = interestelar_100, map = (aes(x = LIQUIDEZ, y = RES))) +
  geom_point(aes(col = EFLO), size = 2, alpha = 0.7) +
  ggtitle("LIQUIDEZ vs RESULTADO DEL EJERCICIO",
          subtitle = "Transporte Interestelar") +
  xlab("Ratio de Liquidez") +
  ylab("Resultado del ejercicio (miles PAVOs)")
```

En los dos gráficos anteriores pueden observarse puntos (casos) candidatos a ser *outliers* particularmente para el caso de la variable RES (beneficio neto o resultado del ejercicio), como ya se pudo advertir al construir los *boxplots*.

Por otro lado, podría ser interesante complementar el gráfico con información sobre las dos variables por separado, es decir, con **información sobre las distribuciones marginales**. Existe un paquete complementario a `{ggplot2}`, llamado `{ggExtra}`, que puede ayudar fácilmente a este cometido. Para ello, hemos de activar dicho paquete con `library()` (si no se ha hecho previamente, en este *script* sí se hizo en la parte inicial). El segundo paso consistirá en **asignar** nuestro *scatterplot*, diseñado con la función `ggplot()`, a un objeto con el nombre que queramos, por ejemplo, "scatter_plus". Luego, ese objeto, que contiene nuestro gráfico, entrará como argumento en la función de `{ggExtra}` llamada `ggMarginal()`, como se muestra en el siguiente código:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
scatter_plus <- ggplot(data = interestelar_100,
                       map = (aes(x = LIQUIDEZ, y = RES))) +
  geom_point(aes(col = EFLO), size = 2, alpha = 0.7) +
  ggtitle("LIQUIDEZ vs RESULTADO DEL EJERCICIO",
          subtitle = "Transporte Interestelar") +
  xlab("Ratio de Liquidez") +
  ylab("Resultado del ejercicio (miles PAVOs)")

ggMarginal(scatter_plus, type = "histogram", groupColour = T,
           groupFill = T, position = "identity", alpha = 0.5)
```

Con el código anterior, apreciamos cómo se añaden los histogramas de cada variable, RENECO y RENFIN, en los márgenes del gráfico:

```{r, eval=TRUE, echo=FALSE, message=FALSE}
scatter_plus <- ggplot(data = interestelar_100,
                       map = (aes(x = LIQUIDEZ, y = RES))) +
  geom_point(aes(col = EFLO), size = 2, alpha = 0.7) +
  ggtitle("LIQUIDEZ vs RESULTADO DEL EJERCICIO",
          subtitle = "Transporte Interestelar") +
  xlab("Ratio de Liquidez") +
  ylab("Resultado del ejercicio (miles PAVOs)")

ggMarginal(scatter_plus, type = "histogram", groupColour = T,
           groupFill = T, position = "identity", alpha = 0.5)
```

Conviene apuntar que el argumento `position = "identity"` hace que las barras del histograma estén perfectamente alineadas con los datos del gráfico de dispersión, sin ningún tipo de desplazamiento.

Adicionalmente, los diámetros de los puntos de los diversos casos podrían contener también información, haciéndolos proporcionales a una tercera variable. Por ejemplo, podrían ser proporcionales al nivel de solvencia (variable SOLVENCIA). Para ello, ejecutaríamos el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
scatter_plus <- ggplot(data = interestelar_100,
                       map = (aes(x = LIQUIDEZ, y = RES))) +
  geom_point(aes(col = EFLO, size = SOLVENCIA), alpha = 0.7) +
  ggtitle("LIQUIDEZ vs RESULTADO DEL EJERCICIO",
          subtitle = "Transporte Interestelar") +
  xlab("Ratio de Liquidez") +
  ylab("Resultado del ejercicio (miles PAVOs)")

ggMarginal(scatter_plus, type = "histogram", groupColour = T,
           groupFill = T, position = "identity", alpha = 0.5)
```

En el código anterior, puede comprobarse que la característica `size =` pasa a incluirse en el `aes`, debido a que el diámetro de cada punto ya no va a ser un parámetro fijo, sino que va a depender de la magnitud de la variable SOLVENCIA.

Finalmente, podría ser útil, en algunos gráficos, añadir una etiqueta (*label*) a cada punto, para **identificar el caso concreto** al que representa. Si bien en esta práctica, el elevado número de casos y el extenso nombre de las empresas hacen poco claro el uso de estas etiquetas, vamos a añadirlas por motivos pedagógicos. Para ello, se añadirá un bloque *geom* llamado `geom_text()`, con una información `label =` que se hace depender de valores que cambian (en concreto, el nombre de los casos, es decir, de las filas del *data frame*), por lo que tendrá que integrarse en una característica `aes`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
scatter_plus <- ggplot(data = interestelar_100,
                       map = (aes(x = LIQUIDEZ, y = RES))) +
  geom_point(aes(col = EFLO, size = SOLVENCIA), alpha = 0.7) +
  geom_text(aes(label=row.names(interestelar_100)),
            size=2, color="black", alpha = 0.7) +
  ggtitle("LIQUIDEZ vs RESULTADO DEL EJERCICIO",
          subtitle = "Transporte Interestelar") +
  xlab("Ratio de Liquidez") +
  ylab("Resultado del ejercicio (miles PAVOs)")

ggMarginal(scatter_plus, type = "histogram", groupColour = T,
           groupFill = T, position = "identity", alpha = 0.5)
```

Las etiquetas de los casos pueden refinarse algo más mediante la función `geom_label_repel()`, disponible al cargar el paquete `{ggrepel}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
scatter_plus <- ggplot(data = interestelar_100,
                       map = (aes(x = LIQUIDEZ, y = RES))) +
  geom_point(aes(col = EFLO, size = SOLVENCIA), alpha = 0.7) +
  geom_label_repel(aes(label = row.names(interestelar_100)),
                   size = 2,
                   color = "black",
                   alpha = 0.5,
                   max.overlaps = 20,   
                   box.padding = 0.1,   
                   point.padding = 0.05,
                   force = 3,
                   max.time = 2,
                   seed = 123) +
  ggtitle("LIQUIDEZ vs RESULTADO DEL EJERCICIO",
          subtitle = "Transporte Interestelar") +
  xlab("Ratio de Liquidez") +
  ylab("Resultado del ejercicio (miles PAVOs)")

ggMarginal(scatter_plus, type = "histogram", groupColour = T,
           groupFill = T, position = "identity", alpha = 0.5)
```

La ventaja de este gráfico, como se puede apreciar, es que se omiten las etiquetas superpuestas, si bien existe el riesgo de que se omitan una gran cantidad de estas. Los principales argumentos que utiliza la función `geom_label_repel()` para controlar la separación entre etiquetas, son:

-   `box.padding` – “Aire” alrededor de cada etiqueta.

-   `point.padding` – “Aire” extra alrededor del punto ancla.

-   `direction` – `"both"`, `"x"` o `"y"` para empujar sólo en ese eje.

-   `force` – Intensidad de repulsión entre etiquetas.

-   `force_pull` – Fuerza que tira de la etiqueta hacia su punto (evita que se vaya demasiado lejos).

-   `max.overlaps` – Nº máximo de solapes tolerados antes de descartar etiquetas (usa `Inf` para no descartar).

-   `max.time` / `max.iter` – Límite de tiempo o iteraciones del algoritmo (según versión del paquete).

-   `seed` – Semilla para reproducibilidad de la colocación.

## ![](figuras/arrow-down-circle.svg){.hicon} Materiales para realizar las prácticas del capítulo.

En esta sección se muestran los links de acceso a los diferentes materiales (*scripts*, datos...) necesarios para llevar a cabo los contenidos prácticos del capítulo.

**Datos (en formato Microsoft® Excel®:**

-   interestelar_100.xlsx ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/interestelar_100.xlsx))

**Scripts:**

-   explora_rstars.R ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/ggplot2_rstars.R))

<!--chapter:end:03-graficos.Rmd-->

# Estadística descriptiva.

![[Pilotos de Shuttlepod Movers.]{.smallcaps}](figuras/04%20pilotos.jpg){width="100%"}

La Estadística Descriptiva es la parte de la Ciencia Estadística que se ocupa de la recopilación de datos, su depuración, y la caracterización, mediante tales datos, de un conjunto de casos o individuos.

Los datos se organizan en variables y/o atributos.

Las **variables** son características de los casos o individuos en estudio que se plasman en valores que están expresados en **escala métrica**. Los **atributos** son características de los casos o individuos en estudio que se concretan en diversas categorías (si el atributo tiene **escala nominal**) o niveles (si el atributo tiene **escala ordinal**). Los atributos se denominan también variables categóricas, cualitativas o factores.

Centrándonos en las variables (características que afectan a un grupo de casos o individuos, y que se concretan en valores que poseen una escala métrica), podemos plantearnos el estudio de una única variable sin tener en cuenta la existencia de otras variables que caracterizan al mismo grupo de casos o individuos. En tal caso estaremos planteando un **análisis estadístico univariante**. Si nuestro análisis se centra en cómo dos variables caracterizan al mismo conjunto de individuos o casos, y la posible relación entre ambas, estaremos planteando un análisis bivariante. Generalizando, si estudiamos cómo un grupo de variables caracterizan de modo conjunto a un mismo grupo de casos o individuos, estaremos planteando un **análisis estadístico multivariante**.

## ![](figuras/book.svg){.hicon} Análisis univariante.

En el análisis estadístico univariante, estudiamos cómo una única característica (nos centraremos en una variable, aunque también puede tratarse de un atributo) afecta a un grupo de casos, individuos o elementos. Por ejemplo, la variable podría ser el salario percibido por un grupo de individuos que podría ser el conjunto de trabajadores en nómina en una empresa. Otro ejemplo podría ser el de la (variable) rentabilidad económica obtenida por un grupo de empresas pertenecientes a un determinado sector económico.

El conjunto de pares formado por cada valor que puede tomar la variable en estudio (o categoría o nivel, en el caso de un atributo) y el número de casos que toman tal valor se denomina **distribución de frecuencias** de la variable.

¿Cómo podemos estudiar el modo en que afecta una variable, de modo global, a un grupo de casos? Mediante el cálculo de una serie de **medidas**. Las medidas son instrumentos matemáticos que extraen y sintetizan la información contenida en una distribución de frecuencias.

Hay diferentes tipos de medidas, principalmente las de **posición**, **dispersión** y **forma**.

Antes de profundizar en las principales medidas, su significado y su obtención; mostraremos el modo de presentar en R, mediante la creación de tablas, los datos referentes a un grupo de individuos y las variables o atributos que los caracterizan, y las distribuciones de frecuencias univariantes.

Se va a utilizar, a modo de ilustración, los datos correspondientes a los salarios percibidos por una de las empresas de transporte de mercancías interestelar del proyecto *R-Stars*.

## ![](figuras/star.svg){.hicon}*Shuttlepod Movers*.

-   **Origen y Contexto:** Shuttlepod Movers nació en el planeta **Arrakis**, en plena **Gran Nube de Magallanes**, un enclave estratégico del comercio galáctico gracias a su proximidad a fuentes de minerales raros y especias. La empresa se fundó como una **Sociedad Anónima Galáctica (SAG)** con el propósito de atender rutas medianas y largas, apoyándose en la tradición logística de la zona y en la creciente demanda de transporte interestelar durante el siglo XXII. Actualmente, la mayor parte de las acciones de la compañía pertenece a varios empresarios del propio Arrakis. La actual CEO es *Tasha Tachybaptus*, que afronta el reto de la renovación de la flota sin debilitar más la liquidez de la empresa.

![[Nuevo carguero TranStar v.6.]{.smallcaps}](figuras/04%20plano%20carguero.jpg){.d-block .mx-auto width="500"}

-   **Características Operativas:**

    -   Activos: 249,39 mil PAVOs
    -   Flota: 47 cargueros espaciales
    -   Edad media de la flota: *Madura*
    -   Rutas operadas: 594
    -   Distancia global anual: 340,93 millones de años luz
    -   Distancia media por ruta: 0,30 años luz

-   **Situación Financiera:**

    -   Ingresos: 241,73 mil PAVOs
    -   Beneficio neto (RES): 180,82 mil PAVOs
    -   Margen operativo: **74,8%**
    -   Rentabilidad económica (RENECO): 72,5%
    -   Rentabilidad financiera (RENFIN): 167,5%
    -   Solvencia: 176,3%
    -   Apalancamiento: 131
    -   Liquidez: 0,56

::: callout-warning
**Atención**: Aunque la empresa presenta márgenes y rentabilidades muy elevados, la **liquidez reducida** supone un riesgo a corto plazo.
:::

-   **Capital Humano:**

    Plantilla de 49 trabajadores, organizados en áreas como almacén, gestión interna, mantenimiento y operaciones de vuelo. El tamaño de la plantilla es coherente con la escala de la compañía y el mantenimiento de 47 naves operativas.

-   **Innovación y Estrategia:**

    -   Inversión en I+D: 18,5 mil PAVOs
    -   Índice de digitalización (IDIG): 28,1
    -   Índice de diversificación (IDIVERSE): 40,7
    -   Índice de fidelización (IFIDE): 43,1

La empresa presenta una **alta diversificación de mercados** y una **fidelización sólida de clientes**, apoyándose en un grado notable de digitalización para su tamaño.

-   **Diagnóstico General:**

    -   **Fortalezas:**
        -   Márgenes de beneficio extraordinariamente altos.
        -   Excelente rentabilidad financiera y económica.
        -   Diversificación y fidelización destacadas.
    -   **Debilidades:**
        -   Liquidez reducida (riesgo en el corto plazo).
        -   Flota madura que requerirá renovación en próximos ciclos.

## ![](figuras/pie-chart.svg){.hicon} Representando datos y distribuciones de frecuencias en tablas con R.

Para aprender a representar los datos referentes a las variables y atributos que caracterizan a un grupo de casos o individuos, y las distribuciones de frecuencias univariantes, vamos a suponer que trabajamos dentro de un proyecto que hemos creado previamente, de nombre “explora”. Dentro de la carpeta del proyecto guardaremos el *script* llamado “describe_rstars.R” y el archivo de *Microsoft® Excel®* llamado “trabajadores.xlsx”. Al final del capítulo se encuentran los enlaces correspondientes a estos archivos.

El archivo de Excel*®* recoge algunos datos correspondientes a la plantilla que compone el personal de la compañía de transporte de mercancías interestelar *Shuttlepod Movers* (hoja "Datos"), tales como el salario percibido (variable **SALARIO**, expresada en cientos de PAVOs), el nivel de estudios (atributo **NESTUDIOS**) y departamento al que se pertenece (atributo **DEP**).

Las primeras líneas del script se refieren, como ya hemos visto en otras secciones del libro, a la limpieza de la memoria o environment, eliminando objetos que se hayan podido crear con anterioridad, carga de los paquetes necesarios, e importación de los datos:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Script para la construcción de tablas de datos 
# y trabajo con distribuciones de frecuencias univariantes.

# Limpiando el Global Environment
rm(list = ls())

# Cargando paquetes
library(readxl)
library(gtExtras)
library (knitr)
library (kableExtra)
library (dplyr)
library(DescTools)
library(moments)
library (ggplot2)

## DATOS

# Importando datos desde Excel
datos <- read_excel("trabajadores.xlsx",
                    sheet = "Datos",
                    na = c("n.d."))
datos <- data.frame(datos, row.names = 1)

# visualizando el data frame de modo elegante con {gtExtras}
datos_df_graph <- gt_plt_summary(datos)
datos_df_graph
```

Los datos se han almacenado en el *data frame* "datos", y hemos presentado las variables contenidas en el mismo con la función `gt_plt_summary()` del paquete `{gtExtras}` Sabemos que, simplemente escribiendo el nombre del *data frame*, aparecerán en la consola los datos almacenados en él. No obstante, esta presentación no es muy elegante para presentar los datos. Vamos a presentarlos de un modo más amigable, mediante la **confección de una "tabla"**.

Un paquete de R muy popular para generar tablas de datos es `{knitr}`. Este paquete contiene la función `kable()`, que permite generar tablas en varios formatos y con diversas características que pueden ser personalizadas (como el título de la tabla). Si queremos personalizar más aún la apariencia de nuestras tablas, podemos usar las facilidades del paquete `{kableExtra}`, que complementa las posibilidades que ofrece la función `kable()` de `{knitr}`.

Para hacer una tabla con nuestros casos y variables, es decir, para escribir nuestro *data frame* "datos" de un modo más elegante, activaremos los paquetes anteriores (si no lo hemos hecho ya), y generaremos nuestra tabla con los datos contenidos en el *data frame* "datos". El código para generar la tabla, y el resultado, es el siguiente:

```{r, eval=TRUE, echo=FALSE, message=FALSE}
tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)
```

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Tabla de datos
datos %>%
  kable(caption = "Trabajadores de Shuttlepod Movers",
        col.names = c("Trabajador", "Salario", "Nivel de estudios",
                      "Departamento")) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped", "bordered", "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(datos)), bold= F, align = "c")
```

Primero llamamos al *data frame* a partir de cuyos datos vamos a generar la tabla, "datos". Con el operador *pipe* `%>%`, ligamos los datos del *data frame* al diseño la tabla realizado con la función `kable()` de `{knitr}`. `kable()` tiene diversos argumentos, entre los que destacan:

-   **`caption =`**: Este argumento informa del título de la tabla.

-   **`col.names =`**: Este argumento, opcional, fija el nombre para las columnas de la tabla, si no queremos que aparezcan los nombres "por defecto", que son los nombres de cada columna en el propio *data frame*.

Luego, con el operador *pipe* `%>%` informamos de que vamos a completar o personalizar el diseño de esta tabla con otras funciones complementarias del paquete `{kableExtra}`. En primer lugar, utilizamos la función `kable_styling()`, que aporta algunas características adicionales a la tabla, según sus argumentos:

-   **`full_width =`** : este argumento ha de tener un valor lógico, y se refiere a si deseamos que la tabla ocupe todo el ancho del documento (TRUE) o solo lo necesario (FALSE).

-   **`bootstrap_options =`** : este argumento es de tipo alfanumérico, y sirve para fijar ciertas características estéticas complementarias. "striped" se refiere a que las filas aparezcan sombreadas de modo alternativo, "bordered" se refiere a que cada fila quede delimitada por unas finas líneas en la parte superior y en la inferior, "condensed" significa que la tabla tendrá un aspecto más compacto.

-   **`position =`** : este argumento se utiliza para situar la tabla centrada, a la izquierda del párrafo, o a la derecha.

-   **`font_size =`** : este argumento numérico se refiere al tamaño de los caracteres, lo cuál es importante a la hora de que una tabla "quepa" en un documento de deteminada anchura.

Por último, hacemos uso dos veces de la función `row_spec()` del paquete `{kableExtra}`. Esta función sirve para personalizar algo más las filas concretas de la tabla que consideremos. El encabezado se identifica como la fila "0". En el ejemplo, se ha utilizado esta función dos veces: una para el encabezado (el primer argumento de la función nos informa de las filas a las que se refiere, en esta ocasión la fila 0), y otra para el resto de filas (desde la fila 1 hasta la que contiene al último caso individuo, la fila con posición `nrow(datos)`). Los otros argumentos definen si se quiere que los caracteres aparezcan en negrita (`bold =` ) y cómo deben estar alineados los elementos, dentro de las columnas ( `align =` ).

El resultado es:

```{r, eval=TRUE, echo=FALSE, message=FALSE}
  if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
datos %>%
  kable(caption = "Trabajadores de Shuttlepod Movers",
        col.names = c("Trabajador", "Salario", "Nivel de estudios",
                      "Departamento")) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped", "bordered", "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(datos)), bold= F, align = "c")
} else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
  kable(caption = "Trabajadores de Shuttlepod Movers",
        col.names = c("Trabajador", "Salario", "Nivel de estudios",
                      "Departamento"))
} else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "pdf") {
  kable(caption = "Trabajadores de Shuttlepod Movers",
        col.names = c("Trabajador", "Salario", "Nivel de estudios",
                      "Departamento")) %>%
       kable_styling(latex_options = c("striped", "hold_position"))
}
```

A veces, puede ocurrir que solo nos interese estudiar una variable (columna del *data frame*). Además, es posible que el conjunto de casos sea muy numeroso, y que, adicionalmente, algunos de los valores de la variable que queremos estudiar estén repetidos para varios casos. Cuando esto ocurre, una opción interesante es, en lugar de representar en una tabla todos nuestros datos, **representar la distribución de frecuencias** de la variable (o atributo) que nos interesa. Es lo que vamos a hacer a continuación, tomando SALARIO como variable a analizar.

Lo primero a tener en cuenta es que, en las distribuciones de frecuencias, **los valores de la variable suelen disponerse de menor a mayor**. Para ello, previamente vamos a ordenar las filas del *data frame* "datos" según el valor que toma, en el caso correspondiente, la variable SALARIO y, si existen casos con el mismo valor de SALARIO, los ordenaremos por orden alfabético del nombre del caso (nombre del trabajador, o de la fila del *data frame*). Para realizar este reordenamiento de casos (filas) del *data frame* de un modo sencillo, vamos a utilizar la función `arrange()` del paquete `{deplyr}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Distribución de frecuencias del salario de los trabajadores de la empresa.

# Colocar los datos
datos <- datos %>% arrange(SALARIO, row.names(datos))
```

Una vez que los casos están ordenados en el data frame de menor a mayor valor de SALARIO, calcularemos, para cada valor de esta variable, el número de casos que lo poseen, es decir, su frecuencia absoluta. Para ello, vamos a crear un objeto denominado "conteo", que va a ser de clase "table", de la variable SALARIO. Todo ello lo realizamos mediante la función `table()`, que contabiliza el número de veces que aparece cada valor de la variable, como se detalla a continuación:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Contar frecuencias
conteo <- table(datos$SALARIO)
conteo
```

Al mostrar en la consola el "conteo", vemos cómo se compone de dos filas de datos. La primera se corresponde con los valores que toma la variable SALARIO en los distintos casos, y la segunda es el número de casos (**frecuencia absoluta**) que se adopta cada valor. Es decir, el objeto "tabla" es la distribución de frecuencias de la variable SALARIO.

Vamos a convertir este objeto "tabla" en un *data frame*, llamado "conteo_df", con el objeto de poder representar de un modo más elegante la distribución de frecuencias. Para ello, ejecutaremos el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Convertir el resultado a un data frame para una mejor visualización
conteo_df <- as.data.frame(conteo)
conteo_df
```

Al mostrar en la consola el *data frame* "conteo_df", observamos que consta de dos columnas o variables. Var1 recoge los valores que toma la variable SALARIO en el grupo de casos, y Freq es el conjunto de frecuencias absolutas de los diferentes valores. Para que se entienda mejor qué es cada columna, las renombraremos:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Renombrar las columnas para mayor claridad
colnames(conteo_df) <- c("Valor", "Frecuencia")
```

A continuación, vamos a calcular el resto de frecuencias que suelen calcularse para una variable. La **frecuencia total**, N, que es la suma de todas las frecuencias absolutas, es decir, el número total de casos, se puede calcular fácilmente como:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Calcular y guardar la frecuencia total
N <- sum(conteo_df$Frecuencia)
```

La serie de frecuencias absolutas acumuladas se calcularán del siguiente modo:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Calcular frecuencias absolutas acumuladas
conteo_df$Frecuencia_acum <- cumsum(conteo_df$Frecuencia)
```

Como sabemos, la última frecuencia absoluta acumulada debe coincidir con la frecuencia total. Por último, calcularemos las frecuencias relativas, que son las frecuencias absolutas divididas por la frecuencia total, y recogen la proporción de casos correspondientes al valor de la variable:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Calcular frecuencias relativas
conteo_df$Frecuencia_R <- conteo_df$Frecuencia / N

# Calcular frecuencias relativas acumuladas
conteo_df$Frecuencia_R_acum <- cumsum(conteo_df$Frecuencia_R)
```

La suma de las frecuencias relativas es siempre 1 (el 100% de los casos). Además, la última frecuencia relativa acumulada siempre es, igualmente, 1.

Ahora pasaremos a construir una tabla que recoja la distribución de frecuencias de la variable SALARIO (con los diversos tipos de frecuencias). Para ello, simplemente hemos de aplicar al *data frame* "conteo_df" las funciones `kable()` del paquete `{knitr}`, y el resto de funciones auxiliares del paquete `{kableExtra}`:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
conteo_df %>%
  kable(caption = "Distribución de frecuencias de salarios. Shuttlepod Movers",
        col.names = c("x(i) = Salario", "Frecuencia absoluta n(i)",
                      "Frecuencia absoluta acum. N(i)", "Frecuencia relativa f(i)",
                      "Frecuencia relativa acum. F(i)"),
        digits  = c(0, 0, 0, 2, 2),
        format.args = list(decimal.mark = ".", scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped", "bordered", "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(conteo_df)), bold= F, align = "c")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
conteo_df %>%
  kable(caption = "Distribución de frecuencias de salarios. Shuttlepod Movers",
        col.names = c("x(i) = Salario", "Frecuencia absoluta n(i)",
                      "Frecuencia absoluta acum. N(i)", "Frecuencia relativa f(i)",
                      "Frecuencia relativa acum. F(i)"),
        digits  = c(0, 0, 0, 2, 2),
        format.args = list(decimal.mark = ".", scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped", "bordered", "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(conteo_df)), bold= F, align = "c")
} else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
conteo_df %>%
  kable(caption = "Distribución de frecuencias de salarios. Shuttlepod Movers",
        col.names = c("x(i) = Salario", "Frecuencia absoluta n(i)",
                      "Frecuencia absoluta acum. N(i)", "Frecuencia relativa f(i)",
                      "Frecuencia relativa acum. F(i)"),
        digits  = c(0, 0, 0, 2, 2),
        format.args = list(decimal.mark = ".", scientific = FALSE))
}
```

Hemos de advertir que en la función `kable()` se han insertado dos nuevos argumentos:

-   **`digits =`** es un vector que establece, para cada columna, el número de decimales a presentar en la tabla (si es una columna con datos categóricos, se indicará con el valor NA).

-   **`format.args =`** es una "lista" que controla aspectos de formato como si los decimales se indican con un punto o una coma, o si en cifras muy grandes se debe utilizar notación científica.

En cuanto a la interpretación de la distribución, podemos apreciar que la distribución de salarios en *Shuttlepod Movers* refleja una estructura claramente concentrada en los niveles intermedios. La mayor parte de los trabajadores percibe entre 12 y 20 cientos de PAVOs, lo que supone cerca del 70% de la plantilla, mientras que los salarios bajos (8–10) y los más altos (25–30) son minoritarios. Este patrón sugiere una organización con un núcleo amplio de personal con remuneraciones medias, posiblemente operativas o técnicas, y una proporción reducida de empleados en los extremos, que podrían corresponder a personal de apoyo poco cualificado (en la base) y a mandos o especialistas (en la parte alta). Desde el punto de vista económico, la distribución es relativamente equilibrada.

Hay ocasiones en las que la cantidad de valores diferentes que toma la variable analizada para los diferentes casos es muy elevado. Esto puede deberse, por ejemplo, a que el número de casos es muy elevado, o a que la variable es de naturaleza continua, y puede tomar una gran variedad de posibles valores (incluso infinitos). En estos casos, un modo de representar la distribución de frecuencias de la variable en una tabla de dimensión reducida es **agrupando los valores en intervalos**. Esto es lo que vamos a hacer ahora con la variable SALARIO.

La primera tarea a realizar será formar los intervalos. Para ello podemos usar la función `cut()`, que permite decir el número de intervalos (de la misma amplitud) en que queremos dividir el intervalo que va desde el menor valor de la distribución (menor salario) al mayor valor (mayor salario). Vamos a controlar el número de intervalos en que deseamos dividir el rango de valores que puede tomar la variable estudiada mediante una constante ***k***. Si en lugar de decidir nosotros este número, preferimos que sea calculado por un método objetivo, como el de Sturges, ligaremos *k* al método. Por ejemplo:

```{r, eval=TRUE, echo=TRUE, message=FALSE}

# Distribución de frecuencias agrupadas en intervalos
# del salario de los trabajadores de la empresa.

# Crear los intervalos (método de Sturges)
k <- nclass.Sturges(datos$SALARIO)
datos$intervalos <- cut(datos$SALARIO, breaks = k, include.lowest = TRUE)
levels(datos$intervalos)
```

El resultado del código anterior es una nueva columna en el *data frame* "datos", llamada "intervalos", que informa, para cada caso, cuál de los intervalos calculados lo contiene. El argumento lógico `include.lowest =` se especifica para indicar que el intervalo inferior es cerrados por la izquierda. Lo usual es que, salvo este, el resto sean abiertos, es decir, que los casos que toman como valor de la variable un extremo de intervalo se contabilicen dentro del intervalo donde ese valor es el extremo superior.

La columna "intervalos" es de la clase ***factor***. Precisamente, los posibles "niveles" de ese factor son los intervalos que se han creado con `cut()`.

Las siguientes líneas de código son similares a las que vimos en el caso de distribuciones de frecuencias no agrupadas: se creará un objeto "tabla" para contabilizar el número de casos que pertenecen a cada intervalo (frecuencias absolutas), se transformará este objeto en un *data frame* para poder trabajar de un modo más fácil, y se cambiarán el nombre de las dos columnas para que se entienda mejor:

```{r, eval=TRUE, echo=TRUE, message=FALSE}

# Contar las frecuencias de cada intervalo
conteo_intervalos <- table(datos$intervalos)

# Convertir el resultado a un data frame para una mejor visualización
conteo_intervalos_df <- as.data.frame(conteo_intervalos)

# Renombrar las columnas para mayor claridad
colnames(conteo_intervalos_df) <- c("Intervalo", "Frecuencia")
```

Con todo lo anterior, se obtiene un *data frame* denominado "conteo_intervalos_df", que contiene dos columnas: la columna "Intervalo", con los intervalos calculados, y la columna "Frecuencia", con el número de casos que tienen un salario incluido dentro de cada intervalo salarial.

Antes de proceder a diseñar la tabla de presentación de la distribución de frecuencia con `kable()`, vamos a obtener, para incluir en la tabla, otras informaciones que suelen ser presentadas junto a las frecuencias absolutas de cada intervalo.

Una de estas informaciones es lo que denominamos ***marca de clase*** de un intervalo. La marca de clase de un intervalo de valores es simplemente el punto medio de dicho intervalo. La obtención en nuestro ejemplo de las marcas de clase puede resultar algo compleja, ya que hemos de recordar que los intervalos, tal y como están almacenados, son los niveles de una variable de clase f*actor*:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
marca_clase <- sapply(strsplit(as.character(conteo_intervalos_df$Intervalo),
                               ",|\\[|\\(|\\]"),
                      function(x) {mean(as.numeric(x[2:3]))})
```

Explicaremos detenidamente el código anterior:

1.  **`conteo_intervalos_df$Intervalo`**: Aquí se está accediendo a la columna "Intervalo" del *data frame* "conteo_intervalos_df".

2.  **`as.character(conteo_intervalos_df$Intervalo)`**: convierte los valores de la columna "Intervalo" a caracteres (*strings*). Esto es necesario porque la función `strsplit()` trabaja con cadenas de texto.

3.  **`strsplit(as.character(conteo_intervalos_df$Intervalo), ",|\\[|\\(|\\]")`**: `strsplit()` divide cada cadena de texto en partes usando los delimitadores especificados. En este caso, se están utilizando como delimitadores las comas ",", los corchetes "[" y "]", y el paréntesis de apertura "(". Cada separador queda "encerrado" entre una doble barra inclinada "\\\\" (o unas comillas, si es el primer separador que se pone), y una barra vertical "\|" (o unas comillas, si es el último separador que se considera). El resultado es una lista de vectores de caracteres, donde cada vector contiene las partes de la cadena original que estaban separadas por los delimitadores.

4.  **`sapply(..., function(x) { ... })`**: `sapply()` aplica una función a cada elemento de una lista y simplifica el resultado a un vector o matriz. Por otro lado, la función anónima `function(x) { ... }` se aplica a cada vector resultante de `strsplit()`.

5.  **`function(x) { mean(as.numeric(x[2:3])) }`**: Esta es la función anónima que se aplica a cada vector "x". Después, **`x[2:3]`** selecciona el segundo y tercer elemento del vector "x". Estos elementos corresponden con los límites del intervalo. **`as.numeric(x[2:3])`** convierte estos elementos a números. **`mean(as.numeric(x[2:3]))`** calcula la media de estos dos números, que representa el punto medio del intervalo.

6.  **`marca_clase <- ...`**: Finalmente, el resultado de `sapply()` se asigna al vector"marca_clase", que contendrá los puntos medios de los 4 intervalos.

El resto de código integra el vector "marca_clase" en el *data frame* "conteo_intervalo_df" como una variable más, reodena con la función select() del paquete {dplyr} el orden de las columnas del *data frame*, calcula el resto de frecuencias (absoluta acumulada, relativa, relativa acumulada), y diseña la tabla de presentación de la distribución de frecuencias de los salarios de los trabajadores de la empresa; pero agrupada en los intervalos de valores que se fijaron con la constante *k*:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Agregar la columna "marca_clase" al data frame
conteo_intervalos_df$marca_clase <- marca_clase

#Cambiar el orden de las columnas en el data frame con dplyr
conteo_intervalos_df <- conteo_intervalos_df %>% select(Intervalo, marca_clase, Frecuencia)

# Calcular y guardar la frecuencia total
N_agre <- sum(conteo_intervalos_df$Frecuencia)

# Calcular frecuencias absolutas acumuladas
conteo_intervalos_df$Frecuencia_acum <- cumsum(conteo_intervalos_df$Frecuencia)

# Calcular frecuencias relativas
conteo_intervalos_df$Frecuencia_R <- conteo_intervalos_df$Frecuencia / N_agre

# Calcular frecuencias relativas acumuladas
conteo_intervalos_df$Frecuencia_R_acum <- cumsum(conteo_intervalos_df$Frecuencia_R)

# Mostrar el resultado
conteo_intervalos_df %>%
  kable(caption = "Distribución de frecuencias agrupadas en intervalos de salarios. Shuttlepod Movers",
        col.names = c("Intervalo salarial",
                      "Marca de clase x(i)",
                      "Frecuencia absoluta n(i)",
                      "Frecuencia absoluta acum. N(i)",
                      "Frecuencia relativa f(i)",
                      "Frecuencia relativa acum. F(i)"),
        digits  = c(NA, 2, 0, 0, 2, 2),
        format.args = list(decimal.mark = ".", scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped", "bordered", "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(conteo_intervalos_df)), bold= F, align = "c")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# Agregar la columna "marca_clase" al data frame
conteo_intervalos_df$marca_clase <- marca_clase

#Cambiar el orden de las columnas en el data frame con dplyr
conteo_intervalos_df <- conteo_intervalos_df %>% select(Intervalo, marca_clase, Frecuencia)

# Calcular y guardar la frecuencia total
N_agre <- sum(conteo_intervalos_df$Frecuencia)

# Calcular frecuencias absolutas acumuladas
conteo_intervalos_df$Frecuencia_acum <- cumsum(conteo_intervalos_df$Frecuencia)

# Calcular frecuencias relativas
conteo_intervalos_df$Frecuencia_R <- conteo_intervalos_df$Frecuencia / N_agre

# Calcular frecuencias relativas acumuladas
conteo_intervalos_df$Frecuencia_R_acum <- cumsum(conteo_intervalos_df$Frecuencia_R)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# Mostrar el resultado
if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
conteo_intervalos_df %>%
  kable(caption = "Distribución de frecuencias agrupadas en intervalos de salarios. Shuttlepod Movers",
        col.names = c("Intervalo salarial",
                      "Marca de clase x(i)",
                      "Frecuencia absoluta n(i)",
                      "Frecuencia absoluta acum. N(i)",
                      "Frecuencia relativa f(i)",
                      "Frecuencia relativa acum. F(i)"),
        digits  = c(NA, 2, 0, 0, 2, 2),
        format.args = list(decimal.mark = ".", scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped", "bordered", "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(conteo_intervalos_df)), bold= F, align = "c")
} else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
conteo_intervalos_df %>%
  kable(caption = "Distribución de frecuencias agrupadas en intervalos de salarios. Shuttlepod Movers",
        col.names = c("Intervalo salarial",
                      "Marca de clase x(i)",
                      "Frecuencia absoluta n(i)",
                      "Frecuencia absoluta acum. N(i)",
                      "Frecuencia relativa f(i)",
                      "Frecuencia relativa acum. F(i)"),
        digits  = c(NA, 2, 0, 0, 2, 2),
        format.args = list(decimal.mark = ".", scientific = FALSE))
}
```

La distribución agrupada de salarios en *Shuttlepod Movers,* expresada en cientos de PAVOs, confirma la concentración de la plantilla en niveles intermedios de ingresos. En particular, los intervalos [7.98, 11.1] y (14.3, 17.4] concentran cada uno un 22% de los trabajadores, mientras que los tramos (11.1, 14.3] y (17.4, 20.6] aportan un 16% adicional cada uno. Esto refleja que casi tres cuartas partes de la fuerza laboral percibe salarios entre 8 y 20 cientos de PAVOs, lo que configura una estructura salarial predominantemente media. Los tramos superiores muestran menor densidad: solo un 10% supera los 23.7 cientos de PAVOs, y el salario máximo (30 cientos) corresponde a un único trabajador.

## ![](figuras/book.svg){.hicon}![](figuras/pie-chart.svg){.hicon} Medidas de posición.

Las medidas de posición son instrumentos matemáticos que pretenden, mediante un único valor o muy pocos valores, **caracterizar de modo global** la distribución de frecuencias de una variable determinada.

Las medidas de posición se pueden clasificar en medidas de posición central, y en medidas de posición no central (principalmente, los llamados cuantiles).

Las principales **medidas de posición central** son: la media, la mediana y la moda. Dentro de la media, podemos distinguir la media aritmética, la geométrica y la armónica. De ellas, nos centraremos en la más común: la media aritmética.

La **media aritmética** de la distribución de frecuencias de una variable *X* se calcula como:

$$
\overline{x} = \frac{1}{N} \sum_{i=1}^{h} x_i n_i
$$

Hemos de tener en cuenta en la fórmula anterior que *N* es la frecuencia total, y *h* es el número de valores diferentes que toma la variable.

Si las frecuencias de todos los valores de la variable son 1 (distribución de frecuencias unitarias), lógicamente la media pasará a ser:

$$
\overline{x} = \frac{1}{N} \sum_{i=1}^{N} x_i
$$ En R, la función para obtener la media de una variable es `mean()`. Así, para obtener el salario medio de la variable SALARIO de los trabajadoes de la empresa, ejecutaremos el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
## MEDIDAS

# Media aritmética.
media <- mean(datos$SALARIO)
media
```

Como podemos observar, el salario medio de los trabajadores de la empresa, recogido en el valor "media", es de 16.04 cientos de PAVOs, es decir, 1604 PAVOs.

¿Qué significado tiene la media aritmética? La media aritmética es el "centro de gravedad" de la distribución, el punto de equilibrio, en el sentido de que, si todos los trabajadores ganaran el salario medio, no habría diferencias salariales aun cuando la "masa" salarial invertida por la empresa permanecería invariable. Es decir, **la media aritmética supone un reparto igualitario de la masa total de la variable**.

Entre sus ventajas destaca el que, para variables (escala métrica) es siempre calculable y única. Como inconvenientes, que pierde su representatividad ante la existencia de casos atípicos o *outliers*, y que no se puede calcular en el caso de trabajar con atributos, variables cualitativas o factores (escalas nominal u ordinal).

La **mediana** es el valor que se corresponde con el caso o casos que dividen a la distribución en dos grupos con el mismo número de elementos (frecuencias), siempre teniendo en cuenta que, previamente, la distribución ha sido ordenada según los valores de la variable en estudio, de menor a mayor. Si la distribución tiene frecuencia total par, los casos "frontera" entre los dos grupos en que queda dividida la distribución son dos, por lo que, si estos casos asumen valores diferentes en la variable estudiada, podría ocurrir que hubiera dos medianas diferentes. En tal circunstancia, se suele tomar, como convenio, el promedio de de ambos valores para tener una única mediana.

En R, la función para obtener la mediana de una variable es `median()`. De este modo, para obtener el salario mediano de la variable SALARIO de los trabajadoes de la empresa, ejecutaremos el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Mediana
mediana <- median(datos$SALARIO)
mediana
```

En el ejemplo de la variable SALARIO, la mediana es 15. Es decir, 15 cientos de PAVOs es el salario percibido por el caso 25, que es el trabajador que divide la distribución de frecuencias en dos grupos de 24 trabajadores: 24 que ganan un salario menor o igual que el caso 25 (menos o igual que 15 cientos de PAVOs), y otros 24 trabajadores que ganan más o lo mismo que el caso en la posición 25 (o sea, igual o más que 15 cientos de PAVOs).

Como ventajas de la mediana, contamos con que no es sensible a la existencia de casos atípicos o outliers, y que se puede calcular en el caso de atributos o factores en escala ordinal. Como desventajas, tenemos que no es necesariamente única, y que no tiene en cuenta la totalidad de los valores de la distribución.

Con **la moda** hacemos referencia al valor (o valores) que posee (o poseen) una mayor frecuencia absoluta.

En R, la moda se calcula mediante la función `Mode()` del paquete `{DescTools}`, que habremos de activar con `library()` (si aún no lo hemos hecho):

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Moda
moda <- Mode(datos$SALARIO)
moda
```

Como podemos apreciar, la moda de la distribución es 15 (un salario de 1500 PAVOs), que aparece en la distribución en 11 ocasiones (la frecuencia absoluta de ese salario es 11).

La moda puede ser calculada en atributos o factores en escala nominal. Como inconveniente principal, tenemos que no necesariamente es un valor único (existen distribuciones multimodales).

Existen otras medidas que son de posición no central, principalmente lo que llamamos **cuantiles**. La naturaleza de los cuantiles es fácil de comprender si los consideramos como una generalización de la mediana. Ya sabemos que, ordenados los valores (y por tanto, los casos que toman dichos valores) de una distribución de frecuencias de una variable de menor a mayor, la mediana es el valor (o valores, porque pueden existir dos medianas, aunque vamos a suponer que solo hay una) de la variable correspondiente al caso que divide a la distribución en dos grupos con el mismo número de frecuencias. Pues bien, si en lugar de dividir a la distribución de frecuencias en dos grupos con el mismo número de elementos, la dividimos en 4 grupos, estaremos hablando de tres valores correspondientes a los casos que delimitan a esos cuatro grupos. Estos valores serán los **cuartiles** de la distribución.

Si queremos dividir la distribución de 9 valores de la variable que toman los casos "frontera" que separan a estos 10 grupos. Esos valores serán los **deciles**. Y si queremos dividir la distribución de frecuencias en 100 grupos con el mismo número de casos o individuos, estaríamos hablando de 99 valores de la variable que toman los casos "frontera" que separan a estos 100 grupos. Esos valores serán los **percentiles**.

En R, la función para calcular los diferentes cuantiles es `quantile()`. Para calcular, por ejemplo, los cuartiles de la variable SALARIO, procederemos así:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Calcular los cuartiles
cuartiles <- quantile(datos$SALARIO, probs = c(0.25, 0.5, 0.75))
cuartiles
```

El argumento `probs =` informa de la proporción de los casos que han de quedar por detrás (con valores menores o iguales) de cada uno de los casos que hacen de "frontera" entre los grupos. En el caso de los cuartiles, estos son 0.25, 0.5 (este cuartil es, a su vez, la mediana de la distribución) y 0.75. Vemos cómo los cuartiles son 12, 15 y 20.

## ![](figuras/book.svg){.hicon}![](figuras/pie-chart.svg){.hicon} Medidas de dispersión o variabilidad.

Las medidas de dispersión cuantifican **lo cerca o lejos que, en general, los valores asumidos por los casos de una distribución de frecuencias se hallan respecto a una medida de posición central**. Si la medida de dispersión toma un valor muy elevado, querrá decir que la medida de posición central no representa bien a la distribución de frecuencias, ya que, en general, los casos toman valores alejados de dicha medida.

La medida de posición central a la que suelen hacer referencia las medidas de dispersión es la media aritmética.

Existen múltiples medidas de dispersión, que principalmente se dividen en **medidas absolutas** (que se expresan en ciertas unidades, como por ejemplo euros, o euros al cuadrado) y **medidas relativas** (que carecen de unidades y siven, por tanto, para comparar la dispersión entre distribuciones de frecuencias expresadas en distintas unidades).

La medida de dispersión absoluta más utilizada es la **varianza**, cuya fórmula es:

$$
S^2 = \frac{1}{N} \sum_{i=1}^{h} (x_i - \overline{x})^2 n_i
$$

Hemos de tener en cuenta en la fórmula anterior que *N* es la frecuencia total, y `h` es el número de valores diferentes que toma la variable.

Si las frecuencias de todos los valores de la variable son 1 (distribución de frecuencias unitarias), lógicamente la varianza pasará a ser:

$$
S^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \overline{x})^2
$$

En realidad, la varianza es el promedio de las diferencias que existen entre los valores que toma la variable y la media aritmética de esta, diferencias que son elevadas al cuadrado para evitar la compensación entre diferencias por los signos.

Una limitación de la varianza viene referida a que, debido al exponente del paréntesis, puede tomar valores muy elevados. Para evitar el inconveniente, una medida alternativa es la **desviación típica**, que queda definida como la raíz cuadrada positiva de la varianza:

$$
S = +\sqrt{S^2}
$$

Otra media de dispersión muy utilizada, sobre todo en *Econometría*, es la *varianza insesgada* o **cuasivarianza**, cuya fórmula es:

$$
{\overline{S}}^2 = \frac{1}{N-1} \sum_{i=1}^{h} (x_i - \overline{x})^2 n_i
$$

Como siempre, si las frecuencias de todos los valores de la variable son 1 (distribución de frecuencias unitarias), la cuasivarianza pasará a ser:

$$
{\overline{S}}^2 = \frac{1}{N-1} \sum_{i=1}^{N} (x_i - \overline{x})^2
$$

En cuanto a una medida de dispersión relativa, cabe nombrar al **coeficiente de variación de Pearson**, definido como el cociente entre la desviación típica y la media aritmética (en valor absoluto):

$$
V = \frac{S}{|\overline{x}|}
$$

El coeficiente de variación informa del número de medias aritméticas que "caben" en la desviación típica de una distribución de frecuencias. A mayor coeficiente, mayor dispersión y menor representatividad de la media aritmética con respecto a la distribución. Además, pueden compararse coeficientes de distribuciones expresadas en unidades diferentes (medida relativa).

A continuación, vamos a calcular varianza, desviación típica, cuasivarianza, y coeficiente de variación en R. Para ello, hemos de tener en cuenta que la función `var()` de R, en realidad, calcula la cuasivarianza. Para obtener la varianza, pues, hemos de realizar una corrección (en realidad, para un número de casos muy grande, ambas medidas prácticamente coinciden):

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Varianza
varianza <- var(datos$SALARIO)*(N-1)/N # recordar que la frecuencia total N ya fue calculada
varianza

# Desviación típica
desv <- varianza ^ (1/2)
desv

# Cuasivarianza
cuasivarianza <- var (datos$SALARIO)
cuasivarianza

# Coeficiente de variación
cvariacion <- desv / abs(media)
cvariacion
```

Según los resultados, puede destacarse que el coeficiente de variación indica que en la desviación típica de la variable SALARIO "cabe" el 34,4% de la media.

## ![](figuras/book.svg){.hicon}![](figuras/pie-chart.svg){.hicon} Medidas de forma.

Las medidas de forma cuantifican el grado de deformación horizontal y vertical de la representación gráfica de una distribución de frecuencias. Son de dos tipos: medidas de asimetría y medidas de apuntamiento o curtosis.

Las **medidas de asimetría** miden el grado de deformación horizontal con respecto a un “eje de simetría”, que es aquel que pasa por el valor medio de la distribución. Si suponemos que la distribución es unimodal y campaniforme, tendremos los casos que se muestran en la figura:

![[Tipos de asimetría]{.smallcaps}](figuras/asimetría.png){width="100%"}

El tipo y grado de asimetría se puede obtener mediante el **coeficiente de asimetría de Fisher**. Este coeficiente toma valor negativo si la distribución es *asimétrica negativa* (mayores frecuencias a la derecha de la media), valor positivo si la distribución es *asimétrica positiva* (mayores frecuencias a la izquierda de la media), y se acerca a 0 en caso de que la distribución sea aproximadamente *simétrica*, aunque pueden darse casos de distribuciones no simétricas con coeficiente 0. En R, se puede obtener el coeficiente de asimetría mediante la función `skewness()` del paquete `{moments}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Coeficiente de asimetría de Fisher
asimetria <- skewness(datos$SALARIO)
asimetria
```

El valor obtenido para la variable SALARIO de nuestra distribución de frecuencias de los trabajadores de la empresa es positivo, lo que indica asimetría positiva: las mayores frecuencias se localizan a la derecha de la media. Esto se puede comprobar fácilmente construyendo el histograma de la variable SALARIO y trazando una línea vertical que pase por la media salarial. Para ello usamos el paquete `{ggplot2}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
ggplot(data = datos,
       map = aes(x = SALARIO)) +
  geom_histogram(bins = k,            # k ya fue definida antes (Sturges)
                 colour = "red",
                 fill = "orange") +
  geom_vline(aes(xintercept = mean(SALARIO)),
             colour = "blue",
             linetype = "dashed",
             linewidth = 1) +
  ggtitle("SALARIO MENSUAL",
          subtitle = "trabajadores de Shuttlepod Movers")+
  xlab("Salario (cientos de PAVOs)") +
  ylab("Frecuencias")
```

Como comprobamos, la distribución es claramente asimétrica positiva.

En cuanto a las **medidas de curtosis**, miden el grado de deformación vertical con respecto a una distribución “tipo”, la distribución normal. Suponemos previamente que la distribución de frecuencias estudiada es campaniforme, unimodal y simétrica (o con ligera asimetría). Pueden darse los casos que se muestran en la figura:

![[Tipos de apuntamiento o curtosis]{.smallcaps}](figuras/curtosis.png){width="100%"}

El tipo y grado de apuntamiento o curtosis se puede obtener mediante el **coeficiente de apuntamiento de Fisher**. Este coeficiente toma valor negativo si la distribución es *platicúrtica* (más aplastada que la distribución normal), valor positivo si la distribución es *leptocúrtica* (más apuntada que la distribución normal), y se acerca a 0 en caso de que la distribución sea aproximadamente igual de apuntada que la distribución normal. En R, se puede obtener el coeficiente de asimetría mediante la función `kurtosis()` del paquete `{moments}`. Hay que tener en cuenta que para que esta versión coincida con lo dicho anteriormente, al valor calculado hay que restarle el valor "3":

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Coeficiente de apuntamiento o curtosis de Fisher
curtosis <- kurtosis(datos$SALARIO) - 3
curtosis
```

Se aprecia como el coeficiente (corregido) es menor que 0, por lo que la distribución de frecuencias de la variable SALARIO es platicúrtica (más "aplastada" que la distribución de frecuencias normal). También hay que tener en cuenta que debemos prestar atención a la aplicación del coeficiente, ya que vimos con anterioridad que la distribución no es aproximadamente simétrica. Gráficamente, podemos comprobar lo anterior representando el histograma y una curva normal que posea la misma moda (que ya calculamos anteriormente). Para que sean comparables, debemos transformar el eje "y" del gráfico, pasando de "frecuencias" a "densidad", para lo cuál se incluye en el `geom_histogram()` el argumento `aes(y = ..density..)`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
ggplot(data = datos, map = aes(x = SALARIO)) +
  geom_histogram(bins = k,
                 colour = "red",
                 fill = "orange",
                 aes(y = after_stat(density))) +
  stat_function(fun = dnorm,
                args = list(mean = moda, sd = sd(datos$SALARIO)),
                colour = "darkblue",
                linewidth = 1) +
  ggtitle("SALARIO MENSUAL",
          subtitle = "trabajadores de la empresa Shuttlepod Movers")+
  xlab("Salario (cientos de PAVOs)") +
  ylab("Densidad")
```

El histograma de salarios de *Shuttlepod Movers* muestra con claridad que la mayor densidad de trabajadores se concentra en los tramos intermedios, especialmente entre los 10 y 17 cientos de PAVOs, donde se alcanza el pico de la distribución. A partir de ahí, la frecuencia va disminuyendo progresivamente hacia los intervalos superiores, lo que indica una asimetría positiva: la cola de la distribución se extiende hacia los salarios altos, aunque con muy pocos casos (apenas uno llega a 30 cientos de PAVOs).

Desde un punto de vista económico, la gráfica confirma que la empresa presenta una **estructura salarial relativamente compacta**, dominada por ingresos medios, lo que sugiere un modelo de retribución homogéneo para la mayor parte de la plantilla. Sin embargo, la escasez de salarios muy altos revela **una débil diferenciación jerárquica**, característica de empresas con poca estratificación en su organigrama. Esto puede favorecer la cohesión interna y el control de costes salariales, pero también limita los incentivos para atraer y retener perfiles altamente cualificados en un sector tecnológicamente exigente como el transporte interespacial.

## ![](figuras/arrow-down-circle.svg){.hicon} Materiales para realizar las prácticas del capítulo.

En esta sección se muestran los links de acceso a los diferentes materiales (*scripts*, datos...) necesarios para llevar a cabo los contenidos prácticos del capítulo.

**Datos (en formato Microsoft® Excel®):**

-   interestelar_100.xlsx ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/trabajadores.xlsx))

**Scripts:**

-   describe_rstars.R ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/describe_rstars.R))

<!--chapter:end:04-descriptiva.Rmd-->

# Análisis previo de datos.

![[Comandante de rogue one freight supervisando los datos de travesía.]{.smallcaps}](figuras/05%20la%20piloto.jpg){width="100%"}

## ![](figuras/book.svg){.hicon}![](figuras/pie-chart.svg){.hicon} Introducción.

Antes de la aplicación de técnicas complejas que permitan extraer de los datos conclusiones relevantes, es necesario realizar unas **tareas previas** destinadas a conseguir dos objetivos:

-   **Preparar nuestros datos** para que puedan ser procesados correctamente sin provocar distorsiones en los resultados. En especial, hay dos puntos clave: el tratamiento de los datos faltantes (***missing values***) y de los casos atípicos o ***outliers***.

-   Obtener una **visión inicial de la información** que esconden los datos, fundamentalmente en cuanto a las medidas básicas que caracterizan la distribución de frecuencias de las variables en las que se estructuran estos, y, en el caso de contar con más de una variable, de la relación estadística que existe entre ellas.

Además, es preciso tener en cuenta que, usualmente, es conveniente que estos rasgos iniciales que caracterizan a nuestra muestra o población sean plasmados de un modo **visualmente amigable**, claro y conciso.

En esta práctica, por medio de un ejemplo basado en una base de datos de las empresas dedicadas al transporte de mercancías interestelar, se mostrarán una serie de buenas prácticas y análisis básicos útiles a la hora de preparar y analizar inicialmente nuestro conjunto de datos.

Vamos a suponer que trabajamos dentro de un **proyecto** que hemos creado previamente, de nombre "explora". Dentro de la carpeta del proyecto guardaremos el *script* llamado "previo_rstars.R", y el archivo de Microsoft® Excel® llamado "interestelar_100.xlsx". Para decargar los ficheros, ve al final de este capítulo y pincha en los enlaces.

Si abrimos "interestelar_100.xlsx", comprobaremos que se compone de tres hojas. La primera muestra un aviso sobre el uso exclusivo que se debe dar a los datos incorporados; la segunda recoge la descripción de las variables consideradas; y la tercera (hoja "Datos") guarda los datos que debemos **importar** desde R-Studio. Estos datos se corresponden con diferentes variables económico-financieras y de diversa índole de una muestra de empresas que se dedican al transporte de mercancías interestelar.

Es muy importante observar que, en la hoja de cálculo, existen variables con datos faltantes (*missing values*). En concreto, podemos identificar estas faltas de dato por la presencia de celdas en blanco; pero también por la existencia de celdas con el texto, por ejemplo, "n.d." (no dato). Así, es clave identificar el modo en que quedan recogidos los datos faltantes en la hoja de cálculo, ya que **tendremos que aplicar código adicional** en el comando de importación de R **para que estos casos queden correctamente recogidos como *NAs*** (*not available*).

Cerraremos el archivo de Microsoft® Excel®, "interestelar_100.xlsx" y volveremos a RStudio. Después, abriremos nuestro *script* "previo_rstars.R" con `File → Open File…` Este *script* contiene el código que vamos a ir ejecutando en la práctica.

La primera línea / instrucción en el *script* es:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
## Tratamiento y análisis previo de datos.

# Limpiando el Global Environment
rm(list = ls())
```

La instrucción tiene como objeto limpiar el *Global Environment* (memoria) de objetos de anteriores sesiones de trabajo.

Luego, si queremos despreocuparnos de la carga de los paquetes que utilizaremos en el script, podemos activarlos ahora:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Cargando paquetes
library (readxl)
library (gtExtras)
library (dplyr)
library (visdat)
library (ggplot2)
library (knitr)
library (kableExtra)
library (moments) # paquete necesario para calcular la curtosis.
library (patchwork)
library (GGally)
```

Para importar los datos que hay en la hoja "Datos" del archivo de Microsoft® Excel® y hacer una primera inspección de las variables que contiene, ejecutaremos el código:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
## DATOS

# Importando datos desde Excel
interestelar_100 <- read_excel("interestelar_100.xlsx",
                               sheet = "Datos",
                               na = c("n.d."))
interestelar_100 <- data.frame(interestelar_100, row.names = 1)

# visualizando el data frame de modo elegante con {gtExtras}
datos_df_graph <- gt_plt_summary(interestelar_100)
datos_df_graph
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
## DATOS

# Importando datos desde Excel
interestelar_100 <- read_excel("interestelar_100.xlsx",
                               sheet = "Datos",
                               na = c("n.d."))
interestelar_100 <- data.frame(interestelar_100, row.names = 1)
```

Por defecto, R considera las celdas en blanco de la hoja de cálculo como NAs; pero hemos de advertirle del resto de posibilidades que existen en la hoja para comunicar que falta un dato determinado, como ya se ha comentado. Para ello, hemos añadido en la función `read_excel()` el argumento `na =`, que recoge los contenidos de celda de la hoja de cálculo que indican que falta el dato en cuestión.

Por otro lado, R ha tomado la primera columna como una variable categórica. En realidad, esta columna no es una variable, sino que está formada por los nombres de los diferentes casos u observaciones. Para evitar que R vea la columna de los nombres de los casos como una variable más, hemos redefinido nuestro *data frame* diciéndole que tome esa primera columna como el conjunto de los *nombres* de los casos (filas).

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# visualizando el data frame de modo elegante con {gtExtras}
datos_df_graph <- gt_plt_summary(interestelar_100)
datos_df_graph
```

En la "tabla" de variables obtenida, se comprueba que 24 variables son métricas, mientras que 4 son categóricas.

## ![](figuras/book.svg){.hicon}![](figuras/pie-chart.svg){.hicon} Análisis de una variable.

### ![](figuras/key.svg){.hicon} Buscando *missing values* y *outliers*.

Vamos a suponer que la variable que queremos estudiar es la variable *Rentabilidad Económica* (RENECO). Es una buena práctica el trabajar con una copia del *data frame* original. Con ello, conseguimos mantener la integridad de este *data frame*, y que el código sea fácilmente utilizable en otros scripts, al utilizar en la copia un nombre genérico. Utilizaremos la función `select()` con el argumento `everything()` del paquete `{dplyr}`:

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, message=FALSE}
# Copia de df
muestra<- select(interestelar_100, everything())
```

La primera acción que debe realizarse es comprobar que todos los casos (empresas) tienen su correspondiente dato o valor para la variable (RENECO), es decir, que no existen **valores perdidos o *missing values***.

Para tener una idea general, se puede utilizar la función `vis_miss()` del paquete `{visdat}`, que localizará gráficamente los *missing values* de las diferentes variables, y calculará el porcentaje de casos que supone, con respecto al total de observaciones. Para limitar el análisis solo a la variable RENECO, filtraremos de *data frame* con la función `select()` del paquete `{dplyr}`:

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, message=FALSE}
## Analisis de una variable.

# Localizando missing values.
muestra %>%
  select (RENECO) %>%
  vis_miss() +
    labs(title = "Rentabilidad Económica: valores ausentes",
      subtitle = "Transporte de mercancías interestelar",
      y = "Observación",
      fill = NULL) +
    scale_fill_manual(
      values = c("TRUE" = "red", "FALSE" = "grey"),
      labels = c("TRUE" = "NA", "FALSE" = "Presente")) +
  theme(
    plot.title = element_text(face = "bold", size = 14))
```

La explicación detallada del código es la siguiente:

-   **`muestra %>%:`** Usa el *pipe* de **dplyr** para encadenar operaciones de forma legible.

-   **`select(RENECO):`** Te quedas solo con la columna `RENECO`. Así, el gráfico se centra en esa variable (una sola columna en la visualización).

-   **`vis_miss():`** Función del paquete **visdat** que crea un mapa de celdas **Present/Missing**:

    -   Eje **Y** = observaciones (filas del data frame).

    -   Eje **X** = variables (aquí, solo `RENECO`).

    -   Cada celda indica si el valor está presente o es NA.

-   **`labs(...):`**

    -   `title` y `subtitle`: títulos del gráfico.

    -   `y = "Observación"`: etiqueta del eje Y.

    -   `fill = NULL`: **quita el título de la leyenda** (por defecto aparecería algo tipo “valueType” o similar).

-   **`scale_fill_manual(...):`** Personaliza los colores y las etiquetas de la leyenda:

    -   `values = c("TRUE" = "red", "FALSE" = "grey"):` En `vis_miss`, internamente se codifica si falta el dato (`TRUE`) o no (`FALSE`). Aquí dices: rojo para `TRUE` (hay NA) y gris para `FALSE` (dato presente).

    -   `labels = c("TRUE" = "NA", "FALSE" = "Presente"):` Cambias el texto mostrado en la leyenda: en lugar de `TRUE/FALSE`, se verá NA/Presente.

-   **`theme(plot.title = element_text(face = "bold", size = 14)):`** Pone el título en negrita y con tamaño 14.

Puede observarse cómo, en el caso concreto de la variable RENECO, un 1% (aproximadamente) de los casos no tienen dato (es decir, uno de los 104 casos). Para localizar el caso concreto, recurriremos a las herramientas de manejo de *data frames* del paquete `{dplyr}`. En concreto, filtraremos los casos para detectar aquellos que carecen de valor en la variable RENECO:

```{r eval=FALSE, echo=TRUE, message=FALSE}
# Mostrar casos concretos de NAs
muestra %>% filter(is.na(RENECO)) %>% select(RENECO)
```

La función `is.na()` comprueba si, en la posición correspondiente a una fila o caso, para la variable escrita en el argumento; hay o no un dato o valor. Como resultado se obtiene una empresa, para la que se puede corroborar que no hay valor para la variable RENECO:

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# Hacer copia del df y mostrar casos concretos de NAs
muestra %>% filter(is.na(RENECO)) %>% select(RENECO)
```

Comprobamos que la empresa sin valor en RENECO es *Photon Pack Freight*.

Ante la existencia de *missing values*, se puede actuar de varios modos. Por ejemplo, se puede intentar obtener por otro canal de información el conjunto de valores de RENECO que no están disponibles, o recurrir a alguna estimación para los mismos y asignarlos. En caso de que esto sea difícil, se puede optar, simplemente, por eliminar estos casos, en especial cuando representan un porcentaje muy reducido respecto al total. En nuestro ejemplo, vamos a suponer que hemos optado por esta última vía, al no conseguir unos valores más o menos verosímiles de RENECO para las empresas de las que se carece de dato. Esta **eliminación de casos** se podrá realizar mediante el código:

```{r, eval=TRUE, echo=TRUE}
muestra <- muestra %>% filter(! is.na(RENECO))
```

El operador **`!`** significa "no".

Podemos comprobar cómo en el *Global Environment* aparece el *data frame* "muestra" con un caso menos (103).

Una vez tratados los casos con valores perdidos o *missing values*, **conviene detectar la posible presencia en la muestra de *outliers*** o casos atípicos, que pudieran desvirtuar los resultados derivados de ciertos análisis. Al trabajar con una sola variable métrica (la rentabilidad económica, RENECO), podemos realizar esta tarea representando gráficamente la variable mediante un ***boxplot*** o *gráfico de caja*. Aplicaremos, por ejemplo, el código siguiente, que utiliza la gramática del paquete `{ggplot2}`:

```{r eval=FALSE, echo=TRUE, message=FALSE}
## Localizando outliers
ggplot(data = muestra, map = (aes(y = RENECO))) +
    geom_boxplot(fill = "orange") +
    ggtitle("Rentabilidad Económica",
            subtitle = "Empresas de transporte interestelar") +
            ylab("Rentabilidad Económica (%)") +
    theme(plot.title = element_text(face = "bold", size = 14))
```

Obteniéndose el gráfico:

```{r eval=TRUE, echo=FALSE, message=FALSE}
## Localizando outliers
ggplot(data = muestra, map = (aes(y = RENECO))) +
    geom_boxplot(fill = "orange") +
    ggtitle("Rentabilidad Económica",
            subtitle = "Empresas de transporte interestelar") +
            ylab("Rentabilidad Económica (%)") +
    theme(plot.title = element_text(face = "bold", size = 14))
```

La "caja" contiene el 50% de los valores de la variable que toman los casos centrales (los que van del primer cuartil al tercero, cuya diferencia se llama *rango intercuartílico*), y contiene una línea horizontal que es la mediana (segundo cuartil). Por arriba sobresale un segmento que llega al mayor valor de la variable que toma algún caso y que no llega a ser atípico; y por debajo de la caja otro segmento que llega al menor valor de la variable que toma algún caso y que no llega a ser atípico. Los casos atípicos o *outliers* son aquellos que toman valores que se alejan más de 1.5 veces del rango intercuartílico (altura de la caja) del tercer cuartil, por arriba; o del primer cuartil, por abajo. Se registran mediante puntos.

En nuestro caso, el *boxplot* ratifica la existencia de dos casos atípicos. Para identificar esos dos casos concretos, podemos recurrir al paquete `{dplyr}`, y **establecer un filtro** con el siguiente código:

```{r eval=FALSE, echo=TRUE, message=FALSE}
# Mostrar casos concretos de outliers
Q1 <- quantile (muestra$RENECO, c(0.25))
Q3 <- quantile (muestra$RENECO, c(0.75))

muestra %>% 
  filter(RENECO > Q3 + 1.5*IQR(RENECO) |
         RENECO < Q1 - 1.5*IQR(RENECO)) %>%
  select(RENECO)
```

En el código anterior, las dos primeras filas calculan los cuartiles primero (Q1) y tercero (Q3) mediante la función `quantile()`. En esta función, es preciso poner como segundo argumento la proporción de casos que van a quedar por debajo del "cuantil" en cuestión (por ejemplo, el primer cuartil se calcula poniendo 0.25, dado que deja por debajo al 25% de casos con menor valor en la variable). Luego se filtran los *outliers* mediante la función `filter()` de `{dplyr}` , calculados como aquellos casos con valores de RENECO mayores que Q3 más 1,5 veces el rango intercuartílico de la variable; o menores que Q1 menos 1,5 veces dicho rango intercuartílico. Para calcular el rango intercuartílico se recurre a la función `IQR()`. Finalmente, con `select()`, se muestran los casos en la consola de R-Studio:

```{r eval=TRUE, echo=FALSE, message=FALSE}
# Mostrar casos concretos de outliers
Q1 <- quantile (muestra$RENECO, c(0.25))
Q3 <- quantile (muestra$RENECO, c(0.75))

muestra %>% 
  filter(RENECO > Q3 + 1.5*IQR(RENECO) |
         RENECO < Q1 - 1.5*IQR(RENECO)) %>%
  select(RENECO)
```

Los casos atípicos o *outliers* en la variable RENECO son *Jovian Logistics* y *Sandworm Freight*.

Como ocurría con los *missing values*, el **tratamiento de los *outliers*** depende de la información que se tenga, y del enfoque que se quiera seguir en la aplicación de las diferentes técnicas, existiendo varias alternativas (corrección del dato, estimación, eliminación del caso, mantenimiento del caso tal y como está y utilización de *técnicas robustas*, etc.) Es un tema que a veces resulta **complejo**, y que no está exento de divergencias en la opinión de los investigadores.

Por tanto, una opción posible es, si no se tiene información fiable y los *outliers* no representan una gran proporción respecto al total de casos, la eliminación de los casos que presentan valores atípicos en las variables en estudio. En este ejemplo, efectivamente, **eliminaremos estas dos empresas con comportamiento atípico** en la rentabilidad económica (RENECO), a fin de que su presencia en la muestra **no** **distorsione los resultados en la aplicación posterior de ciertas técnicas** (por ejemplo, un ANOVA o un análisis de regresión). Podemos hacerlo creando un nuevo *data frame* a partir de "muestra"; pero sin esos dos casos. Ese nuevo *data frame* se llamará, por ejemplo, "muestra_so":

```{r eval=TRUE, echo=TRUE, message=FALSE}
# Eliminar outliers
muestra_so <- muestra %>%
              filter(RENECO <= Q3 + 1.5*IQR(RENECO) &
                     RENECO >= Q1 - 1.5*IQR(RENECO))
```

Es importante observar que, en el código de la función `filter()`, las desigualdades deben cambiar, así como el operador "**\|**" por el operador "**&**". En el *Global Environment* podemos comprobar cómo el *data frame* "muestra_so" posee el mismo número de variables que el *data frame* "muestra"; pero con dos observaciones o casos menos (101).

### ![](figuras/key.svg){.hicon} Descripción de una variable.

Una vez que se tiene preparada la base de datos, con un tratamiento adecuado de los *missing values* y de los *outliers,* y **antes** de proceder a la aplicación de una técnica adecuada según los objetivos perseguidos en el estudio; suelen presentarse una serie de **gráficos** básicos y **medidas** descriptivas que proporcionan una **idea inicial de la estructura** del sector para la variable o variables analizadas. Nos referimos a medidas y/o gráficos de posición, dispersión y forma (asimetría y curtosis).

Antes de ello, es conveniente presentar una tabla donde se recoja la distribución de frecuencias de la variable. Si son muchos los casos (en el ejemplo, 101), la distribución podría presentarse agrupada en intervalos, como ya se vio en el capítulo 4 de este libro. De este modo, el código para generar la tabla podría ser (recordar que tienen que estar activos los paquetes `{knitr}` y `{kableExtra}`):

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Tabla de datos (distribución de frecuencias agrupadas en intervalos)
# Colocando casos
muestra_so <- muestra_so %>% arrange(RENECO, row.names(muestra_so))

# Fijar k como número de intervalos (método de Sturges)
k <- nclass.Sturges(muestra_so$RENECO)

# Crear los intervalos
muestra_so$intervalos <- cut(muestra_so$RENECO, breaks = k, include.lowest = TRUE)

# Contar las frecuencias de cada intervalo
conteo_intervalos <- table(muestra_so$intervalos)

# Convertir el resultado a un data frame para una mejor visualización
conteo_intervalos_df <- as.data.frame(conteo_intervalos)

# Renombrar las columnas para mayor claridad
colnames(conteo_intervalos_df) <- c("Intervalo", "Frecuencia")

# Calcular y guardar la frecuencia total
N_agre <- sum(conteo_intervalos_df$Frecuencia)

# Calcular frecuencias absolutas acumuladas
conteo_intervalos_df$Frecuencia_acum <- cumsum(conteo_intervalos_df$Frecuencia)

# Calcular frecuencias relativas
conteo_intervalos_df$Frecuencia_R <- conteo_intervalos_df$Frecuencia / N_agre

# Calcular frecuencias relativas acumuladas
conteo_intervalos_df$Frecuencia_R_acum <- cumsum(conteo_intervalos_df$Frecuencia_R)

# Mostrar el resultado
conteo_intervalos_df %>%
  kable(caption = "Distribución de frecuencias agrupadas en intervalos de Rentabilidad Económica",
        col.names = c("Intervalo rentabilidad", "Frecuencia absoluta n(i)",
                      "Frecuencia absoluta acum. N(i)", "Frecuencia relativa f(i)",
                      "Frecuencia relativa acum. F(i)"),
        digits    = c(NA, 0, 0, 2, 2),
        format.args = list(decimal.mark = ".", scientific = FALSE)) %>%
  kable_styling(full_width = F, bootstrap_options = "striped",
                "bordered", "condensed",
                position = "center", font_size = 11) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(conteo_intervalos_df)), bold= F, align = "c")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# Tabla de datos (distribución de frecuencias agrupadas en intervalos)
# Colocando casos
muestra_so <- muestra_so %>% arrange(RENECO, row.names(muestra_so))

# Fijar k como número de intervalos (método de Sturges)
k <- nclass.Sturges(muestra_so$RENECO)

# Crear los intervalos
muestra_so$intervalos <- cut(muestra_so$RENECO, breaks = k, include.lowest = TRUE)

# Contar las frecuencias de cada intervalo
conteo_intervalos <- table(muestra_so$intervalos)

# Convertir el resultado a un data frame para una mejor visualización
conteo_intervalos_df <- as.data.frame(conteo_intervalos)

# Renombrar las columnas para mayor claridad
colnames(conteo_intervalos_df) <- c("Intervalo", "Frecuencia")

# Calcular y guardar la frecuencia total
N_agre <- sum(conteo_intervalos_df$Frecuencia)

# Calcular frecuencias absolutas acumuladas
conteo_intervalos_df$Frecuencia_acum <- cumsum(conteo_intervalos_df$Frecuencia)

# Calcular frecuencias relativas
conteo_intervalos_df$Frecuencia_R <- conteo_intervalos_df$Frecuencia / N_agre

# Calcular frecuencias relativas acumuladas
conteo_intervalos_df$Frecuencia_R_acum <- cumsum(conteo_intervalos_df$Frecuencia_R)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}

# Mostrar el resultado
if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
conteo_intervalos_df %>%
  kable(caption = "Distribución de frecuencias agrupadas en intervalos de Rentabilidad Económica",
        col.names = c("Intervalo rentabilidad", "Frecuencia absoluta n(i)",
                      "Frecuencia absoluta acum. N(i)", "Frecuencia relativa f(i)",
                      "Frecuencia relativa acum. F(i)"),
        digits    = c(NA, 0, 0, 2, 2),
        format.args = list(decimal.mark = ".", scientific = FALSE)) %>%
  kable_styling(full_width = F, bootstrap_options = "striped",
                "bordered", "condensed",
                position = "center", font_size = 11) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(conteo_intervalos_df)), bold= F, align = "c")
} else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
conteo_intervalos_df %>%
  kable(caption = "Distribución de frecuencias agrupadas en intervalos de Rentabilidad Económica",
        col.names = c("Intervalo rentabilidad", "Frecuencia absoluta n(i)",
                      "Frecuencia absoluta acum. N(i)", "Frecuencia relativa f(i)",
                      "Frecuencia relativa acum. F(i)"),
        digits    = c(NA, 0, 0, 2, 2),
        format.args = list(decimal.mark = ".", scientific = FALSE))
}
```

En conjunto, la **rentabilidad económica** de estas empresas se concentra en valores **medios–altos**: la gran mayoría se mueve en torno al 50–60%, y dos de cada cuatro intervalos centrales (47,4–53,9 y 53,9–60,3) reúnen la mayor parte de los casos. En términos prácticos, alrededor de ocho de cada diez compañías quedan entre 41% y 66,8%, lo que sugiere un sector relativamente estable, con resultados parecidos entre sí. En los extremos hay pocas: apenas un 5% presenta rentabilidades muy bajas (por debajo del 34,5%) y sólo un 2% llega a niveles muy altos (por encima del 73,2%). Económicamente, esto indica que la competencia y la eficiencia operativa tienden a “empujar” a las empresas hacia un rendimiento **razonable y sostenido**, mientras que los casos de fracaso o de éxito excepcional son **minoría**.

Por otro lado, el **análisis gráfico** suele dar una idea atractiva e intuitiva de la estructura de la distribución de frecuencias de nuestro conjunto de casos en relación con la variable a analizar.

Un gráfico fundamental es el **histograma** de la variable estudiada. Para ello, utilizaremos la gramática del paquete `{ggplot2}`:

```{r eval=TRUE, echo=TRUE, message=FALSE}
## Descriptivos básicos

# Gráficos básicos
g1 <-
ggplot(data = muestra_so, map = aes(x = RENECO)) +
  geom_histogram(bins = k,
                 colour = "red",
                 fill = "orange",
                 alpha = 0.7) +
  geom_vline(xintercept = mean(muestra_so$RENECO),
             color = "dark blue",
             linewidth = 1.2,
             alpha = 0.8) +
  ggtitle("Histograma")+
  xlab("Rentabilidad Económica (%)") +
  ylab("Frecuencias")

g1
```

En el gráfico vemos de un modo nítido la distribución de frecuencias en cuanto a la rentabilidad económica (RENECO). Se ha incorporado una línea vertical azul (mediante `geom_vline()`) para localizar la rentabilidad media. Entre otras cosas, se puede apreciar que la distribución de frecuencias es acampanada y prácticamente simétrica.

Como complemento al histograma, podemos realizar un gráfico de densidad de RENECO, al que añadiremos una curva normal con la misma media y desviación típica que nuestra distribución de frecuencias, y que se añade mediante `stat_function()` y el argumento `fun = dnorm`. Este gráfico representa la distribución de probabilidad empírica de la muestra, es una especie de histograma "suavizado". De este modo, se podrán verificar de un modo fácil algunas de las características avanzadas con la observación del histograma, como la asimetría positiva. El código es:

```{r eval=TRUE, echo=TRUE, message=FALSE}
g2 <-
ggplot(data = muestra_so, map = aes(x = RENECO)) +
  geom_density(colour = "red",
               fill = "orange",
               alpha = 0.7) +
  geom_vline(xintercept = mean(muestra_so$RENECO),
             color = "dark blue",
             linewidth = 0.8,
             alpha = 0.8) +
  stat_function(fun = dnorm, args = list(mean = mean(muestra_so$RENECO),
                                         sd = sd(muestra_so$RENECO)),
                                         geom = "area",
                                         color = "darkblue", 
                                         fill = "yellow",
                                         alpha = 0.2) +
  ggtitle("Gráfico de densidad vs curva normal")+
  xlab("Rentabilidad Económica (%)") +
  ylab("Densidad")

g2
```

La curva naranja muestra cómo se distribuye la **rentabilidad económica**: tiene forma de “campana”, con la mayoría de empresas concentradas alrededor del 50–60%. La línea vertical azul marca el valor central (media), en torno al 52%. Comparada con la curva azul de referencia (la normal), la distribución real es muy parecida, aunque algo más ancha y con un ligero peso extra en rentabilidades algo por debajo del centro; en el extremo alto hay menos casos de lo que predeciría una campana perfecta. En términos económicos: el sector muestra un rendimiento **bastante estable** y similar entre compañías, con **pocas** muy rezagadas o extraordinariamente rentables.

Un tercer gráfico útil es el ***box-plot*** una vez se eliminaron los casos *outliers*, con la incorporación de los valores que toman los casos que componen la muestra, para lo cual se utiliza el `geom_jitter`:

```{r eval=TRUE, echo=TRUE, message=FALSE}
g3 <-
ggplot(data = muestra_so, map = (aes(x = "", y = RENECO))) +
  geom_boxplot(color = "red",
               fill = "orange",
               outlier.shape = NA) +
  stat_summary(fun = "mean",
               geom = "point",
               size = 3,
               col = "darkblue") +
  geom_jitter(width = 0.1,
              size = 1,
              col = "darkred",
              alpha = 0.50) +
  ggtitle("Box-Plot") +
  ylab("Rentabilidad Económica (%)")

g3
```

El *box-plot* confirma que la rentabilidad económica está bastante concentrada en la zona central. La **mediana** ronda el **50%** (línea dentro de la caja) y la mitad de las empresas cae aproximadamente entre 46% y 59% (altura de la caja, IQR), lo que sugiere variabilidad moderada. El rombo azul (media) está muy cerca de la mediana, indicando casi simetría. Los bigotes se extienden hacia valores en torno a 35–70% y aparecen pocos *outliers* tanto por abajo (≈30%) como por arriba (hasta ≈80%). En términos económicos: la mayoría de compañías rinde de forma similar y estable alrededor del 50–60%, con casos extremos poco frecuentes.

Por otro lado, conviene tener conocimiento del valor de las principales **medidas descriptivas** (de posición, dispersión, forma) que caracterizan a la distribución de la variable a analizar. Para ello, vamos a crear un *data frame* llamado, por ejemplo, "estadisticos", que recogerá las diferentes medidas, calculadas al aplicar a la variable RENECO del *data frame* "muestra_so" la función de `{deplyr}` llamada `summarise()`. Se calcularán la media, desviación típica, valor mínimo, mediana, valor máximo, el coeficiente de asimetría de *Fisher*, y el coeficiente de apuntamiento o curtosis de *Fisher*. Precisamente, para poder calcular esta última medida, es preciso activar el paquete `{moments}`, que contiene la función `kurtosis()`. La versión del coeficiente de apuntamiento de esta función dispone como distribución perfectamente mesocúrtica el valor de 3, por lo que se le restará 3 en la versión que manejaremos para que la distribución mesocúrtica se sitúe en un coeficiente de 0. El código es:

```{r eval=TRUE, echo=TRUE, message=FALSE}
# Calcular estadísticos
estadisticos <- muestra_so %>% summarise( Media = mean(RENECO),
                                          DT = sd(RENECO),
                                          Mínimo = min(RENECO),
                                          Mediana = median(RENECO),
                                          Maximo = max(RENECO),
                                          Asimetria = skewness(RENECO),
                                          Curtosis = kurtosis(RENECO) - 3)
```

La ventaja de volcar las medidas y estadísticos en el *data frame* (de un solo caso) "estadisticos" es que se pueden mostrar los valores en una tabla elegante generada a partir de el mismo mediante las funciones `knitr()` y `kableExtra()`, como ya sabemos:

```{r eval=FALSE, echo=TRUE, message=FALSE}
# Mostrar estadisticos
estadisticos %>%
  kable(caption = "Principales Estadísticos de la Rentabilidad Económica",
        col.names = c("Media", "Mediana",
                      "Desviación Típica", "Valor mínimo",
                      "Valor Máximo", "C. Asimetría Fisher",
                      "C. Curtosis Fisher"),
        digits    = c(2, 2, 2, 2, 2, 2, 2),
        format.args = list(decimal.mark = ".", scientific = FALSE)) %>%
  kable_styling(full_width = F, bootstrap_options = "striped",
                "bordered", "condensed",
                position = "center", font_size = 11) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(estadisticos)), bold= F, align = "c")
```

```{r eval=TRUE, echo=FALSE, message=FALSE}
# Determinar el formato de salida actual
output_format <- knitr::opts_knit$get("rmarkdown.pandoc.to")

# Formatear la tabla según el formato de salida
if (output_format == "html") {
  estadisticos %>%
  kable(caption = "Principales Estadísticos de la Rentabilidad Económica",
        col.names = c("Media", "Mediana",
                      "Desviación Típica", "Valor mínimo",
                      "Valor Máximo", "C. Asimetría Fisher",
                      "C. Curtosis Fisher"),
        digits    = c(2, 2, 2, 2, 2, 2, 2),
        format.args = list(decimal.mark = ".", scientific = FALSE)) %>%
  kable_styling(full_width = F, bootstrap_options = "striped",
                "bordered", "condensed",
                position = "center", font_size = 11) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(estadisticos)), bold= F, align = "c")
} else if (output_format == "latex") {
estadisticos %>%
  kable(caption = "Principales Estadísticos de la Rentabilidad Económica",
        col.names = c("Media", "Mediana",
                      "Desviación Típica", "Valor mínimo",
                      "Valor Máximo", "C. Asimetría Fisher",
                      "C. Curtosis Fisher"),
        digits    = c(2, 2, 2, 2, 2, 2, 2),
        format.args = list(decimal.mark = ".", scientific = FALSE))
}  
```

La interpretación de estas medidas fueron comentadas en el capítulo 4. En general, puede decirse que:

-   La rentabilidad típica del sector está en torno al 51–52% (media ≈ 51,6% y mediana ≈ 50,4%), lo que indica un nivel medio-alto y sin sesgos fuertes.

-   La variabilidad es moderada (desv. típica ≈ 10 p.p.): la mayoría de empresas se mueve, grosso modo, entre \~42% y \~62%. Es un sector relativamente estable.

-   La asimetría ligeramente positiva (0,14) sugiere que hay algunas compañías con rentabilidades algo superiores a lo habitual, pero no dominan el panorama.

-   La curtosis negativa (−0,19, platicúrtica) implica una distribución más “plana” que la normal: menos extremos (tanto muy altos como muy bajos) y también algo menos de concentración en el centro. Económicamente: hay diversidad razonable de resultados, pero pocos casos extremos de fracaso o de éxito extraordinario.

-   Los extremos observados (≈ 28% mínimo y ≈ 80% máximo) existen, pero son minoría.

-   **Conclusión:** el negocio de transporte interestelar muestra una **rentabilidad sostenida y relativamente homogénea**; hay diferencias entre empresas (gestión, escala, rutas, mix tecnológico), pero el **riesgo de resultados extremos** parece **contenido**, y los “superéxitos” son poco frecuentes.

### ![](figuras/key.svg){.hicon} Normalidad.

En muchas técnicas multivariantes basadas en métodos inferenciales (por ejemplo, análisis de la varianza, o en la regresión lineal), se requiere que las variables sigan una **distribución normal**. Para comprobarlo, se puede a recurrir a análisis gráficos o a análisis formales, estos últimos basados en contrastar la hipótesis nula de normalidad.

Vamos a mostrar un **método gráfico** muy extendido. Comprobaremos la normalidad de la variable RENECO mediante un ***gráfico qq*** (cuantil-cuantil), que compara los cuantiles de nuestra muestra con los de una distribución normal teórica (con la misma media y desviación típica). Si los puntos se sitúan cercanos a la diagonal, entonces se asumirá un comportamiento (aproximadamente) normal. El código para realizar el gráfico con las herramientas del paquete `{ggplot2}` es:

```{r eval=FALSE, echo=TRUE, message=FALSE}
## Normalidad

# Grafico QQ
g4 <-
ggplot(data = muestra_so, aes(sample = RENECO)) +
  stat_qq(colour = "red") + 
  stat_qq_line(colour = "dark blue") +
  ggtitle("QQ-Plot")

g4
```

Y el resultado:

```{r eval=TRUE, echo=FALSE, message=FALSE}
## Normalidad

# Grafico QQ
g4 <-
ggplot(data = muestra_so, aes(sample = RENECO)) +
  stat_qq(colour = "red") + 
  stat_qq_line(colour = "dark blue") +
  ggtitle("QQ-Plot")

g4
```

A veces, es difícil obtener una conclusión sólida con el gráfico *qq*; aunque en el ejemplo la separación de los puntos respecto al eje diagonal induce a pensar en que podría seguirse una distribución normal.

Si queremos ser más precisos, en lugar de un análisis gráfico se puede recurrir a realizar un análisis formal, basado en la realización de **contrastes de hipótesis**. Una prueba muy usual es la **prueba de normalidad de *Shapiro y Wilk***, que tiene un buen comportamiento en muestras relativamente reducidas. En esta prueba, la hipótesis nula equivale al supuesto de normalidad. Para un 5% de significación estadística, un p-valor superior a 0.05 implicará el no-rechazo de la hipótesis de normalidad. Para realizar la prueba, se ejecutará el código:

```{r eval=FALSE, echo=TRUE, message=FALSE}
# Prueba de Shapiro-Wilk

shapiro.test(x = muestra_so$RENECO)
```

El resultado obtenido en la consola es:

```{r eval=TRUE, echo=FALSE, message=FALSE}
# Prueba de Shapiro-Wilk

shapiro.test(x = muestra_so$RENECO)
```

Como el p-valor es (muy) superior a 0.05, **no se rechaza la hipótesis nula de normalidad en la distribución**, lo que implica que, para una significación estadística del 5%, la muestra apoya la hipótesis de que RENECO sigue un comportamiento normal, como ya se anticipó con el gráfico *qq*.

### ![](figuras/paperclip.svg){.hicon} Resumen: 4 gráficos básicos en la descripción de una variable.

En definitiva, para describir de un modo inicial una distribución de frecuencias de una variable (en escala métrica), podrían analizarse los 4 gráficos que se han comentado anteriormente. Estos gráficos se pueden presentar conjuntamente, para ahorrar espacio en un informe, utilizando el paquete `{patchwork}`, que permite combinar e integrar en una sola imagen varios gráficos generados con `{ggplot2}`. En nuestro ejemplo, vamos a generar una figura que integra los 4 gráficos anteriores (que hemos denominado "g1", "g2", "g3" y "g4"). Para ello creamos el objeto "resumen", que integra los gráficos, mediante una asignación con la sintaxis del paquete `{patchwork}`: El operador `/` indica que los gráficos siguientes se dispondrán inmediatamente debajo; mientras que `|` indica que el gráfico siguiente se dispone al lado del anterior. Luego, se le añade también un título y un subtítulo personalizados:

```{r eval=TRUE, echo=TRUE, message=FALSE}
## Resumen gráfico

resumen <- (g1 | g2)/(g3 | g4)
resumen <- resumen + 
  plot_annotation(
    title = "Rentabilidad Económica",
    subtitle = "Empresas TMI (sin outliers)",
    theme = theme(
      # TÍTULO de la composición
      plot.title = element_text(
        size = 16,          # tamaño
        face = "bold",      # negrita
      ),
      # SUBTÍTULO de la composición
      plot.subtitle = element_text(
        size = 12
      )))
resumen
```

La composición gráfica se guarda en el *Global Environment* con el nombre "resumen". El bloque `plot_annotation()` determina y caracteriza el título y subtítulo del conjunto de gráficos.

## ![](figuras/book.svg){.hicon}![](figuras/pie-chart.svg){.hicon} Análisis de múltiples variables.

Son muchas las técnicas aplicadas al análisis de datos económicos basadas en una distribución de frecuencias multivariante. En este apartado nos centraremos en el caso de **variables métricas**, ya que al caso de atributos, variables categóricas o factores; le dedicaremos un capítulo en exclusiva. Algunas técnicas multivariantes son: el análisis de componentes principales, el análisis de regresión o el análisis clúster...

Todas estas metodologías requieren, de nuevo, de una fase inicial que ponga a punto la base de datos y ofrezca una fotografía de cómo es la situación en cuanto a las variables en estudio. En este sentido, es conveniente aplicar, para cada variable por separado, algunos de los análisis gráficos básicos vistos anteriormente.

A estos análisis básicos hay que añadir, principalmente, algún análisis previo adicional, destinado fundamentalmente a cuantificar el **grado de intensidad en la relación estadística** entre las variables implicadas, mediante el estudio de la ***correlación***. Antes de abordar esta cuestión, hemos de pararnos en una casuística específica que se presenta cuando trabajamos con numerosas variables: la detección de casos atípicos o *outliers*.

### ![](figuras/key.svg){.hicon} Localización de *missing values* y *outliers*.

Para trabajar con múltiples variables, en primer lugar es preciso localizar los casos con **valores perdidos o *missing values***, para decidir cómo procesarlos (eliminación del caso, estimación del valor faltante, etc.)

Vamos a imaginar que queremos realizar un análisis en el que tendremos en cuenta las variables RENECO (rentabilidad económica), ACTIVO (volumen de activos de la empresa), MARGEN (margen de beneficio) y RES (resultado del ejercicio).

En primer lugar, generaremos una copia del *data frame* original, "interestelar_100", para preservar su integridad. A esa copia la hemos llamado "muestra2". Ya vimos cómo el siguiente código nos aporta gráficamente una idea de la posible existencia de valores faltantes:

```{r echo=TRUE, message=FALSE, warning=FALSE, message=FALSE}
## Trabajando con multiples variables.

# Copia de df original.
muestra2<- select(interestelar_100, everything())

# Localizando missing values.
muestra2 %>%
  select (RENECO, ACTIVO, MARGEN, RES) %>%
  vis_miss() +
    labs(title = "Rentabilidad Económica: valores ausentes",
      subtitle = "Transporte de mercancías interestelar",
      y = "Observación",
      fill = NULL) +
    scale_fill_manual(
      values = c("TRUE" = "red", "FALSE" = "grey"),
      labels = c("TRUE" = "NA", "FALSE" = "Presente")) +
  theme(
    plot.title = element_text(face = "bold", size = 14))
```

Podemos apreciar cómo existen varios casos con *missing values* en las variables objeto de estudio.

Para localizar los casos concretos con *missing values*, podemos recurrir al siguiente código. En él, hemos sometido a "muestra2" a un *filtro* para detectar los casos en los que no hay valor para alguna (o varias) de las variables analizadas. El operador **`|`** significa "o". Posteriormente, hemos decidido eliminar esos casos (podría optarse por otro tipo de tratamiento, como la estimación de valores). Para ello asignamos a "muestra2" el resultado de pasar un filtro en el que se eligen los casos que no tienen valores faltantes en ninguna de las variables. El operador **`&`** significa "y":

```{r eval=TRUE, echo=TRUE, message=FALSE}
## Trabajando con multiples variables.

# Localizando y descartando casos con missing values.
muestra2 %>% filter(is.na(RENECO) |
                      is.na(ACTIVO) |
                      is.na(MARGEN) |
                      is.na(RES))%>%
             select(RENECO, ACTIVO, MARGEN, RES)
muestra2 <- muestra2 %>%
            filter(! is.na(RENECO) &
                     ! is.na(ACTIVO) &
                     ! is.na(MARGEN) &
                     ! is.na(RES))
```

El *data frame* "muestra2" contiene los mismos datos que "interestelar_100", salvo los 3 casos con *missing values* que han sido eliminados (101): *Vega Transport*, *Photon Pack Freight* y *Poe Dameron Cargo*.

Para la **detección de *outliers***, si las variables que entran en el análisis son numerosas, podría ser poco operativo estudiar las variables una a una. Una alternativa consiste en calcular la ***distancia de Mahalanobis*** de las variables del estudio, como **"resumen" del comportamiento de cada caso** en todas las variables del análisis, consideradas conjuntamente. En concreto, se puede considerar como una suerte de “z-score multivariante”: mide cuántas desviaciones típicas se aleja una observación del “centro” del conjunto, teniendo en cuenta todas las variables a la vez y cómo se relacionan entre sí.

Su utilización, intuitivamente, se puede justificar de este modo:

-   Con una sola variable, el “raro” es el que está a muchos **z-scores** de la media.

-   Con varias variables (p. ej., rentabilidad, activo, margen, resultado), el “centro” es el vector de medias y la forma de la nube viene dada por la covarianza (las variables pueden estar en escalas distintas y estar correlacionadas).

-   La *distancia de Mahalanobis:*

    -   **Estandariza** las escalas (una variable en millones no pesa más que otra en porcentajes).

    -   **Gira y estira** el espacio según las correlaciones (si dos variables están muy correlacionadas, moverte a lo largo de su diagonal “cuesta” menos que moverte perpendicularmente).

-   En 2D lo verías como **elipses** alrededor del centro (no círculos). Los puntos fuera de la elipse “grande” son **potenciales *outliers***.

La distancia de Mahalanobis de un vector $\mathbf{x}\in\mathbb{R}^p$ respecto del centro $\boldsymbol{\mu}$ y la matriz de covarianzas $\mathbf{\Sigma}$ es:

$$
D_M(\mathbf{x})
= \sqrt{(\mathbf{x}-\boldsymbol{\mu})^{\top}\,\mathbf{\Sigma}^{-1}\,(\mathbf{x}-\boldsymbol{\mu})}.
$$

Así, primero vamos a calcular una columna más en el *data frame* "muestra2" con los valores de la *distancia de* *Mahalanobis* del conjunto de las 4 variables en cada uno de los casos. Esta columna o variable la denominaremos, por ejemplo, MAHALANOBIS*.* Para ello, se utiliza las funciones `mutate()` y `pick()` del paquete `{dplyr}`, y como argumento de esta la función `mahalanobis()`, en la que hay que especificar:

-   Las columnas de "muestra2" para las que se van a calcular las distancias.

-   El vector de medias de las variables para las que se calcula la distancia (argumento `center =`).

-   La matriz de varianzas y covarianzas de las variables para las que se calcula la distancia (argumento `cov =`).

En definitiva, el código es:

```{r eval=TRUE, echo=TRUE, message=FALSE}
# Identificando y descartando outliers con distancia de Mahalanobis.
muestra2 <- muestra2 %>%
  mutate(
    MAHALANOBIS = {
      X <- pick(RENECO, ACTIVO, MARGEN, RES)
      mahalanobis(as.matrix(X),
                  center = colMeans(X),
                  cov = cov(X))
    }
  )
```

Posteriormente, se puede construir el diagrama de caja de la variable MAHALANOBIS, como cualquier otra variable:

```{r eval=TRUE, echo=TRUE, message=FALSE}
ggplot(data = muestra2, map = (aes(y = MAHALANOBIS))) +
    geom_boxplot(fill = "orange") +
    ggtitle("DISTANCIA DE MAHALANOBIS",
            subtitle = "RENECO, ACTIVO, MARGEN, RES. Empresas TMI.") +
    ylab("MAHALANOBIS")
```

Se observa cómo existen varios casos *outliers*. Para saber de qué casos concretos se trata, se podrá ejecutar el código:

```{r eval=FALSE, echo=TRUE, message=FALSE}
Q1M <- quantile (muestra2$MAHALANOBIS, c(0.25))
Q3M <- quantile (muestra2$MAHALANOBIS, c(0.75))

muestra2 %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
         MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(MAHALANOBIS, RENECO, ACTIVO, MARGEN, RES) 
```

En la consola se obtendrá el listado:

```{r eval=TRUE, echo=FALSE, message=FALSE}
Q1M <- quantile (muestra2$MAHALANOBIS, c(0.25))
Q3M <- quantile (muestra2$MAHALANOBIS, c(0.75))

df <- muestra2 %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
           select(MAHALANOBIS, RENECO, ACTIVO, MARGEN, RES)
  
# Número de columnas por bloque
columnas_por_bloque <- 3

# Dividir las columnas en bloques y mostrar los datos en la consola
for (i in seq(1, ncol(df), columnas_por_bloque)) {
  print(df[, i:min(i + columnas_por_bloque - 1, ncol(df))])
  cat("\n")
}
rm(i)
rm(df)
rm(columnas_por_bloque)
```

Si se opta por eliminar estos casos cara al análisis a aplicar posteriormente, se podrá crear un nuevo *data frame*, por ejemplo "muestra2_so", con el código siguiente:

```{r eval=TRUE, echo=TRUE, message=FALSE}
muestra2_so <- muestra2 %>%
  filter(MAHALANOBIS <= Q3M + 1.5*IQR(MAHALANOBIS) &
           MAHALANOBIS >= Q1M - 1.5*IQR(MAHALANOBIS)) 
```

El *data frame* "muestra2_so" será una réplica de "muestra2", aunque sin incluir los casos detectados como atípicos o *outliers* (91 casos).

### ![](figuras/key.svg){.hicon} Correlación entre variables.

Cuando trabajamos con más de una variable, una característica muy importante viene dada por la intensidad con la que tales variables están relacionadas estadísticamente entre sí, es decir, el estudio de las correlaciones. Un modo atractivo y rápido de visualizar la **matriz de correlaciones** de las variables es a través de la función `ggpairs()` del paquete `{GGally}`. Para aplicar la función, hemos creado el *data frame* "temporal" con las variables (métricas) del estudio, que borramos tras general el gráfico:

```{r eval=TRUE, echo=TRUE, warning=FALSE, error=FALSE, message=FALSE}
## Correlaciones entre variables.
temporal <- muestra2_so %>% select(RENECO, ACTIVO, MARGEN, RES)
corr_plot_so <- ggpairs(temporal, 
                        lower = list(continuous = wrap("cor",
                        size = 4.5,
                        method = "pearson",
                        stars = TRUE)),
                        title = "Matriz de Correlación (sin outliers)")
rm(temporal)
corr_plot_so
```

La interpretación detallada del código es la siguiente:

-   **`ggpairs(temporal)`**: crea una cuadrícula con una fila/columna por variable del data frame `temporal`.

    -   **Triángulo inferior (`lower`)**: lo personalizamos para que muestre números de correlación.

-   **`lower = list(continuous = wrap("cor", ...))`**\
    En las celdas del triángulo inferior, cuando ambas variables son numéricas (“continuous”):

    -   **`"cor"`**: escribe el **coeficiente de correlación** entre esas dos variables.

    -   **`method = "pearson"`**: usa la correlación de *Pearson* (relación lineal).

    -   **`size = 4.5`**: tamaño del número que aparece.

    -   **`stars = TRUE`**: añade asteriscos de significación estadística junto al número. Habitual: `*` p \< 0.05, `**` p \< 0.01, `***` p \< 0.001 (más asteriscos ⇒ relación menos probable por azar).

-   **`title = "Matriz de Correlación sin outliers"`**: título del conjunto.

-   **Asignación `corr_plot_so <- ...`**: guarda la figura en el objeto `corr_plot_so`. Para verla, basta con escribir `corr_plot_so` en la consola.

Un coeficiente de correlación puede tomar un valor entre **-1** (fuerte relación, en sentido opuesto) y **+1** (fuerte relación, en el mismo sentido). Como puede apreciarse en el gráfico, las variables ACTIVO y RES mantienen una relación muy intensa y en sentido positivo. Entre MARGEN y RENECO existe también una relación de intensidad destacable. En cambio, ACTIVO y MARGEN; y RENECO y ACTIVO apenas están estadísticamente relacionadas.

Para finalizar, vamos a comparar las correlaciones anteriores con las que se dan si se incluyen los casos outliers en la muestra:

```{r eval=TRUE, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
temporal <- muestra2 %>% select(RENECO, ACTIVO, MARGEN, RES)
corr_plot_co <- ggpairs(temporal, 
                        lower = list(continuous = wrap("cor",
                                                       size = 4.5,
                                                       method = "pearson",
                                                       stars = TRUE)),
                        title = "Matriz de Correlación (con outliers)")
rm(temporal)
corr_plot_so_gg <- ggmatrix_gtable(corr_plot_so)
corr_plot_co_gg <- ggmatrix_gtable(corr_plot_co)

library (gridExtra)

grid.arrange(corr_plot_so_gg, corr_plot_co_gg, ncol = 2, top = "CORRELACIONES")
```

Se aprecia cómo la presencia de outliers puede variar la relación entre las variables. Salvo el caso de la correlación entre ACTIVO y RES, que se fortalece; las correlaciones entre RENECO y MARGEN, y RENECO y RES se debilitan. Esto redunda en la idea, ya expuesta, de que **la decisión de trabajar con o sin los casos *outliers*** es una cuestión, a veces, muy **relevante** y compleja.

## ![](figuras/arrow-down-circle.svg){.hicon} Materiales para realizar las prácticas del capítulo.

En esta sección se muestran los links de acceso a los diferentes materiales (scripts, datos...) necesarios para llevar a cabo los contenidos prácticos del capítulo.

**Datos (en formato Microsoft (R) Excel (R)):**

-   interestelar_100.xlsx ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/interestelar_100.xlsx))

**Scripts:**

-   previo_rstars.R ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/previo_rstars.R))

<!--chapter:end:05-analisis_previo_de_datos.Rmd-->

# Componentes principales.

![[Carguero a dos niveles de resolución.]{.smallcaps}](figuras/06%20carguero%20renderizado.jpg){width="100%"}

## ![](figuras/book.svg){.hicon} Introducción.

A veces, menos es más. Esta es la filosofía que subyace a las **técnicas de reducción de la dimensión** de la información.

Imaginemos una serie de casos (por ejemplo, las empresas de un sector económico) caracterizados por múltiples variables. Puede ocurrir que, paradójicamente, el contar con tantas variables haga difícil la caracterización de los casos. Esto ocurre cuando algunas de las variables aportan una información muy parecida sobre tales casos. Por ejemplo, es más difícil hacerse una idea del comportamiento global en el ámbito económico o financiero de un grupo de empresas si tenemos que atender a los valores que toman en un conjunto de 10 variables, que si solo tenemos que atender a un par de indicadores.

Las técnicas de reducción de la dimensión de la información tratan, precisamente, de disminuir el número de variables necesarias para caracterizar un grupo de casos, aprovechando la posibilidad de que las (múltiples) variables originales compartan información sobre estos. Es decir, la idea fundamental es pasar de un planteamiento basado en manejar muchas variables con información compartida o redundante (variables que "dicen lo mismo" sobre el comportamiento de los casos) a un planteamiento en el que hay **menos variables**, pero que **no comparten información** (variables que "dicen" cosas diferentes sobre el comportamiento de los casos). En este proceso es importante que la pérdida de información sea mínima, y que solo se pierda la información redundante o repetida.

La principal técnica de reducción de la dimensión de la información es la de **componentes principales,** y es la que se expondrá y ejemplificará en el resto del capítulo. Pero antes, es preciso concretar la relación entre dos conceptos muy presentes en esta técnica: información y varianza.

## ![](figuras/book.svg){.hicon} Información y varianza.

En el apartado anterior hemos hablado de la posibilidad de que algunas variables compartan "información" sobre el comportamiento de los casos que constituyen nuestra muestra u objeto de estudio. Pero, ¿qué es, en este contexto, la "información"?

La ***información*** que una variable contiene sobre un conjunto de casos puede entenderse como su **capacidad para diferenciar a unos casos de otros**.

Observemos este ejemplo, en el que se representan los valores que toman un grupo de 20 empresas en 3 variables.

![[Información y Varianza]{.smallcaps}](figuras/varianza.png){width="100%"}

En la variable 1, todas las empresas toman el mismo valor. Por tanto, la capacidad que tiene la variable para distinguir a los casos (empresas), unos de otros, es nula. Eso es debido a que, en definitiva, esta variable no contiene información sobre el grupo de 20 empresas.

En la variable 2, existe cierta dispersión, aunque reducida, en los valores que adoptan los casos. Esto permite distinguir a unos de otros, aunque a veces con cierta dificultad. Por ejemplo, la empresa 17 se distingue del resto por ser la que tiene un valor (un poco) mayor. Aun así, como la dispersión es reducida, no se distinguen algunos casos de otros demasiado bien. En definitiva, la variable 2 contiene cierta cantidad de información sobre el conjunto de empresas de la muestra, aunque no demasiado grande.

Por último, la variable 3 muestra una dispersión considerablemente mayor que las otras dos variables. Existe un amplio abanico de valores que toman los diferentes casos (empresas). Esto hace que puedan diferenciarse con facilidad, en general, unos de otros. Esta variable posee, por tanto, una cantidad de información superior respecto a las empresas, ya que observando los valores que toman en la variable pueden diferenciarse con facilidad unas de otras.

Como conclusión, podemos establecer que cuanto mayor dispersión muestra una variable para un grupo de casos, mayor cantidad de información contiene sobre ellos, en el sentido de disfrutar de un mayor "poder" de diferenciación de unos casos respecto a otros.

Una medida de la dispersión de una variable usualmente utilizada es la **varianza**. Por tanto, en cierta manera, la varianza sirve para medir la cantidad de información que contiene la variable: a mayor varianza, mayor dispersión. Y a mayor dispersión, mayor cantidad de información.

En el ejemplo, puede observarse cómo la variable 3 es la que mayor varianza tiene, luego la variable 2, y la variable 1 tiene una varianza de 0 (y no posee información sobre las 20 empresas). Esta comparación de varianzas es válida siempre y cuando las tres variables estén expresadas en las mismas unidades, ya que la varianza es una medida de dispersión absoluta. Por ello, para poder comparar, hemos añadido también en el ejemplo una medida de dispersión relativa: el coeficiente de variación. Podemos comprobar cómo el mayor coeficiente de variación pertenece a la variable 3 (que es la que tiene una mayor cantidad de información), luego la variable 2 (que cuenta con menor cantidad de información), y por último la variable 1, con un coeficiente de 0 (no contiene información sobre las empresas).

## ![](figuras/star.svg){.hicon} La elección de *Arg-Us Korp*.

Vamos a considerar el caso del sector del "Transporte Interestelar". Tenemos una selección de 104 empresas o compañías. El "magnate" de los negocios a escala interplanetaria, *Arg-us* *Korp* en la imagen), está de compras: quiere adquirir una de las empresas que mejor proyección a futuro en el sector tenga.

![[Arg-Us Korp.]{.smallcaps}](figuras/Arg-Us%20Korp.jpg){.d-block .mx-auto width="500"}

Hay varias variables disponibles que pueden ser interpretadas como *indicadores* de la preparación de las diferentes compañías de transporte para afrontar el futuro, como por ejemplo:

1.  **IMD (Gasto en I+D)**

    -   Justificación: Una inversión alta en investigación y desarrollo indica que la empresa está preparándose para innovaciones tecnológicas, lo que le permitirá mejorar su eficiencia, reducir costos y mantenerse competitiva en el futuro.

2.  **IDIG (Índice de Digitalización)**

    -   Justificación: La digitalización es un factor clave en la eficiencia operativa y la adaptabilidad a nuevas tecnologías. Un alto IDIG sugiere que la empresa está invirtiendo en automatización, software avanzado y optimización de procesos.

3.  **EFLO (Edad Media de la Flota)**

    -   Justificación: Contar con una flota renovada significa menor riesgo de fallas mecánicas, menores costos de mantenimiento y mayor eficiencia en las operaciones, lo que permite mantener ventajas competitivas a largo plazo.

4.  **CAPEX (Gastos de Capital)**

    -   Justificación: Empresas que invierten en infraestructura y equipamiento moderno están mejor preparadas para el crecimiento y la adaptación a nuevas demandas del mercado.

5.  **IDIVERSE (Índice de Diversificación)**

    -   Justificación: Empresas con operaciones diversificadas tienen mayor resiliencia ante cambios del mercado, ya que no dependen de una única fuente de ingresos o de un solo tipo de carga.

6.  **RUTAS (Número de Rutas Atendidas)**

    -   Justificación: La expansión de rutas refleja una empresa con visión de crecimiento y acceso a mercados emergentes, lo que fortalece su sostenibilidad a largo plazo.

7.  **SOLVENCIA**

    -   Justificación: Empresas con una solvencia alta tienen mayor capacidad de enfrentar crisis económicas o períodos de baja demanda sin comprometer su estabilidad financiera.

8.  **BMAL (Beneficio Medio por Año Luz)**

    -   Justificación: Un alto BMAL indica eficiencia en la operación y rentabilidad sostenible, lo que contribuye a la capacidad de la empresa para reinvertir y mejorar su competitividad.

9.  **IFIDE (Índice de Fidelización)**

    -   Justificación: Un alto nivel de fidelización sugiere que la empresa ha construido relaciones sólidas con sus clientes, lo que le proporciona estabilidad de ingresos y ventajas competitivas a futuro.

Estos indicadores reflejan la capacidad de innovación, estabilidad financiera, expansión de mercado y eficiencia operativa de las empresas, factores esenciales para su sostenibilidad en el tiempo y preparación para los desafíos futuros.

De entre ellas, *Korp* ha seleccionado **IDIVERSE**, **IFIDE** e **IDIG** como aspectos que le importan especialmente. Aun así, su equipo sabe que su jefe quiere una respuesta precisa. Un nombre de una empresa.

Así, el equipo ha pensado en crear un *ranking* de compañías basándose en estos tres indicadores. Pero, si tienen 3 variables, ¿cómo combinar su análisis para obtener un solo *ranking* de modo objetivo? ¿Cómo ponderar las tres variables?

La respuesta ha venido al comprobar que las tres variables guardan entre sí unas correlaciones relativamente altas (en valor absoluto). Es decir: aportan una información bastante parecida sobre cada una de las empresas de la selección. Esto es importante porque, si en gran medida comparten información "redundante", pueden ser, seguramente, "resumidas" en un solo indicador, cuyo valor o puntuación para cada caso (empresa) podría servir para establecer el ranking de compañías candidatas a ser adquiridas.

El método para obtener este indicador a partir de las tres variables originales se llama ***Análisis de Componentes Principales*** **(PCA)**, y puede ser fácilmente desarrollado con unas líneas de código de R. Ese indicador, que es la clave de toda la estrategia, será la primera "componente" del PCA, una combinación lineal de las tres variables originales; siempre y cuando su poder para "retener" la información global ofrecida por esas tres variables sea lo suficientemente alto.

## ![](figuras/pie-chart.svg){.hicon} Preparación previa de datos.

Vamos a suponer que trabajamos dentro de un **proyecto** que hemos creado previamente, de nombre **"componentes"**. Dentro de la carpeta del proyecto guardaremos estos dos elementos:

-   El *script* llamado "componentes_rstars.R".

-   El archivo de Microsoft® Excel® llamado "interestelar_100.xlsx". Si abrimos este último archivo, comprobaremos que se compone de tres hojas. La primera muestra un mensaje sobre el uso de los datos, la segunda recoge la descripción de las variables consideradas, y la tercera (hoja "Datos") almacena los datos que debemos importar. Estos datos se corresponden con diferentes variables económicas y financieras de 104 empresas dedicadas a los servicios de transporte interestelar de mercancías.

Comenzaremos a ejecutar el código usual en cualquier *script*, esto es, limpiar el *Global Environment*, cargar los paquetes necesarios, importar los datos del archivo de Excel®, y tratar los casos con datos faltantes o *missing values*, y los casos que, para las variables estudiadas, se comportan como *ouliers*. En cuanto a los primeros pasos, tenemos:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
### Análisis de Componentes Principales ###

# Limpiando el Global Environment
rm(list = ls())

# Cargando paquetes
library(readxl)
library(dplyr)
library(visdat)
library(ggplot2)
library(gtExtras)
library (GGally)
library (knitr)
library (kableExtra)
library (patchwork)

## DATOS

# Importando datos desde Excel
interestelar_100 <- read_excel("interestelar_100.xlsx",
                               sheet = "Datos",
                               na = c("n.d."))
interestelar_100 <- data.frame(interestelar_100, row.names = 1)

# Seleccionando variables metricas para el analisis.
seleccion <- interestelar_100 %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion_df_graph <- gt_plt_summary(seleccion)
seleccion_df_graph
```

Básicamente, en el código anterior se han almacenado los datos de la hoja de cálculo en el *data frame* "interestelar_100". Luego, se ha creado un nuevo *data frame* más reducido con las únicas variables del análisis que vamos a realizar (IDIVERSE, IFIDE e IDIG), al que hemos llamado "seleccion". Estas tres variables han sido exploradas a traves de una tabla gráfica a partir de la función `gt_plt_summary()` del paquete `{gtExtras}`.

El siguiente consistirá en localizar los posibles ***missing values***, ya que para obtener componentes principales es necesario que todos los casos posean dato en todas las variables del análisis. Para tener una idea general, se puede utilizar la función `vis_miss()` del paquete `{visdat}`, que localizará gráficamente los *missing values* de las diferentes variables, y calculará el porcentaje de casos que supone, con respecto al total de observaciones:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Localizando missing values.
seleccion %>%
  vis_miss() +
  labs(title = "Indicadores: Diversificación, Fidelidad, Digitalización",
       subtitle = "Transporte de mercancías interestelar",
       y = "Observación",
       fill = NULL) +
  scale_fill_manual(
    values = c("TRUE" = "red", "FALSE" = "grey"),
    labels = c("TRUE" = "NA", "FALSE" = "Presente")) +
  theme(
    plot.title = element_text(face = "bold", size = 14))
```

Del gráfico anterior se desprende que existen 2 *missing values* repartidos en 2 de las 3 variables del estudio. Para localizarlos, podemos filtrar nuestro *data frame* con las herramientas de `{dplyr}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
seleccion %>% filter(is.na(IDIVERSE) |
                     is.na(IFIDE) |
                     is.na(IDIG)) %>%
              select(IDIVERSE, IFIDE, IDIG)
```

Los casos con datos faltantes son las empresas *Ezra Bridger Haulage* y *Alderaan Freight*.

Ante la existencia de *missing values*, se puede actuar de varios modos. Por ejemplo, **se puede intentar obtener el conjunto de valores que no están disponibles por otro canal de información,** o recurrir **a alguna estimación**. En caso de que esto sea difícil, se puede optar, simplemente, por **eliminar** estos casos, en especial cuando representan un porcentaje muy reducido respecto al total de observaciones. En nuestro ejemplo, supondremos que hemos optado por esta última vía, y eliminaremos estos casos con el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
seleccion <- seleccion %>%
             filter(! is.na(IDIVERSE) &
                    ! is.na(IFIDE) &
                    ! is.na(IDIG))
```

Verificamos en el *Global Environment* que el *data frame* “seleccion” ha pasado a tener 102 casos.

Por otro lado, la técnica de componentes principales **es muy sensible a la existencia de *outliers***. En concreto, las observaciones atípicas pueden afectar a los resultados a través de tres vías:

- **Distorsión de las componentes principales:** pueden influir en la dirección de las componentes principales, haciendo que estas se ajusten más a los valores atípicos que a la mayoría de los datos. Esto puede llevar a una interpretación incorrecta de las relaciones entre las variables.

- **Afectación a las cargas de las variables:** estas cargas son relevantes porque indican la importancia relativa de cada variable original en la formación de la componente, es decir, cuánto contribuye o se correlaciona una variable específica con una componente principal.

- **Modificación de la varianza explicada:** pueden *inflar* la varianza total de los datos, lo que puede afectar a la proporción de varianza explicada por cada componente principal. Esto puede ocasionar una selección incorrecta del número de componentes a retener.

En consecuencia, deberán ser identificados y, en su caso, eliminados. Para realizar este proceso, y dado que en nuestro análisis contamos con 4 variables, primero “resumiremos” el valor que toman dichas variables para cada observación (empresa), mediante el cálculo de la *distancia de Mahalanobis*. De hecho, las distancias de los diferentes casos se almacenarán en una nueva columna o variable de nuestro *data frame,* a la que llamaremos MAHALANOBIS:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Identificando y descartando outliers con distancia de Mahalanobis.
seleccion <- seleccion %>%
  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),
                                   center = colMeans(.),
                                   cov    = cov(.)))
```

Dentro de los argumentos de la función `mahalanobis()` incluida en la función `mutate()` hay unos puntos entre paréntesis. Recordemos que estos puntos deben ser añadidos cuando una función no es la primera del operador "*pipe*" (`%>%`), para indicar que las variables de los paréntesis hacen referencia al *data frame* "seleccion" (o, en general, el objeto que fluye a través del "*pipe*").

A continuación, hemos construido un *box-plot* o diagrama de caja de la variable MAHALANOBIS, como si fuera cualquier otra variable, a partir de la función `ggplot()` del paquete `{ggplot2}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}

ggplot(data = seleccion, map = (aes(y = MAHALANOBIS))) +
  geom_boxplot(fill = "orange") +
  ggtitle("DISTANCIA DE MAHALANOBIS",
          subtitle = "IDIVERSE, IFIDE, IDIG. Empresas TMI.") +
  ylab("MAHALANOBIS")
```

En el gráfico se aprecia que existen, por encima de la caja, varios *outliers*. Para identificarlos de modo concreto, hemos de calcular los cuartiles primero y tercero de la variable MAHALANOBIS y pasar el correspondiente filtro:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
Q1M <- quantile (seleccion$MAHALANOBIS, c(0.25))
Q3M <- quantile (seleccion$MAHALANOBIS, c(0.75))

seleccion %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(MAHALANOBIS, IDIVERSE, IFIDE, IDIG) 
```

Las compañías que se comportan como *outliers*, considerando conjuntamente las tres variables (a través de la distancia de Mahalanobis), son 13. La eliminación de estos casos puede realizarse fácilmente con el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Creando nuevo df sin outliers.
seleccion_so <- seleccion %>%
  filter(MAHALANOBIS <= Q3M + 1.5*IQR(MAHALANOBIS) &
           MAHALANOBIS >= Q1M - 1.5*IQR(MAHALANOBIS))  

# Eliminando variable MAHALANOBIS de los df
seleccion    <- seleccion    %>% select(-MAHALANOBIS)
seleccion_so <- seleccion_so %>% select(-MAHALANOBIS)
```

Se ha creado un nuevo *data frame* llamado "seleccion_so” con los casos (89) que **no** son *outliers* (y que no contienen *missing values*), y se ha eliminado la variable MAHALANOBIS, puesto que su única utilidad era la de localizar y filtrar los *outliers*. Con este *data frame* “seleccion_so” es con el que se procederá al cálculo de las componentes.

## ![](figuras/pie-chart.svg){.hicon} Cálculo de componentes.

La **condición previa** para el **cálculo de componentes** es que las variables originales del análisis contengan información redundante, es decir, que en buena medida tengan una capacidad para diferenciar a los casos (empresas) parecida. Esto se verifica con la existencia de **altas correlaciones**, en valor absoluto, entre ellas (al menos, entre *algunas*). Por tanto, hemos de calcular la matriz de correlaciones correspondiente. Un modo gráfico visualmente efectivo es utilizar las posibilidades que nos ofrece el paquete `{GGally}`, mediante la función `ggpairs()`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Correlaciones.
corr_plot_so <- ggpairs(seleccion_so, 
                        lower = list(continuous = wrap("cor",
                                                       size = 4.5,
                                                       method = "pearson",
                                                       stars = TRUE)),
                        title = "Matriz de Correlación (sin outliers).")
corr_plot_so
```

Puede apreciarse cómo existen altas correlaciones (en valor absoluto) entre todas las variables. Por tanto, tiene sentido hacer un análisis de componentes principales, ya que hay variables que parecen **compartir información**.

La obtención de las componentes se va a realizar mediante la función `prcomp()` del paquete `{stats}`, que es un paquete cargado por defecto al abrir R. Es conveniente que activemos el argumento `scale =` con “T” (*true*) para que las variables originales sean consideradas en sus **versiones tipificadas**. Vamos a asignar los resultados a un objeto de nombre, por ejemplo, “componentes”. Por último, guardaremos el `summary()` o resumen de los resultados con un nombre provisional, por ejemplo, "temporal". El código es el siguiente:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Obtencion de componentes.
componentes <- prcomp (seleccion_so, scale=T)
temporal <- summary (componentes)
temporal
```

La “*Standard deviation*” es la raíz cuadrada de los autovalores asociados a cada componente. “*Proportion of Variance*” nos dice la proporción de la suma de varianzas de las variables originales (*comunalidad*) recogida por cada componente, proporción que se acumula en “*Cumulative Proportion*”. Nótese que las componentes aparecen ordenadas de más a menos importantes en función de la cantidad de varianza que capturan.

En este caso, a partir de la tabla anterior podemos destacar que la primera componente recoge más del 71% de la varianza (*comunalidad*) o información puesta en juego globalmente por las tres variables originales. Las dos primeras componentes, en conjunto, aglutinan casi el 89% de la información de las tres variables ofrecen sobre el comportamiento de las empresas. Entre las tres componentes, lógicamente se recoge el 100% de la *comunalidad* o varianza global.

Si el elemento "*importance*" del `summary()` o resumen "temporal" lo convertimos en un *data frame*, por ejemplo "summary_df", podremos presentar los resultados por medio de una tabla estéticamente más atractiva, a partir de la función `kable()` del paquete `{knitr}`, y las funciones complementarias del paquete `{kableExtra}`:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Convertir el resumen en un data frame
summary_df <- as.data.frame(temporal$importance)
summary_df <- t(summary_df)  # Transponer para mejor visualización
rm (temporal)

# Crear la tabla con kable y personalizarla con kableExtra
summary_df %>%
  kable(caption = "Resumen de Componentes",
        col.names = c("Componente", 
                      "Desv. típica",
                      "Proporción de varianza (comunalidad)",
                      "Proporción de varianza (comunalidad) acumulada"),
        digits = c(2, 2, 2),
        format.args = list(decimal.mark = ".", scientific = FALSE)) %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "condensed"),
                full_width = F, 
                position = "center") %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(summary_df)), bold= F, align = "c") %>%
  column_spec(1, bold = TRUE, extra_css = "text-align: center;")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# Convertir el resumen en un data frame
summary_df <- as.data.frame(temporal$importance)
summary_df <- t(summary_df)  # Transponer para mejor visualización
rm (temporal)

# Crear la tabla con kable y personalizarla con kableExtra
library (knitr)
library (kableExtra)
tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
summary_df %>%
  kable(caption = "Resumen de Componentes",
        col.names = c("Componente", 
                      "Desv. típica",
                      "Proporción de varianza (comunalidad)",
                      "Proporción de varianza (comunalidad) acumulada"),
        digits = c(2, 2, 2),
        format.args = list(decimal.mark = ".", scientific = FALSE)) %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "condensed"),
                full_width = F, 
                position = "center") %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(summary_df)), bold= F, align = "c") %>%
  column_spec(1, bold = TRUE, extra_css = "text-align: center;")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
summary_df %>%
  kable(caption = "Resumen de Componentes",
        col.names = c("Componente", 
                      "Desv. típica",
                      "Proporción de varianza (comunalidad)",
                      "Proporción de varianza (comunalidad) acumulada"),
        digits = c(2, 2, 2),
        format.args = list(decimal.mark = ".", scientific = FALSE)) 
}  
```

Los coeficientes o **cargas** de cada componente se obtienen pidiendo a nuestro objeto “*componentes*” el elemento “*rotation*”. Estas cargas las vamos a guardar en un nuevo objeto que llamaremos, por ejemplo, “cargas”, que presentaremos mediante una pequeña tabla diseñada con la función `kable()` del paquete `{knitr}` y otras funciones del paquete `{kableExtra}`:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Cargas de cada componente.
cargas <- componentes$rotation
cargas %>%
  kable(caption = "Cargas de las componentes obtenidas",
        digits = c(3, 3, 3),
        format.args = list(decimal.mark = ".", scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = c("striped", "bordered", "condensed"),
                position = "center") %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(cargas)), bold= F, align = "c") %>%
  column_spec(1, bold = TRUE, extra_css = "text-align: left;")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# Cargas de cada componente.

cargas <- componentes$rotation
tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
cargas %>%
  kable(caption = "Cargas de las componentes obtenidas",
        digits = c(3, 3, 3),
        format.args = list(decimal.mark = ".", scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = c("striped", "bordered", "condensed"),
                position = "center") %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(cargas)), bold= F, align = "c") %>%
  column_spec(1, bold = TRUE, extra_css = "text-align: left;")
} else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
cargas %>%
  kable(caption = "Cargas de las componentes obtenidas",
        digits = c(3, 3, 3),
        format.args = list(decimal.mark = ".", scientific = FALSE))
}
```

En la tabla, se muestran las cargas (*loads*), que son los coeficientes que intervienen en las combinaciones lineales que definen cada componente, a partir de las variables originales (tipificadas). Por tanto, con base en las cargas se pueden explicitar las ecuaciones correspondientes a cada componente. Por ejemplo, para la primera componente, la ecuación será:

$$
\text{CP}_{i1} = 0.6006 \cdot \text{IDIVERSE}_{i1} + 0.5442 \cdot \text{IFIDE}_{i1} + 0.5858 \cdot \text{IDIG}_{i1}
$$

Puede observarse que, en cuanto a la primera componente, que es la que especialmente nos interesa como *indicador* de la "preparación de las diferentes compañías de transporte para afrontar el futuro", las 3 cargas tienen signo positivo, lo que implica que, cuanto mayores sean los valores de una empresa en las variables **IDIVERSE** (*índice de diversificación*), **IFIDE** (*índice de fidelidad*) e **IDIG** (*índice de digitalización*), mayor será el valor del indicador y, por tanto, su preparación. Además, como las variables fueron tipificadas, los valores de las cargas son comparables. De este modo, vemos cómo, dentro de la primera componente, que es la que adoptamos como indicador, la mayor importancia la tiene **IDIVERSE**, seguido de **IDIG** y, por último, **IFIDE**.

## ![](figuras/pie-chart.svg){.hicon} Retención de componentes.

La etapa de retención de componentes consiste en decidir cuántas de las componentes generadas (recordemos que, en un principio, se calculan tantas componentes como variables originales) consideramos que resumen de un modo aceptable la información contenida en las variables originales. Estas **componentes "retenidas" se convertirán en las componentes principales**.

La primera componente siempre es retenida y, por tanto, es una "componente principal". El resto, que van capturando proporciones cada vez menores de la varianza común de las variables originales (*comunalidad*), podrán o no retenerse; aunque, siempre, la retención de una componente implica que se han retenido todas las anteriores. En este caso práctico, buscamos un único indicador de la "preparación de las diferentes compañías de transporte para afrontar el futuro", por lo que solo vamos a "retener" la primera componente. En otras aplicaciones, podría ser necesario retener varias para recoger la suficiente comunalidad.

Hay varios procedimientos o criterios para decidir cuántas componentes retener. Uno de ellos, comúnmente aplicado, es el de **retener aquellas componentes cuyo autovalor es mayor que 1** (suponiendo que se ha trabajado con las variables en sus versiones tipificadas). Los autovalores son el cuadrado de los elementos “Desviación típica” (valores “*Standard deviation*” (sdev) del objeto “componentes” que hemos generado a partir de la función `prcomp()`).

Hemos creado un *data frame* con estos autovalores calculados (y su orden de importancia, al que hemos llamado variable o columna “orden”, y que es un vector de números enteros consecutivos que va desde uno hasta número de variables originales o de componentes) y los hemos dispuesto en un gráfico de barras:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Determinacion Componentes a retener.
# Criterio del Autovalor mayor que 1.
orden <- c(1:ncol(seleccion_so))
autovalor <- componentes$sdev^2
autovalores <- data.frame(orden, autovalor)

autograph <- ggplot(data = autovalores, map = (aes(x = orden,
                                                   y = autovalor))) +
             geom_bar(stat = "identity",
                      colour = "red",
                      fill = "orange",
                      alpha = 0.7) +
             scale_x_continuous(breaks=c(1:nrow(autovalores)))+
             geom_hline(yintercept = 1,
                        colour = "dark blue") +
             geom_text(aes(label = round(autovalor,2)),
                       vjust = 1,
                       colour = "dark blue",
                       size = 3) +
             ggtitle("AUTOVALORES DE LAS COMPONENTES",
                     subtitle = "Empresas TMI") +
             xlab ("Número de componente") +
             ylab("Autovalor")

autograph
```

Respecto al gráfico, conviene recordar que, al ser de barras, si no se quieren representar las frecuencias sino los valores que toma una variable (en este caso, “autovalor”) para cada valor de la otra variable (en este caso, “orden”); en el `geom_bar()` habrá que añadir el argumento `stat =` con el valor “identity”. Además, se utiliza el elemento `scale_x_continuous()` para pesonalizar la escala del eje x, y que se divida dicho eje en tantos tramos como componentes hay.

En el gráfico obtenido, las componentes cuyas barras atraviesan la línea que pasa por el valor "1" son las que deberían ser retenidas, ya que son las componentes que resumen de modo suficiente la información ofrecida sobre el comportamiento de los casos por las variables originales. En este caso, basta con retener solo la primera componente (por lo tanto, solo contamos con una **componente principal**). Este es un resultado favorable cara a nuestro propósito, ya que nos indica que podemos usar como indicador solo una componente, la primera, y con ella resumiremos suficientemente las tres variables originales. Por tanto, es un buen indicador, en ese sentido.

Un gráfico complementario útil es el que muestra, para cada componente, el porcentaje de varianza total (comunalidad) acumulada al ir reteniendo las sucesivas componentes, un resultado que ya se obtuvo anteriormente en forma de tabla:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Comunalidad acumulada.
autovalores <- autovalores %>%
  mutate(variacum = 100*(cumsum((autovalor/nrow(autovalores)))))
checkcp <- ifelse(autovalores$autovalor >= 1, "CP", "NCP")
checkcp
             
vacumgraph <- ggplot(data = autovalores, map = (aes(x = orden,
                                                    y = variacum))) +
              geom_bar(stat = "identity",
                       aes(fill = checkcp),
                       colour = "red",
                       alpha = 0.7) +
              scale_x_continuous(breaks=c(1:nrow(autovalores)))+
              geom_text(aes(label = round(variacum,2)),
                        vjust = 1,
                        colour = "dark blue",
                        size = 3) +
              ggtitle("COMUNALIDAD ACUMULADA POR COMPONENTES",
                      subtitle = "Empresas TMI") +
              xlab ("Número de componente") +
              ylab("Varianza acumulada")
vacumgraph
```

Para obtener el gráfico anterior, se comienza añadiendo al *data frame* “autovalores” una columna o variable que es la *suma acumulada del porcentaje de comunalidad* recogido por las sucesivas componentes, que están ordenadas de mayor a menor autovalor. Para calcular el porcentaje, se usa la función `cumsum()`, y se tiene en cuenta que, como las variables fueron tipificadas para calcular las componentes, la *comunalidad*, que coincide con la suma de las varianzas de las componentes (autovalores), es igual al número de variables o componentes (valor que toma la función `nrow()`).

Después, se ha creado un vector que contiene tantos elementos como variables o componentes hay en el análisis (vector “checkcp”). Con la función condicional `ifelse()` se consigue que los elementos de "checkcp" sean "CP" o "NCP" según los correspondientes autovalores sean mayores o no que 1. Finalmente, según sea el valor de cada elemento de "checkcp", las barras del gráfico se colorearán de uno u otro modo.

Posteriormente, mediante el paquete `{patchwork}`, se han unido los dos gráficos creados en esta fase, poniendo uno debajo del otro:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
combinado <- autograph / vacumgraph
combinado <- combinado + 
  plot_annotation(
    title = "Retención de componentes (Autovalor >1)",
    subtitle = "Empresas TMI (sin outliers)",
    theme = theme(
      # TÍTULO de la composición
      plot.title = element_text(
        size = 16,          # tamaño
        face = "bold",      # negrita
      ),
      # SUBTÍTULO de la composición
      plot.subtitle = element_text(
        size = 12
      )))
combinado
```

Un a vez confirmado el hecho de que la primera componente es suficiente para contar con un buen indicador de "preparación de las diferentes compañías de transporte para afrontar el futuro", considerando los aspectos de *digitalización*, *diversificación* y *fidelización* de clientes, pasaremos a estudiar qué casos concretos ofrecen mejores (mayores) valores en el indicador, para lo cual hemos de calcular sus puntuaciones.

## ![](figuras/pie-chart.svg){.hicon} Puntuaciones de los casos (scores).

Para obtener las puntuaciones de cada caso (empresa) en el indicador de "preparación de las diferentes compañías de transporte para afrontar el futuro" (y que es nuestra componente principal, que a su vez coincide con la primera componente), simplemente debemos tener en cuenta que tales puntuaciones están guardadas en la matriz “x” del objeto `prcomp()` creado. Vamos a renombrar a las primera columna (componente) de esta matriz como “scores” y vamos a recolocar las filas (empresas) de mayor a menor valor de la puntuación (lo que se consigue mediante la función `arrange()` del paquete `{dplyr}`. Finalmente, mostraremos en una tabla el *ranking* de las 10 mejores empresas (según sus puntuaciones en el indicador), "cortando" el *data frame* con la función `slice()`:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
## Puntuaciones o Scores
scores <- componentes$x[,1]  #tantas columnas como componentes retenidas
scores_df <- as.data.frame(scores)
scores_df <- cbind(scores_df,seleccion_so)
scores_top10 <- scores_df %>%
  arrange(desc(scores)) %>%
  slice(1:10)

scores_top10 %>%
  kable(caption = "Puntuaciones emporesas TMI (Top-10, sin outliers)",
        col.names = c("Empresa",
                      "Puntuación",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(3, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                                    "bordered",
                                    "condensed",
                position = "center",
                font_size = 12) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(scores_top10)),
           bold= F,
           align = "c") %>%
  column_spec(1, bold = TRUE,
              extra_css = "text-align: left;")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
## Puntuaciones o Scores
scores <- componentes$x[,1]  #tantas columnas como componentes retenidas
scores_df <- as.data.frame(scores)
scores_df <- cbind(scores_df,seleccion_so)
scores_top10 <- scores_df %>%
  arrange(desc(scores)) %>%
  slice(1:10)
  
tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
scores_top10 %>%
  kable(caption = "Puntuaciones emporesas TMI (Top-10, sin outliers)",
        col.names = c("Empresa",
                      "Puntuación",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(3, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                                    "bordered",
                                    "condensed",
                position = "center",
                font_size = 12) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(scores_top10)),
           bold= F,
           align = "c") %>%
  column_spec(1, bold = TRUE,
              extra_css = "text-align: left;")
} else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
scores_top10 %>%
  kable(caption = "Puntuaciones emporesas TMI (Top-10, sin outliers)",
        col.names = c("Empresa",
                      "Puntuación",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(3, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE))
}
```

De la tabla y gráfico anteriores podemos concluir que las empresas más preparadas para afrontar el futuro, según nuestro indicador (primera componente del análisis PCA) son, por este orden, *Shuttlepod Movers*, *Kamino Movers* e *Ícarus Star Transport*.

¿Seguro?

Este *ranking* se ha elaborado a partir de las compañías que formaron la muestra para realizar el análisis de componentes principales. Estas empresas eran aquellas que tenían dato en las tres variables originales y que **no habían sido calificadas como *outliers***. Los *outliers* se apartaron de la muestra para evitar distorsiones y sesgos en el análisis. Pero una cuestión que podríamos plantearnos es si, una vez calculadas las combinaciones lineales que son las *componentes* sin su influencia, podrían ser ahora, en virtud de esas componentes calculadas, puntuadas. Que una compañía se comporte como *outlier* en una o varias de las variables originales, y no se cuente con ella a la hora de calcular las componentes; no quiere decir necesariamente que no sea una buena candidata a ser la elegida para ser adquirida por su preparación para afrontar el futuro (incluso podría concluirse que en algún aspecto está "especialmente preparada").

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Scores puntuando outliers
scores_all <- predict(componentes, newdata = seleccion)
scores_all <- scores_all[,1]
scores_all_df <- as.data.frame(scores_all)
scores_all_df <- cbind(scores_all_df,seleccion)
scores_all_top10 <- scores_all_df %>%
  arrange(desc(scores_all)) %>%
  slice(1:10)

scores_all_top10 %>%
  kable(caption = "Puntuaciones emporesas TMI (Top-10, con outliers)",
        col.names = c("Empresa",
                      "Puntuación",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(3, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                "bordered",
                "condensed",
                position = "center",
                font_size = 12) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(scores_all_top10)),
           bold= F,
           align = "c") %>%
  column_spec(1, bold = TRUE,
              extra_css = "text-align: left;")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# Scores puntuando outliers
scores_all <- predict(componentes, newdata = seleccion)
scores_all <- scores_all[,1]
scores_all_df <- as.data.frame(scores_all)
scores_all_df <- cbind(scores_all_df,seleccion)
scores_all_top10 <- scores_all_df %>%
  arrange(desc(scores_all)) %>%
  slice(1:10)

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
scores_all_top10 %>%
  kable(caption = "Puntuaciones emporesas TMI (Top-10, con outliers)",
        col.names = c("Empresa",
                      "Puntuación",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(3, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                "bordered",
                "condensed",
                position = "center",
                font_size = 12) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(scores_all_top10)),
           bold= F,
           align = "c") %>%
  column_spec(1, bold = TRUE,
              extra_css = "text-align: left;")
} else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {  
scores_all_top10 %>%
  kable(caption = "Puntuaciones emporesas TMI (Top-10, con outliers)",
        col.names = c("Empresa",
                      "Puntuación",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(3, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE))
}
```

Vemos como el *ranking* cambia radicalmente, y pasa a estar encabezado por las compañías ***Arrakis Freight***, ***Chakotay Cargo Systems*** e ***Hyperdrive Express***. Si un análisis particularizado de estas empresas por parte del equipo encargado del estudio da como resultado que sus datos son correctos, serían, seguramente, las candidatas para ser adquiridas por el magnate *Arg-Us Korp*.

## ![](figuras/paperclip.svg){.hicon} Puntuaciones de los casos (scores).

Cabe hacerse una pregunta antes de cerrar esta historia. ¿Qué hubiera ocurrido si se hubieran calculado las componentes sin eliminar los *outliers* previamente? ¿Hubieran cambiado los resultados? ¿era tan relevante el efecto de estos casos sobre los resultados, como comentamos al comienzo del tema, en la parte teórica?

En R, el código es fácil de adaptar: basta con sustituir el *data frame* "seleccion_so" por el *data frame* "seleccion", desde el apartado del cálculo de componentes hasta la formación del primer *ranking* basado en las puntuaciones de las compañías en la primera componente.

El *ranking* obtenido de este modo es el siguiente:

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# Obtencion de componentes (sin eliminación de outliers en cálculo).
componentes2 <- prcomp (seleccion, scale=T)

# Cargas de cada componente.
cargas2 <- componentes2$rotation
## Puntuaciones o Scores
scores2 <- componentes2$x[,1]  #tantas columnas como componentes retenidas
scores2_df <- as.data.frame(scores2)
scores2_df <- cbind(scores2_df,seleccion)
scores2_top10 <- scores2_df %>%
  arrange(desc(scores2)) %>%
  slice(1:10)

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
scores2_top10 %>%
  kable(caption = "Puntuaciones emporesas TMI (Top-10, outliers en cálculo)",
        col.names = c("Empresa",
                      "Puntuación",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(3, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                "bordered",
                "condensed",
                position = "center",
                font_size = 12) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(scores2_top10)),
           bold= F,
           align = "c") %>%
  column_spec(1, bold = TRUE,
              extra_css = "text-align: left;")
} else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {  
scores2_top10 %>%
  kable(caption = "Puntuaciones emporesas TMI (Top-10, outliers en cálculo)",
        col.names = c("Empresa",
                      "Puntuación",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(3, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE))
}  
```

Como puede repararse, los cambios, en este caso, no parecen ser relevantes. Solo a partir de la séptima posición se registran cambios en el orden de las empresas.

No obstante, hay una tercera vía para proceder al análisis de componentes principales que nos libera de la necesidad de tener que decidir si es conveniente incluir en el cálculo los *outliers* o no: la utilización de **técnicas robustas**.

## ![](figuras/paperclip.svg){.hicon} Utilización de técnicas robustas: método de Hubert.

La presencia de *outliers*, según comentamos, puede distorsionar los resultados del análisis. Esto se debe, en esencia, a los efectos que estos elementos tienen en el cálculo de *la matriz de varianzas-covarianzas de las variables originales*, básica en el cálculo de componentes. Este efecto se manifiesta, por ejemplo, en un incremento *artificial* de las varianzas, debido a la mayor dispersión inducida por los *outliers*.

En la literatura se han desarrollado métodos de cálculo de esta matriz de varianzas-covarianzas que minimizan este efecto distorsionador de los *outliers*. Son los **métodos robustos**. Este es el caso de la variante del análisis de componentes principales de ***Hubert***, que emplea estimadores robustos de la matriz de varianzas y covarianzas tales como *MCD* o *S-estimators*.

El siguiente código permite obtener las componentes por *Hubert*, y realizar el ranking de empresas de acuerdo a las puntuaciones de la primera componente. Hay que tener en cuenta que, en esta ocasión, las cargas de la primera componente son negativas, luego las mejores compañías en términos de diversificación de operaciones, fidelización de clientes y digitalización serán aquellas que presenten **menores** puntuaciones:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# --- Obtención de componentes (ROBPCA de Hubert) ---
# "seleccion" debe ser un data.frame/matriz numérica (obs x vars)
library(rrcov)

componentes3 <- PcaHubert(
  x        = seleccion,
  k        = ncol(seleccion),        # 0 => deja que el método elija nº de comp.
  scale    = TRUE,     # estandariza como en prcomp(scale=TRUE)
  mcd      = TRUE,     # fase inicial robusta
)

summary(componentes3)

# --- Cargas de cada componente ---
cargas3 <- as.data.frame(unclass(componentes3@loadings))
cargas3    # Atención! Las cargas de CP1 son negativas: el ranking debe ascender

# --- Puntuaciones (Scores) ---
# Si quieres la 1ª componente (equivalente a componentes2$x[,1]):
scores3 <- as.numeric(componentes3@scores[, 1])
# (Si quieres todas: scores3_all <- as.data.frame(componentes3@scores))

scores3_df <- data.frame(scores3 = scores3) %>%
  cbind(seleccion)

# --- Top-10 por la 1ª componente (orden descendente, como en tu código original) ---
scores3_top10 <- scores3_df %>%
  arrange(scores3_df) %>%      # orden ascendente porque las cargas de PC1 son negativas (mejor empresa => menor puntuación)
  slice(1:10)

scores3_top10 %>%
  kable(caption = "Puntuaciones emporesas TMI (Top-10, Hubert)",
        col.names = c("Empresa",
                      "Puntuación",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(3, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                "bordered",
                "condensed",
                position = "center",
                font_size = 12) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(scores3_top10)),
           bold= F,
           align = "c") %>%
  column_spec(1, bold = TRUE,
              extra_css = "text-align: left;")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# --- Obtención de componentes (ROBPCA de Hubert) ---
# "seleccion" debe ser un data.frame/matriz numérica (obs x vars)
library(rrcov)

componentes3 <- PcaHubert(
  x        = seleccion,
  k        = ncol(seleccion),        # 0 => deja que el método elija nº de comp.
  scale    = TRUE,     # estandariza como en prcomp(scale=TRUE)
  mcd      = TRUE,     # fase inicial robusta
)

summary(componentes3)

# --- Cargas de cada componente ---
cargas3 <- as.data.frame(unclass(componentes3@loadings))
cargas3    # Atención! Las cargas de CP1 son negativas: el ranking debe ascender

# --- Puntuaciones (Scores) ---
# Si quieres la 1ª componente (equivalente a componentes2$x[,1]):
scores3 <- as.numeric(componentes3@scores[, 1])
# (Si quieres todas: scores3_all <- as.data.frame(componentes3@scores))

scores3_df <- data.frame(scores3 = scores3) %>%
  cbind(seleccion)

# --- Top-10 por la 1ª componente (orden descendente, como en tu código original) ---
scores3_top10 <- scores3_df %>%
  arrange(scores3_df) %>%      # orden ascendente porque las cargas de PC1 son negativas (mejor empresa => menor puntuación)
  slice(1:10)

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
scores3_top10 %>%
  kable(caption = "Puntuaciones emporesas TMI (Top-10, Hubert)",
        col.names = c("Empresa",
                      "Puntuación",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(3, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                "bordered",
                "condensed",
                position = "center",
                font_size = 12) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:(nrow(scores3_top10)),
           bold= F,
           align = "c") %>%
  column_spec(1, bold = TRUE,
              extra_css = "text-align: left;")
} else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
  scores3_top10 %>%
  kable(caption = "Puntuaciones emporesas TMI (Top-10, Hubert)",
        col.names = c("Empresa",
                      "Puntuación",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(3, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE))
}  
```
En la tabla se comprueba que existen leves variaciones con respecto a las clasificaciones anteriores. A destacar la inclusión en el *top-10* de *Ripley Interestellar Freight*, y la desparición de  *Kamino Movers*.

¿cuál de los 3 ránkings elegirías tú?

## ![](figuras/arrow-down-circle.svg){.hicon} Materiales para realizar las prácticas del capítulo.

En esta sección se muestran los links de acceso a los diferentes materiales (*scripts*, datos...) necesarios para llevar a cabo los contenidos prácticos del capítulo.

**Datos (en formato Microsoft (R) Excel (R)):**

-   interestelar_100.xlsx ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/interestelar_100.xlsx))

**Scripts:**

-   componentes_rstars.R ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/componentes_rstars.R))

<!--chapter:end:06-componentes_principales.Rmd-->

# Análisis Clúster.

![[Grupos de cargueros en travesía.]{.smallcaps}](figuras/07%20naves%20en%20cluster.jpg){width="100%"}

## ![](figuras/book.svg){.hicon} Introducción.

El análisis de conglomerados —o análisis de clústeres (AC)— agrupa casos con características similares en función de un conjunto de variables clasificadoras. El objetivo es que:

- Los casos de un mismo clúster sean lo más homogéneos posible entre sí.
- Los clústeres entre sí sean lo más heterogéneos posible, de acuerdo con las variables consideradas.


En general, el proceso de determinación de los grupos, conglomerados o clústeres de casos es el siguiente:

-   Se parte de un conjunto de **n** casos, y para cada uno de ellos se cuenta con el valor de **m** variables clasificadoras.
-   Se establece una **medida de distancia** que cuantifica lo que dos casos se parecen, **considerando en conjunto** los valores que poseen para las variables clasificadoras.
-   Se crean los grupos, conglomerados o clústeres con los casos que poseen entre sí una **menor distancia**. Existen dos **enfoques** principales a la hora de crear los grupos de casos a partir de las distancias observadas entre los casos: los *métodos jerárquicos* y los *métodos no-jerárquicos*.
-   Finalmente, se **caracterizan** los grupos, conglomerados o clústeres obtenidos, y se comparan unos con otros para extraer conclusiones.

En lo que respecta a la medida de **distancia** entre los casos, la medida más habitual es la **distancia euclídea**. Así, la distancia euclídea entre dos caso, i e i', para las m variables clasificadoras x, será:

$$
d(i, i') = \sqrt{\sum_{j=1}^{m} (x_{ij} - x_{i'j})^2}
$$ Esta distancia es muy sensible a la escala de las variables clasificadoras. Para evitar este inconveniente, se trabaja con las variables previamente **tipificadas**.

## ![](figuras/book.svg){.hicon} ![](figuras/pie-chart.svg){.hicon} Métodos de agrupación jerárquicos.

Como se acaba de comentar, existen dos enfoques fundamentales de realizar el análisis clúster, dependiendo de cómo son los métodos de agrupación de los casos (y grupos de casos): el enfoque de los métodos jerárquicos, y el enfoque que reúne a los métodos no-jerárquicos.

Ambos enfoques tienen sus ventajas e inconvenientes, y pueden adaptarse mejor a cada problema concreto. Es importante seleccionar un buen método de agrupación, puesto que pueden proporcionar soluciones muy diferentes entre sí.

En los **métodos jerárquicos,** se van formando sucesivamente grupos como agrupación de otros grupos precedentes, hasta llegar a un único grupo que recoge a todos los individuos; tomando el proceso una **estructura piramidal** (también existen métodos jerárquicos descendientes, que parten de un único grupo que contiene a todos los casos, para acabar el n grupos de un solo caso, aunque son menos frecuentes).

Estos métodos suelen aplicarse cuando hay un número reducido de casos. También, cuando nuestro objetivo pasa por crear **grupos que recojan a todos los casos**, más que definir simplemente tipologías más o menos homogéneas de casos (lo que se obtiene caracterizando los grupos obtenidos). Es decir, cuando se incluyen en el análisis a todos los individuos, incluidos los *outliers*. De hecho, estos métodos pueden emplearse, de por sí, como técnicas de localización de *outliers*. Por último, también se suelen emplearse cuando se desconoce a priori el número de grupos, conglomerados o clústeres a formar.

Entre los métodos jerárquicos de agrupación más extendidos, figuran los siguientes:

-   **Método del vecino más cercano (single linkage):** la distancia que se considera entre grupos es la distancia entre sus elementos más próximos.

-   **Método del vecino más lejano (complete linkage):** la distancia que se considera entre grupos es la distancia entre sus elementos más lejanos.

-   **Método de Ward (Ward method):** se unen los grupos que dan lugar a otro grupo cuyos casos tienen una menor suma de los cuadrados de sus distancias respecto al centro de dicho grupo (menor varianza intra-clúster).

-   **Otros métodos:** vinculación intergrupos (average linkage between groups), vinculación intragrupos (whithin-group)...

De entre ellos, ¿cuál elegir?

La cuestión no es fácil de resolver, y no tiene por qué tener una única respuesta. Por otro lado, cada método proporciona soluciones que pueden variar mucho entre sí. Una estrategia puede pasar por probar con varios métodos y se seleccionar la solución que parezca más coherente desde el punto de vista teórico, y estable desde el punto de vista empírico.

En la práctica, uno de los métodos más utilizados es el **método de Ward**, porque proporciona grupos muy homogéneos, ya que se basa en la minimización de la varianza o dispersión de los elementos que componen cada grupo con respecto a su centro de gravedad o **centroide.** Precisamente, este método será aplicado en el ejemplo práctico que desarrollaremos en R a continuación.

### ![](figuras/star.svg){.hicon} El Informe *Bluebird*.

![[Agencia Interplanetaria del Transporte de Mercancías.]{.smallcaps}](figuras/aitm.jpg){width="100%"}

La *Agencia Interplanetaria de Transporte de Mercancías* es un organismo dedicado a estudiar el funcionamiento del sector. Entre sus actividades, hay una consistente en la selección de un grupo de empresas para su segmentación en términos de fidelidad de los clientes (**IFIDE**), diversificación del negocio (**IDIVERSE**), y digitalización de la compañía (**IDIG**). En esta ocasión, se ha elegido un panel de 25 empresas o compañías. La investigación corre a cargo de una de las investigadoras de la agencia, la doctora *Xelia Bluebird* (en la imagen), por lo que al informe que contiene los resultados de la segmentación se le denomina *Informe Bluebird*.

![[Xelia Bluebird, investigadora de la AITM.]{.smallcaps}](figuras/Bluebird.jpg){.d-block .mx-auto width="400"}

En esta edición del informe, y tras la reciente compra de la compañía *Home One Cargo* por parte del magnate *Arg-us Korp*, la doctora *Bluebird* está especialmente interesada en cómo queda encuadrada dicha empresa.

### ![](figuras/pie-chart.svg) Preparación de los datos.

Dado que son pocos los casos (empresas) a segmentar, vamos a utilizar un método jerárquico de agrupación de casos. En concreto, utilizaremos el **método de Ward**.

Vamos a suponer que trabajamos dentro de un **proyecto** que hemos creado previamente, de nombre **"cluster"**. Dentro de la carpeta del proyecto guardaremos estos dos elementos:

-   El *script* llamado "cluster_rstars.R".

-   El archivo de Microsoft® Excel® llamado "interestelar_25.xlsx". Si abrimos este último archivo, comprobaremos que se compone de tres hojas. La primera muestra un mensaje sobre el uso de los datos, la segunda recoge la descripción de las variables consideradas, y la tercera (hoja "Datos") almacena los datos que debemos importar. Estos datos se corresponden con diferentes variables económicas y financieras de 25 empresas dedicadas a los servicios de transporte interestelar de mercancías.

Comenzaremos a ejecutar el código del *script*. En primer lugar, el código se ocupa de limpiar el *Global Environment* y cargar los paquetes necesarios:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
### CLUSTER jerárquico 25 empresas TMI.###

# Limpiando el Global Environment
rm(list = ls())

# Cargando paquetes
library(readxl)
library(dplyr)
library(ggplot2)
library(gtExtras)
library(visdat)
library (factoextra)
library (knitr)
library (kableExtra)
library (patchwork)
```

La siguiente sección de código es algo especial. Se trata de una **función**, en la que no se entrará en detalle, dada la complejidad de su código. Basta decir que el *input* de la función es una **lista** de gráficos generados con el paquete `{ggplot2}`. El *output* será una serie de **composiciones** de dimensión 4X4 realizada con los gráficos de la lista de modo automático, dejando los huecos en blanco necesarios en caso de que el número de gráficos no sea múltiplo de 4. La función se denomina `create_patchwork()`.

A lo largo del script se llamará dos veces a esta función, lo que ahorrará una buena cantidad de código. Se ha ubicado al comienzo del script para asegurar su ejecución previa a sus llamadas, aunque, en realidad, lo más adecuado sería integrar la función en un paquete, instalarlo y activarlo, y así evitar alargar el script con su código (se deja esta opción para usuarios avanzados).

```{r, eval=TRUE, echo=TRUE, message=FALSE}
##### Función para crear composiciones de gráficos con patchwork ###############
create_patchwork <- function(plot_list) {
  n <- length(plot_list)
  if (n == 0) return(NULL)
  full_rows <- n %/% 4
  remaining <- n %% 4
  patchworks <- list()
  
  if (full_rows > 0) {
    for (i in seq(1, full_rows * 4, by = 4)) {
      patchworks <- c(patchworks, list((plot_list[[i]] + plot_list[[i+1]]) / 
                                         (plot_list[[i+2]] + plot_list[[i+3]])))
    }
  }
  
  if (remaining > 0) {
   last_plots <- plot_list[(full_rows * 4 + 1):n]
   empty_plots <- lapply(1:(4 - remaining), function(x) ggplot() + theme_void())
   last_patchwork <- do.call(patchwork::wrap_plots, c(last_plots, empty_plots))
   patchworks <- c(patchworks, list(last_patchwork))
  }
  return(patchworks)
}
################################################################################
```

Posteriormente importaremos los datos del archivo de Excel®, y trataremos los casos con datos faltantes o *missing values*:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
## DATOS

# Importando datos desde Excel
interestelar_25 <- read_excel("interestelar_25.xlsx",
                               sheet = "Datos",
                               na = c("n.d."))
interestelar_25 <- data.frame(interestelar_25, row.names = 1)

# Seleccionando variables metricas para el analisis.
seleccion <- interestelar_25 %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion_df_graph <- gt_plt_summary(seleccion)
seleccion_df_graph

# Localizando missing values.
seleccion %>%
  vis_miss() +
  labs(title = "Indicadores: Diversificación, Fidelidad, Digitalización",
       subtitle = "Transporte de mercancías interestelar",
       y = "Observación",
       fill = NULL) +
  scale_fill_manual(
    values = c("TRUE" = "red", "FALSE" = "grey"),
    labels = c("TRUE" = "NA", "FALSE" = "Presente")) +
  theme(
    plot.title = element_text(face = "bold", size = 14))

seleccion %>% filter(is.na(IDIVERSE) |
                       is.na(IFIDE) |
                       is.na(IDIG)) %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion <- seleccion %>%
  filter(! is.na(IDIVERSE) &
           ! is.na(IFIDE) &
           ! is.na(IDIG)) 
```

No se ha localizado ninguna observación con *missing values*, luego no se ha de realizar ningún tipo de tratamiento.

El siguiente paso es la **identificación de *outliers*.** Para realizar este proceso, y dado que en nuestro análisis contamos con 3 variables, primero “resumiremos” el valor que toman dichas variables para cada caso, mediante el cálculo de la *distancia de Mahalanobis*. De hecho, las distancias de los diferentes casos se almacenarán en una nueva variable, a la que llamaremos MAHALANOBIS, que se incorporará al *data frame* "originales" por medio de la función `mutate()` de `{dplyr}`, y la función `mahalanobis()`. Recordemos que, en los diferentes argumentos de esta función, el punto "." hace referencia al *data frame* que está delante del operador pipe (%\>%).

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Identificando y descartando outliers con distancia de Mahalanobis.
seleccion <- seleccion %>%
  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),
                                   center = colMeans(.),
                                   cov    = cov(.)))
```

Una vez creada la variable MAHALANOBIS, se estudia la existencia en sus valores de *outliers* mediante la construcción de un diagrama de caja o *boxplot*:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Identificando outliers con distancia de Mahalanobis.
seleccion <- seleccion %>%
  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),
                                   center = colMeans(.),
                                   cov    = cov(.)))

ggplot(data = seleccion, map = (aes(y = MAHALANOBIS))) +
  geom_boxplot(fill = "orange") +
  ggtitle("DISTANCIA DE MAHALANOBIS",
          subtitle = "IDIVERSE, IFIDE, IDIG. Empresas TMI.") +
  ylab("MAHALANOBIS")
```

En el gráfico se observa que existen, por encima de la caja, 4 *outliers*. Para identificarlos de modo concreto, hemos de calcular los cuartiles primero y tercero de la variable MAHALANOBIS y pasar el correspondiente filtro:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
Q1M <- quantile (seleccion$MAHALANOBIS, c(0.25))
Q3M <- quantile (seleccion$MAHALANOBIS, c(0.75))

seleccion %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(MAHALANOBIS, IDIVERSE, IFIDE, IDIG)
```

De la tabla anterior se desprende que los outliers identificados son las empresas *Chakotay Cargo Systems*, *Home One Cargo*, y de modo más moderado, *Tannhäuser Freight* e *Hyperion Star Haulage*. Estos son, en definitiva, los 4 casos en los que nos fijaremos específicamente más adelante, al analizar los resultados del análisis clúster.

Estas empresas cuentan con un valor atípico en la variable MAHALANOBIS, lo que, a su vez, implica que muestren valores atípicos en una o varias de las variables originales (IDIVERSE, IFIDE, IDIG). En el desarrollo de otras técnicas, en este punto localizaríamos y eliminaríamos los *outliers*. En este caso **no** lo vamos a hacer, ya que queremos agrupar **todos los casos** que tenemos en el análisis. Precisamente, si hay algún caso que permanece aislado, sin agruparse con otros en el proceso de agrupación hasta las últimas etapas, quizá se trate de un candidato a *outlier*, por lo que el análisis clúster **también puede considerarse una técnica de localización de casos atípicos**.

Por último, borramos la variable MAHALANOBIS del *data frame* "seleccion", puesto que ya ha cumplido la función de localizar los casos atípicos:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Eliminando variable MAHALANOBIS del df
seleccion    <- seleccion    %>% select(-MAHALANOBIS)
```

La siguiente etapa se refiere a la aplicación propia del análisis clúster al grupo de 25 compañías que toman valores para las tres variables incluidas en el análisis.

### ![](figuras/pie-chart.svg) Aplicación del método de *Ward*.

Los métodos de agrupación usualmente se basan en la **distancia euclídea**. Como la distancia euclídea es sensible a las unidades de medida de las diferentes variables clasificadoras, es preciso trabajar con las **variables tipificadas**, lo que lograremos creando, por ejemplo, un *data frame* “zseleccion” con la función `scale()`. Luego, aplicaremos el método elegido a este *data frame,* en lugar de al *data frame* que contiene los datos originales sin tipificar:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# CLUSTER JERARQUICO CON VARIABLES ORIGINALES.

# Tipificando variables
zseleccion <- data.frame(scale(seleccion))
summary(zseleccion)
```

Este nuevo *data frame* contiene las mismas variables del análisis; pero tipificadas (obsérvese, en el `summary()`, las medias de las variables).

Previamente a aplicar un método de agrupación concreto, es necesario calcular la **matriz de distancias** entre los casos, a la que llamaremos, por ejemplo, **“d”**. Esta matriz se calcula con la función `dist()`. Para visualizarla, una opción es representarla mediante el *gráfico de temperatura* que ofrece la función `fviz_dist()` del paquete `{factoextra}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Matriz de distancias
d <- dist(zseleccion)
fviz_dist(d, lab_size = 8)  # Del paquete {factoextra}
```

Los casos con intersecciones en tonos anaranjados tenderán a agruparse con mayor facilidad (o a agruparse antes); mientras que los casos cuyas intersecciones están en tonos azulados tenderán a pertenecer a grupos diferentes (o a agruparse más tarde). Cabe destacar los colores azulados asociados a las empresas *Chakotay Cargo Systems*, *Home One Cargo*, que precisamente eran las dos compañías identificadas más claramente como *outliers*.

Vamos a realizar el análisis clúster jerárquico mediante uno de los métodos más habituales, el de ***Ward***, como es común en las aplicaciones prácticas, ya que este método proporciona grupos muy homogéneos (mínima varianza). La función a utilizar es `hclust()`. La solución la guardaremos en el objeto (lista) que hemos llamado, por ejemplo, “cluster_j”. Luego se visualizará el ***dendograma*** construido con la función `fviz_dend()` del paquete `{factoextra}`, que permite personalizar el gráfico con una gramática similar a la utilizada con los gráficos del paquete `{ggplot2}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Método de Ward.
cluster_j<-hclust(d, method="ward.D2")

fviz_dend(cluster_j,
          cex = 0.6,
          rect = FALSE,
          labels_track_height = 5.5) +
  labs(title = "Empresas TMI.",
       subtitle = "Método de Ward. Variables originales tipificadas.") +
  theme_grey()
```

En el código anterior:

-   **`cluster_j`**: Es el objeto que contiene el dendrograma que se desea visualizar.

-   **`cex = 0.6`**: Este argumento ajusta el tamaño del texto de las etiquetas en el dendrograma. Un valor de **`0.6`** significa que el texto será más pequeño que el tamaño predeterminado.

-   **`rect = FALSE`**: Este argumento indica si se deben dibujar rectángulos alrededor de los clústeres en el dendrograma. **`FALSE`** significa que no se dibujarán rectángulos.

-   **`labels_track_height = 5.5`**: Este argumento ajusta la altura de la pista de etiquetas, que es el espacio reservado para las etiquetas de los objetos en el dendrograma. Un valor de **`5.5`** proporciona más espacio para las etiquetas.

Además, el código incluye funciones adicionales para mejorar la visualización:

-   **`labs(title = "Empresas TMI.", subtitle = "Método de Ward. Variables originales tipificadas.")`**: Esta función añade un título y un subtítulo al gráfico.

-   **`theme_grey()`**: Esta función aplica un tema gris al gráfico, que es el tema predeterminado en `{ggplot2}`, proporcionando un fondo gris claro y un estilo de texto específico.

El eje vertical del *dendograma* recoge las distancias (o disimilitud) entre los casos y/o grupos previos que se van agrupando sucesivamente. La escala depende de cada método empleado. En el caso del método de *Ward*, la escala refleja la suma de cuadrados de la distancia de los casos dentro del clúster.

Por otro lado, en este ejemplo, es interesante observar que las empresas *Chakotay Cargo Systems*, *Home One Cargo*, las *outliers* comentadas anteriormente, permanecen sin agruparse hasta una zona muy avanzada del proceso de agrupación, en coherencia con el gráfico de temperatura de la matriz de distancias euclídeas. En cambio, compañías como *Betazoid Transport* y *Skywalker Freight Co.* se han unido en el mismo grupo casi inmediatamente, lo que cuadra con el tono anaranjado de su intersección en la matriz de distancias.

Una cuestión importante consiste en determinar con **cuántos grupos** hemos de quedarnos. Aunque existen algoritmos y paquetes de R que aconsejan un número (por ejemplo, la función `NbClust()` del paquete `{NbClust}`); a veces puede ser preferible que el propio investigador decida el número de grupos a crear, mediante la observación del dendograma, y de acuerdo a los objetivos de su propia investigación.

Dentro de los métodos *objetivos*, uno muy extendido es el del *método de la anchura media de silueta*, muy utilizado en este tipo de análisis.

Para cada observación $i$, la **anchura de la silueta** $s_i$ se calcula como:

$$
s_i = \frac{b_i - a_i}{\max(a_i, b_i)}
$$

Donde:

-   $a_i$ es la distancia promedio de la observación $i$ a todas las demás observaciones dentro de su propio clúster.
-   $b_i$ es la distancia promedio de la observación $i$ al clúster más cercano.

La anchura de la silueta varía entre **-1 y 1.** Un valor cercano a 1 indica que la observación está bien agrupada dentro de su clúster; un valor cercano a 0 quiere decir que la observación está en el límite entre dos clústeres; y un valor negativo implica que la observación podría estar mal clasificada en su clúster actual. De este modo, para evaluar cuál es el mejor número de conglomerados a retener:

1.  Se prueban diferentes valores de $k$ (número de clústeres).

2.  Se calcula el promedio de los valores de silueta $k$ .

3.  Se selecciona el $k$ que maximiza el **promedio de la silueta**, lo que indica que la partición es más adecuada.

De todos modos, es necesario insistir en que estos métodos no llevan a una conclusión única ni irrevocable; por lo que puede ser preferible que el **propio investigador** decida el número de grupos a crear, mediante la observación del dendograma, y de acuerdo a los objetivos de su propia investigación, tomando los métodos cuantitativos solo como **orientación**.

En nuestro caso, el gráfico del *método de la silueta* se obtiene a partir de la función `fviz_nbclust` del paquete `{factoextra}`, con el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Método de obtención de número de grupos (k) del Ancho de Silueta.
p <- fviz_nbclust(
  x = zseleccion, 
  FUNcluster = hcut, 
  method = "silhouette",
  hc_method = "ward.D2",
  k.max = 15
)
p
```

El método aconseja una división de los casos en 8 grupos o conglomerados diferentes. Sin embargo, parece un número excesivo si se quieren caracterizar posteriormente los grupos e incidir en sus diferencias. Un segundo número de grupos apropiado según el método es 2; pero podría ocurrir lo contrario que el caso anterior: sería un valor demasiado bajo, lo que provocaría que tan solo se distinguiese un grupo con los dos outliers de otro con los 23 casos restantes. Así, el investigador puede decidir qué número de grupos parece equilibrado y se ajusta a sus intereses. En este ejemplo, **un número de grupos razonable podría ser 5**, que contaría con el aval de mantener individualizadas a 2 de las empresas etiquetadas como *outliers*.

Si se acepta esta opción, se podrá visualizar de nuevo el dendograma coloreando los grupos formados, con el código siguiente (debe modificarse el código para igualar el argumento `k =` al número de grupos seleccionado):

```{r, eval=TRUE, echo=TRUE, message=FALSE}
ngrupos = 5   # números de conglomerados decidido! #############################
fviz_dend(cluster_j,
          cex = 0.6,
          k = ngrupos, # número de conglomerados que se ha decidido formar!
          k_colors = "black",
          labels_track_height = 5.5,
          rect = TRUE,
          rect_border = "npg",
          rect_fill = TRUE) +
  labs(title = "Empresas TMI.",
       subtitle = "Método de Ward. Variables originales tipificadas.") +
  theme_grey()
```

En el código anterior:

-   **`k = 5`**: Especifica el número de grupos o clústeres que se desea formar en el dendrograma. En este caso, se han decidido formar 5 grupos.

-   **`k_colors = "black"`**: Define el color de las etiquetas de los clústeres. Aquí, se ha elegido el color negro para las etiquetas.

-   **`rect = TRUE`**:Significa que se dibujarán rectángulos delimitando los grupos formados.

-   **`rect_border = "npg"`**: Define el color del borde de los rectángulos que rodean los clústeres. **`"npg"`** es un conjunto de colores predefinidos (el de las publicaciones del *Nature Publishing Group*) en el paquete **`{ggsci}`**, que proporciona paletas de colores científicas.

-   **`rect_fill = TRUE`**: Indica si los rectángulos que rodean los clústeres deben estar rellenos. **`TRUE`** significa que los rectángulos estarán rellenos con el color especificado.

Al observar el gráfico, pueden destacarse varios conglomerados:

- Primer grupo (rojo, izquierda):

Integrado por *Hyperion Star Haulage* y *Event Horizon Haulage*. Estas dos empresas están muy próximas entre sí, indicando un alto grado de similitud en sus características. Se separan tempranamente del resto, lo que sugiere que poseen un perfil muy diferenciado respecto al resto de compañías (quizá por especialización extrema o tamaño atípico).

- Segundo grupo (azul, central):

Es el grupo más numeroso (14), e incluye empresas como *Bib Fortuna Haulage*, *Razor Freight Lines*, *Kashyyyk Logistics* o *Tatooine Movers*, entre otras. Aglutina compañías con comportamientos relativamente homogéneos, posiblemente representando el segmento “medio” del sector. Dentro de este grupo se distinguen subgrupos internos, lo que refleja cierta diversidad en estrategias o mercados específicos.

- Tercer grupo (verde, derecha):

Formado por empresas (7) como *Terminator Freight*, *Blizzard Transport* y *Rebel Alliance Transport*. Su distancia con respecto al grupo azul indica una diferenciación moderada, quizá por operar en mercados más especializados o disponer de tecnologías más avanzadas.

- Por último, aparecen a la derecha las compañías *Chakotay Cargo Systems* y *Home One Cargo*, que son nuestros *outliers* más destacados en el análisis previo y en el estudio de la matriz de distancias. Su aislamiento sugiere perfiles muy particulares.

A continuación vamos a **identificar con mayor detalle los casos** que integran cada uno de los grupos, así como a **caracterizar tales grupos** en función de los valores medios de las variables originales. Para ello, crearemos el vector de valores enteros que indica el grupo al que pertenece cada caso (empresa). A este vector se le llamará, por ejemplo, “whatcluster_j”, y se construirá mediante la función `cutree()`, donde el primer argumento es el nombre del objeto que guarda la solución del análisis clúster (“cluster_j”), y el segundo argumento es el número de grupos que hemos decidido crear (k = 5).

Lamentablemente, el "número de grupo" que `cutree()` asigna a cada caso no tiene por qué coincidir con el orden de grupos del dendograma (es decir, el segundo grupo del dendograma puede ser denominado por `cutree()`, por ejemplo, grupo "4"). Para que los grupos se llamen según el orden en que se disponen en el dendograma, hay que hacer varios ajustes adicionales, como se muestra en el código.

Además, conviene convertir esta variable "whatcluster_j" en un factor con la función `as.factor()`, para que deje de ser variable métrica, a efectos de incorporar una leyenda en gráficos posteriores, . Finalmente, ese factor se incorporará al *data frame* “seleccion” (importante: **no a ”zseleccion”**; sino al *data frame* que contiene a las variables no tipificadas):

```{r, eval=TRUE, echo=TRUE, message=FALSE}
## CARACTERIZACIÓN Y COMPOSICIÓN DE GRUPOS.

# cortando árbol en grupos
cl <- cutree(cluster_j, k = ngrupos)

# etiquetas en el orden exacto del dendrograma (izq→der)
ord_labels <- cluster_j$labels[cluster_j$order]

# posición (1..n) de cada caso según ese orden
pos <- match(names(cl), ord_labels)

# mapa de id antiguo -> id nuevo (1..ngrupos) según el bloque más a la izquierda
mins       <- tapply(pos, cl, min)             # posición más a la izq por clúster
old_order  <- names(sort(mins))                # ids antiguos ordenados izq→der
map        <- setNames(seq_along(old_order), old_order)

# Formar etiqueta de grupos (factor "whatcluster")
whatcluster_j <- factor(map[as.character(cl)], levels = 1:ngrupos)
seleccion$whatcluster_j <- whatcluster_j
```

### ![](figuras/pie-chart.svg) Caracterización de los grupos o conglomerados.

Una vez incorporado el grupo de pertenencia de cada empresa al *data frame* "seleccion", se podrán calcular y almacenar las medias de cada grupo de las distintas variables originales, usando las funciones `by_group()` y `summarise()` de `{dplyr}`. Toda la información se asigna al *data frame* “tablamedias” para poder representarla en una tabla mediante las facilidades que ofrecen los paquetes `{knitr}` y `{kableExtra}`:

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Tabla con centroides de grupos.
tablamedias <- seleccion %>%
               group_by(whatcluster_j) %>%
               summarise(obs = length(whatcluster_j),
                                      Idiverse = mean(IDIVERSE),
                                      Ifide = mean(IFIDE),
                                      Idig = mean(IDIG))

tablamedias %>%
  kable(caption = "Método de Ward. 5 grupos. Medias de variables",
        col.names = c("Clúster",
                      "Observaciones",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(0, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                                    "bordered",
                                    "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T,
           align = "c") %>%
  row_spec(1:nrow(tablamedias),
           bold= F,
           align = "c")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE}
# Tabla con centroides de grupos.

# Tabla con centroides de grupos.
tablamedias <- seleccion %>%
               group_by(whatcluster_j) %>%
               summarise(obs = length(whatcluster_j),
                                      Idiverse = mean(IDIVERSE),
                                      Ifide = mean(IFIDE),
                                      Idig = mean(IDIG))

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
tablamedias %>%
  kable(caption = "Método de Ward. 5 grupos. Medias de variables",
        col.names = c("Clúster",
                      "Observaciones",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(0, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                                    "bordered",
                                    "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T,
           align = "c") %>%
  row_spec(1:nrow(tablamedias),
           bold= F,
           align = "c")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
tablamedias %>%
  kable(caption = "Método de Ward. 5 grupos. Medias de variables",
        col.names = c("Clúster",
                      "Observaciones",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(0, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE))
}  
```

Obviamente, también se podrían comparar las medias de los grupos, para cada variable, con un simple gráfico de barras. A fin de crear un método que valga para cualquier número de variables, realizaremos la tarea con un bucle. El código es el siguiente:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Gráficos de centroides

  # Vector de nombre de variables excluyendo la variable no deseada
    variables <- setdiff(names(tablamedias), c("whatcluster_j", "obs"))

  # Lista para almacenar los gráficos
    graficos.centroides <- list()

  # Bucle para crear y almacenar los gráficos
    for (i in seq_along(variables)) {
    var1 <- variables[[i]]
    grafico <- ggplot(data= tablamedias,
                      map = (aes_string(y = var1, x = "whatcluster_j"))) +
               geom_bar(stat = "identity",
                        colour = "red",
                        fill = "orange",
                        alpha = 0.7) +
               ggtitle(paste0(var1, ". Media por grupos."),
                       subtitle = "Empresas eólicas")+
               xlab ("Grupo") +
               ylab(var1)
    graficos.centroides[[paste0("grafico_", var1)]] <- grafico
}  
```

En el código anterior, `setdiff()` crea un vector "variables" que contiene todos los nombres de las columnas del *data frame* "tablamedias", excepto "whatcluster_j" y "obs", que no son las variables originales. Luego se crea una lista vacía "graficos.centroides" para almacenar los gráficos generados. Con `for (i in seq_along(variables))` comienza el bucle, que recorre cada elemento del vector "variables". En el código de gráfico, "var1" toma el nombre, en cada iteración, de la variable a representar. En el "mapeo", es importante utilizar `aes_string()`, que requiere que los nombres de las variables se pasen como cadenas de texto (entre comillas), lo que es útil cuando los nombres de las variables se generan dinámicamente o se pasan como argumentos de función, y cuando se necesitan construir *mapeos estéticos* de manera programática. Finalmente, con `graficos.centroides[[paste0("grafico_", var1)]] <- grafico` se guarda el gráfico en la lista "graficos.centroides" con un nombre basado en "var1".

Los gráficos guardados en la lista "gráficos.centroides" se pueden agrupar en composiciones, de, por ejemplo, 2x2, utilizando el paquete `{patchwork}`, empleando la función `create.patchwork()`que ya incorporamos al comienzo del script. Con esta función, crearemos la lista de composiciones de gráficos denominada, por ejemplo, "grupos.graficos.centroides":

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Aplicar función de composiciones a gráficos de centroides.
  grupos.graficos.centroides <- create_patchwork(graficos.centroides)
  
# Presentar las composiciones
  for (n in 1:length(grupos.graficos.centroides)){
       print(grupos.graficos.centroides[[n]])
  }
```

Como solo hay 3 variables originales en el análisis; "grupos.graficos.centroides" solo cuenta con un elemento o composición, que contiene tres gráficos (valores medios de IDIVERSE, IFIDE e IDIG, por conglomerado).

Hablando siempre en términos de la media (centroides), puede comprobarse cómo el grupo 4 (compañía *Chakotay Cargo Systems*) destaca por su alto valor en el índice de diversificación y en el Fidelización, quedando en segunda posición en el índice de digitalización. El otro grupo de una única compañía, el grupo 5, que contiene a la empresa *Home One Cargo*, destca por poseer el mayor índice de digitalización. En adición, ocupa la segunda posición en el índice de fidelización. Por último, en cuanto al índice de diversificación, ocupa una posición media. En cuanto al grupo 3, em promedio es un conglomerado formado por empresas con índices de fidelización y digitalización medios, mientras que en cuando a la diversificación ocupa una destacada segunda posición, solo por detrás del grupo 4. Por su parte, las empresas del grupo 2, en promedio, presentan un índice de fidelización medio, mientras que los índices de diversificación y digitalización son más bien bajos. Por último, el grupo 1, compuesto por las compañías *Hyperion Star Haulage* y *Event Horizon Haulage*, se caracteriza por mostrar los menores valores medios en los tres indicadores.

Por otro lado, se pueden presentar en diferentes **tablas** las informaciones de **cada grupo**. Vamos a automatizar de nuevo el proceso de generación de las tablas mediante el empleo de un *bucle.* Las diferentes tablas se irán guardando en una lista de nombre, por ejemplo, "tablascompo":

```{r, eval=FALSE, echo=TRUE, message=FALSE}
# Tablas con composiciones de grupos

  # Número de tablas y lista para guardarlas

  numclusters <- nlevels(seleccion$whatcluster_j)
  tablascompo <- list()

  # Bucle para generar las tablas

  for (n in 1:numclusters){
      tabla <- seleccion %>%
      filter(whatcluster_j == as.character(n)) %>%
      select(IDIVERSE, IFIDE, IDIG) %>%
      kable(caption = paste("Método de Ward. Grupo ", n, "."),
            col.names = c("I. Diversificación",
                          "I. Fidelización",
                          "I. Digitalización"),
            digits = c(0, 3, 3, 3),
            format.args = list(decimal.mark = ".",
                               scientific = FALSE)) %>%
      kable_styling(full_width = FALSE, 
                    bootstrap_options = c("striped",
                                          "bordered",
                                          "condensed"),
                    position = "center",
                    font_size = 12) %>%
      row_spec(0, bold = TRUE, align = "c")
      tablascompo[[n]] <- tabla
  }

  # Presentar las tablas
  for (n in 1:numclusters){
    print(tablascompo[[n]])
  }
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# Tablas con composiciones de grupos

  # Número de tablas y lista para guardarlas

  numclusters <- nlevels(seleccion$whatcluster_j)
  tablascompo <- list()

# Bucle para generar las tablas

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
for (n in 1:numclusters){
      tabla <- seleccion %>%
      filter(whatcluster_j == as.character(n)) %>%
      select(IDIVERSE, IFIDE, IDIG) %>%
      kable(caption = paste("Método de Ward. Grupo ", n, "."),
            col.names = c("I. Diversificación",
                          "I. Fidelización",
                          "I. Digitalización"),
            digits = c(0, 3, 3, 3),
            format.args = list(decimal.mark = ".",
                               scientific = FALSE)) %>%
      kable_styling(full_width = FALSE, 
                    bootstrap_options = c("striped",
                                          "bordered",
                                          "condensed"),
                    position = "center",
                    font_size = 12) %>%
      row_spec(0, bold = TRUE, align = "c")
      tablascompo[[n]] <- tabla
   }
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
for (n in 1:numclusters){
      tabla <- seleccion %>%
      filter(whatcluster_j == as.character(n)) %>%
      select(IDIVERSE, IFIDE, IDIG) %>%
      kable(caption = paste("Método de Ward. Grupo ", n, "."),
            col.names = c("I. Diversificación",
                          "I. Fidelización",
                          "I. Digitalización"),
            digits = c(0, 3, 3, 3),
            format.args = list(decimal.mark = ".",
                               scientific = FALSE))
  tablascompo[[n]] <- tabla
  }
}
```

Las tablas obtenidas serán:

```{r, eval=TRUE, echo=FALSE, message=FALSE, results='asis'}
  # Presentar las tablas
  for (n in 1:numclusters){
    print(tablascompo[[n]])
  }
```

Para representar los grupos gráficamente, tenemos la dificultad de contar con más de dos variables clasificadoras. Una idea es **generar todas las combinaciones de variables posibles** y los correspondientes gráficos de dispersión con los casos coloreados de modo diferente según el grupo de pertenencia. Finalmente, los gráficos de modo compacto utilizando la función `create_patchwork()` que se incluyó anteriormente en el *script* para hacer mediante `{patchwork}`.composiciones de 2x2 gráficos.

Vamos a realizar la tarea de generar todos los gráficos de modo automatizado. Primero generaremos un vector con el nombre de todas las variables (excluyendo al *factor* whatcluster_j) mediante la función `setdiff()`. Luego, crearemos una lista para ir almacenando los gráficos (lista "graficos"). Por último, calcularemos todas las combinaciones de nombres de variables posibles, con la función `combn()`, y las almacenaremos en la lista "combinaciones". En esta función, el argumento **`simplify = FALSE`** le dice a la función que no vuelque el resultado a una matriz. En su lugar, devuelve una lista donde cada elemento de la misma es una combinación de los elementos del vector original.

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Gráficos Variable vs Variable

  # Lista de variables excluyendo la variable no deseada
    variables <- setdiff(names(seleccion), "whatcluster_j")

  # Lista para almacenar los gráficos
    graficos <- list()

  # Generar todas las combinaciones posibles de pares de variables
    combinaciones <- combn(variables, 2, simplify = FALSE)
```

El siguiente paso consiste en utilizar un bucle para generar los gráficos de dispersión de acuerdo a las combinaciones de variables obtenidas y guardarlos en la lista "graficos". El bucle itera tantas veces como elementos guarda la lista "combinaciones" (argumento/función `seq_along()`):

```{r, eval=TRUE, echo=TRUE, message=FALSE}
  # Bucle para crear y almacenar los gráficos
    for (i in seq_along(combinaciones)) {
      var1 <- combinaciones[[i]][1]
      var2 <- combinaciones[[i]][2]
      grafico <- ggplot(seleccion,
                        map = aes_string(x = var1,
                                         y = var2,
                                         color = "whatcluster_j")) +
                 geom_point() +
                 labs(title = paste("GRÁFICO", var1, "-", var2),
                      subtitle = "Empresas TMI") +
                 xlab (var1) +
                 ylab (var2) +
                 scale_color_brewer(palette = "Set1") 
      graficos[[paste0("grafico_", var1, "_", var2)]] <- grafico
}
```

Una vez generados los gráficos de todas las combinaciones de variables (6 gráficos en el ejemplo), y almacenados en la lista "graficos", se podrán reagrupar y presentar en composiciones de 2x2 mediante el empleo de la función `create_patchwork()`:

```{r, eval=TRUE, echo=TRUE, message=FALSE}
  # Hacer agrupaciones con la función de patchworks creada anteriormente
    gruposgraficos <- create_patchwork(graficos)

  # Presentar las composiciones
    for (n in 1:length(gruposgraficos)){
      print(gruposgraficos[[n]])
    }
```

Pueden extraerse algunas conclusiones, a partir de estos gráficos, sobre cada uno de los grupos, que confirman las ideas anteriores:

**1. Grupos 1 y 2:** empresas tradicionales de baja diversificación.

Los clústeres 1 y 2 se sitúan en la parte baja del eje de diversificación (IDIVERSE \< 30).

El clúster 2, el más numeroso, agrupa empresas con niveles medios de fidelidad (IFIDE ≈ 37-42) y baja digitalización (IDIG \< 20). Estas compañías parecen representar un modelo tradicional y estable, con una clientela relativamente fiel pero escasa inversión tecnológica. Son negocios consolidados, pero con riesgo de perder competitividad en entornos cada vez más digitalizados.

El clúster 1, aunque pequeño, muestra menor fidelidad y ligeramente más digitalización, sugiriendo empresas que están experimentando cambios estructurales o en transición, probablemente buscando nuevos nichos o adaptándose a la digitalización con dificultades.

**2. Grupo 3:** empresas equilibradas y diversificadas.

El clúster 3 reúne compañías con diversificación y fidelidad moderadamente altas (IDIVERSE e IFIDE entre 35 y 45) y digitalización intermedia (IDIG entre 20 y 35). Estas firmas parecen situarse en una posición estratégica óptima, combinando una cartera de servicios variada con clientelas estables y cierto grado de modernización. Representan el núcleo competitivo del sector, con potencial para liderar la transición tecnológica sin perder estabilidad comercial.

**3. Grupo 4:** empresa líder altamente diversificada y digitalizada

El clúster 4, compuesto por una única empresa (*Chakotay Cargo Systems*), se sitúa en los valores máximos de los tres indicadores: diversificación, fidelidad y digitalización. Esta empresa se distingue como referente del sector, con una clara orientación a la innovación y una base de clientes sólida. Su posición sugiere una estrategia de diferenciación tecnológica y de servicio, que le otorga ventajas competitivas sostenibles.

**4. Grupo 5:** empresa digital pero poco diversificada

Finalmente, el clúster 5 (*Home One Cargo*) presenta muy alta digitalización (IDIG \> 60) y fidelidad elevada, pero una diversificación reducida. Se trataría de una compañía especializada en un nicho muy digitalizado, posiblemente centrado en transporte premium o servicios automatizados. Su modelo es eficiente y tecnológicamente avanzado, aunque dependiente de un ámbito de negocio limitado.

En cuanto a las tres variables consideradas, se pueden inferir algunos patrones de relación también interesantes:

**1. Relación entre IDIVERSE e IFIDE (diversificación y fidelidad):**

En el gráfico IDIVERSE–IFIDE, los puntos muestran una ligera pendiente ascendente, aunque no muy marcada. Esto sugiere una correlación positiva débil a moderada: las empresas con una mayor diversificación tienden a mantener una clientela algo más fiel.

En el clúster 2 (el grupo mayoritario y más tradicional), la nube de puntos es bastante compacta y casi horizontal, indicando poca relación entre ambas variables. En este segmento, la fidelidad parece más determinada por la trayectoria o reputación que por la diversificación.

En cambio, el clúster 3 (empresas más diversificadas y equilibradas) sí muestra una relación más consistente: conforme aumenta la diversificación, también se mantiene o eleva la fidelidad.

El clúster 4 (chakotay cargo systems) refuerza esa tendencia, al situarse en los valores más altos de ambas variables — es decir, una empresa muy diversificada y con gran fidelidad, un ejemplo de sinergia entre ambas dimensiones.

EN conclusión, existe una correlación positiva leve, que se hace más fuerte en las empresas modernas y diversificadas.

**2. Relación entre IDIVERSE e IDIG (diversificación y digitalización):**

En el gráfico IDIVERSE–IDIG la relación parece algo más heterogénea. En general, no hay una correlación lineal clara entre ambas variables para el conjunto total, pero sí aparecen patrones diferenciados por clúster.

En los clústeres 1 y 2, la digitalización es baja y relativamente independiente de la diversificación: empresas poco diversificadas pueden tener tanto baja como media digitalización.

En el clúster 3, se aprecia una correlación positiva más nítida: las empresas con mayor diversificación presentan también mayores niveles de digitalización. Esto sugiere que estas firmas han apostado por diversificar apoyándose en la tecnología.

Los casos extremos (clústeres 4 y 5) marcan los límites de esa relación:

Clúster 4: alta diversificación + alta digitalización → perfil de liderazgo.

Clúster 5: baja diversificación + digitalización muy alta → modelo de especialización tecnológica.

EN definitiva, la relación entre diversificación y digitalización es positiva en los grupos más avanzados, pero inexistente o débil en los tradicionales.

**3. Relación entre IFIDE e IDIG (fidelidad y digitalización):**

El gráfico IFIDE–IDIG muestra una dispersión considerable, aunque con algunas señales interesantes. En la mayor parte de los casos (clústeres 1 y 2), a mayor digitalización no necesariamente corresponde mayor fidelidad; las empresas digitalizadas no parecen haber consolidado aún esa ventaja comercial.

Sin embargo, en el clúster 3 la nube de puntos tiende ligeramente al alza: las empresas con niveles medios-altos de digitalización también presentan fidelidad superior, lo que podría indicar una mejor experiencia de cliente o una oferta más personalizada gracias a la tecnología.

Los casos atípicos refuerzan la idea de estrategias distintas:

Clúster 4 (*Chakotay Cargo Systems*) combina altos niveles de ambas, señal de madurez digital y consolidación de la relación con clientes.

Clúster 5 (*Home One Cargo*) muestra muy alta digitalización pero fidelidad algo menor, típico de modelos disruptivos o muy especializados donde la clientela puede fluctuar.

Se puede concluir que la correlación global entre fidelidad y digitalización es débil, pero se vuelve positiva en empresas más equilibradas o maduras tecnológicamente.

## ![](figuras/book.svg){.hicon} ![](figuras/pie-chart.svg){.hicon} Métodos de agrupación no-jerárquicos.

Dentro del análisis clúster, los métodos de agrupación no-jerárquicos se utilizan en situaciones en las que hay un elevado número de casos que clasificar. Son especialmente útiles cuando nuestro objetivo pasa por crear grupos que definan **una tipología de casos o individuos**, más que clasificar casos o individuos concretos. En definitiva, identificar patrones de subpoblaciones a partir de una muestra. Por eso es conveniente, previamente, detectar los *outliers* y, en su caso, eliminarlos; ya que podrían distorsionar las características de los grupos debido a la sensibilidad de los algoritmos a la presencia de casos atípicos.

Una diferencia clave con respecto a los métodos jerárquicos es que **es necesario decidir *a priori* el número de conclomerados** o grupos de casos a formar. Por otro lado, son métodos más eficientes, y permiten el traslado de casos de unos grupos a otros.

Aunque existen otros métodos, la técnica no-jerárquica más común es la de **k-medias**. Este es un método iterativo. Se establece un *centroide* inicial (“semilla”) para cada uno de los k grupos que se quieren crear, y se van asignando a cada grupo los casos que se sitúen más cerca de su centro. Una vez asignados los casos, se recalculan los *centroides* de los grupos, y se repite el proceso en una nueva iteración. El procedimiento termina cuando el algoritmo encuentra la solución convergente (estable). Precisamente, la elección de las "semillas" iniciales es otro de las debilidades que presenta el método, ya que de ello puede depender la obtención de soluciones diferentes.

¿Cómo fijar el **número de grupos o conglomerados a formar**? Hay ocasiones en que las que el investigador establecerá un número que le sea manejable o útil según los objetivos que persiga. Si esto no es así, y no se tiene claro el número de grupos a construir, se podrá optar por probar con varios números, y evaluar las soluciones obtenidas. También puede ayudar el realizar previamente un análisis jerárquico para estudiar el dendograma. Además, existen métodos como el del *Ancho de Silueta*; o algoritmos, como *NbClust* en R, que sugieren un número de grupos en función de una batería de pruebas presentes en la literatura.

La segunda cuestión clave es **cómo determinar las "semillas"** o centroides iniciales. Una opción es generar las semillas de modo aleatorio, aunque no es un método muy conveniente. De hecho, cada vez que se aplicara el algoritmo de *k-medias*, podría obtenerse una solución diferente. Otra alternativa es la fijación de las "semillas" por parte del investigador. Una idea, en este sentido, es hacer un *clúster jerárquico* previo, y tomar los *centroides* de la solución final como “semillas” de k-medias. Otra posibilidad interesante es aplicar el método del *centroide más lejano*: se fija el primer centroide al azar, pero luego el 2º centroide coincidirá con el punto de datos más alejado de él. En general, el jº centroide coincidirá con el punto cuya distancia mínima a los centroides precedentes sea mayor. Se pretende que los centroides estén bien separados unos de otros. Una versión mejorada de este procedimiento es el método *k-medias++*.

Para nuestro ejemplo práctico, vamos a volver a utilizar como variables clasificadoras las del ejemplo desarrollado para los métodos jerárquicos; pero esta vez para una muestra de 300 empresas de transporte de mercancías interestelar. Estas variables son, de nuevo, los índices de diversificación (IDIVERSE), fidelización de clientes (IFIDE) y de digitalización (IDIG). Con base en ellos, queremos **establecer una serie de perfiles o tipologías** de las empresas que componen el sector.

Trabajaremos, como en el ejemplo de clúster jerarquizado, en el proyecto llamado "cluster". Vamos a ir a la carpeta del proyecto y vamos a guardar en ella los dos archivos de esta práctica: un archivo de Microsoft® Excel® llamado “interestelar_300.xlsx" y un *script* denominado "kmedias_rstars.R". En la última sección del capítulo dispones de los enlaces a dicho material.

Si abrimos el archivo de Microsoft® Excel®, "interestelar_300.xlsx", comprobaremos que tiene la misma estructura y "hojas" que nos encontramos en "interestelar_25.xlsx"; pero extendido a 300 empresas o compañías.

### ![](figuras/pie-chart.svg) Preparación de los datos.

La primera parte del script es semejante a la del *script* de la parte de "clúster jerárquico", *cluster_rstars.R*. Solo cambia en los siguientes aspectos:

-   No utiliza el paquete `{factoextra}`, y sí utiliza los paquetes `{cluster}`y `{ClusterR}`.
-   El archivo a importar es *interestelar_300.xlsx*, y los datos se almacenan en el *data frame* "interestelar_300".
-   Los *outliers*, en esta ocasión, sí son eliminados, con lo que se trabajará con un *data frame* sin casos atípicos, denominado "seleccion_so".

Dicho esto, pasamos a transcribir el código correspondiente a la limpieza del *Global Environment*, carga de paquetes, función para crear composiciones de 2X2 gráficos con `{patchwork}`, importación de datos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
### CLUSTER de k-medias empresas TMI. ###

# Limpiando el Global Environment
rm(list = ls())

# Cargando paquetes
library(readxl)
library(dplyr)
library(ggplot2)
library(gtExtras)
library(visdat)
library (cluster)
library (ClusterR)
library (knitr)
library (kableExtra)
library (patchwork)
library (pgirmess)

##### Función para crear composiciones de gráficos con patchwork ###############
create_patchwork <- function(plot_list) {
  n <- length(plot_list)
  if (n == 0) return(NULL)
  full_rows <- n %/% 4
  remaining <- n %% 4
  patchworks <- list()
  
  if (full_rows > 0) {
    for (i in seq(1, full_rows * 4, by = 4)) {
      patchworks <- c(patchworks, list((plot_list[[i]] + plot_list[[i+1]]) / 
                                         (plot_list[[i+2]] + plot_list[[i+3]])))
    }
  }
  
  if (remaining > 0) {
    last_plots <- plot_list[(full_rows * 4 + 1):n]
    empty_plots <- lapply(1:(4 - remaining), function(x) ggplot() + theme_void())
    last_patchwork <- do.call(patchwork::wrap_plots, c(last_plots, empty_plots))
    patchworks <- c(patchworks, list(last_patchwork))
  }
  return(patchworks)
}
################################################################################

## DATOS

# Importando datos desde Excel
interestelar_300 <- read_excel("interestelar_300.xlsx",
                              sheet = "Datos",
                              na = c("n.d."))
interestelar_300 <- data.frame(interestelar_300, row.names = 1)

# Seleccionando variables metricas para el analisis.
seleccion <- interestelar_300 %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion_df_graph <- gt_plt_summary(seleccion)
seleccion_df_graph

# Localizando missing values.
seleccion %>%
  vis_miss() +
  labs(title = "Indicadores: Diversificación, Fidelidad, Digitalización",
       subtitle = "Transporte de mercancías interestelar",
       y = "Observación",
       fill = NULL) +
  scale_fill_manual(
    values = c("TRUE" = "red", "FALSE" = "grey"),
    labels = c("TRUE" = "NA", "FALSE" = "Presente")) +
  theme(
    plot.title = element_text(face = "bold", size = 14))

seleccion %>% filter(is.na(IDIVERSE) |
                       is.na(IFIDE) |
                       is.na(IDIG)) %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion <- seleccion %>%
  filter(! is.na(IDIVERSE) &
           ! is.na(IFIDE) &
           ! is.na(IDIG)) 
```

Aunque se ha incluido el código para identificar y eliminar los casos con ***missing values***, el gráfico generado con la función `vis_miss()` ya informó de que no había ningún dato ausente en las tres variables de interés, almacenadas en el *data frame* "seleccion", luego se siguen manteniendo los 300 casos.

Para la detección de ***outliers***, se ha utilizado también el mismo código que en el análisis clúster jerárquico: obtención de las distancias de *Mahalanobis*, y aplicación de diagrama de caja o *boxplot*: 

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Identificando outliers con distancia de Mahalanobis.
seleccion <- seleccion %>%
  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),
                                   center = colMeans(.),
                                   cov    = cov(.)))

ggplot(data = seleccion, map = (aes(y = MAHALANOBIS))) +
  geom_boxplot(fill = "orange") +
  ggtitle("DISTANCIA DE MAHALANOBIS",
          subtitle = "IDIVERSE, IFIDE, IDIG. Empresas TMI.") +
  ylab("MAHALANOBIS")
```

Una vez se constata que existen casos *outliers*, se pueden identificar con el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
Q1M <- quantile (seleccion$MAHALANOBIS, c(0.25))
Q3M <- quantile (seleccion$MAHALANOBIS, c(0.75))

seleccion %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
         MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(MAHALANOBIS, IDIVERSE, IFIDE, IDIG)
```

Puede comprobarse que son 26 casos. En esta ocasión, y a diferencia del caso del análisis jerárquico, vamos a eliminar estas observaciones, ya que lo que deseamos no es agrupar casos concretos, sino identificar patrones que caractericen a la mayor parte de las empresas. Para eliminar los *outliers*, ejecutaremos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Eliminando outliers.
seleccion_so <-seleccion %>%
  filter(MAHALANOBIS <= Q3M + 1.5*IQR(MAHALANOBIS) &
         MAHALANOBIS >= Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(IDIVERSE, IFIDE, IDIG)
```

Hemos creado un *data frame* llamado "seleccion_so" con las variables de nuestro análisis; pero sin los casos considerados atípicos. Por último, eliminaremos la variable MAHALANOBIS, ya que no será necesaria para el resto del análisis:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Eliminando variable MAHALANOBIS del df con outliers
seleccion <- seleccion %>% select(-MAHALANOBIS)
```

### ![](figuras/pie-chart.svg) Determinación del número de grupos o clústeres a formar.

Una vez preparados los datos, vamos a proceder a aplicar la técnica de formación de conglomerados no-jerárquica de **k-medias**. Primero, y puesto que se basa en el cálculo de las distancias euclídeas entre casos, procederemos a tipificar los valores de las variables, utilizando la función `scale()`, y creando el data frame "zseleccion_so". Después, calcularemos la matriz de distancias **d**:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
## CLUSTER K-MEDIAS CON VARIABLES ORIGINALES

# Tipificando variables
zseleccion_so <- data.frame(scale(seleccion_so))
summary(zseleccion_so)

d <- dist(zseleccion_so)
```

Una cuestión clave es la determinación previa del número de grupos a formar. Si no se tiene decidido un número de conglomerados *a priori* como consecuencia del interés o de los objetivos de la propia investigación, se puede recurrir como orientación a algún método o algoritmo "objetivo". De nuevo, puede recurrirse al **método de la *anchura media de silueta**, proponiendo diferentes números de grupos (parámetro *k*).

La función `KMeans_rcpp()` del paquete `{ClusterR}` permite calcular la *anchura media de Silueta* para cada número de grupos propuesto, estimando los grupos mediante el método de **k-medias++**. Aplicaremos el método para cada número de grupos *k* propuesto, mediante una función que guarda el ancho medio de silueta en un vector:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# --- Calcular el ancho medio de silueta para distintos k ---
set.seed(123)
res_sil <- sapply(2:10, 
                  function(k) {
                    km <- KMeans_rcpp(zseleccion_so,
                    clusters = k,
                    num_init = 10,
                    max_iters = 100,
                    initializer = "kmeans++")
  mean(silhouette(km$clusters, d)[, "sil_width"])
}
)
```

El código anterior ejecuta la siguiente secuencia:

1. `set.seed(123)` fija la “semilla” aleatoria. *k-medias* (incluso con *kmeans++*) usa un poco de azar al empezar; fijar la semilla hace que siempre se obtengan los mismos resultados al repetir el código (reproducibilidad).

2.  `sapply(2:10, function(k) { ... })`:

  - 2:10 es la secuencia de valores de *k* que vamos a probar (de 2 a 10 clústeres).

  - `sapply(..., function(k) { ... })` aplica la función a cada *k* y devuelve el vector numérico *res_sil* con los resultados.
  
3.  Dentro de la función anónima `function(k) { ... }`:

  - `KMeans_rcpp()` (del paquete `{ClusterR}`) ejecuta *k-medias* sobre los datos del *data frame* "zseleccion_so".

  - `clusters = k`: número de grupos que queremos formar.
  
  - `num_init = 10`: ejecuta el algoritmo 10 veces con diferentes inicios y se queda con la mejor solución (esto reduce la mala suerte de una mala inicialización).
  
  - `max_iters = 100`: tope de iteraciones por ejecución.
  - `initializer = "kmeans++"`: estrategia de inicio que suele dar mejores resultados que un inicio totalmente aleatorio.
  
El objeto `km` contiene, entre otras cosas, `km$clusters`, que es el vector de asignaciones: dice a qué cluster pertenece (1, 2, 3, …, k) cada caso.

`silhouette(...)` (del paquete cluster) calcula, para cada observación, su ancho de silueta. Necesita:

  - `km$clusters`: las etiquetas de clúster.

  - `d`: matriz de distancias entre observaciones.

El resultado de silhouette es una matriz con columnas; una de ellas es `"sil_width" =`, que es el ancho de silueta de cada punto.

  - `[...] [, "sil_width"]` extrae esa columna.

  - `mean(...)` hace la media: se obtiene el ancho medio de la silueta para ese k.

El resultado final es el vector "res_sil", con 9 valores (para k = 2, 3, …, 10). Cada valor es el ancho medio de silueta correspondiente. Servirá para crear, junto a cada valor de "k", un pequeño *data frame* de nombre "df_sil" que será la base, a su vez, para representar los *anchos de silueta* gráficamente con `{ggplot2}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# --- Crear data frame para graficar ---
df_sil <- data.frame(
  k = 2:10,
  Silhouette = res_sil
)

# --- Gráfico con ggplot2 ---
ggplot(df_sil, aes(x = k, y = Silhouette)) +
  geom_line(color = "steelblue", linewidth = 1) +
  geom_point(size = 3, color = "darkred") +
  labs(title = "Ancho medio de Silueta para distintos k (k-means++)",
       x = "Número de clústeres (k)",
       y = "Silueta media") +
  theme_minimal(base_size = 13)
```

### ![](figuras/pie-chart.svg) Aplicación del método de k-medias con determinación de semillas *kmeans++*.^

Una vez decidido el número de grupos (en el ejemplo, 7), se aplica (de nuevo) el método de ***k-medias***, en la versión de la función `KMeans_rcpp()` del paquete `{ClusterR}`, que permite obtener las "semillas" (centroides iniciales) mediante el algoritmo *kmeans++*, que ofrece buenos resultados frente a otras posibilidades de obtención de "semillas", como la generación puramente aleatoria. La solución final se guarda en el objeto “cluster_k”:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Aplicación definitiva de k-medias.
  k <- 7  # poner aquí número de grupos decidido!!!!

  # Aplicando k-means con inicializacion kmeans++

  cluster_k <-KMeans_rcpp (zseleccion_so,
                           clusters = k,
                           num_init = 10,
                           max_iters = 100,
                           initializer = "kmeans++")
```

Como se acaba de explicar, en la función `KMeans_rcpp()`, el primer argumento es el *data frame* con las variables clasificadoras (en sus versiones tipificadas). El segundo es el número de clústeres o grupos a obtener (que lo hemos asignado anteriormente al parámetro “**k**”). El tercero, `num_init =`, es el número de veces que se repite el procedimiento a fin de retener la mejor solución. `Max_iters =` fija el máximo de iteraciones del procedimiento de *k-medias* hasta obtener una solución estable. `Initializer =` define el método de obtención de las semillas (en nuestro caso, *k-means++*). La solución final se guarda en el objeto “cluster_k”, como se puede apreciar en el *Environment*.

Dentro de la solución, el vector con el grupo de pertenencia de cada empresa se obtiene con el elemento “\$clusters”. Conviene guardar ese vector como factor. En concreto, se ha añadido como el factor **“whatcluster_k”**, integrado dentro del *data frame* "seleccion_so".

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  seleccion_so$whatcluster_k <- as.factor(cluster_k$clusters)
```

### ![](figuras/pie-chart.svg) Caracterización de los grupos o conglomerados.

A continuación vamos a **caracterizar los grupos** formados en función de las medias de las variables originales (coordenadas de los *centroides*). Así, se podrán mostrar en pantalla las medias de cada grupo de las distintas variables originales, usando las funciones `by_group()` y `summarise()` de `{dplyr}`. Toda la información se asigna al *data frame* “tablamedias”; para poder, posteriormente, representarla en una tabla mediante las facilidades que ofrecen los paquetes `{knitr}` y `{kableExtra}`:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# CARACTERIZANDO GRUPOS FORMADOS
  
  # Tabla con centroides de grupos.
  tablamedias <- seleccion_so %>%
    group_by(whatcluster_k) %>%
    summarise(obs = length(whatcluster_k),
              Idiverse = mean(IDIVERSE),
              Ifide = mean(IFIDE),
              Idig = mean(IDIG))
  
  tablamedias %>%
    kable(caption = "Método de k-medias. 7 grupos. Medias de variables",
          col.names = c("Clúster",
                        "Observaciones",
                        "I. Diversif.",
                        "I. Fidelizac.",
                        "I. Digitalizac."),
          digits = c(NA, 0, 3, 3, 3),
          format.args = list(decimal.mark = ".",
                             scientific = FALSE)) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped",
                  "bordered",
                  "condensed",
                  position = "center",
                  font_size = 11) %>%
    row_spec(0, bold= T,
             align = "c") %>%
    row_spec(1:nrow(tablamedias),
             bold= F,
             align = "c")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# CARACTERIZANDO GRUPOS FORMADOS
  
  # Tabla con centroides de grupos.
  tablamedias <- seleccion_so %>%
    group_by(whatcluster_k) %>%
    summarise(obs = length(whatcluster_k),
              Idiverse = mean(IDIVERSE),
              Ifide = mean(IFIDE),
              Idig = mean(IDIG))

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

    if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {

    tablamedias %>%
    kable(caption = "Método de k-medias. 7 grupos. Medias de variables",
          col.names = c("Clúster",
                        "Observaciones",
                        "I. Diversif.",
                        "I. Fidelizac.",
                        "I. Digitalizac."),
          digits = c(NA, 0, 3, 3, 3),
          format.args = list(decimal.mark = ".",
                             scientific = FALSE)) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped",
                  "bordered",
                  "condensed",
                  position = "center",
                  font_size = 11) %>%
    row_spec(0, bold= T,
             align = "c") %>%
    row_spec(1:nrow(tablamedias),
             bold= F,
             align = "c")
    }else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {      
tablamedias %>%
    kable(caption = "Método de k-medias. 7 grupos. Medias de variables",
          col.names = c("Clúster",
                        "Observaciones",
                        "I. Diversif.",
                        "I. Fidelizac.",
                        "I. Digitalizac."),
          digits = c(NA, 0, 3, 3, 3),
          format.args = list(decimal.mark = ".",
                             scientific = FALSE))
    }
```

Obviamente, también se podrían comparar las medias de los grupos, para cada variable, con un gráfico de barras. El código es el siguiente:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Gráficos de centroides
  
  # Vector de nombre de variables excluyendo la variable no deseada
  variables <- setdiff(names(tablamedias), c("whatcluster_k", "obs"))
  
  # Lista para almacenar los gráficos
  graficos.centroides <- list()
  
  # Bucle para crear y almacenar los gráficos
  for (i in seq_along(variables)) {
    var1 <- variables[[i]]
    grafico <- ggplot(data= tablamedias,
                      map = (aes_string(y = var1, x = "whatcluster_k"))) +
      geom_bar(stat = "identity",
               colour = "red",
               fill = "orange",
               alpha = 0.7) +
      ggtitle(paste0(var1, ". Media por grupos."),
              subtitle = "Empresas TMI.")+
      xlab ("Grupo") +
      ylab(var1)
    graficos.centroides[[paste0("grafico_", var1)]] <- grafico
  }          
```

Para hacer composiciones de 4 gráficos (que en este caso será solo una, dado que hemos almacenado en la lista "graficos.centroides" 4 elementos correspondientes a las 4 variables clasificadoras), volveremos a utilizar la función `create_patchwork()`, que ya se mostró en el ejemplo de clúster jerárquico. De nuevo se incluye su código y se aplica a la lista de gráficos "graficos.centroides":

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Aplicar función de composiciones a gráficos de centroides.
  grupos.graficos.centroides <- create_patchwork(graficos.centroides)
  
  # Presentar las composiciones
  for (n in 1:length(grupos.graficos.centroides)){
    print(grupos.graficos.centroides[[n]])
  }
```

La lista "grupos.graficos.centroides" almacena las composiciones de 4 (o menos) gráficos generadas. En este ejemplo, solo tiene un elemento, que es mostrado al ejecutar el bucle de presentación de composiciones.

Por medio de la tabla y los gráficos anteriores se pueden trazar algunas conclusiones, comparando las medias de las variables para cada grupo de casos formado (coordenadas de los centroides). El análisis revela una clara heterogeneidad estructural entre las empresas de transporte de mercancías interestelar. Las tres dimensiones —diversificación, fidelización y digitalización— se combinan de manera diferente en cada grupo, reflejando distintos modelos de negocio y grados de madurez estratégica. Así, podemos distinguir tres grandes perfiles:

- Empresas avanzadas e innovadoras (altos niveles en las tres dimensiones).

- Empresas especializadas tradicionales, con baja digitalización y diversificación.

- Empresas en transición o de nicho, con fortalezas parciales (por ejemplo, alta fidelidad pero baja digitalización).

Analizando cada grupo de modo individualizado:

- **Grupo 1** (57 empresas). Alta diversificación (35.8), alta fidelidad (40.2), digitalización media (22.2). En general, el grupo se compone de empresas sólidas, con una base de clientes fiel y estrategias diversificadas que les permiten operar en distintos segmentos o rutas. Aunque su digitalización es moderada, su estructura sugiere estabilidad y resiliencia. Perspectiva futura: Si invierten más en tecnología, podrían convertirse en líderes del sector, con ventajas competitivas sostenibles a largo plazo.

- **Grupo 2** (17 empresas). Diversificación media-baja (17.9), muy alta fidelidad (43.1), alta digitalización (26.0). Se compone de empresas tecnológicamente dinámicas y con gran fidelidad, pero con un negocio poco diversificado. Probablemente dominan un nicho específico del transporte interestelar.
Perspectiva futura: Su foco estratégico y digitalización pueden asegurarles crecimiento, aunque deberían explorar una mayor diversificación para reducir riesgos sectoriales.

- **Grupo 3** (17 empresas). Baja diversificación (15.8), fidelidad media (34.8), digitalización media (21.8). Las empresas que conforman este conglomerado se encuentran, en general, en fase de consolidación, con cierto desarrollo digital pero sin un posicionamiento claro ni fidelización destacable. Perspectiva futura: Si no fortalecen su propuesta de valor o amplían su mercado, podrían quedar rezagadas frente a competidores más innovadores.

- **Grupo 4** (30 empresas). Muy baja diversificación (7.5), fidelidad baja (31.6), muy baja digitalización (7.8). Este grupo reúne las empresas más rezagadas, con escasa adaptación tecnológica y un modelo de negocio poco flexible.Perspectiva futura: Riesgo elevado de obsolescencia o pérdida de cuota de mercado. Necesitan transformarse urgentemente mediante digitalización y estrategias de retención de clientes.

- **Grupo 5** (74 empresas, grupo más numeroso). Diversificación baja (11.1), fidelidad media-alta (38.1), muy baja digitalización (6.8). Grupo formado, en término medio, por empresas tradicionales, que conservan buena relación con sus clientes pero no han avanzado en digitalización. Representan el núcleo clásico del sector TMI. Perspectiva futura: Son vulnerables a la disrupción tecnológica. Sin inversión en digitalización y diversificación, podrían perder relevancia en mercados más competitivos.

- **Grupo 6** (18 empresas). Alta diversificación (35.9), muy alta fidelidad (43.7), muy alta digitalización (33.7). Grupo constituido por las empresas punteras del sector, con máximos en las tres dimensiones. Combinan diversificación de servicios, lealtad de clientes y transformación digital. Perspectiva futura: Tienen el liderazgo asegurado y marcan el estándar competitivo. Probablemente sean grandes corporaciones interplanetarias con capacidad de expansión internacional.

- **Grupo 7** (61 empresas). Diversificación media (24.6), fidelidad alta (39.3), digitalización baja-media (14.8). Grupo formado por empresas en transición digital, con buen posicionamiento comercial y cierta diversificación. Perspectiva futura: Si continúan su proceso de digitalización, pueden convertirse en empresas de alto rendimiento similares al grupo 1 o incluso al 6.

Conclusión **general**: El Grupo 6 es el referente: altamente competitivo, innovador y sostenible. El Grupo 4 y parte del Grupo 5 enfrentan los mayores desafíos estructurales. Los Grupos 1 y 7 son prometedores si logran acelerar su digitalización. Los Grupos 2 y 3 podrían especializarse o asociarse para sobrevivir en un mercado cada vez más concentrado y tecnológico. En conjunto, el análisis sugiere que la digitalización se convierte en el factor diferenciador clave para la supervivencia y crecimiento en el transporte de mercancías interestelar, actuando como catalizador de la fidelidad y la diversificación.

El análisis gráfico anterior se puede complementar de un modo más formal, a fin de verificar si las diferencias observadas entre los valores medios de los grupos, para cada variable, son significativas. Para ello, y teniendo en cuenta que los grupos se pueden considerar submuestras que representan a subpoblaciones, se puede aplicar alguna prueba de comparaciones múltiples de las medias de los grupos formados, de modo que se pueda confirmar, para cierta significación estadística (usualmente 0,05), **si las diferencias en las medias de los grupos para cada variable son estadísticamente relevantes** (significativas) o no.

Una prueba clásica para llevar a cabo esta tarea es aplicar el *test de comparaciones múltiples de Tuckey*. No obstante, y dado que esta prueba requiere del cumplimiento de ciertos requisitos previos (normalidad de los grupos, varianzas homogéneas); hemos optado por la ***prueba robusta de Kruskal-Wallis*****.** Para cada una de las variable originales, realizaremos un gráfico múltiple de diagramas de caja, y procederemos a mostrar los resultados de la prueba.

En primer lugar, vamos a definir el vector con el nombre de las variables que entran en el análisis (vector "variables"), e inicializaremos las listas para guardar los gráficos y los resultados de la prueba para cada variable ("graficos_kw" y "tablas_kw", respectivamente): 

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # ¿Centroides estadísticamente significativos?

    # Vector de nombre de variables excluyendo la variable no deseada
      variables <- setdiff(names(seleccion_so), "whatcluster_k")

    # Inicializar listas para almacenar gráficos y tablas
      graficos_kw <- list()
      tablas_kw <- list()
```

Luego, haremos un bucle para que se realice el gráfico de caja para cada una de las variables. Los gráficos se almacenan en la lista "graficos_kw", que luego pasan a la función `create_patchwork()`para agruparse en elementos de la lista "gruposgraficos_kw" (que solo tendrá un elemento, puesto que únicamente hay tres gráficos que agrupar en composiciones de 4 o menos. 

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # ¿Diferencias entre centroides estadísticamente significativas?

   # Vector de nombre de variables excluyendo la variable no deseada
      variables <- setdiff(names(seleccion_so), "whatcluster_k")

   # Bucle para generar gráficos
     for (i in seq_along(variables)) {
          variable <- variables[i]
  
    # Crear el gráfico
      p <- ggplot(data = seleccion_so,
                  aes_string(x = "whatcluster_k",
                             y = variable,
                             fill = "whatcluster_k")) +
           geom_boxplot(outlier.shape = NA) +
           stat_summary(fun = "mean",
                        geom = "point",
                        size = 3,
                        col = "red") +
           stat_summary(fun = "mean",
                        geom = "line",
                        col = "red",
                        aes(group = TRUE)) +   
           geom_jitter(width = 0.1,
                       size = 1,
                       col = "red",
                       alpha = 0.40) +
           ggtitle(paste(variable, ". Comparación de grupos."),
                   subtitle = "Empresas TMI.") +
           ylab("Valor")
  
    # Almacenar el gráfico en la lista
      graficos_kw[[i]] <- p
    }

    # Crear composiciones de gráficos.
            gruposgraficos_kw <- create_patchwork(graficos_kw)    

            for (i in seq_along(gruposgraficos_kw)) {
              print(gruposgraficos_kw[[i]])
    }   
```

En cuanto a la prueba de *comparaciones múltiples*, la función que se ocupa de llevarla a cabo es `kruskalmc()`, del paquete `{pgirmess}`. la solución se guarda en un objeto, por ejemplo, "datos_kmc", y los argumentos son, por un lado, la variable analizada y el factor que se utiliza para denominar los grupos ("whatcluster_k"), unidos por el símbolo "\~". La función se aplica dentro del bucle que permite aplicar la prueba a cada una de las tres variables. Además, los resultados se pasan en tablas que son almacenadas en la lista "tablas_kw":

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
    # Realizar el análisis de Kruskal-Wallis  y llevar resultados a tablas

      tablas_kw <- list()
      for (i in seq_along(variables)) {
         variable <- variables[i]
         datos_kmc <- kruskalmc(as.formula(paste(variable, "~ whatcluster_k")),
                                 data = seleccion_so)
              
         tabla <- datos_kmc$dif.com %>%
         kable(caption = paste("k-medias. Diferencias de Centroides", variable),
               col.names = c("Diferencias centros",
                             "Diferencias críticas",
                             "Significación"),
               digits = c(3, 3, NA),
               format.args = list(decimal.mark = ".",
                                  scientific = FALSE)) %>%
         kable_styling(full_width = F,
                bootstrap_options = c("striped",
                                      "bordered",
                                      "condensed"),
                position = "center",
                font_size = 11) %>%
         row_spec(0, bold = T, align = "c") %>%
         row_spec(1:nrow(datos_kmc$dif.com), bold = F, align = "c")

    # Almacenar la tabla en la lista
      
      tablas_kw[[i]] <- tabla
      }
  
    # Mostrar tablas almacenadas

      for (i in seq_along(tablas_kw)) {
           print(tablas_kw[[i]])
      }
```

A partir del código anterior se obtienen los siguientes resultados, concernientes al análisis de comparaciones múltiples (de medias) de *Kruskal-Wallis*. Cada fila compara dos grupos. Las columnas indican:

- Diferencias centros: diferencia absoluta entre los rangos medios de esos dos grupos.

- Diferencias críticas: umbral mínimo que esa diferencia debe superar para considerarse significativa.

- Significación (TRUE/FALSE): indica si la diferencia es estadísticamente significativa (TRUE) o no (FALSE).

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

    # Realizar el análisis de Kruskal-Wallis  y llevar resultados a tablas

            tablas_kw <- list()
            for (i in seq_along(variables)) {
              variable <- variables[i]
              datos_kmc <- kruskalmc(as.formula(paste(variable, "~ whatcluster_k")),
                                     data = seleccion_so)
if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
              tabla <- datos_kmc$dif.com %>%
                kable(caption = paste("k-medias. Diferencias de Centroides", variable),
                      col.names = c("Diferencias centros",
                                    "Diferencias críticas",
                                    "Significación"),
              digits = c(3, 3, NA),
              format.args = list(decimal.mark = ".",
                                 scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = c("striped",
                                      "bordered",
                                      "condensed"),
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold = T, align = "c") %>%
  row_spec(1:nrow(datos_kmc$dif.com), bold = F, align = "c")
  }else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
tabla <- datos_kmc$dif.com %>%
                kable(caption = paste("k-medias. Diferencias de Centroides", variable),
                      col.names = c("Diferencias centros",
                                    "Diferencias críticas",
                                    "Significación"),
              digits = c(3, 3, NA),
              format.args = list(decimal.mark = ".",
                                 scientific = FALSE))
  }
   # Almacenar la tabla en la lista
     tablas_kw[[i]] <- tabla
  }
  
    # Mostrar tablas almacenadas

      for (i in seq_along(tablas_kw)) {
           print(tablas_kw[[i]])
      }  
```

En el caso de la variable **IDIVERSE** (grado de diversificación del negocio), se aprecia que los grupos 1 y 6, con los mayores valores medios de diversificación (~35 puntos), no difieren entre sí, pero sí del resto. Son empresas altamente diversificadas, posiblemente grandes corporaciones con presencia en múltiples rutas galácticas y segmentos de carga. Por otro lado, los grupos 4 y 5, con valores muy bajos (≈7–11), se sitúan en el extremo opuesto, significativamente diferentes de casi todos los demás. Representan compañías especializadas o locales, con operaciones limitadas y modelos menos flexibles. Los grupos 2, 3 y 7 ocupan posiciones intermedias, sin diferencias significativas entre ellos. Podrían estar en una fase de transición o exploración de nuevos mercados, con estrategias aún en consolidación.

En cuanto a la variable **IFIDE** (grado de fidelización de los clientes del negocio), se puede deducir que los grupos 2 y 6 son los “campeones de fidelización”: no muestran diferencias significativas entre sí, pero sí con casi todos los demás grupos. Representan empresas muy consolidadas comercialmente, con redes de clientes leales. Económicamente, disfrutan de ingresos estables y bajos costes de adquisición de clientes, lo que les confiere ventaja competitiva sostenida. Por otro lado, los grupos 1 y 7 exhiben una “alta fidelidad consolidada”, similar entre ambos, sin diferencias significativas con los campeones (2 y 6) en algunos casos. Estas compañías probablemente combinan buen servicio y reputación con cierto margen para mejorar la digitalización o la personalización de su oferta. Adicionalmente, el grupo 5 se caracteriza por una fidelidad media, es decir, alcanzan un nivel aceptable, pero significativamente inferior al de los grupos líderes. Finalmente, los grupos 3 y 4 adolecen de una fidelización débil. No difieren entre sí, pero sí del resto. Son, en general grupos formados por empresas con poca orientación al cliente o escaso conocimiento del mismo, probablemente con modelos operativos más básicos y precios competitivos. Desde una perspectiva de futuro, son las más vulnerables a la pérdida de clientes y a la competencia.

PO último, en relación con la variable **IDIG** (grado de digitalización de cada empresa: integración de sistemas inteligentes, automatización logística, uso de IA en gestión de flotas, plataformas de clientes, etc.), podemos distinguir cuatro niveles tecnológicos:

- Grupo 6 (media 33.7). Difere significativamente de casi todos los grupos, menos 1, 2 y 3. Representa a las empresas más avanzadas tecnológicamente, probablemente grandes corporaciones con sistemas integrados de navegación, inteligencia artificial en mantenimiento predictivo y plataformas de comercio galáctico.

- Grupos 1 (22.2), 2 (26.0), 3 (21.8). No hay diferencias significativas entre ellos, lo que indica un bloque intermedio tecnológicamente.Han avanzado en digitalización (probablemente con sistemas de trazabilidad o automatización parcial), pero aún no alcanzan la madurez total del grupo 6.

- Grupo 7 (14.8). Difiere significativamente de casi todos los grupos más avanzados, pero no de 3. Empresas con digitalización parcial o básica (gestión digital de clientes o control de flota limitado).

- Grupos 4 (7.8) y 5 (6.8). No difieren entre sí, pero sí de todos los demás.Son los más rezagados tecnológicamente: probablemente dependientes de procesos manuales, escasa automatización y baja conectividad. 

Finalmente, es interesante mostrar cómo los elementos de cada grupo se disponen en los diagramas de dispersión producto de cruzar las variables clasificadoras entre sí. Para generar estos gráficos de un modo automatizado, primero generaremos un vector con el nombre de todas las variables (excluyendo al *factor* whatcluster_k mediante la función `setdiff()`. Luego, crearemos una lista para ir almacenando los gráficos (lista "graficos").a continuación, calcularemos todas las combinaciones de nombres de variables posibles, con la función `combn()`, y las almacenaremos en la lista "combinaciones". En esta función, el argumento **`simplify = FALSE`** le dice a la función que no vuelque el resultado a una matriz:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# GRÁFICOS Variable vs Variable

  # Lista de variables excluyendo la variable no deseada
    variables <- setdiff(names(seleccion_so), "whatcluster_k")

  # Lista para almacenar los gráficos
    graficos <- list()

  # Generar todas las combinaciones posibles de pares de variables
    combinaciones <- combn(variables, 2, simplify = FALSE)
```

Una vez almacenados los elementos anteriores, procederemos a crear los gráficos de dispersión mediante un bucle. Estos gráficos se guardarán en la lista "graficos", que se pasará por la función `create_patchwork()` para que se sinteticen en composiciones de 4 (o menos) gráficos. Estas composiciones se almacenan en la lista "gruposgraficos", y se presentan mediante un nuevo bucle:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Bucle para crear y almacenar los gráficos
    for (i in seq_along(combinaciones)) {
         var1 <- combinaciones[[i]][1]
         var2 <- combinaciones[[i]][2]
         grafico <- ggplot(seleccion_so,
                           map = aes_string(x = var1,
                                            y = var2,
                                            color = "whatcluster_k")) +
         geom_point() +
         labs(title = paste("GRÁFICO", var1, "-", var2),
              subtitle = "Empresas eólicas") +
         xlab (var1) +
         ylab (var2) +
         scale_color_brewer(palette = "Set1") 
         graficos[[paste0("grafico_", var1, "_", var2)]] <- grafico
    }

  # Aplicar función de composiciones de patchwork

    gruposgraficos <- create_patchwork(graficos)

  # Presentar las composiciones
    for (n in 1:length(gruposgraficos)){
         print(gruposgraficos[[n]])
    }
```

Los gráficos de dispersión muestran con claridad la estructura espacial de los siete clústeres y su correspondencia con los niveles de **diversificación**, **fidelización** y **digitalización** de las empresas. Los grupos 4 y 5 se concentran en la zona inferior izquierda de los diagramas, caracterizados por los valores más bajos en todas las variables —empresas con escasa diversificación y digitalización, y menor fidelidad de clientes—, mientras que el grupo 6 se sitúa de forma destacada en el extremo superior derecho, evidenciando su liderazgo tecnológico y su alta diversificación. Los grupos 1 y 7 se ubican en posiciones intermedias, aunque algo más desplazados hacia niveles medios-altos de fidelidad y diversificación, lo que los perfila como empresas con potencial de crecimiento. Finalmente, los grupos 2 y 3, más concentrados en la parte superior central, presentan altos niveles de fidelización y moderada digitalización, pero con menor dispersión en diversificación, lo que sugiere estrategias más focalizadas.

En conjunto, los diagramas reflejan relaciones positivas entre las tres variables, especialmente entre diversificación y digitalización (IDIVERSE–IDIG), donde se observa un patrón casi lineal: las empresas más diversificadas tienden también a estar más digitalizadas. La relación entre fidelización y digitalización (IFIDE–IDIG) es más dispersa, aunque se vislumbra una pendiente positiva: la digitalización parece acompañarse de cierta mejora en la fidelidad de los clientes. En cambio, la asociación fidelidad–diversificación (IFIDE–IDIVERSE) resulta más compleja, con un grupo principal en torno a valores medios-altos de fidelización independientemente de la diversificación, lo que podría indicar que la fidelidad no depende directamente de la amplitud del negocio, sino de otros factores como la calidad del servicio o la relación cliente–empresa.

## ![](figuras/book.svg){.hicon} ![](figuras/pie-chart.svg){.hicon} Clustering basado en densidad (DBSCAN).

El método **DBSCAN** —acrónimo de *Density-Based Spatial Clustering of Applications with Noise*— pertenece a la familia de algoritmos de agrupación basados en densidad.

A diferencia de los métodos jerárquicos o de partición, que buscan minimizar distancias o varianzas internas, DBSCAN identifica regiones del espacio donde los puntos están suficientemente “apiñados” y las considera clústeres naturales, mientras que los puntos aislados son clasificados como ruido o outliers.

La idea central es muy intuitiva. Imaginemos que recorremos el espacio de datos con un círculo (en tres dimensiones, una esfera) de radio eps. Si ese círculo abarca al menos minPts observaciones, el punto central se considera parte de una zona densa. Si la densidad se propaga a los vecinos, se forma un clúster; si no, el punto se etiqueta como ruido.

Así, el algoritmo forma clústeres a partir de regiones densas y deja fuera los puntos dispersos.

En este método, cada punto o caso se clasifica según su *vecindad*:

- **Punto núcleo (core point):** tiene al menos `minPts` vecinos a una distancia menor que `eps`.  
- **Punto borde (border point):** no cumple esa condición, pero pertenece a la vecindad de un punto núcleo.  
- **Ruido:** punto que no pertenece a ningún clúster.

El proceso se repite hasta que todos los puntos han sido etiquetados.
De este modo, DBSCAN no necesita especificar el número de clústeres a priori: el número total emerge de los patrones de densidad del propio conjunto de datos.

Los dos **hiperparámetros clave** de la técnica son:

| Parámetro | Significado | Efecto principal |
|------------|--------------|------------------|
| `eps` | Radio máximo para considerar vecindad | Si es muy grande, fusiona clústeres distintos. Si es muy pequeño, genera exceso de ruido. |
| `minPts` | Número mínimo de puntos para formar un clúster denso | Valores típicos entre 4 y 10 según el tamaño de muestra. |

Como **ventajas** frente a los métodos *jerárquicos* y de *k-medias* podemos destacar:

- **No requiere fijar k**: a diferencia de *k-medias*, donde el número de grupos se define manualmente, *DBSCAN* detecta automáticamente cuántos clústeres existen.

- **Identifica ruido y *outliers* **: los métodos clásicos fuerzan la asignación de todos los casos a algún grupo; *DBSCAN* puede dejarlos fuera, lo que resulta muy útil cuando hay observaciones anómalas.

- **Detecta clústeres de forma irregular**: *k-medias* tiende a producir grupos esféricos; *DBSCAN* permite clústeres de formas arbitrarias y de tamaños distintos.

- **Escala bien con datos medianos o grandes**: es eficiente para conjuntos de varios miles de observaciones.

En cuanto a sus **limitaciones**, y las **precauciones que se han de tener en cuenta**, hemos de advertir:

- **Sensibilidad a los parámetros *eps* y *minPts* **: su resultado depende fuertemente de estos valores. Si *eps* es demasiado pequeño, surgen muchos puntos de ruido; si es demasiado grande, los clústeres se fusionan.

- **Dificultad en densidades heterogéneas**: cuando existen regiones con densidades muy distintas, un único eps puede ser inadecuado para todas (en esos casos, el algoritmo *HDBSCAN*, su versión jerárquica, ofrece mejores resultados).

- **Escalado y distancia**: requiere que las variables estén en la misma escala y que la métrica de distancia sea representativa del problema.

Una de las mayores virtudes de DBSCAN es su **robustez ante outliers**. En los métodos jerárquicos o de partición (*k-medias*, *Ward*), un punto extremo puede distorsionar el cálculo de distancias o medias y alterar significativamente la estructura de los grupos. En cambio, *DBSCAN* identifica explícitamente los puntos atípicos como ruido, y no los utiliza para formar ni expandir clústeres. Dicho de otro modo: los *outliers* no influyen en los límites de los clústeres; y el algoritmo no intenta “forzarlos” a pertenecer a algún grupo, sino que los deja fuera de la estructura principal. Esto convierte a *DBSCAN* en una técnica intrínsecamente más robusta frente a observaciones anómalas, especialmente cuando los datos han sido previamente tipificados.

En el ejemplo práctico volveremos a utilizar la misma muestra de 300 empresas de transporte de mercancías interestelar empleada en el apartado de *k-medias*. Las variables clasificadoras son, de nuevo, los tres índices ya conocidos: diversificación (IDIVERSE), fidelización de clientes (IFIDE) y digitalización (IDIG).

Trabajaremos dentro del proyecto denominado "cluster", donde guardaremos los dos archivos necesarios para esta práctica: el archivo de datos en formato Microsoft® Excel® (interestelar_300.xlsx) y el *script* de R denominado *dbscan_rstars.R*. En la última sección del capítulo se facilitan los enlaces a ambos materiales.

### ![](figuras/pie-chart.svg) Preparación de los datos.

La primera parte del script reproduce exactamente el mismo código que vimos en el ejemplo de k-medias: limpieza de *Global Environment*, activación de paquetes, código de la función `create_patchwork()`, importación de los datos, selección y presentación de las variables del análisis, tratamiento de *missing values* e identificación de *outliers*:

```{r, eval=FALSE, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
### CLUSTER por algoritmo DBScan empresas TMI. ###

# Limpiando el Global Environment
rm(list = ls())

# Cargando paquetes
library(readxl)
library(dplyr)
library(ggplot2)
library(gtExtras)
library(visdat)
library(dbscan)
library (knitr)
library (kableExtra)
library (patchwork)

##### Función para crear composiciones de gráficos con patchwork ###############
create_patchwork <- function(plot_list) {
  n <- length(plot_list)
  if (n == 0) return(NULL)
  full_rows <- n %/% 4
  remaining <- n %% 4
  patchworks <- list()
  
  if (full_rows > 0) {
    for (i in seq(1, full_rows * 4, by = 4)) {
      patchworks <- c(patchworks, list((plot_list[[i]] + plot_list[[i+1]]) / 
                                         (plot_list[[i+2]] + plot_list[[i+3]])))
    }
  }
  
  if (remaining > 0) {
    last_plots <- plot_list[(full_rows * 4 + 1):n]
    empty_plots <- lapply(1:(4 - remaining), function(x) ggplot() + theme_void())
    last_patchwork <- do.call(patchwork::wrap_plots, c(last_plots, empty_plots))
    patchworks <- c(patchworks, list(last_patchwork))
  }
  return(patchworks)
}
################################################################################

## DATOS

# Importando datos desde Excel
interestelar_300 <- read_excel("interestelar_300.xlsx",
                               sheet = "Datos",
                               na = c("n.d."))
interestelar_300 <- data.frame(interestelar_300, row.names = 1)

# Seleccionando variables metricas para el analisis.
seleccion <- interestelar_300 %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion_df_graph <- gt_plt_summary(seleccion)
seleccion_df_graph

# Localizando missing values.
seleccion %>%
  vis_miss() +
  labs(title = "Indicadores: Diversificación, Fidelidad, Digitalización",
       subtitle = "Transporte de mercancías interestelar",
       y = "Observación",
       fill = NULL) +
  scale_fill_manual(
    values = c("TRUE" = "red", "FALSE" = "grey"),
    labels = c("TRUE" = "NA", "FALSE" = "Presente")) +
  theme(
    plot.title = element_text(face = "bold", size = 14))

seleccion %>% filter(is.na(IDIVERSE) |
                       is.na(IFIDE) |
                       is.na(IDIG)) %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion <- seleccion %>%
  filter(! is.na(IDIVERSE) &
           ! is.na(IFIDE) &
           ! is.na(IDIG)) 

# Identificando outliers con distancia de Mahalanobis.
seleccion <- seleccion %>%
  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),
                                   center = colMeans(.),
                                   cov    = cov(.)))

ggplot(data = seleccion, map = (aes(y = MAHALANOBIS))) +
  geom_boxplot(fill = "orange") +
  ggtitle("DISTANCIA DE MAHALANOBIS",
          subtitle = "IDIVERSE, IFIDE, IDIG. Empresas TMI.") +
  ylab("MAHALANOBIS")

Q1M <- quantile (seleccion$MAHALANOBIS, c(0.25))
Q3M <- quantile (seleccion$MAHALANOBIS, c(0.75))

seleccion %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(MAHALANOBIS, IDIVERSE, IFIDE, IDIG)

# Eliminando variable MAHALANOBIS del df con outliers
seleccion <- seleccion %>% select(-MAHALANOBIS)
```

Aunque el *script* incluye el código para detectar y eliminar posibles *missing values*, el gráfico generado por `vis_miss()` confirma que no existen datos ausentes en las tres variables de interés. Por tanto, el *data frame* "seleccion" mantiene los 300 casos originales:

```{r, eval=TRUE, echo=FALSE, results='hide', fig.show='hide', message=FALSE, warning=FALSE}
### CLUSTER por algoritmo DBScan empresas TMI. ###

# Limpiando el Global Environment
rm(list = ls())

# Cargando paquetes
library(readxl)
library(dplyr)
library(ggplot2)
library(gtExtras)
library(visdat)
library(dbscan)
library (knitr)
library (kableExtra)
library (patchwork)

##### Función para crear composiciones de gráficos con patchwork ###############
create_patchwork <- function(plot_list) {
  n <- length(plot_list)
  if (n == 0) return(NULL)
  full_rows <- n %/% 4
  remaining <- n %% 4
  patchworks <- list()
  
  if (full_rows > 0) {
    for (i in seq(1, full_rows * 4, by = 4)) {
      patchworks <- c(patchworks, list((plot_list[[i]] + plot_list[[i+1]]) / 
                                         (plot_list[[i+2]] + plot_list[[i+3]])))
    }
  }
  
  if (remaining > 0) {
    last_plots <- plot_list[(full_rows * 4 + 1):n]
    empty_plots <- lapply(1:(4 - remaining), function(x) ggplot() + theme_void())
    last_patchwork <- do.call(patchwork::wrap_plots, c(last_plots, empty_plots))
    patchworks <- c(patchworks, list(last_patchwork))
  }
  return(patchworks)
}
################################################################################

## DATOS

# Importando datos desde Excel
interestelar_300 <- read_excel("interestelar_300.xlsx",
                               sheet = "Datos",
                               na = c("n.d."))
interestelar_300 <- data.frame(interestelar_300, row.names = 1)

# Seleccionando variables metricas para el analisis.
seleccion <- interestelar_300 %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion_df_graph <- gt_plt_summary(seleccion)
seleccion_df_graph
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Localizando missing values.
seleccion %>%
  vis_miss() +
  labs(title = "Indicadores: Diversificación, Fidelidad, Digitalización",
       subtitle = "Transporte de mercancías interestelar",
       y = "Observación",
       fill = NULL) +
  scale_fill_manual(
    values = c("TRUE" = "red", "FALSE" = "grey"),
    labels = c("TRUE" = "NA", "FALSE" = "Presente")) +
  theme(
    plot.title = element_text(face = "bold", size = 14))
```

En la detección de *outliers*, se calcularon las *distancias de Mahalanobis* y se representaron mediante un *boxplot*. El gráfico evidenció la presencia de casos atípicos, que fueron identificados mediante el filtro habitual (26 observaciones):

```{r, eval=TRUE, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
seleccion %>% filter(is.na(IDIVERSE) |
                       is.na(IFIDE) |
                       is.na(IDIG)) %>%
  select(IDIVERSE, IFIDE, IDIG)
seleccion <- seleccion %>%
  filter(! is.na(IDIVERSE) &
           ! is.na(IFIDE) &
           ! is.na(IDIG)) 

# Identificando outliers con distancia de Mahalanobis.
seleccion <- seleccion %>%
  mutate(MAHALANOBIS = mahalanobis(as.matrix(.),
                                   center = colMeans(.),
                                   cov    = cov(.)))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data = seleccion, map = (aes(y = MAHALANOBIS))) +
  geom_boxplot(fill = "orange") +
  ggtitle("DISTANCIA DE MAHALANOBIS",
          subtitle = "IDIVERSE, IFIDE, IDIG. Empresas TMI.") +
  ylab("MAHALANOBIS")
```

```{r, eval=TRUE, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
Q1M <- quantile (seleccion$MAHALANOBIS, c(0.25))
Q3M <- quantile (seleccion$MAHALANOBIS, c(0.75))

seleccion %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(MAHALANOBIS, IDIVERSE, IFIDE, IDIG)

# Eliminando outliers.
seleccion_so <-seleccion %>%
  filter(MAHALANOBIS <= Q3M + 1.5*IQR(MAHALANOBIS) &
           MAHALANOBIS >= Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(IDIVERSE, IFIDE, IDIG)

# Eliminando variable MAHALANOBIS del df con outliers
seleccion <- seleccion %>% select(-MAHALANOBIS)
```

A partir de aquí iniciaremos un procedimiento de análisis diferente. *DBSCAN* es una técnica robusta frente a los *outliers*, ya que estos no afectan a la formación de los clústeres, sino que el propio algoritmo los identifica y clasifica como “ruido”. Por tanto, mantendremos las 300 observaciones en el análisis.

### ![](figuras/pie-chart.svg) Aplicación de la técnica.

Antes de calcular distancias, es imprescindible que todas las variables estén en la **misma escala**. Si no lo hacemos, aquellas con un rango numérico más amplio dominarán el cálculo de distancias y distorsionarán la formación de los grupos. Para evitarlo, aplicamos una **tipificación estándar (z-score)** a cada variable: restamos su media y dividimos entre su desviación típica.

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
## APLICACION DE DBSCAN

# 1) Tipificación (z-scores)
zseleccion <- scale(seleccion)
zseleccion <- as.matrix(zseleccion)  # dbscan espera matriz/numérico
```

Como ya se precisó, *DBSCAN* depende de dos parámetros principales que determinan cómo se definen las regiones densas del espacio de datos:

- **minPts**: número mínimo de observaciones que deben encontrarse dentro de un vecindario para que se considere suficientemente denso como para formar parte de un clúster.

- **eps**: radio del vecindario, es decir, la distancia máxima que define si dos puntos son vecinos.

En la práctica, una regla empírica útil es **minPts = 2 × p**, donde *p* es el número de variables métricas. Este valor garantiza que el algoritmo considere suficiente densidad en función de la dimensionalidad del problema. Como en nuestro caso p = 3, usaremos minPts = 6.

Además, definiremos una **proporción objetivo de ruido**, que representa el porcentaje aproximado de observaciones que esperamos que el algoritmo clasifique como *outliers* o no pertenecientes a ningún clúster. A partir de este nivel deseado de ruido, el procedimiento buscará automáticamente el valor de *eps* que lo produzca.

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# 2) Determinación de minPts y eps
minPts0 <- 6  # regla general: 2*p (p=3)
ruido_obj <- 0.20 # proporción objetivo de ruido (p. ej., 0.10, 0.20, 0.30)
tol_ruido <- 0.02 # tolerancia (±2 p.p.)
max_iter  <- 30   # tope de iteraciones
```

En este bloque de código, los parámetros cumplen las siguientes funciones:

- `minPts0`: define la **densidad mínima** requerida para formar un clúster (en este caso, seis puntos).

- `ruido_obj`: fija la **proporción** deseada de observaciones que se considerarán **ruido** o casos atípicos.

- `tol_ruido`: margen de **tolerancia** aceptado respecto al valor objetivo de ruido (±2 puntos porcentuales).

- `max_iter`: número *máximo de iteraciones* permitidas para ajustar *eps* y alcanzar la proporción de ruido deseada.

Para comenzar la búsqueda de un valor adecuado para el radio *eps*, analizamos la distribución de las **distancias al (minPts − 1)-ésimo vecino más cercano** de cada observación. Este gráfico, conocido como *curva kNN*, suele presentar una primera zona plana (puntos en regiones densas) y, a partir de cierto valor, un “codo” o cambio brusco de pendiente que marca la transición hacia las zonas menos densas o de ruido. Ese punto de inflexión nos orienta sobre el **rango de valores razonables** para *eps*.

A partir de esta información inicial, el siguiente paso consistirá en acotar automáticamente la búsqueda dentro de un intervalo que incluya tanto la zona densa como la de transición.

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# 3) Rango inicial inteligente para eps usando kNN (k = minPts0-1)
knn_d <- kNNdist(zseleccion, k = minPts0 - 1)
d_sorted <- sort(as.numeric(knn_d))

eps_lo <- max(min(d_sorted) * 0.9, 1e-6)      # límite inferior
eps_hi <- max(d_sorted) * 1.5                  # límite superior amplio
```

EL código anterior calcula un rango inicial de búsqueda para el parámetro *eps* a partir de las distancias entre observaciones más cercanas. La función `kNNdist()` (del paquete `{dbscan}`) obtiene, para cada punto del conjunto tipificado "zseleccion", la distancia a su (minPts − 1)-ésimo vecino más próximo. Estas distancias reflejan cuán denso es el entorno de cada observación: los puntos en zonas compactas tienen distancias pequeñas, mientras que los puntos aislados presentan distancias más grandes.

A continuación:

- `cknn_d` guarda todas esas distancias.

- `d_sorted` las ordena de menor a mayor para facilitar su análisis.

- `eps_lo` y `eps_hi` definen, respectivamente, los límites inferior y superior del rango dentro del cual se buscará el valor óptimo de *eps*.

El límite inferior (`eps_lo`) se fija ligeramente por debajo de la distancia mínima observada. El límite superior (`eps_hi`) se amplía un 50 % por encima del valor máximo, para garantizar que el rango incluya tanto las zonas densas como las menos densas del espacio de datos.

En conjunto, este paso permite acotar de forma inteligente la búsqueda de *eps*, evitando probar valores arbitrarios y asegurando que el algoritmo explore un **intervalo realista** basado en la estructura de los propios datos.

En el paso o código siguiente se define una función auxiliar denominada `noise_rate()`, que servirá para **evaluar la proporción de ruido** generada por el modelo *DBSCAN* para distintos valores del parámetro *eps*. Su objetivo es permitirnos ajustar automáticamente el radio óptimo que produce el nivel de ruido deseado:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# 4) Función auxiliar: calcula proporción de ruido para un eps
noise_rate <- function(eps) {
  fit <- dbscan::dbscan(zseleccion, eps = eps, minPts = minPts0)
  mean(fit$cluster == 0)
}
```

La lógica del código anterior es sencilla. Cada vez que se ejecute `noise_rate()` con un valor concreto de *eps*, la función aplicará internamente el algoritmo `dbscan()` sobre el conjunto de datos tipificados "zseleccion". Se mantienen constantes el parámetro minPts (almacenado en minPts0) y el número de variables.

El argumento *eps* se actualiza en cada llamada, de modo que se prueban distintos radios de vecindad. La función calcula el porcentaje de observaciones clasificadas como ruido, es decir, aquellas que el modelo identifica con la etiqueta *cluster = 0*.

La expresión `mean(fit$cluster == 0)` devuelve el **proporción de puntos fuera de los clústeres**, ya que cuenta cuántos casos cumplen esa condición y los divide entre el total.

En resumen, este paso crea una herramienta compacta que nos permitirá, más adelante, probar de forma sistemática diferentes valores de *eps* y comprobar qué proporción de ruido generan. Gracias a ello, podremos automatizar la elección de eps en lugar de depender exclusivamente de la inspección visual del gráfico *kNN*.

En le siguiente paso pretendemos encontrar de forma automática el valor de *eps* que produzca una proporción de ruido cercana al objetivo fijado (`ruido_obj`). Para ello, este bloque de código implementa una búsqueda iterativa que ajusta progresivamente el valor de eps hasta aproximarse al nivel deseado de ruido. El procedimiento se basa en una estrategia conocida como **búsqueda binaria** *(binary search)*, un método eficiente que va dividiendo el intervalo de búsqueda por la mitad en cada paso, descartando la zona que no cumple el criterio.

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# 5) Búsqueda binaria para acercarnos a ruido_obj
tested <- data.frame(eps = numeric(0), ruido = numeric(0))

for (i in seq_len(max_iter)) {
  eps_mid <- (eps_lo + eps_hi) / 2
  r_mid   <- noise_rate(eps_mid)
  tested  <- rbind(tested, data.frame(eps = eps_mid, ruido = r_mid))
  
  if (abs(r_mid - ruido_obj) <= tol_ruido) break
  if (r_mid > ruido_obj) {
    # demasiado ruido -> aumentar eps para unir vecindarios y reducir ruido
    eps_lo <- eps_mid
  } else {
    # poco ruido -> disminuir eps para ser más estricto
    eps_hi <- eps_mid
  }
}
```

En este código:

- Se crea primero el *data frame* "tested", que almacenará los pares de valores *eps* y la proporción de ruido (ruido) obtenidos en cada iteración.

- En cada paso del bucle for:

  1.  Se calcula un **valor intermedio del radio**, `eps_mid`, como el punto medio entre los límites actuales del rango (`eps_lo` y `eps_hi`).

  2.  Se ejecuta la función `noise_rate()` para ese valor de *eps*, obteniendo la proporción de ruido `r_mid`.

  3.  El resultado se guarda en el *data frame* "tested" para poder consultarlo o graficarlo posteriormente.

- A continuación, el código compara el ruido obtenido `r_mid` con el ruido objetivo `ruido_obj`:

  - Si la diferencia está dentro de la tolerancia admitida (`tol_ruido`), el bucle se detiene porque se ha encontrado un *eps* adecuado.

  - Si el ruido actual es demasiado alto, significa que los clústeres son demasiado pequeños; para corregirlo, se aumenta *eps*, lo que amplía los vecindarios y une más puntos.

  - Si el ruido es demasiado bajo, se reduce *eps*, haciendo los clústeres más estrictos y separando más puntos como atípicos.

Este proceso se repite hasta alcanzar la proporción deseada de ruido o hasta completar el número máximo de iteraciones definido en `max_iter`.

El resultado final es un conjunto de valores *eps* probados con sus correspondientes niveles de ruido, entre los cuales se encontrará el radio más adecuado para aplicar el modelo *DBSCAN* con los parámetros óptimos.

Una vez completadas las iteraciones del paso anterior, disponemos de varios valores de *eps* junto con las proporciones de ruido que generaron. El siguiente paso consiste en seleccionar, pues, el **valor óptimo** de eps, es decir, aquel que produce un **nivel de ruido más próximo al objetivo fijado** (`ruido_obj`).

Este bloque de código realiza esa selección de forma automática y guarda los resultados finales para ser utilizados en el modelo definitivo:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# 6) Elegimos el eps con ruido más cercano a la diana
ix_best   <- which.min(abs(tested$ruido - ruido_obj))
eps_final <- tested$eps[ix_best]
ruido_est <- tested$ruido[ix_best]
```

En este fragmento de código:

- "tested" es el *data frame* generado en la etapa anterior, que contiene todas las combinaciones de *eps* evaluadas junto con su proporción de ruido.

- La instrucción `which.min(abs(tested$ruido - ruido_obj))` identifica el índice (`ix_best`) del valor de *eps* cuya proporción de ruido está más cerca del objetivo deseado. A partir de ese índice:

  - `eps_final` guarda el valor de **radio óptimo** que se empleará finalmente en el algoritmo *DBSCAN*.

  - `ruido_est` almacena la **proporción real de ruido** que produce ese valor óptimo.

Este paso resume todo el proceso de búsqueda en un resultado claro y cuantitativo: el radio `eps_final`, ajustado para lograr la densidad de agrupación coherente con el nivel de ruido que se considera adecuado para el análisis.

Con el valor óptimo de *eps* ya determinado, podemos ajustar el modelo *DBSCAN* definitivo. En este paso se aplica el algoritmo utilizando los parámetros seleccionados (`eps_final` y `minPts0`) sobre el conjunto de datos tipificados "zseleccion". El objetivo es obtener la estructura final de clústeres, junto con la asignación de cada observación a su correspondiente grupo o al conjunto de ruido.

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# 7) Modelo final
modelo_db <- dbscan::dbscan(zseleccion, eps = eps_final, minPts = minPts0)
modelo_db
```

En este bloque:

- La función `dbscan()` ejecuta el algoritmo con los parámetros óptimos:

  - `eps = eps_final`, el radio de vecindad que determina qué puntos se consideran cercanos.

  - `minPts = minPts0`, el número mínimo de observaciones necesarias en un vecindario para formar un clúster.

El resultado se guarda en el objeto "modelo_db", que contiene toda la información del agrupamiento: número de clústeres detectados, tamaño de cada grupo y proporción de ruido. AL mostrarlo en pantalla, R presenta un resumen con el conteo de puntos por clúster y el porcentaje de observaciones no asignadas (ruido).

Finalmente, se muestra un resumen sintético de los resultados obtenidos por el modelo *DBSCAN* y se asocian las etiquetas de clúster a las observaciones originales. Este paso permite comprobar de un vistazo la coherencia entre los parámetros elegidos y los resultados alcanzados:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# 8) Resumen rápido
cat("\n---\n",
    "eps_final =", round(eps_final, 3), 
    "| ruido_obj =", scales::percent(ruido_obj),
    "| ruido_logrado =", scales::percent(ruido_est), "\n")

table(Cluster = modelo_db$cluster)

seleccion$whatcluster_dbs <- as.factor(modelo_db$cluster)
```

En este bloque:

- La función `cat()` imprime un resumen limpio con los valores finales:

- `eps_final`, el radio óptimo de vecindad.

- `ruido_obj`, la proporción de ruido objetivo definida por el usuario.

- `ruido_est`, el porcentaje real de observaciones que el modelo ha clasificado como ruido.

Este breve informe permite verificar si el algoritmo alcanzó el nivel de ruido esperado o si se desvió ligeramente dentro del margen de tolerancia.

La función `table()` genera un recuento del número de observaciones asignadas a cada clúster, incluido el *clúster 0*, que representa las observaciones no agrupadas o de baja densidad.

Finalmente, la instrucción `seleccion$whatcluster_dbs <- as.factor(modelo_db$cluster)` incorpora al *data frame* original una nueva columna denominada **whatcluster_dbs**, que almacena la etiqueta de clúster asignada por *DBSCAN* a cada empresa. Esta variable será muy útil en la siguiente fase del análisis, cuando visualicemos los clústeres y analicemos sus características internas.

### ![](figuras/pie-chart.svg) Caracterización de los clústeres o grupos formados.

Una vez obtenidos los clústeres, y asignados los elementos (teniendo en cuenta que el *grupo 0*
no es realmente un clúster; sino que reúne a los casos considerados atípicos, *ouliers* o *ruido*), puede aplicarse el mismo código que se empleó en el caso de *k-medias* a la hora de caracterizar tales grupos (omitimos por no extender el código la prueba de comparaciones múltiples de *Kruskal-Wallis*):

```{r, eval=FALSE, echo=TRUE, results='hide', fig.show='hide', message=FALSE, warning=FALSE}
# CARACTERIZANDO GRUPOS FORMADOS

# Tabla con centroides de grupos.
tablamedias <- seleccion %>%
  group_by(whatcluster_dbs) %>%
  summarise(obs = length(whatcluster_dbs),
            Idiverse = mean(IDIVERSE),
            Ifide = mean(IFIDE),
            Idig = mean(IDIG))

tablamedias %>%
  kable(caption = "Método DBSCAN. Medias de variables (Grupo 0 = Ruido)",
        col.names = c("Clúster",
                      "Observaciones",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(NA, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                "bordered",
                "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T,
           align = "c") %>%
  row_spec(1:nrow(tablamedias),
           bold= F,
           align = "c")

# Gráficos de centroides

# Vector de nombre de variables excluyendo la variable no deseada
variables <- setdiff(names(tablamedias), c("whatcluster_dbs", "obs"))

# Lista para almacenar los gráficos
graficos.centroides <- list()

# Bucle para crear y almacenar los gráficos
for (i in seq_along(variables)) {
  var1 <- variables[[i]]
  grafico <- ggplot(data= tablamedias,
                    map = (aes_string(y = var1, x = "whatcluster_dbs"))) +
    geom_bar(stat = "identity",
             colour = "red",
             fill = "orange",
             alpha = 0.7) +
    ggtitle(paste0(var1, ". Media por grupos."),
            subtitle = "Empresas TMI.")+
    xlab ("Grupo") +
    ylab(var1)
  graficos.centroides[[paste0("grafico_", var1)]] <- grafico
}                 

# Aplicar función de composiciones a gráficos de centroides.
grupos.graficos.centroides <- create_patchwork(graficos.centroides)

# Presentar las composiciones
for (n in 1:length(grupos.graficos.centroides)){
  print(grupos.graficos.centroides[[n]])
}

# GRÁFICOS Variable vs Variable

# Lista de variables excluyendo la variable no deseada
variables <- setdiff(names(seleccion), "whatcluster_dbs")

# Lista para almacenar los gráficos
graficos <- list()

# Generar todas las combinaciones posibles de pares de variables
combinaciones <- combn(variables, 2, simplify = FALSE)

# Bucle para crear y almacenar los gráficos
for (i in seq_along(combinaciones)) {
  var1 <- combinaciones[[i]][1]
  var2 <- combinaciones[[i]][2]
  grafico <- ggplot(seleccion,
                    map = aes_string(x = var1,
                                     y = var2,
                                     color = "whatcluster_dbs")) +
    geom_point() +
    labs(title = paste("GRÁFICO", var1, "-", var2),
         subtitle = "Empresas TMI.") +
    xlab (var1) +
    ylab (var2) +
    scale_color_brewer(palette = "Set1") 
  graficos[[paste0("grafico_", var1, "_", var2)]] <- grafico
}

# Aplicar función de composiciones de patchwork

gruposgraficos <- create_patchwork(graficos)

# Presentar las composiciones
for (n in 1:length(gruposgraficos)){
  print(gruposgraficos[[n]])
}
```

Con el código anterior, por ejemplo, podemos construir la tabla de número de elementos y medias de cada clúster:

```{r, eval=TRUE, echo=FALSE, fig.show='hide', message=FALSE, warning=FALSE}
# Tabla con centroides de grupos.
tablamedias <- seleccion %>%
               group_by(whatcluster_dbs) %>%
               summarise(obs = length(whatcluster_dbs),
                                      Idiverse = mean(IDIVERSE),
                                      Ifide = mean(IFIDE),
                                      Idig = mean(IDIG))

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
tablamedias %>%
  kable(caption = "Método DBSCAN. Medias de variables (Grupo 0 = Ruido)",
        col.names = c("Clúster",
                      "Observaciones",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(0, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                                    "bordered",
                                    "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T,
           align = "c") %>%
  row_spec(1:nrow(tablamedias),
           bold= F,
           align = "c")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
tablamedias %>%
  kable(caption = "Método DBSCAN. Medias de variables (Grupo 0 = Ruido)",
        col.names = c("Clúster",
                      "Observaciones",
                      "I. Diversif.",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(0, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE))
}  
```


Además, obtenemos los gráficos de las medias de los clústeres y de dispersión de las variables 2 a 2:

```{r, eval=TRUE, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# Gráficos de centroides

# Vector de nombre de variables excluyendo la variable no deseada
variables <- setdiff(names(tablamedias), c("whatcluster_dbs", "obs"))

# Lista para almacenar los gráficos
graficos.centroides <- list()

# Bucle para crear y almacenar los gráficos
for (i in seq_along(variables)) {
  var1 <- variables[[i]]
  grafico <- ggplot(data= tablamedias,
                    map = (aes_string(y = var1, x = "whatcluster_dbs"))) +
    geom_bar(stat = "identity",
             colour = "red",
             fill = "orange",
             alpha = 0.7) +
    ggtitle(paste0(var1, ". Media por grupos."),
            subtitle = "Empresas TMI.")+
    xlab ("Grupo") +
    ylab(var1)
  graficos.centroides[[paste0("grafico_", var1)]] <- grafico
}                 

# Aplicar función de composiciones a gráficos de centroides.
grupos.graficos.centroides <- create_patchwork(graficos.centroides)

# Presentar las composiciones
for (n in 1:length(grupos.graficos.centroides)){
  print(grupos.graficos.centroides[[n]])
}

# GRÁFICOS Variable vs Variable

# Lista de variables excluyendo la variable no deseada
variables <- setdiff(names(seleccion), "whatcluster_dbs")

# Lista para almacenar los gráficos
graficos <- list()

# Generar todas las combinaciones posibles de pares de variables
combinaciones <- combn(variables, 2, simplify = FALSE)

# Bucle para crear y almacenar los gráficos
for (i in seq_along(combinaciones)) {
  var1 <- combinaciones[[i]][1]
  var2 <- combinaciones[[i]][2]
  grafico <- ggplot(seleccion,
                    map = aes_string(x = var1,
                                     y = var2,
                                     color = "whatcluster_dbs")) +
    geom_point() +
    labs(title = paste("GRÁFICO", var1, "-", var2),
         subtitle = "Empresas TMI.") +
    xlab (var1) +
    ylab (var2) +
    scale_color_brewer(palette = "Set1") 
  graficos[[paste0("grafico_", var1, "_", var2)]] <- grafico
}

# Aplicar función de composiciones de patchwork

gruposgraficos <- create_patchwork(graficos)

# Presentar las composiciones
for (n in 1:length(gruposgraficos)){
  print(gruposgraficos[[n]])
}
```
Se deja al lector, como ejercicio práctico, la interpretación de estos gráficos a fin de caracterizar los 4 clústeres identificados (ha de tenerse en cuenta que el grupo 0 no es un clúster, sino que recoge los elementos *ruido*, y por tanto, su interpretación carece de sentido, al menos en térmisnos de valores medios de las variables).

Para concluir, conviene hacerse una pregunta clave. **¿Cómo ha clasificado *DBSCAN* a los casos 26 que fueron identificados como *outliers*?** ¿Los ha etiquetado como "ruido", como sería lo coherente?

La siguiente tabla nos muestra la respuesta:

```{r, eval=FALSE, echo=TRUE, fig.show='hide', message=FALSE, warning=FALSE}
# ¿Los outliers son ruido?

seleccion <- seleccion %>%
  mutate(MAHALANOBIS = mahalanobis(select(., IDIVERSE, IFIDE, IDIG),
                                   colMeans(select(., IDIVERSE, IFIDE, IDIG)),
                                   cov(select(., IDIVERSE, IFIDE, IDIG))))
Q1M <- quantile (seleccion$MAHALANOBIS, c(0.25))
Q3M <- quantile (seleccion$MAHALANOBIS, c(0.75))

seleccion_out <- seleccion %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(whatcluster_dbs, MAHALANOBIS, IDIVERSE, IFIDE, IDIG)

seleccion_out %>%
  kable(caption = "¿Outliers son ruido? (Grupo 0 = Ruido)",
        col.names = c("Caso",
                      "Observaciones",
                      "Grupo (0=ruido)",
                      "D. Mahalanobis",
                      "I. Diversificación",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(NA, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                "bordered",
                "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T,
           align = "c") %>%
  row_spec(1:nrow(seleccion_out),
           bold= F,
           align = "c")
```

```{r, eval=FALSE, echo=TRUE, fig.show='hide', message=FALSE, warning=FALSE}
# ¿Los outliers son ruido?

seleccion <- seleccion %>%
  mutate(MAHALANOBIS = mahalanobis(select(., IDIVERSE, IFIDE, IDIG),
                                   colMeans(select(., IDIVERSE, IFIDE, IDIG)),
                                   cov(select(., IDIVERSE, IFIDE, IDIG))))
Q1M <- quantile (seleccion$MAHALANOBIS, c(0.25))
Q3M <- quantile (seleccion$MAHALANOBIS, c(0.75))

seleccion_out <- seleccion %>%
  filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
           MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS))%>%
  select(whatcluster_dbs, MAHALANOBIS, IDIVERSE, IFIDE, IDIG)

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
seleccion_out %>%
  kable(caption = "¿Outliers son ruido? (Grupo 0 = Ruido)",
        col.names = c("Caso",
                      "Observaciones",
                      "Grupo (0=ruido)",
                      "D. Mahalanobis",
                      "I. Diversificación",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(NA, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE)) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped",
                "bordered",
                "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T,
           align = "c") %>%
  row_spec(1:nrow(seleccion_out),
           bold= F,
           align = "c")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
  seleccion_out %>%
  kable(caption = "¿Outliers son ruido? (Grupo 0 = Ruido)",
        col.names = c("Caso",
                      "Observaciones",
                      "Grupo (0=ruido)",
                      "D. Mahalanobis",
                      "I. Diversificación",
                      "I. Fidelizac.",
                      "I. Digitalizac."),
        digits = c(NA, 0, 3, 3, 3),
        format.args = list(decimal.mark = ".",
                           scientific = FALSE))
}
```

En efecto, todas las compañías que fueron identificadas como *outliers* han sido clasificadas como *ruido* (es decir, no pertenecen a ninguno de los clústeres identificados). De hecho, *DBSCAN* ha sido más exigente, y ha clasificado como *ruido* otras empresas que no fueron localizadas, por el método tradicional, como *outliers*.

## Materiales para realizar las prácticas del capítulo.

En esta sección se muestran los links de acceso a los diferentes materiales (*scripts*, datos...) necesarios para llevar a cabo los contenidos prácticos del capítulo.

**Datos (en formato Microsoft (R) Excel (R)):**

-   interestelar_25.xlsx ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/interestelar_25.xlsx))
-   interestelar_300.xlsx ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/interestelar_300.xlsx))

**Scripts:**

-   cluster_rstars.R ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/cluster_rstars.R))
-   kmedias_rstars.R ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/kmedias_rstars.R))
-   dbscan_rstars.R ([obtener aquí](https://raw.githubusercontent.com/teckel71/RStars-book/main/download/dbscan_rstars.R))

<!--chapter:end:07-analisis_cluster.Rmd-->

# Análisis de la varianza.

## Introducción.

El análisis de la varianza (ANOVA) puede considerarse una generalización del contraste de hipótesis de medias poblacionales iguales para el caso de poblaciones normales y con varianzas desconocidas, pero iguales. La generalización consiste en poder considerar más de dos poblaciones. La **hipótesis nula** será la que afirma que **las medias poblacionales de la variable métrica en estudio, para todas las poblaciones, son iguales**. La hipótesis alternativa, por su lado, afirmará que existe al menos dos poblaciones con medias diferentes. Como todo contraste, para llevarlo acabo hemos de tener una muestra representativa de cada población, y fijar un nivel de significación (usualmente 0.05).

También se puede considerar el ANOVA como un **tipo especial de análisis de regresión**, en la que la variable dependiente es una variable métrica, y las variables explicativas son atributos o factores (en escala nominal u ordinal). La misión de los factores es clasificar a los casos que constituyen nuestra muestra en distintas submuestras, cada una representativa de una de las subpoblaciones cuyas medias en la variable en estudio se quiere comparar.

## ANOVA de un solo factor.

Aunque se pueden realizar ANOVAs con más de un atributo o factor, en este ejemplo nos ceñiremos al caso más simple, en el que solo hay un atributo o factor que se ocupa de distribuir los casos de la muestra entre los distintos grupos o submuestras (a partir de las categoría o nivel que toma cada caso).

En concreto, en esta práctica, comprobaremos **si** **la dimensión del grupo empresarial al que pertenecen las empresas eólicas** (medida en función del número de empresas integradas en el grupo empresarial, y concretada en el factor DIMENSION) **tiene una influencia significativa sobre la rentabilidad económica** (variable RENECO), **en términos medios**. Para ello se ha seleccionado una muestra constituida por 50 empresas productoras de electricidad mediante tecnología eólica. Así, la población, constituida por todas las empresas de generación eléctrica eólica de España, queda dividida en tres subpoblaciones: la subpoblación de empresas que pertenecen a grupos empresariales de DIMENSION (según el número de filiales contenidas) “GRANDE”, la subpoblación de empresas que pertenecen a grupos empresariales de DIMENSION “MEDIA”, y la subpoblación de empresas que pertenecen a grupos empresariales de DIMENSION “PEQUEÑA”. Cada una de estas subpoblaciones tendrán sus respectivas rentabilidades económicas medias, que desconocemos (ya que no tenemos los datos de la población, es decir, de todas las empresas eólicas del país; sino solo de una muestra de 50 empresas). Lo que si tenemos para cada subpoblación es una submuestra que la representa (parte de las 50 empresas de la muestra, que queda fraccionada en tres según el factor DIMENSION). Y de cada submuestra, tenemos la correspondiente rentabilidad media muestral. Lo que comprobaremos con el contraste de ANOVA, en definitiva, es si las diferencias observadas entre las rentabilidades medias de cada submuestra son lo suficientemente amplias como para pensar que, puede considerarse que existen diferencias importantes (significativas) entre las rentabilidades medias de las subpoblaciones (considerando todas las empresas eólicas que conforman la población).

Los datos están almacenados en el archivo de Microsoft Excel "[eolica_50.xls](https://docs.google.com/spreadsheets/d/1PVRwLo1eAbFLOgSscLZw0bD9rgMk2X-w/edit?usp=sharing&ouid=115375878280465826079&rtpof=true&sd=true)", y el script con el código del ejemplo se halla contenido en el fichero "[anova_cluster.R](https://drive.google.com/file/d/1p4p17OudRprwkpF7wvvF1gwHZG82uv89/view?usp=sharing)". Vamos a suponer que trabajaremos en un proyecto de RStudio al que denominaremos "anova".

Una vez abierto el *script* en el editor de RStudio , comprobaremos que la primera parte del código está dedicada a la limpieza de la memoria (*Environment*) y a la importación de los datos. Para ello, activaremos el paquete `{readxl}` y utilizaremos la función `read_excel()`, indicando en los argumentos el archivo a explorar, y la *hoja* en la cual se encuentran los datos (hoja "Datos"). También hemos de prestar atención a la cuestión de si existen en la hoja de *Excel* anotaciones en las celdas donde no haya dato, para completar adecuadamente el argumento `na=` . Los datos se almacenarán en el *data frame* "datos". En este *data frame*, la primera columna no es una verdadera variable, sino que se compone de los nombres de los casos o empresas. Con una línea de código adicional transformaremos esa primera columna en el *nombre de las filas*, de modo que tal columna abandona su *rol* de variable. En definitiva, el código para importar los datos es:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# Análisis ANOVA de un factor.

rm(list = ls())

# DATOS

library (readxl)
datos <- read_excel("eolica_50.xlsx", sheet = "Datos",
                    na = c("n.d.", "s.d."))
datos <- data.frame(datos, row.names = 1)
summary (datos)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# Análisis ANOVA de un factor.

rm(list = ls())
library (dplyr)

# DATOS

library (readxl)
datos <- read_excel("eolica_50.xlsx", sheet = "Datos",
                    na = c("n.d.", "s.d."))
datos <- data.frame(datos, row.names = 1)
df <- select(datos, everything())

# Número de variables por bloque
variables_por_bloque <- 3

# Dividir las variables en bloques
for (i in seq(1, ncol(df), variables_por_bloque)) {
  # Comentario: No mostramos el nombre del bloque
  print(summary(df[, i:min(i + variables_por_bloque - 1, ncol(df))]))
  cat("\n")
}
rm (i)
rm (variables_por_bloque)
rm (df)
```

Antes de proceder al *análisis de la varianza* propiamente dicho, hemos de preparar nuestros datos mediante la localización de *missing values* y *outliers*. Los *outliers*, en el ANOVA, pueden tener una gran influencia (ya que se trabaja en *términos medios*) sobre los resultados, por lo que deben ser tratados convenientemente.

Para localizar los casos concretos de ***missing values***, puede recurrirse a utilizar las herramientas de manejo de *data frames* del paquete `{dplyr}`. Previamente, realizaremos una **copia** del *data frame* original, “datos", a la que llamaremos “muestra”, que es con la que trabajaremos (para mantener la integridad del primer *data frame*). Con la función `vismiss()` del paquete `{visdat}` podemos tener una visión gráfica general de los valores faltantes, en especial en el caso de la variable RENECO y el factor DIMENSION. Si hay casos faltantes en una de estas variables, los identificaremos filtrando el *data frame* con la función `filter()` de `{dplyr}`. Ante la existencia de *missing values*, se puede actuar de varios modos. Por ejemplo, **se puede intentar obtener por otro canal de información el conjunto de valores** de RENECO que no están disponibles, **o recurrir a alguna estimación** para los mismos y asignarlos. En caso de que esto sea difícil, se puede optar, simplemente, por **eliminar** estos casos, en especial cuando representan un porcentaje muy reducido respecto al total de casos. En nuestro ejemplo, vamos a suponer que hemos optado por esta última vía, al no conseguir unos valores más o menos verosímiles de RENECO para las empresas de las que se carece de dato. Así, estos casos, finalmente, será exluidos del análisis, utilizando para ello un nuevo filtro:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Missing values

  library (dplyr)
  library(visdat)
  vis_miss(datos)

  datos %>% filter(is.na(RENECO) | is.na(DIMENSION)) %>%
            select(RENECO, DIMENSION)
  muestra <- datos %>%
             filter(! is.na(RENECO) & ! is.na(DIMENSION))
```

Tras el código anterior, se han eliminado del *data frame* "muestra" las empresas "Sargon Energías S. L. U." y "Viesgo Renovables S. L.", empresas pertenecientes a grupos empresariales de dimensión media (DIMENSION), debido a que carecían de dato de rentabilidad económica (RENECO).

Una vez tratados los casos con valores perdidos o *missing values*, **es necesario detectar la presencia de *outliers*** o casos atípicos en la muestra, que pudieran desvirtuar los resultados derivados del ANOVA. Para ello, realizaremos un ***boxplot*** o gráfico de caja. Aplicaremos, por ejemplo, el código siguiente, que utiliza la gramática del paquete `{ggplot2}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Outliers

  library (ggplot2)
  ggplot(data = muestra,
         map = (aes(y = RENECO))) +
  geom_boxplot(fill = "orange") +
  ggtitle("RENTABILIDAD ECONÓMICA",
          subtitle = "100 empresas eólicas") +
  ylab("Rentabilidad Económica (%)")
```

En el gráfico de caja se aprecia claramente que existe un *outlier*. Para identificar tal empresa, calcularemos, respecto a la variable RENECO, el primer y tercer cuartiles, que nos servirán para construir el filtro que capturará el caso atípico. Así, en el código, las dos primeras filas calculan los cuartiles primero (Q1) y tercero (Q3) mediante la función `quantile()`. Luego se filtran, mediante la función `filter()` de `{dplyr}`, los *outliers*, calculadoscomo aquellos casos con valores de RENECO mayores que Q3 más 1,5 veces el *rango intercuartílico* de la variable; o menores que Q1 menos 1,5 veces dicho rango intercuartílico. Para calcular el rango intercuartílico se recurre a la función `IQR()`. Finalmente, con `select()` se muestran los casos en la consola de RStudio:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  Q1 <- quantile (muestra$RENECO, c(0.25))
  Q3 <- quantile (muestra$RENECO, c(0.75))

  muestra %>%
    filter(RENECO > Q3 + 1.5*IQR(RENECO) |
           RENECO < Q1 - 1.5*IQR(RENECO)) %>%
    select(RENECO)
```

La empresa identificada como outlier es "Molinos del Ebro S. A". Como ocurría con los *missing values*, el tratamiento de los *outliers* depende de la información que se tenga, existiendo varias alternativas (corrección del dato, estimación, etc.) Si no se tiene información fiable, y los *outliers* no representan una gran proporción respecto al total de casos, puede optarse por su eliminación de la muestra, como haremos en este ejemplo. Podemos hacerlo creando un nuevo *data frame* a partir de “muestra"; pero sin ese caso. Ese nuevo *data frame* se llamará, por ejemplo, “**muestra_so**”:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  
  muestra_so <- muestra %>% filter(RENECO <= Q3 + 1.5*IQR(RENECO) &
                                   RENECO >= Q1 - 1.5*IQR(RENECO))
```

Una vez preparados los datos, vamos a presentar los grupos de empresas eólicas y sus rentabilidades económicas medias. Para ello, diseñaremos una tabla con la función `kable()` del paquete `{knitr}`, y personalizada con algunas funciones incluidas en el paquete `{kableExtra}`. Para construir la tabla, hemos de crear anteriormente un pequeño *data frame*, llamado por ejemplo "tablamedias", en el que cada caso o fila sea uno de los grupos en que queda dividida la muestra a partir de los niveles del factor "DIMENSION", y que contenga tres variables: la DIMENSION de cada grupo o submuestra, su número de casos contenidos (variable "observaciones"), y las respectivas rentabilidades medias (variable "media"):

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
  # Visualizando número de frecuencias y medias de los grupos con dplyr:
  
  library (knitr)
  library (kableExtra)
  knitr.table.format = "html" 

  tablamedias <-  muestra_so %>%
                    group_by(DIMENSION) %>%
                    summarise (observaciones = length(DIMENSION),
                               media = mean(RENECO))

  tablamedias %>%
    kable(format = knitr.table.format,
      caption = "Rentabilidad Económica. Medias por grupos (tamaño matriz).",
      col.names = c("Tamaño", "Observaciones", "Rentabilidad Económica")) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped", "bordered", "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:nrow(tablamedias), bold= F, align = "c")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
  # Visualizando número de frecuencias y medias de los grupos con dplyr:
  
  library (knitr)
  library (kableExtra)
tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)
  knitr.table.format = "html" 

  tablamedias <-  muestra_so %>%
                    group_by(DIMENSION) %>%
                    summarise (observaciones = length(DIMENSION),
                               media = mean(RENECO))

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
  tablamedias %>%
    kable(caption = "Rentabilidad Económica. Medias por grupos (tamaño matriz).",
      col.names = c("Tamaño", "Observaciones", "Rentabilidad Económica")) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped", "bordered", "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:nrow(tablamedias), bold= F, align = "c")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
 tablamedias %>%
    kable(caption = "Rentabilidad Económica. Medias por grupos (tamaño matriz).",
      col.names = c("Tamaño", "Observaciones", "Rentabilidad Económica"))
}
```

Gráficamente, las tres submuestras de empresas pueden caracterizarse, en cuanto a la rentabilidad económica (RENECO), mediante dos tipos de gráficos: gráficos de densidad y gráficos de caja. Los gráficos serán construidos utilizando las facilidades del paquete `{ggplot2}`. Además, combinaremos los gráficos en una composición utilizando el paquete `{patchwork}`. El código es el siguiente:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Graficando casos y medias.

  # Crear el gráfico de densidad
  gdensidad <- ggplot(data= muestra_so, aes(x = RENECO, color = DIMENSION, fill = DIMENSION)) +
    geom_density(alpha = 0.3) +
    geom_vline(data = tablamedias, aes(xintercept = media, color = DIMENSION), linetype = "dashed", size = 1) +
    labs(title = "Diagramas de Densidad por Grupo con Medias",
         x = "Rentabilidad Económica (%)",
         y = "Densidad") +
    theme_grey()
  
  # Crear box-plot
  gbox <- ggplot(data = muestra_so,
         map = (aes(y = DIMENSION,
                    x = RENECO,
                    color = DIMENSION,
                    fill = DIMENSION))) +
  geom_boxplot(outlier.shape = NA,
               alpha = 0.3) +
  stat_summary(fun = "mean",
               geom = "point",
               size = 3,
               map = aes(col = DIMENSION),
               alpha = 0.60) +
  geom_jitter(width = 0.1,
              size = 1,
              map = (aes(col = DIMENSION)),
              alpha = 0.40) +
  labs(tittle ="Diagramas de caja por Grupo con Medias",
       xlab = "Rentabilidad Económica (%)",
       ylab = "Submuestras")

  # Combinar gráficos.
  
  library(patchwork)
  
  gdensidad / gbox
```

Cada grupo o submuestra "representa" a una subpoblación, según la dimensión que tenga la matriz empresarial. El ANOVA lo que intenta determinar es si, observadas las diferencias en las medias muestrales de la variable en estudio (aquí, RENECO), esas diferencias pueden considerarse o no estadísticamente significativas a nivel poblacional. En realidad, la hipótesis nula a contrastar es que ninguna de las diferencias entre las medias de la variable de las subpoblacionales es estadísticamente significativa; mientras que la hipótesis alternativa es que existe al menos una diferencia significativa entre las medias de las subpoblaciones. En definitiva, la **hipótesis nula** a contrastar sugiere **que no existen diferencias entre las rentabilidades económicas medias de los grupos** (subpoblaciones) de empresas eólicas (discriminadas por el tamaño del grupo empresarial al que pertenecen). De ser así, el factor DIMENSION no tendría una influencia estadísticamente significativa sobre el valor medio de la variable RENECO.

Las conclusiones a las que lleguemos con el *contraste F de ANOVA* serán válidas en la medida en que se cumplan las **hipótesis de normalidad y homogeneidad en las varianzas** de la variable dependiente o métrica (RENECO), en los tres grupos o subpoblaciones en que queda dividida la población atendiendo los niveles o categorías del factor (DIMENSION). Hemos de contrastar, pues, ambas hipótesis, a partir de la información de las tres submuestras que tenemos y que representan, respectivamente, a cada una de esas subpoblaciones.

En cuanto a la hipótesis de **normalidad**, se pueden usar dos vías para verificar su cumplimiento: el análisis gráfico y la realización de contrastes estadísticos.

El análisis gráfico puede llevarse a cabo mediante la realización *de gráficos QQ* de la variable RENECO, a partir de las submuestras o grupos de empresas en que queda fragmentada la muestra a partir de la variable DIMENSION. Estos gráficos pueden generarse con la gramática del paquete `{ggplot2}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# PRERREQUISITOS / HIPÓTESIS ANOVA  

    # Normalidad / Gráfico QQ

    ggplot(data = muestra_so,
           aes(sample = RENECO)) +
    stat_qq(colour = "red") + 
    stat_qq_line(colour = "dark blue") +
    ggtitle("RENTABILIDAD ECONÓMICA: QQ-PLOT",
            subtitle = "Empresas eólicas") +
    facet_grid(. ~ DIMENSION)
```

Los *gráficos QQ* parecen inidicar que la variable RENECO tiene un comportamiento distante a la Ley Normal en las submuestras (y, por tanto, en las correspondientes subpoblaciones), al localizarse, algunos de los puntos, relativamente alejados de la diagonal, especialmente en el caso de las empresas pertenecientes a matrices de dimensión "grande". Para extraer una conclusión de un modo más preciso, vamos a realizar el contraste de normalidad de *Shapiro-Wilk*:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
    # Normalidad: Shapiro-Wilk para cada grupo
    normalidad <- muestra_so %>%
      group_by(DIMENSION) %>%
    summarise(shapiro_p_value = round(shapiro.test(RENECO)$p.value, 3)) %>%
    mutate(decide = if_else(shapiro_p_value > 0.05,
                            "NORMALIDAD",
                            "NO-NORMALIDAD"))

    tablashapiro <- normalidad %>%
    kable(format = knitr.table.format,
          caption = "Normalidad (Shapiro-Wilks)",
          col.names = c("Dimensión", "p-valor", "Conclusión")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped", "bordered", "condensed",
                  position = "center",
                  font_size = 11) %>%
    row_spec(0, bold= T, align = "c") %>%
    row_spec(1:nrow(normalidad), bold= F, align = "c")

    tablashapiro

```

En el código anterior, se crea un pequeño *data frame* denominado "normalidad", que incluye, para cada submuestra definida por los niveles del factor DIMENSION, la variable "decide" con los *p-valores* de la prueba de *Shapiro-Wilk*, y un atributo llamado "decide", creado mediante la función `mutate()` de `{dplyr}`, que adopta la categoría "NORMALIDAD" o "NO-NORMALIDAD" dependiendo de los p-valores. Posteriormente, el data frame "normalidad" se presenta como una tabla de nombre "tablashapiro".

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
    # Normalidad: Shapiro-Wilk para cada grupo
    normalidad <- muestra_so %>%
      group_by(DIMENSION) %>%
    summarise(shapiro_p_value = round(shapiro.test(RENECO)$p.value, 3)) %>%
    mutate(decide = if_else(shapiro_p_value > 0.05,
                            "NORMALIDAD",
                            "NO-NORMALIDAD"))

    tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)
if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
    tablashapiro <- normalidad %>%
    kable(caption = "Normalidad (Shapiro-Wilks).",
          col.names = c("Dimensión", "p-valor", "Conclusión")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped", "bordered", "condensed",
                  position = "center",
                  font_size = 11) %>%
    row_spec(0, bold= T, align = "c") %>%
    row_spec(1:nrow(normalidad), bold= F, align = "c")

    tablashapiro
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
tablashapiro <- normalidad %>%
  kable(caption = "Normalidad (Shapiro-Wilks).",
      col.names = c("Dimensión", "p-valor", "Conclusión"))
}
```

En esta prueba, la hipótesis nula equivale al supuesto de normalidad. Para un 5% de significación estadística, un **p-valor superior a 0,05 implicará el no-rechazo de la hipótesis nula de normalidad**. En el ejemplo, la submuestra de empresas pertenecientes a matrices de dimensión "grande" llevan a pensar que esta subpoblación no sigue una distribución normal. En los otros dos casos, en cambio, podemos aceptar la existencia de normalidad.

En cuanto a la **homogeneidad de las varianzas**, contrastamos este supuesto mediante la prueba de *Bartlett*.

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
    # Homogeneidad en las varianzas

    bartlett.test(muestra_so$RENECO ~ muestra_so$DIMENSION)
```

El p-valor es menor que 0,05; luego **se rechaza la hipótesis nula de homogeneidad de las varianzas de los grupos (subpoblaciones)**.

Puesto que en uno de los grupos (subpoblaciones) se rechaza la hipótesis de normalidad, y (sobre todo) puesto que no se puede considerar una dispersión similar en las tres subpoblaciones (varianzas homogéneas), los resultados de la prueba ***F de ANOVA*** pierden validez, y habría que optar por **una alternativa robusta**. No obstante, **a modo ilustrativo**, seguiremos adelante con la prueba *F de ANOVA*.

El contraste *F de ANOVA* de igualdad en las medias de la variable en estudio (rentabilidad económica, RENECO) de las distintas (sub)poblaciones (grupos de empresas según el tamaño del grupo empresarial de pertenencia) se realiza en R mediante la función `aov()`, que guardaremos, por ejemplo, como el objeto “Datos.aov”, y que se almacenará en forma resumida en la lista "summary_aov", de un solo elemento. Este elemento, que contiene la solución resumida del contraste ANOVA, se convierte en un *data frame* (de nombre "aov_table") con el objetivo de presentarlo como una tabla de `kable()`:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# Test F de ANOVA

Datos.aov <- aov(muestra_so$RENECO ~ muestra_so$DIMENSION)
summary_aov <- summary(Datos.aov)

  # Extraer los resultados del ANOVA
  aov_table <- as.data.frame(summary_aov[[1]])

  # Convertir la tabla en una tabla de kable
  aov_table %>%
    kable(format = knitr.table.format,
          caption = "Resultados del ANOVA",
          col.names = c("Grados Libertad",
                        "Suma cuadrados",
                        "Media suma cuadrados",
                        "Estadístico F",
                        "p-valor")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped", "bordered", "condensed",
                  position = "center",
                  font_size = 11) %>%
    row_spec(0, bold= T, align = "c") %>%
    row_spec(1:nrow(aov_table), bold= F, align = "c")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# Test F de ANOVA

Datos.aov <- aov(muestra_so$RENECO ~ muestra_so$DIMENSION)
summary_aov <- summary(Datos.aov)

  # Extraer los resultados del ANOVA
  aov_table <- as.data.frame(summary_aov[[1]])

  # Convertir la tabla en una tabla de kable
  tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)
if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
  aov_table %>%
    kable(caption = "Resultados del ANOVA",
          col.names = c("Grados Libertad",
                        "Suma cuadrados",
                        "Media suma cuadrados",
                        "Estadístico F",
                        "p-valor")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped", "bordered", "condensed",
                  position = "center",
                  font_size = 11) %>%
    row_spec(0, bold= T, align = "c") %>%
    row_spec(1:nrow(aov_table), bold= F, align = "c")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
    aov_table %>%
    kable(caption = "Resultados del ANOVA",
          col.names = c("Grados Libertad",
                        "Suma cuadrados",
                        "Media suma cuadrados",
                        "Estadístico F",
                        "p-valor"))
}  
```

El valor del estadístico F de ANOVA es de 8,721; con un p-valor asociado de 0,0006. Como el p-valor es menor que 0,05, **se rechaza la hipótesis nula de medias iguales**; por lo que podremos afirmar (para una significación del 5%) que **el tamaño o dimensión del grupo empresarial de pertenencia influye, en media, en la rentabilidad económica obtenida**.

## Comparaciones múltiples.

Cuando el resultado del contraste *F de ANOVA* es de rechazo de la hipótesis nula, se presenta otra cuestión interesante. La hipótesis alternativa dice que existe al menos una diferencia entre las medias de dos grupos que es "importante". Pero no tienen porque ser todas. Entonces, cabe preguntarse que diferencias concretas entre medias son estadísticamente significativas, y cuáles no. Para dilucidar esta cuestión se han desarrollado diversas pruebas de **comparaciones múltiples**. Una de ellas es la prueba *HSD de Tuckey*. Un modo de obtener los resultados de esta prueba es ejecutarla a partir de las funciones del paquete `{emmeans}`. En concreto, los resultados de la prueba *HSD de* *Tuckey* se obtendrán aplicando la función `pairs()` a un objeto creado previamente con la función `emmeans()`, al que hemos llamado, por ejemplo, “medias”, y que tiene como argumentos el nombre de nuestra solución del contraste ANOVA anterior y, entrecomillado, el nombre de la variable que actúa como factor que divide a la muestra en los tres grupos (submuestras) comparados (DIMENSION). Los resultados se han guardado en el objeto "pares", que posteriormente se ha transformado en un *data frame* para poder ser presentado como una tabla de `kable()`:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# COMPARACIONES MÚLTIPLES

library(emmeans)
medias <- emmeans(Datos.aov, "DIMENSION")
pares <- pairs(medias)
pares_df <- as.data.frame(pares)
pares_df %>%
  kable(format = knitr.table.format,
        caption = "Resultados comparaciones múltiples",
        col.names = c("Grupos",
                      "Diferencia estimada",
                      "Desviación Típica",
                      "Grados de libertad",
                      "Estadístico t",
                      "p-valor")) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped", "bordered", "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:nrow(pares_df), bold= F, align = "c")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# COMPARACIONES MÚLTIPLES

library(emmeans)
medias <- emmeans(Datos.aov, "DIMENSION")
pares <- pairs(medias)
pares_df <- as.data.frame(pares)

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)
if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
pares_df %>%
  kable(caption = "Resultados comparaciones múltiples",
        col.names = c("Grupos",
                      "Diferencia estimada",
                      "Desviación Típica",
                      "Grados de libertad",
                      "Estadístico t",
                      "p-valor")) %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped", "bordered", "condensed",
                position = "center",
                font_size = 11) %>%
  row_spec(0, bold= T, align = "c") %>%
  row_spec(1:nrow(pares_df), bold= F, align = "c")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
  pares_df %>%
  kable(caption = "Resultados comparaciones múltiples",
        col.names = c("Grupos",
                      "Diferencia estimada",
                      "Desviación Típica",
                      "Grados de libertad",
                      "Estadístico t",
                      "p-valor"))
}  
```

En la tabla generada, cada fila recoge la diferencia entre la rentabilidad media de las empresas incluidas en las submuestras de los distintos niveles de dimensión de los grupos empresariales a los que perteneces dichas empresas. En la última columna, se muestran los *p-valores.* La hipótesis nula implica que la diferencia entre las rentabilidades medias de los dos grupos implicados son tan pequeñas que pueden considerarse nulas. Por tanto, p-valores muy pequeños (menores que 0,05) implican un rechazo de esta hipótesis, y la admisión de que esas difrencias son significativamente distintas a 0, o sea, "importantes". Por tanto, en el ejemplo se concluye que las rentabilidad media del grupo de empresas de matrices de dimensión "grande" difiere significativamente con respecto a las rentabilidades medias de los otros dos grupos. En cambio, las rentabilidades medias de los grupos de empresas de matrices de dimensión "media" y "pequeña" no difieren significativamente.

## ¿Y si no se cumplen las condiciones para realizar el contraste F de ANOVA?

En el ejemplo anterior hemos visto cómo, aunque hemos seguido el procedimiento de realización del contraste F de ANOVA y la prueba HSD de Tuckey de comparaciones múltiples; no sé cumplían algunos de los requisitos necesarios para confiar en los resultados de estos contrastes: la normalidad de las subpoblaciones (medida a través de las submuestras) y la homogeneidad de las varianzas de estas.

Cuando esto ocurre, es necesario recurrir a técnicas robustas, puesto que el comportamiento de la variable analizada en las subpoblaciones (grupos) no se ajusta al necesario para poder aplicar los contrastes anteriores.

Una prueba robusta que puede suplir al contraste F de ANOVA es la de Kruskal-Wallis. En esta prueba, la hipótesis nula vuelve a ser que no existen diferencias significativas entre las medias de la variable estudiada de las diferentes subpoblaciones (representadas por las correspondientes submuestras), mientras que la hipótesis alternativa apuesta porquela existencia al menos una diferencia significativa. Para aplicar la prueba de Kruskal-Wallis en R, puede recurrirse a la función `kruskal.test()` del paquete `{pgirmess}`. Así, en nuestro ejemplo, tenemos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
#Cuando se incumplen las hipotesis de ANOVA (test robusto)

library(pgirmess)

Datos.K <- kruskal.test(muestra_so$RENECO ~ muestra_so$DIMENSION)
Datos.K
```

Puede comprobarse cómo el *p-valor* es muy pequeño (menor a 0,05), por lo que hemos de rechazar la hipótesis nula de medias iguales y admitir que **existe al menos dos grupos (subpoblaciones) en los que la diferencia entre la rentabilidad económica media es significativa**.

De nuevo, si se rechaza la hipótesis nula, cabe preguntarse cuáles son los grupos o subpoblaciones concretas cuyas medias de rentabilidad económica son significativamente diferentes. Kruskal-Wallis desarrollaron también la versión robusta de la prueba *HSD de Tuckey*, que se incluye en `{pgirmess}`, con la función `kruskalmc()`. En el siguiente código se aplica la función, cuya solución se guarda en el objeto "Datos.kmc". El elemento de la solución "dif.com", que contiene las comparaciones entre las medias, se pasa a un *data frame* para poder ser mostrado como una tabla de `kable()`:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
  # Comparaciones múltiples

  Datos.kmc <- kruskalmc(muestra_so$RENECO ~ muestra_so$DIMENSION)

  # Convertir los resultados a un data frame

  Datos.kmc.df <- as.data.frame(Datos.kmc$dif.com)

  Datos.kmc.df %>%
    kable(format = knitr.table.format,
          caption = "Kruskal-Wallis. Múltiples diferencias",
          col.names = c("Diferencias medias",
                        "Diferencias críticas",
                        "Significación")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = c("striped",
                                        "bordered",
                                        "condensed"),
                  position = "center",
                  font_size = 11) %>%
    row_spec(0, bold = T, align = "c") %>%
    row_spec(1:nrow(Datos.kmc.df), bold = F, align = "c")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
  # Comparaciones múltiples

  Datos.kmc <- kruskalmc(muestra_so$RENECO ~ muestra_so$DIMENSION)

  # Convertir los resultados a un data frame

  Datos.kmc.df <- as.data.frame(Datos.kmc$dif.com)

  tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)
if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
  Datos.kmc.df %>%
    kable(caption = "Kruskal-Wallis. Múltiples diferencias",
          col.names = c("Diferencias medias",
                        "Diferencias críticas",
                        "Significación")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = c("striped",
                                        "bordered",
                                        "condensed"),
                  position = "center",
                  font_size = 11) %>%
    row_spec(0, bold = T, align = "c") %>%
    row_spec(1:nrow(Datos.kmc.df), bold = F, align = "c")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
  Datos.kmc.df %>%
    kable(caption = "Kruskal-Wallis. Múltiples diferencias",
          col.names = c("Diferencias medias",
                        "Diferencias críticas",
                        "Significación"))
}
```

En la tabla generada se comprueba cómo las diferencias que la media de la rentabilidad económica de la subpoblación de empresas pertenecientes a matrices de dimensión "grande" son significativas (para un 0,05 de significación). En cambio, las rentabilidades económicas medias de las empresas pertenecientes a matrices de dimensiones "media" y "pequeña" no difieren entre sí de modo significativo.

## Materiales para realizar las prácticas del capítulo.

En esta sección se muestran los links de acceso a los diferentes materiales (*scripts*, datos...) necesarios para llevar a cabo los contenidos prácticos del capítulo.

**Datos (en formato Microsoft (R) Excel (R)):**

-   eolica_50.xlsx ([obtener aquí](https://docs.google.com/spreadsheets/d/1PVRwLo1eAbFLOgSscLZw0bD9rgMk2X-w/edit?usp=sharing&ouid=115375878280465826079&rtpof=true&sd=true))

**Scripts:**

-   anova_eolica.R [(obtener aquí](https://drive.google.com/file/d/1p4p17OudRprwkpF7wvvF1gwHZG82uv89/view?usp=sharing))

<!--chapter:end:08-Analisis_de_la_Varianza.Rmd-->

# Análisis de Regresión Lineal Múltiple.

## Introducción.

El análisis de regresión (lineal) múltiple es una de las técnicas de análisis de dependencias más profusamente utilizadas. En el modelo de regresión múltiple la variable dependiente tiene escala métrica. Las variables explicativas pueden ser métricas o ser atributos.

Según los datos de los que se alimenta el modelo, se aplicarán diferentes métodos de estimación, especificaciones y pruebas:

-   Series temporales.

-   Datos de corte transversal.

-   Paneles de datos.

En este capítulo nos centraremos en los modelos estimados con base en datos de corte transversal (es decir, las variables tienen datos referentes a distintos casos o individuos: personas, empresas, países, etc.)

La construcción de un modelo de regresión cuenta con una serie de **etapas**, que son:

1.  **Especificación del modelo:** establecer las variables que entrarán a formar parte del modelo (dependiente, explicativas).

2.  **Estimación:** calcular el valor de los parámetros o coeficientes estructurales del modelo.

3.  **Contraste y validación:** verificar si el modelo estimado cumple con las hipótesis que garantizan unas buenas propiedades y si es adecuado para representar la realidad.

4.  **Utilización del modelo:** a efectos de previsión, análisis estructural o simulación de escenarios.

Vamos a partir del modelo básico de regresión (MBR). Es cierto que, para superar ciertas carencias de este, se ha procedido a desarrollar especificaciones y métodos de estimación más elaborados; pero no es menos cierto que no es conveniente "quemar" etapas sin conocer las características del modelo fundamental, como cimiento donde se posan modelados más complejos.

En el MBR vamos a suponer que existen:

-   Una variable dependiente *y*.

-   k variables explicativas $x_j$.

-   Variable o ***perturbación aleatoria*** u, que recoge el efecto conjunto de todas aquellas variables que afectan al comportamiento de *y* pero que no están explicitadas en la especificación como variables *x*.

-   El tamaño de la muestra es *n*.

El modelo que se plantea es:

$$
y_i=\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_kx_{ik}+u_i
$$

con $i=1,2,...,n$ .

O, en notación matricial:

$$
y=X\beta+u
$$

Donde *y* es un vector (nx1), *X* una matriz (nxk), $\beta$ un vector (kx1), y *u* un vestor (nx1).

En el MBR, la perturbación aleatoria *u* debe cumplir con una serie de **hipótesis básicas**: normalidad en su comportamiento, esperanza nula, homoscedasticidad o varianza constante, y ausencia de autocorrelación (covarianza nula entre diferentes elementos del vector de la perturbación). Estas hipótesis, junto a las de permanencia estructural (valores de los elementos de $\beta$ constantes a lo largo de la muestra), no endogeneidad o regresores no-estocásticos (covarianza nula entre la matriz *X* y el vector *u*), y rango pleno (las columnas de la matriz X o variables explicativas no han de ser combinaciones lineales unas de otras); permiten que el MBR pueda ser estimado por el método de mínimos cuadrados ordinarios (MCO), obteniendo estimadores con las mejores propiedades: **insesgadez, eficiencia, consistencia**.

En la medida en que alguna o algunas de las hipótesis básicas no se cumplan, la calidad de los estimadores MCO perderan calidad, en el sentido de no gozar de las propiedades deseables, desde un punto de vista inferencial. En tal caso, podrán aplicarse otros métodos de estimación, diversos métodos econométricos, o asumir que los estimadores carecen de algunas de las propiedades deseables.

El modelo estimado será:

$$
\hat{y}_i=\hat{\beta}_1x_{i1}+\hat{\beta}_2x_{i2}+\cdots+\hat{\beta}_kx_{ik}
$$

Y el **error o residuo** será, para cada observación, $\hat{u}_i=y_i-\hat{y}_i$. El vector de residuos se considera una estimación del vector de perturbaciones aleatorias. Es por ello que el vector de residuos se utiliza para verificar el cumplimiento de las hipótesis del modelo básico referentes al comportamiento de la perturbación (normalidad, homoscedasticidad, ausencia de autocorrelación…)

Tras estas breves notas formales del MBR, pasaremos a construir un modelo que intentará explicar el comportamiento de la rentabilidad económica de un grupo de empresas en función de una serie de variables aleatorias.

## Especificación del MBR. Datos de corte transversal.

Vamos a explicar, mediante un modelo de regresión múltiple, el comportamiento de la rentabilidad económica (RENECO) de las empresas de producción eléctrica mediante tecnología eólica en función del resultado del ejercicio (RES), el activo (ACTIVO), del grado de endeudamiento (ENDEUDA), del grado de apalancamiento (APALANCA), y del tamaño del grupo corporativo (matriz) al que pertenece (DIMENSION). Para ello se ha seleccionado una muestra constituida por 50 empresas.

Supondremos que trabajamos en un proyecto de RStudio de nombre, por ejemplo, "regresion". Los datos de las empresas se encuentran en la hoja "Datos" del archivo de Microsoft® Excel® "[eolica_50.xlsx](https://docs.google.com/spreadsheets/d/1oq17lI4trI8k5oeDgj_fYg9WhbRIqDLG/edit?usp=sharing&ouid=115375878280465826079&rtpof=true&sd=true)". El script con el código que vamos a ir ejecutando se llama "[regresion_eolica.R](https://drive.google.com/file/d/1ilNEAA3-Co9kbs5Gv4arwCBCcCnbn0zY/view?usp=sharing)".

En primer lugar, como de costumbre, hemos de preparar los datos: importarlos en R y gestionar *missing values* y *outliers*.

Así, una vez abierto el *script* en el editor de RStudio , comprobaremos que la primera parte del código está dedicada a la limpieza de la memoria (*Environment*) y a la importación de los datos. Para ello, activaremos el paquete `{readxl}` y utilizaremos la función `read_excel()`, indicando en los argumentos el archivo a explorar, y la *hoja* en la cual se encuentran los datos (hoja "Datos"). También hemos de prestar atención a la cuestión de si existen en la hoja de *Excel* anotaciones en las celdas donde no haya dato, para completar adecuadamente el argumento `na=` . Los datos se almacenarán en el *data frame* "datos". En este *data frame*, la primera columna no es una verdadera variable, sino que se compone de los nombres de los casos o empresas. Con una línea de código adicional transformaremos esa primera columna en el *nombre de las filas*, de modo que tal columna abandona su *rol* de variable. En definitiva, el código para importar los datos es:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
## Regresion multiple empresas eolicas. Disculpen la falta de tildes.

rm(list = ls())

# DATOS

  # Importando

    library(readxl)
    eolicos <- read_excel("eolica_50.xlsx", sheet = "Datos",
                          na = c("n.d.", "s.d."))
    eolicos <- data.frame(eolicos, row.names = 1)
    summary (eolicos)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
## Regresion multiple empresas eolicas. Disculpen la falta de tildes.

rm(list = ls())

# DATOS

  # Importando

    library(readxl)
    eolicos <- read_excel("eolica_50.xlsx", sheet = "Datos",
                          na = c("n.d.", "s.d."))
    eolicos <- data.frame(eolicos, row.names = 1)

library(dplyr)
df <- select(eolicos, everything())

# Número de variables por bloque
variables_por_bloque <- 3

# Dividir las variables en bloques
for (i in seq(1, ncol(df), variables_por_bloque)) {
  # Comentario: No mostramos el nombre del bloque
  print(summary(df[, i:min(i + variables_por_bloque - 1, ncol(df))]))
  cat("\n")
}
rm (i)
rm (variables_por_bloque)
rm (df)
```

Posteriormente, seleccionaremos las variables que vamos a utilizar en el análisis, almacenándolas en otro data frame de nombre, por ejemplo, "originales":

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
  # Seleccionando variables clasificadoras para el analisis

    library(dplyr)
    originales<-select(eolicos,
                       RENECO,
                       RES,
                       ACTIVO,
                       ENDEUDA,
                       APALANCA,
                       DIMENSION)
    summary (originales)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
  # Seleccionando variables clasificadoras para el analisis

    library(dplyr)
    originales<-select(eolicos,
                       RENECO,
                       RES,
                       ACTIVO,
                       ENDEUDA,
                       APALANCA,
                       DIMENSION)

df <- select(originales, everything())

# Número de variables por bloque
variables_por_bloque <- 3

# Dividir las variables en bloques
for (i in seq(1, ncol(df), variables_por_bloque)) {
  # Comentario: No mostramos el nombre del bloque
  print(summary(df[, i:min(i + variables_por_bloque - 1, ncol(df))]))
  cat("\n")
}
rm (i)
rm (variables_por_bloque)
rm (df)

```

Para localizar los casos concretos de ***missing values***, puede recurrirse a utilizar las herramientas de manejo de *data frames* del paquete `{dplyr}`. Con la función `vismiss()` del paquete `{visdat}` podemos tener una visión gráfica general de los valores faltantes. Si hay casos faltantes en una de las variables, los identificaremos filtrando el *data frame* con la función `filter()` de `{dplyr}`. Ante la existencia de *missing values*, se puede actuar de varios modos. Por ejemplo, **se puede intentar obtener por otro canal de información el conjunto de valores** de las variables que no están disponibles, **o recurrir a alguna estimación** para los mismos y asignarlos. En caso de que esto sea difícil, se puede optar, simplemente, por **eliminar** estos casos, en especial cuando representan un porcentaje muy reducido respecto al total de casos. En nuestro ejemplo, vamos a suponer que hemos optado por esta última vía. Así, estos casos, finalmente, será exluidos del análisis, utilizando para ello un nuevo filtro:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Identificando missing values.

    library(visdat)
    vis_miss(originales)

    originales %>%
      filter(is.na(RENECO) | is.na(RES) | is.na(ACTIVO) |
             is.na(ENDEUDA) | is.na(APALANCA) | is.na(DIMENSION)) %>%
      select(RENECO, RES, ACTIVO, ENDEUDA, APALANCA, DIMENSION)  

    originales <- originales %>%
      filter(! is.na(RENECO) & ! is.na(RES) & ! is.na(ACTIVO) &
             ! is.na(ENDEUDA) & ! is.na(APALANCA) & ! is.na(DIMENSION))
```

Tras el código anterior, se han eliminado del *data frame* "originales" las empresas "Sargon Energías S. L. U." y "Viesgo Renovables S. L.", debido a que carecían de dato de rentabilidad económica (RENECO), y "La Caldera Energía Burgos, S. L.", al no tener dato sobre el valor de sus activos (ACTIVO).

Una vez tratados los casos con valores perdidos o *missing values*, **es necesario detectar la presencia de *outliers*** o casos atípicos en la muestra, que pudieran desvirtuar los resultados del análisis de regresión. Para realizar esta etapa, y dado que en nuestro análisis contamos con 5 variables, primero “resumiremos” el valor que toman dichas variables para cada caso, mediante el cálculo de la *distancia de Mahalanobis*. De hecho, las distancias de los diferentes casos se almacenarán en una nueva variable, a la que llamaremos MAHALANOBIS, que se incorporará al *data frame* "originales" por medio de la función `mutate()` de `{dplyr}`, y la función `mahalanobis()`. Recordemos que, en los diferentes argumentos de esta función, el punto "." hace referencia al *data frame* que está delante del operador pipe (%\>%). Posteriormente, construiremos el diagrama de caja de MAHALANOBIS para verificar la existencia de *outliers* (puntos), e identificaremoslos casos correspondientes con el filtro adecuado. Obtaremos por crear un nuevo *data frame*, "originales_so", aplicando un nuevo filtro que elimine esos casos localizados como *outliers*:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Identificando outliers.

    originales <- originales %>%
      mutate(MAHALANOBIS = mahalanobis(select(.,
                                          RENECO,
                                          RES,
                                          ACTIVO,
                                          ENDEUDA,
                                          APALANCA),
                                        center = colMeans(select(.,
                                                            RENECO,
                                                            RES,
                                                            ACTIVO,
                                                            ENDEUDA,
                                                            APALANCA)),
                                        cov=cov(select(.,
                                                  RENECO,
                                                  RES,
                                                  ACTIVO,
                                                  ENDEUDA,
                                                  APALANCA))))

    library (ggplot2)
    ggplot(data = originales, map = (aes(y = MAHALANOBIS))) +
    geom_boxplot(fill = "orange") +
    ggtitle("DISTANCIA DE MAHALANOBIS", subtitle = "Empresas eólicas") +
    ylab("MAHALANOBIS")

    Q1M <- quantile (originales$MAHALANOBIS, c(0.25))
    Q3M <- quantile (originales$MAHALANOBIS, c(0.75))
    
    originales %>% filter(MAHALANOBIS > Q3M + 1.5*IQR(MAHALANOBIS) |
                      MAHALANOBIS < Q1M - 1.5*IQR(MAHALANOBIS)) %>%
                   select(MAHALANOBIS)
    originales_so <- originales %>%
                     filter(MAHALANOBIS <= Q3M + 1.5*IQR(MAHALANOBIS) &
                            MAHALANOBIS >= Q1M - 1.5*IQR(MAHALANOBIS)) 
    originales <- originales %>% select(-MAHALANOBIS)
    originales_so <- originales_so %>% select(-MAHALANOBIS)
```

Se han localizado 7 empresas con valores de la *distancia de Mahalanobis* atípicos, lo que hace pensar que, a su vez, estas empresas registran valores atípicos en una o varias de las variables originales. Como ya hemos señalado, eliminamos estos 7 casos de la muestra. Así, el *data frame* "originales_so", que es el que emplearemos para estimar el modelo, cuenta con 40 observaciones.

Una de las variables explicativas es un atributo o factor, llamado DIMENSION, que cuenta con 3 niveles: "GRANDE", "MEDIANA" y "PEQUEÑA", según el número de empresas integradas en la matriz a la que pertenece cada empresa de la muestra. Hemos de informar a R de la condición de atributo o factor de esta variable (pues de momento solo la contempla como una variable cualitativa o alfanumérica). Para ello ejecutaremos el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Convertir variable DIMENSION en Factor.

    originales_so$DIMENSION <- as.factor(originales_so$DIMENSION)
    levels(originales_so$DIMENSION)
```

Una vez preparadas todas las variables, es el momento de **especificar y estimar una regresión múltiple inicial**, que contenga todas las variables explicativas candidatas a formar parte de la versión final.

Para especificar y estimar este modelo inicial, deben de cargarse previamente algunos paquetes que es necesario utilizar: `{knitr}` y `{kableExtra}` para construir tablas con los resultados de la regresión, y `{broom}`. Este último paquete es fundamental, ya que permite disponer de un modo cómodo de todos los elementos de los que se compone un modelo estimado (no solo lineal, como es nuestro caso).

Por otro lado, El MBR lineal, estimado mediante el método de mínimos cuadrados ordinarios (MCO), se obtendrá mediante la función `lm()`. Los resultados los guardaremos en un objeto de nombre, por ejemplo, “ecua0”. Luego, se realizará un `summary()` para ver dicha estimación.

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# ESPECIFICACION Y ESTIMACION

  # Cargar las librerías necesarias

    library (knitr)
    library (kableExtra)
    library (broom)
    library (car) # para obtener el vif

  # Especificar el modelo de regresión lineal
    ecua0 <- lm(data = originales_so,
                RENECO ~ RES + ACTIVO + ENDEUDA + APALANCA + DIMENSION)
    summary(ecua0)
```

El resultado es:

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# ESPECIFICACION Y ESTIMACION

  # Cargar las librerías necesarias

    library (knitr)
    library (kableExtra)
    library (broom)
    library (car) # para obtener el vif

  # Especificar el modelo de regresión lineal
    ecua0 <- lm(data = originales_so,
                RENECO ~ RES + ACTIVO + ENDEUDA + APALANCA + DIMENSION)
    summary(ecua0)
```

Podemos observar, en cuanto a la **bondad del modelo**, cómo es capaz de recoger el 52% de la varianza o comportamiento de la rentabilidad económica (RENECO), atendiendo al valor del coeficiente de determinación lineal corregido (*Adjusted R-squared*). Los coeficientes o parámetros estimados, en su conjunto, son estadísticamente significativos para una significación de 0,05 (p-valor muy pequeño, en el *contraste F de significación conjunta*). En cuanto a los coeficientes estimados considerados individualmente, encontramos que son **estadísticamente significativos** tanto el término independiente (intercept), como los asociados a las variables RES y ACTIVO, a juzgar por los *p-valores* correspondientes al *contraste t de significación individual*. En ambos casos, además, son coeficientes con signo positivo, lo que se interpreta como que ambas variables influyen sobre RENECO de modo que, a mayor valor de estas variables, en general se obtiene una mayor rentabilidad económica.

Por último, es conveniente advertir que el factor DIMENSION se especifica mediante dos variables dicotómicas que representan a dos de los niveles del factor ("MEDIA" y "PEQUEÑA"). sus coeficientes muestran el efecto relativo de ese nivel en relación con el nivel que no es especificado explicitamente ("GRANDE"), ya que no se pueden especificar todos los niveles de un factor para no generar un problema de multicolinealidad perfecta.

Hay otras informaciones importantes a la hora de valorar la especificación (inicial) del modelo que no se recogen en el `summary()` del mismo. Además, vamos a presentar todo en modo de tablas diseñadas con `kable()`. Para poder hacer esto no solo con este modelo inicial, sino con cualquier otra estimación, vamos a integrar el código correspondiente en una función de R. Llamaremos a esta función `presenta_modelo()`, y recibirá como input la estimación lineal de un modelo, ofreciendo como output tres tablas contenidas en una *lista*: la primera es una *versión* del `summary()`, la segunda es una tabla con otras informaciones adicionales, como el valor del *Criterio de Información de Akaike* (AIC), y la tercera contiene los valores del *factor de inflación de la varianza* (VIF) de las variables del modelo. Previamente a mostrar el código de la función, fijaremos un parámetro de nombre, por ejemplo, "knitr.table.format", para recoger el formato en el que se generarán las tablas diseñadas:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  #diseña salida ordenador

    knitr.table.format = "html"
```

El código de la función se basa en el aprovechamiento, a su vez, de dos de las funciones del paquete `{broom}`. Para realizar la versión en tabla del `summary()` del modelo, se aplica la función `tidy()`. Esta función crea un data frame donde se almacenan las columnas con las distintas informaciones (coeficientes, desviaciones típicas, valores del estadístico t, p-valores...) con tantas filas como variables explicativas especificadas. La función `glance()`, por su lado, extre las siguientes informaciones:

1.  **r.squared**: El coeficiente de determinación R², que indica el porcentaje de variación explicada por el modelo.

2.  **adj.r.squared**: El R² ajustado, que toma en cuenta los grados de libertad.

3.  **sigma**: El error estándar de los residuos.

4.  **statistic**: El estadístico F del modelo.

5.  **p.value**: El valor p asociado con el estadístico F, que indica la significancia global del modelo.

6.  **df**: Los grados de libertad del numerador del estadístico F.

7.  **logLik**: El logaritmo de la verosimilitud del modelo.

8.  **AIC**: El criterio de información de Akaike.

9.  **BIC**: El criterio de información bayesiano.

10. **deviance**: La desviación del modelo.

El código es, en definitiva:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
  # Definir la función de presentación de resultados:  presenta_modelo() #####

    presenta_modelo <- function(modelo) {

    # Lista de piezas
      modelo_piezas <-list()
  
    # Aplicar la función tidy() al modelo
      resultados <- tidy(modelo)
  
    # Seleccionar las columnas deseadas
      resultados <- resultados[, c("term",
                                   "estimate",
                                   "std.error","statistic",
                                   "p.value")]
    # Añadir la columna 'stars' según los valores de 'p.value'
      resultados$stars <- cut(resultados$p.value,
                            breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
                            labels = c("***", "**", "*", "·", " "),
                            right = FALSE)
    # formatear los valores de la columna "estimate" a 5 decimales
      resultados$estimate <- formatC(resultados$estimate,
                                     format = "f",
                                     digits = 5)
  
    # Crear la tabla con kable
      tabla1 <- resultados %>%
        kable(format = knitr.table.format,
          caption = "Modelo Lineal",
          col.names = c("Variable", "Coeficiente", "Desv. Típica",
                        "Estadístico t", "p-valor", "Sig."),
          digits = 3,
          align = c("l", "c", "c", "c", "c", "c")) %>%
        kable_styling(full_width = F,
                      bootstrap_options = "striped",
                                          "bordered",
                                          "condensed",
                      position = "center",
                      font_size = 11)
 
      modelo_piezas[[1]] <- tabla1
  
    # Aplicar la función glance
      estadisticos <- glance(modelo)
      estadisticos <- estadisticos[,c("r.squared",
                                      "adj.r.squared",
                                      "sigma",
                                      "statistic",
                                      "p.value",
                                      "AIC",
                                      "nobs")]
    # Crear la tabla con kable
      tabla2 <- estadisticos %>%
        kable(format = knitr.table.format,
          caption = "Estadísticos del modelo",
          col.names = c("R2", "R2 ajustado", "Sigma", "Estadístico F",
                        "p-valor", "AIC", "num. observaciones"),
          digits = 3,
          align = "c") %>%
        kable_styling(full_width = F,
                      bootstrap_options = "striped",
                                          "bordered",
                                          "condensed",
                      position = "center",
                      font_size = 11)
      
      modelo_piezas[[2]] <- tabla2  

    # Obtener VIF
      vif_df <- as.data.frame(vif(modelo))

    # Añadir nombres de filas
      library(tibble)
      vif_df <- vif_df %>%
      rownames_to_column(var = "Variable")
  
    # Crear tabla con kable
      tabla3 <- vif_df[,1:2] %>%
        kable(format = knitr.table.format,
              caption = "Factor de inflación de la varianza",
              col.names = c("Variable","Valor VIF"),
              digits = 3,
              align = "c") %>%
        kable_styling(full_width = F,
                      bootstrap_options = "striped",
                                          "bordered",
                                          "condensed",
                      position = "center",
                      font_size = 11)
      
      modelo_piezas[[3]] <- tabla3
  
    return(modelo_piezas)
  }
############################################################################

```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
  # Definir la función de presentación de resultados:  presenta_modelo() #####

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

    presenta_modelo <- function(modelo) {

    # Lista de piezas
      modelo_piezas <-list()
  
    # Aplicar la función tidy() al modelo
      resultados <- tidy(modelo)
  
    # Seleccionar las columnas deseadas
      resultados <- resultados[, c("term",
                                   "estimate",
                                   "std.error","statistic",
                                   "p.value")]
    # Añadir la columna 'stars' según los valores de 'p.value'
      resultados$stars <- cut(resultados$p.value,
                            breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
                            labels = c("***", "**", "*", "·", " "),
                            right = FALSE)
    # formatear los valores de la columna "estimate" a 5 decimales
      resultados$estimate <- formatC(resultados$estimate,
                                     format = "f",
                                     digits = 5)
  
    # Crear la tabla con kable

      if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {      
      tabla1 <- resultados %>%
        kable(caption = "Modelo Lineal",
          col.names = c("Variable", "Coeficiente", "Desv. Típica",
                        "Estadístico t", "p-valor", "Sig."),
          digits = 3,
          align = c("l", "c", "c", "c", "c", "c")) %>%
        kable_styling(full_width = F,
                      bootstrap_options = "striped",
                                          "bordered",
                                          "condensed",
                      position = "center",
                      font_size = 11)
      }else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
       tabla1 <- resultados %>%
        kable(caption = "Modelo Lineal",
          col.names = c("Variable", "Coeficiente", "Desv. Típica",
                        "Estadístico t", "p-valor", "Sig."),
          digits = 3,
          align = c("l", "c", "c", "c", "c", "c"))
      }
      modelo_piezas[[1]] <- tabla1
  
    # Aplicar la función glance
      estadisticos <- glance(modelo)
      estadisticos <- estadisticos[,c("r.squared",
                                      "adj.r.squared",
                                      "sigma",
                                      "statistic",
                                      "p.value",
                                      "AIC",
                                      "nobs")]
    # Crear la tabla con kable
      if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
      tabla2 <- estadisticos %>%
        kable(caption = "Estadísticos del modelo",
          col.names = c("R2", "R2 ajustado", "Sigma", "Estadístico F",
                        "p-valor", "AIC", "num. observaciones"),
          digits = 3,
          align = "c") %>%
        kable_styling(full_width = F,
                      bootstrap_options = "striped",
                                          "bordered",
                                          "condensed",
                      position = "center",
                      font_size = 11)
      }else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
        tabla2 <- estadisticos %>%
        kable(caption = "Estadísticos del modelo",
          col.names = c("R2", "R2 ajustado", "Sigma", "Estadístico F",
                        "p-valor", "AIC", "num. observaciones"),
          digits = 3,
          align = "c")
      }
      modelo_piezas[[2]] <- tabla2  

    # Obtener VIF
      vif_df <- as.data.frame(vif(modelo))

    # Añadir nombres de filas
      library(tibble)
      vif_df <- vif_df %>%
      rownames_to_column(var = "Variable")
  
    # Crear tabla con kable
      if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
      tabla3 <- vif_df[,1:2] %>%
        kable(caption = "Factor de inflación de la varianza",
              col.names = c("Variable","Valor VIF"),
              digits = 3,
              align = "c") %>%
        kable_styling(full_width = F,
                      bootstrap_options = "striped",
                                          "bordered",
                                          "condensed",
                      position = "center",
                      font_size = 11)
      }else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
        tabla3 <- vif_df[,1:2] %>%
        kable(caption = "Factor de inflación de la varianza",
              col.names = c("Variable","Valor VIF"),
              digits = 3,
              align = "c")
      }
      modelo_piezas[[3]] <- tabla3
  
    return(modelo_piezas)
  }
############################################################################

```

Una vez definida la función, podemos aplicarla al modelo estimado inicial, guardando las tres tablas generadas en la lista "modelo_0", que pueden ser visualizadas:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  modelo_0 <- presenta_modelo(ecua0)

  modelo_0[[1]]
  modelo_0[[2]]
  modelo_0[[3]]

```

Los resultados de la primera y segunda tabla ya fueron comentados en casi su totalidad anteriormente, al comentar el `summary()` del modelo. Se ha añadido el valor del *Criterio de Información de Akaike* (AIC). Esta es una medida basada en la *función de verosimilitud* que permite comparar la adecuación de especificaciones alternativas para representar la realidad, de modo que, **a menor AIC, mejor especificación.**

La tercera tabla muestra los valores del *factor de la inflación de la varianza* (VIF) para cada variable explicativa. El VIF **mide el riesgo de que, debido a la influencia de la variable en cuestión, exista un problema de multicolinealidad** entre las variables explicativas del modelo. Un valor de 5/10 (dependiendo de los autores) sugiere que puede existir un problema de multicolinealidad importante. En el caso del modelo inicial, ningún valor del VIF sugiere un posible problema de multicolinealidad.

La búsqueda de una especificación alternativa que sea más adecuada puede atender a múltiples estrategias del analista, y del propio propósito con el que se quiere utilizar el modelo. Por tanto, implica una gran carga de subjetividad. No obstante, existen métodos automatizados para que, una vez se tiene la estimación inicial, se obtenga una especificación más sencilla (*Principio de Parsimonia*) sin una pérdida grande de bondad de la regresión. Por ejemplo, un método es el *step / backward* que, en función del *Criterio de Información de Akaike (AIC)*, irá probando a estimar especificaciones más simples que disminuyan de AIC (lo que implica una mejor especificación). En nuestro caso, se aplicará con el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  ecuaDEF <- step(ecua0, scale = 0,
                  direction = c("backward"),
                  trace = 1, steps = 1000, k = 2)
  summary (ecuaDEF)
```

El resultado del algoritmo es una especificación del modelo, cuya estimación se almacena en memoria con el nombre, por ejemplo, "ecuaDEF". Se ha mostrado el `summary()` del modelo. También se puede aplicar la función `presenta_modelo()`, para obtener un *output* de la estimación más detallado:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  modelo_DEF <- presenta_modelo(ecuaDEF)

  modelo_DEF[[1]]
  modelo_DEF[[2]]
  modelo_DEF[[3]]

```

En el modelo definitivo (ecuaDEF), tan solo permanecen 3 variables explicativas: RES, ACTIVO (ambas significativas para una significación de 0,05) y APALANCA (significativa para una significación de 0,1). Una diferencia importante respecto al modelo inicial es que, el signo del coeficiente asociado a ACTIVO pasa a ser negativo. También el grado de apalancamiento tiene asociado un coeficiente negativo (a mayor apalancamiento, menor rentabilidad económica). SI observamos la segunda tabla, puede observarse que el valor de AIC es de 216,20, inferior al valor de AIC del modelo inicial (220,4). La bondad del ajuste, medida por medio del R2 ajustado, es de 0,541; superior al de modelo inicial (0,521). Finalmente, en la tercera tabla se muestran unos valores de VIF bajos, por lo que se descartan problemas de multicolinealidad.

Una vez decidida la especificación (final) del modelo, es necesario desarrollar la **etapa de contrastación de las hipótesis básicas del modelo**, con la intención de determinar el grado en que los estimadores obtenidos mediante MCO gozan de buenas propiedades, o si es necesario aplicar métodos de estimación alternativos, métodos econométricos específicos, o incluso re-especificar el modelo).

Es conveniente, previamente al análisis de cada hipótesis, generar varios gráficos de gran utilidad.

El primer paso, no obstante, es recurrir a la función `augment()` del paquete `{broom}`. Esta función, aplicada a un modelo, genera algunas series de datos fundamentales relacionadas con el mismo, y las almacena junto a las variables del modelo especificado en un data frame. En nuestro caso, hemos llamado al data frame "series_ecuaDEF", y hemos cambiado el nombre a las series que vamos a utilizar posteriormente: los valores ajustados de la variable dependiente (RENECO.est), los residuos (residuos), y los valores de la distancia de Cook (cooksd). Además, hemos creado una variable llamada ORDEN para asignar un valor correlativo a cada observación o caso de la muestra:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# MODELO FINAL: CONTRASTACIÓN.

  series_ecuaDEF <- augment(ecuaDEF)
  series_ecuaDEF <- series_ecuaDEF %>%
  rename(RENECO.est = .fitted,
         residuos = .resid,
         cooksd = .cooksd)
  series_ecuaDEF$ORDEN = c(1:nrow(series_ecuaDEF))
  summary (series_ecuaDEF)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# MODELO FINAL: CONTRASTACIÓN.

  series_ecuaDEF <- augment(ecuaDEF)
  series_ecuaDEF <- series_ecuaDEF %>%
  rename(RENECO.est = .fitted,
         residuos = .resid,
         cooksd = .cooksd)
  series_ecuaDEF$ORDEN = c(1:nrow(series_ecuaDEF))
library(dplyr)
df <- select(series_ecuaDEF, everything())

# Número de variables por bloque
variables_por_bloque <- 3

# Dividir las variables en bloques
for (i in seq(1, ncol(df), variables_por_bloque)) {
  # Comentario: No mostramos el nombre del bloque
  print(summary(df[, i:min(i + variables_por_bloque - 1, ncol(df))]))
  cat("\n")
}
rm (i)
rm (variables_por_bloque)
rm (df)
```

Una vez obtenidas todas las series necesarias, diseñaremos los gráficos que facilitarán la comprensión y contraste del modelo final. El primero compara los valores reales de RENECO con las estimaciones del modelo, RENECO.est. Lógicamente, es deseable que, para cada caso, la distancia entre ambos puntos sea mínima. El segundo muestra los residuos. El tercero es el *gráfico de densidad* de los residuos, y el cuarto representa, para cada caso, el *valor de la distancia de Cook*. Los valores altos de la distancia de Cook en un modelo de regresión indican observaciones que tienen una influencia significativa en los coeficientes estimados del modelo. Son considerados "altos" los valores que superan el valor inverso de 4 por el número de observaciones muestrales.

Los 4 gráficos se agrupan mediante la gramática del paquete {patchwork}. Finalmente, se genera una tabla a partir de un filtro para identificar los casos concretos que tienen valores "altos" de la distancia de Cook. En definitiva, el código es:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
  # Gráficos.
  
    g_real_pred <- ggplot(data = series_ecuaDEF) +
      geom_point(aes(x = ORDEN, y = RENECO.est),
                 size= 2,
                 alpha= 0.6,
                 color = "blue") +    
      geom_point(aes(x = ORDEN, y = RENECO),
                 size= 2,
                 alpha= 0.6,
                 color = "red") +
      geom_line(aes(x = ORDEN, y = RENECO.est),
                color = "blue",
                linetype = "dashed",
                size= 1) +
      geom_line(aes(x = ORDEN, y = RENECO),
                color = "red",
                linetype = "dashed",
                size= 1) +
      geom_segment(aes(x = ORDEN, xend = ORDEN, y = RENECO.est, yend = RENECO),
                   color = "orange") +
      ggtitle("RENTABILIDAD ECONÓMICA.",
              subtitle= "VALORES REALES (rojo) vs PREDICCIONES (azul).") +
      xlab("Casos") + 
      ylab("Rentabilidad Económica: Real y Predicción")

  g_resid <- ggplot(data = series_ecuaDEF, aes(x = ORDEN, y = residuos)) +
    geom_point(size=2, alpha= 0.6, color = "blue") +
    geom_smooth(color = "firebrick", span = 0.5) +
    geom_hline(yintercept = 0, color = "red")+
    ggtitle("RENTABILIDAD ECONÓMICA.", subtitle= "Residuos.")+
    xlab("Casos") + 
    ylab("Residuos")

  g_hresid <- ggplot(data = series_ecuaDEF, map = aes(x = residuos)) +
    geom_density(colour = "red", fill = "orange", alpha = 0.6) +
    ggtitle("RENTABILIDAD ECONÓMICA", subtitle = "Densidad Residuos")+
    xlab("Rentabilidad Económica") +
    ylab("Densidad")

  g_cook <- ggplot(data = series_ecuaDEF, aes(x = ORDEN, y = cooksd)) +
    geom_bar(stat = "identity") +
    geom_hline(yintercept = 4/nrow(series_ecuaDEF),
               linetype = "dashed",
               color = "red") +
    ggtitle("RENTABILIDAD ECONÓMICA.", subtitle= "Distancia de Cook.")+
    xlab("Casos") + 
    ylab("Distancias")

  library (patchwork)

  (g_real_pred | g_hresid) / (g_resid | g_cook)

  tablaCook <- series_ecuaDEF %>%
    filter ( cooksd > 4/nrow(series_ecuaDEF)) %>%
    select (.rownames, cooksd) %>%
    kable(format = knitr.table.format,
          caption = "Casos destacados distancia de Cook",
          col.names = c("Caso","Distancia de Cook"),
          digits = 3,
          align = c("l","c")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped",
                                      "bordered",
                                      "condensed",
                  position = "center",
                  font_size = 11)

    tablaCook
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
  # Gráficos.
  
    g_real_pred <- ggplot(data = series_ecuaDEF) +
      geom_point(aes(x = ORDEN, y = RENECO.est),
                 size= 2,
                 alpha= 0.6,
                 color = "blue") +    
      geom_point(aes(x = ORDEN, y = RENECO),
                 size= 2,
                 alpha= 0.6,
                 color = "red") +
      geom_line(aes(x = ORDEN, y = RENECO.est),
                color = "blue",
                linetype = "dashed",
                size= 1) +
      geom_line(aes(x = ORDEN, y = RENECO),
                color = "red",
                linetype = "dashed",
                size= 1) +
      geom_segment(aes(x = ORDEN, xend = ORDEN, y = RENECO.est, yend = RENECO),
                   color = "orange") +
      ggtitle("RENTABILIDAD ECONÓMICA.",
              subtitle= "VALORES REALES (rojo) vs PREDICCIONES (azul).") +
      xlab("Casos") + 
      ylab("Rentabilidad Económica: Real y Predicción")

  g_resid <- ggplot(data = series_ecuaDEF, aes(x = ORDEN, y = residuos)) +
    geom_point(size=2, alpha= 0.6, color = "blue") +
    geom_smooth(color = "firebrick", span = 0.5) +
    geom_hline(yintercept = 0, color = "red")+
    ggtitle("RENTABILIDAD ECONÓMICA.", subtitle= "Residuos.")+
    xlab("Casos") + 
    ylab("Residuos")

  g_hresid <- ggplot(data = series_ecuaDEF, map = aes(x = residuos)) +
    geom_density(colour = "red", fill = "orange", alpha = 0.6) +
    ggtitle("RENTABILIDAD ECONÓMICA", subtitle = "Densidad Residuos")+
    xlab("Rentabilidad Económica") +
    ylab("Densidad")

  g_cook <- ggplot(data = series_ecuaDEF, aes(x = ORDEN, y = cooksd)) +
    geom_bar(stat = "identity") +
    geom_hline(yintercept = 4/nrow(series_ecuaDEF),
               linetype = "dashed",
               color = "red") +
    ggtitle("RENTABILIDAD ECONÓMICA.", subtitle= "Distancia de Cook.")+
    xlab("Casos") + 
    ylab("Distancias")

  library (patchwork)

  (g_real_pred | g_hresid) / (g_resid | g_cook)

  tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {        
tablaCook <- series_ecuaDEF %>%
    filter ( cooksd > 4/nrow(series_ecuaDEF)) %>%
    select (.rownames, cooksd) %>%
    kable(caption = "Casos destacados distancia de Cook",
          col.names = c("Caso","Distancia de Cook"),
          digits = 3,
          align = c("l","c")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped",
                                      "bordered",
                                      "condensed",
                  position = "center",
                  font_size = 11)
 }else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
   tablaCook <- series_ecuaDEF %>%
    filter ( cooksd > 4/nrow(series_ecuaDEF)) %>%
    select (.rownames, cooksd) %>%
    kable(caption = "Casos destacados distancia de Cook",
          col.names = c("Caso","Distancia de Cook"),
          digits = 3,
          align = c("l","c"))
 }
    tablaCook
```

De los gráficos anteriores se desprende, en general, que los residuos parecen mantener un comportamiento conforme a una distribución normal, y que hay una observación o caso con una distancia de Cook que puede influir de modo relevante en el valor de los coeficientes estimados. Esta empresa es identificada como "Innogy Spain S.A." Podría estudiarse en detalle este caso o incluso reestimar el modelo sin su presencia, a fin de comprobar el efecto que tiene esta observación sobre la estimación en general.

Tras estudiar los gráficos, vamos a pasar a contrastar las hipótesis básicas del MBR referidas a la *forma funcional*, y la *normalidad* y *comportamiento homoscedástico* de la perturbación aleatoria. Para ello se aplicarán las pruebas *Ramsey-Reset*, *Shapiro-Wilk* y *Breusch-Pagan*, respectivamente. Los resultados de las tres pruebas se presentarán condensados en una tabla, en la que además se incluirá la conclusión del contraste, para una significación de 0,05. Para crear la tabla, primero se generará un data frame con sus elementos, denominado, por ejemplo, "check_hipotesis". El código es:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
  # Hipótesis básicas MBR.

    library (lmtest)
    reset_test <- resettest(ecuaDEF, data= originales_so) # F. Funcional
    shapiro_test <- shapiro.test(series_ecuaDEF$residuos) # Normalidad
    bp_test <- bptest(ecuaDEF) # Homoscedasticidad

  # Tabla resultados

    # Crear un data frame con los resultados

      check_hipotesis <- data.frame(
        "Tipo_de_prueba" = c("Forma Funcional",
                             "Normalidad de perturbación aleatoria",
                             "Homoscedasticidad de perturbación aleatoria"),
        "Prueba" = c("Ramsey-Reset",
                     "Shapiro-Wilk",
                     "Breusch-Pagan"),
        "Estadistico" = c(reset_test$statistic,
                          shapiro_test$statistic,
                          bp_test$statistic),
        "P_valor" = c(reset_test$p.value,
                      shapiro_test$p.value,
                      bp_test$p.value),
        "Conclusion" = c(ifelse(reset_test$p.value >= 0.05,
                                 "F. funcional correcta",
                                 "F. funcional incorrecta"),
                         ifelse(shapiro_test$p.value >= 0.05,
                                "Normalidad",
                                "No-Normalidad"),
                         ifelse(bp_test$p.value >= 0.05,
                                "Homoscedasticidad",
                                "Heteroscedasticidad")))

    row.names(check_hipotesis) <- NULL

  # Crear la tabla con kable
    
    tabla_check <- check_hipotesis %>%
      kable(format = knitr.table.format,
            caption = "Contrastes de hipótesis del MBR",
            col.names = c("Tipo de prueba",
                          "Prueba",
                          "Estadístico",
                          "P-valor",
                          "Conclusión"),
            digits = 3,
            align = c("l", "c", "c", "c", "c")) %>%
      kable_styling(full_width = F,
                    bootstrap_options = "striped",
                                        "bordered",
                                        "condensed",
                    position = "center",
                    font_size = 11)

    tabla_check
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
  # Hipótesis básicas MBR.

    library (lmtest)
    reset_test <- resettest(ecuaDEF, data= originales_so) # F. Funcional
    shapiro_test <- shapiro.test(series_ecuaDEF$residuos) # Normalidad
    bp_test <- bptest(ecuaDEF) # Homoscedasticidad

  # Tabla resultados

    # Crear un data frame con los resultados

      check_hipotesis <- data.frame(
        "Tipo_de_prueba" = c("Forma Funcional",
                             "Normalidad de perturbación aleatoria",
                             "Homoscedasticidad de perturbación aleatoria"),
        "Prueba" = c("Ramsey-Reset",
                     "Shapiro-Wilk",
                     "Breusch-Pagan"),
        "Estadistico" = c(reset_test$statistic,
                          shapiro_test$statistic,
                          bp_test$statistic),
        "P_valor" = c(reset_test$p.value,
                      shapiro_test$p.value,
                      bp_test$p.value),
        "Conclusion" = c(ifelse(reset_test$p.value >= 0.05,
                                 "F. funcional correcta",
                                 "F. funcional incorrecta"),
                         ifelse(shapiro_test$p.value >= 0.05,
                                "Normalidad",
                                "No-Normalidad"),
                         ifelse(bp_test$p.value >= 0.05,
                                "Homoscedasticidad",
                                "Heteroscedasticidad")))

    row.names(check_hipotesis) <- NULL

  # Crear la tabla con kable

      tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {      
    tabla_check <- check_hipotesis %>%
      kable(caption = "Contrastes de hipótesis del MBR",
            col.names = c("Tipo de prueba",
                          "Prueba",
                          "Estadístico",
                          "P-valor",
                          "Conclusión"),
            digits = 3,
            align = c("l", "c", "c", "c", "c")) %>%
      kable_styling(full_width = F,
                    bootstrap_options = "striped",
                                        "bordered",
                                        "condensed",
                    position = "center",
                    font_size = 11)
 }else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
    tabla_check <- check_hipotesis %>%   
      kable(caption = "Contrastes de hipótesis del MBR",
            col.names = c("Tipo de prueba",
                          "Prueba",
                          "Estadístico",
                          "P-valor",
                          "Conclusión"),
            digits = 3,
            align = c("l", "c", "c", "c", "c"))    
 }  
    tabla_check
```

En la tabla de resultados del contraste de las hipótesis básicas del MBR, encontramos que el modelo no plantea problemas de falta de normalidad o heteroscedasticidad en la perturbación aleatoria. En cambio, la prueba de *Ramsey-Reset* rechaza la hipótesis nula de especificación lineal correcta. En concreto, este contraste plantea estas dos hipótesis:

-   **Hipótesis nula (H0)**: El modelo está correctamente especificado, es decir, no hay errores de especificación. En términos más técnicos, esto significa que las combinaciones no lineales de los valores ajustados no tienen poder explicativo adicional sobre la variable dependiente.

-   **Hipótesis alternativa (H1)**: El modelo está mal especificado, lo que implica que las combinaciones no lineales de los valores ajustados sí tienen poder explicativo adicional sobre la variable dependiente.

Si se asume que las variables especificadas son las correctas, y no hay omisión de variables relevantes; un rechazo de la hipótesis nula implica que la relación funcional planteada (lineal), no es correcta.

El incumplimiento de la hipótesis de linealidad podría acarrear que los estimadores MCO obtenidos adolecen de la pérdida de la propiedad de insesgadez.

Si el modelo supera razonablemente la fase de contraste de las hipótesis básicas; podría ser utilizado para los objetivos planteados en la investigación: análisis estructural, previsión, simulación. En este ejemplo, vamos a realizar un ejercicio de simulación.

Vamos a obtener la respuesta de la rentabilidad económica (RENECO), ante 3 escenarios alternativos. Dichos escenarios están guardados en la hoja "Simula" del archivo de Microsoft® Excel® “[eolica_escenarios.xlsx](https://docs.google.com/spreadsheets/d/1QUqpOHC_XK38MBggb3hYibDReHVfd7wL/edit?usp=sharing&ouid=115375878280465826079&rtpof=true&sd=true)”. Hay que tener en cuenta que en los escenarios se aportan valores para algunas variables que no aparecen en el modelo final. Lógicamente, lo importante son los datos del escenario correspondientes a las variables que sí están especificadas. Los escenarios se importan y se almacenan en el *data frame* de nombre, por ejemplo, "escenario":

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# SIMULACIÓN

  # Cargar escenario de Excel

    escenario <- read_excel("eolica_escenarios.xlsx", sheet = "Simula")
    escenario <- data.frame(escenario, row.names = 1)
    escenario

```

Mediante la función `predict()`, se generará la respuesta a los escenarios propuestos, por parte del modelo definitivo (ECUADEF). Esta respuesta se guardará en el *data frame* de nombre, por ejemplo, "estimación", que luego uniremos al *data frame* "escenario" mediante la función `cbind()`, creando un único *data frame* llamado, por ejemplo, "simulacion":

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Simulación con el modelo

    estimacion <-predict (object= ecuaDEF,
                          newdata = escenario,
                          interval="prediction",
                          level=0.95)
    estimacion
    simulacion <- cbind(escenario, estimacion)
```

Finalmente, vamos a presentar la simulación en forma de tabla. En primer lugar, vamos a dar un formato específico a cada variable, cara a su volcado a la tabla, con la función `format()`. El argumento `nsmall=` indica el número mínimo de decimales que habrá a la derecha del punto decimal:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Formatear las columnas con el número mínimo de decimales deseado

    simulacion$ENDEUDA <- format(simulacion$ENDEUDA, nsmall = 3)
    simulacion$APALANCA <- format(simulacion$APALANCA, nsmall = 3)
    simulacion$fit <- format(simulacion$fit, nsmall = 3)
    simulacion$lwr <- format(simulacion$lwr, nsmall = 3)
    simulacion$upr <- format(simulacion$upr, nsmall = 3)
```

Tras definir el formato numérico de las variables, se construirá la tabla, de nombre, por ejemplo, "tablasimula":

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
  # Tabla

    tablasimula <- simulacion %>%
      kable(format = knitr.table.format,
            caption = "Simulación Modelo Rentabilidad Económica",
            col.names = c("Escenario",
                          "Resultado",
                          "Activo",
                          "Endeuda",
                          "Apalancamiento",
                          "Dimensión",
                          "Previsión",
                          "Inferior 95%",
                          "Superior 95%"),
        digits = 3) %>%
      kable_styling(full_width = F,
                    bootstrap_options = "striped",
                    "bordered",
                    "condensed",
                    position = "center",
                    font_size = 11) %>%
      row_spec(0, bold= T, align = "c") %>%
      row_spec(1:(nrow(simulacion)), bold= F, align = "c")

  tablasimula

```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
  # Tabla

      tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {      
    tablasimula <- simulacion %>%
      kable(caption = "Simulación Modelo Rentabilidad Económica",
            col.names = c("Escenario",
                          "Resultado",
                          "Activo",
                          "Endeuda",
                          "Apalancamiento",
                          "Dimensión",
                          "Previsión",
                          "Inferior 95%",
                          "Superior 95%"),
        digits = 3) %>%
      kable_styling(full_width = F,
                    bootstrap_options = "striped",
                    "bordered",
                    "condensed",
                    position = "center",
                    font_size = 11) %>%
      row_spec(0, bold= T, align = "c") %>%
      row_spec(1:(nrow(simulacion)), bold= F, align = "c")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {    
tablasimula <- simulacion %>%
      kable(caption = "Simulación Modelo Rentabilidad Económica",
            col.names = c("Escenario",
                          "Resultado",
                          "Activo",
                          "Endeuda",
                          "Apalancamiento",
                          "Dimensión",
                          "Previsión",
                          "Inferior 95%",
                          "Superior 95%"),
        digits = 3)
}
  tablasimula
```

## Materiales para realizar las prácticas del capítulo.

En esta sección se muestran los links de acceso a los diferentes materiales (*scripts*, datos...) necesarios para llevar a cabo los contenidos prácticos del capítulo.

**Datos (en formato Microsoft (R) Excel (R)):**

-   eolica_50.xlsx ([obtener aquí](https://docs.google.com/spreadsheets/d/1oq17lI4trI8k5oeDgj_fYg9WhbRIqDLG/edit?usp=sharing&ouid=115375878280465826079&rtpof=true&sd=true))

-   eolica_escenarios.xlsx ([obtener aquí](https://docs.google.com/spreadsheets/d/1QUqpOHC_XK38MBggb3hYibDReHVfd7wL/edit?usp=sharing&ouid=115375878280465826079&rtpof=true&sd=true))

-   **Scripts:**

-   regresion_eolica.R [(](https://drive.google.com/file/d/1p4p17OudRprwkpF7wvvF1gwHZG82uv89/view?usp=sharing)[obtener aquí](https://drive.google.com/file/d/1ilNEAA3-Co9kbs5Gv4arwCBCcCnbn0zY/view?usp=sharing))

<!--chapter:end:09-Analisis_de_Regresion_Lineal.Rmd-->

# Análisis de Datos Cualitativos.

## Introducción.

En numerosas ocasiones la información con que el analista debe enfrentarse es de naturaleza **cualitativa**, esto es, la información se recoge en características no numéricas o **atributos** (o factores o variables categóricas o cualitativas). Cuando ocurre esto, es necesario recurrir a técnicas de explotación específicas para este tipo de datos. Así, en este capítulo vamos a explorar algunos métodos para extraer información útil cuando los datos de **los que disponemos son categóricos.**

En estos casos, la información suele sintetizarse y presentarse mediante las denominadas “**tablas de contingencia**”. En este tipo de tablas, se muestran las frecuencias conjuntas, es decir, el número de casos que comparten los distintos niveles o categorías de los diferentes factores.

Cuando se trabaja con varios factores o atributos, representados en su correspondiente tabla de contingencia, uno de los análisis más interesantes es determinar si existe **asociación** entre los factores o atributos. Esto es, si se aprecia algún tipo de relación estadística entre estas variables categóricas o cualitativas, en el sentido de si se puede afirmar que el hecho de que los casos de la muestra tomen ciertos niveles o categorías en unos factores, hace que estos mismos casos tiendan a tomar ciertos niveles o categorías de otro u otros factores.

## Tablas de contingencia, y asociación entre dos atributos o factores.

Comenzaremos con el caso más sencillo, en el que los casos de nuestra muestra se distribuyen entre las categorías o niveles de dos atributos o factores. En este caso, la tabla de contingencia que presente los datos de esta situación será una simple tabla de doble entrada. En las filas de la tabla se dispondrán las categorías de uno de los factores, y en las columnas de la tabla se situarán las categorías del otro factor.

Para ilustrar el tratamiento de una tabla de contingencia bidimensional, vamos a trabajar con los datos correspondientes a 51 empresas de generación eléctrica presentes en el archivo de Microsoft® Excel® "[eolica_contingencia.xlsx](https://docs.google.com/spreadsheets/d/1N5QIRKbm6OKAOaN8DB2YFDptpZ_0pbPc/edit?usp=sharing&ouid=115375878280465826079&rtpof=true&sd=true)". Este archivo cuenta con 3 hojas, los datos en cuestión se encuentran en la que tiene por nombre "Datos".

Dentro de "Datos", existen dos variables categóricas, atributos o factores: la variable DIMENSION, que tiene, a su vez, tres categorías dependiendo del número de empresas que integradas en la matriz de la empresa en cuestión ("GRANDE", "MEDIA" o "REDUCIDA"); y la variable VALORACION, que cuenta con tres categorías dependiendo de la opnión de un panel de expertos en cuanto a la situación económica de la empresa ("OPTIMA", "NORMAL", "PESIMA"). Nuestra intención es analizar si existe **evidencia sobre asociación** entre las dos variables categóricas, en el sentido de que los casos tienden a concentrarse con cierta facilidad en ciertas combinaciones de categorías de uno y otro factor, y/o a no posicionarse en otras combinaciones; o si por el contrario no se puede afirmar que exista asociación de este tipo.

El código de R que iremos aplicando se encuentra en el *script* "[correspondencias_eolica.R](https://drive.google.com/file/d/1u8gJa3jQH2cwuQ8KsISQ9esHKrFAO_au/view?usp=sharing)". Trabajaremos en un proyecto específico de RStudio, si así lo valoramos como conveniente (en este ejemplo, el proyecto "correspondencias").

Al abrir el *script* en el editor de RStudio, lo primero que veremos será la instrucción para limpiar la memoria de objetos. Tras ello, se procede a la importación de los datos del fichero de *Excel*. Tras importar las variables, se procede a ajustar la importación para que la primera columna pase a ser el nombre de los casos (filas) del *data frame* que recibe y almacena los datos. Este *data frame* se denomina, por ejmplo, "eolicas". Con un `summary()` comprobamos que las variables se han importado correctamente:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# Analisis de correspondencias simple de eolicas
# Disculpen por la falta de tildes!

rm(list = ls())

# DATOS

  # Importando datos

    library (readxl)
    eolicas <- read_excel("eolica_contingencia.xlsx", sheet ="Datos")
    eolicas <- data.frame(eolicas, row.names = 1)
    summary (eolicas)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# Analisis de correspondencias simple de eolicas
# Disculpen por la falta de tildes!

rm(list = ls())

# DATOS

  # Importando datos

    library (readxl)
    eolicas <- read_excel("eolica_contingencia.xlsx", sheet ="Datos")
    eolicas <- data.frame(eolicas, row.names = 1)
library(dplyr)
df <- select(eolicas, everything())

# Número de variables por bloque
variables_por_bloque <- 3

# Dividir las variables en bloques
for (i in seq(1, ncol(df), variables_por_bloque)) {
  # Comentario: No mostramos el nombre del bloque
  print(summary(df[, i:min(i + variables_por_bloque - 1, ncol(df))]))
  cat("\n")
}
rm (i)
rm (variables_por_bloque)
rm (df)

```

A continuación, creamos otro *data frame*, de nombre, por ejemplo, "originales", con los dos atributos analizados: el factor DIMENSION y el factor VALORACION. Se comprueba si existen observaciones con *missing values* y, en tal caso, se eliminan dichas observaciones aplicando el filtro apropiado, a partir de la función `filter()` del paquete `{dplyr}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Seleccionando factores/atributos para el analisis

    library(dplyr)
    originales<-select(eolicas, DIMENSION, VALORACION)
    summary (originales)

  # Identificando missing values.

    library(visdat)
    vis_miss(originales)
    originales %>% filter(is.na(DIMENSION) | is.na(VALORACION)) %>%
      select(DIMENSION, VALORACION)  
    originales <- originales %>%
      filter(! is.na(DIMENSION) & ! is.na(VALORACION))  
```

En el ejemplo, se comprueba que no existen casos con *missing values*, por lo que la tabla queda en blanco y no se descarta ninguna observación.

La siguiente etapa es la construcción de la "**tabla de contingencia**". La función fundamental es `table()` que, como sabemos, hace un recuento de los casos (frecuencias) en los que una variable toma un determinado valor (o en los que un atributo adopta una determinada categoría o nivel). Cuando esta función se aplica a más de una variable o atributo, hace un recuento de los casos que adoptan las posibles combinacionesse entre valores/categorías o niveles de las variables o atributos implicados. Cuando se trata de dos atributos, por tanto, `table()` construye una tabla de contingencia mediante la formulación de una tabla de doble entrada. En nuestro ejemplo, a la tabla de contingencia la hemos denominado, por ejemplo, "tab.originales". Luego, la hemos presentado con un formato de tabla elaborado con la función `kable()` del paquete `{knitr}`, y algunas funciones adicionales del paquete `{kableExtra}`:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# TABLA DE CONTINGENCIA

  tab.originales <- table(originales)

 library(knitr)
 library(kableExtra)
 knitr.table.format = "html"

 addmargins(tab.originales) %>%
  kable(format = knitr.table.format,
        caption="Empresas eólicas") %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped", "bordered", "condensed",
                position = "center",
                font_size = 12) %>%
  add_header_above(c(DIMENSION = 1, VALORACION = 3, " " = 1),
                   bold=T,
                   line=T) %>%
  row_spec(0, bold= T, align = "c") %>%
  column_spec(1, bold = T)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# TABLA DE CONTINGENCIA

  tab.originales <- table(originales)

 library(knitr)
 library(kableExtra)
 knitr.table.format = "html"
 
 tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

      if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {      
 addmargins(tab.originales) %>%
  kable(caption="Empresas eólicas") %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped", "bordered", "condensed",
                position = "center",
                font_size = 12) %>%
  add_header_above(c(DIMENSION = 1, VALORACION = 3, " " = 1),
                   bold=T,
                   line=T) %>%
  row_spec(0, bold= T, align = "c") %>%
  column_spec(1, bold = T)
      }else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
 addmargins(tab.originales) %>%
  kable(caption="Empresas eólicas") %>%
  kable_styling(full_width = F,
                bootstrap_options = "striped", "bordered", "condensed",
                position = "center",
                font_size = 12)
      }        
```

Es de destacar que, en el código de la tabla, se ha añadido la función `addmargin()` para que se añadan las frecuencias marginales de las categorias de ambos atributos (sumas de filas y columnas). Además, se ha utilizado la función `add_header_above()` del paquete `{kableExtra}` para añadir filas superiores al encabezado, que ocupen distintas cantidades de columnas.

La tabla también se puede representar gráficamente mediante la función `mosaic()` de la librería `{vcd}`, con lo que se percibirá mejor la magnitud de las frecuencias conjuntas (celdas de la tabla). A mayor frecuencia, mayor área del rectángulo correspondiente:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
    library (vcd)
    mosaic(tab.originales,
           main = "Eólicas: Dimensión Matriz y Valoración Expertos.",
           shade = T,
           gp = shading_Marimekko(tab.originales),
           main_gp = gpar(fontsize = 14),
           sub_gp = gpar(fontsize = 12))
```

Los argumentos de la función `mosaic()` juegan el siguiente papel:

1.  **`tab.originales`**: Es la tabla de contingencia que contiene los datos a visualizar en el gráfico mosaico. Debe ser una tabla de contingencia creada con la función `table()` o una matriz con datos categóricos.

2.  **`main`**: Título principal del gráfico.

3.  **`shade`**: Si se establece en `TRUE`, aplica un coloreado a las celdas del mosaico; si es `FALSE` todas los rectángulos serán del mismo color.

4.  **`gp`**: Parámetros gráficos para personalizar la apariencia del gráfico. En este caso, se usa `shading_Marimekko()` para aplicar un sombreado específico. Hay otras funciones de parámetros gráficos como `shading_hcl()`, `shading_max`, o cualquier función personalizada que devuelva un objeto de clase `gpar`.

5.  **`main_gp`**: Parámetros gráficos para el título principal, como el tamaño de la fuente. Se pueden especificar atributos como, por medio de la función `gpar()`, como `fontsize`, `fontfamily`, `col`, etc.

6.  **`sub_gp`**: Igual que el caso anterior; pero para el subtítulo del gráfico de mosaico.

En nuestro ejemplo, podemos apreciar con claridad como una importante proporción de los casos (empresas) se concentran en la combinación de valoración óptima y dimensión de la empresa matriz reducida. También es destacable la combinación de valoración normal y dimensión de la compañía matriz grande. Por el lado opuesto, destaca la combinación de valoración normal y dimensión de la matriz media, que no posee ningún caso (frecuencia 0), y valoración pésima y dimesión de la matriz reducida.

Otro modo de visualizar la estructura de la tabla es hacer gráficos de barras que muestren las frecuencias de las categorías de cada factor (frecuencias marginales), aunque en cada barra se pueda diferenciar, además, los casos o frecuencias que pertenecen a las categorías del otro factor. En nuestro caso:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Representando frecuencias de categorias en factores

    library (ggplot2)
    library (patchwork)

    g1 <- ggplot(originales, mapping= aes(x= DIMENSION, fill = VALORACION)) +
          geom_bar() +
          ggtitle("Tamaño de la matriz.", subtitle = "Empresas eólicas") + 
          ylab("Frecuencias") +
          xlab("Dimensión")

    g2 <- ggplot(originales, mapping= aes(x= VALORACION, fill = DIMENSION)) +
          geom_bar() +
          ggtitle("Valoración Expertos", subtitle = "Empresas eólicas") + 
          ylab("Frecuencias") +
          xlab("Valoración")

    (g1 + g2) + plot_annotation(title = "Frecuencias Marginales.",
                  theme = theme(plot.title = element_text(size = 14)))


```

Se observa cómo, en cuanto al atributo o factor DIMENSION, la categoría más frecuente es la dimensión "REDUCIDA", mientras que la categoría que menos se da en la muestra es, destacadamente, "MEDIA". Además, como ya se ha comentado, destaca que la mayor parte de los casos de la categoría "REDUCIDA" tienen una valoración de "OPTIMA" en el factor VALORACION. También llama la atención que en la categoría "MEDIA" no existen casos con valoración "NORMAL". En cuanto al gráfico del atributo o factor "VALORACION", el mayor número de frecuencias, de modo muy destacado, se concentran en la categoría "OPTIMA", estando las otras dos categorías bastante igualadas en cuanto al número de casos. Es reseñable también que, en la categoría "OPTIMA", la mayor parte de casos tienen una dimensión "REDUCIDA".

Como ya hemos dicho, una de las cuestiones más importantes en el análisis de tablas de contingencia es **determinar si existe asociación entre ambos atributos o factores** (valoración de las empresas y dimensión de las compañías matrices), en el sentido de poder plantear que los casos (empresas) concentradas en ciertas categorías concretas de uno de los factores o atributos tienden a concentrarse, simultáneamente, en ciertas categorías concretas del otro factor o atributo; o al revés: que el hecho de que los casos no tiendan a concentrarse en ciertas categorías de uno de los factores o atributos está relacionado con que no se concentren en ciertas categorías del otro factor o atributo. En nuestro ejemplo, ya hemos señalado algunas combinaciones de categorías que podrían llevar a pensar a que existe cierto grado de asociación entre los atributos DIMENSION y VALORACION.

Existen diferentes pruebas para verificar la posible existencia de asociación entre dos factores. Una de ellas es el ***contraste o prueba de asociación de Pearson**.* Este contraste se base en un estadístico del contraste, que bajo la hipótesis nula de que no existe asociación sigue una distribución Chi/Ji Cuadrado. El estadístico, en realidad, es una medida global de **lo distante que está la tabla de contingencia observada (sus frecuencias conjuntas) respecto a la estructura "ideal" que tendría que tener si existiera independencia** "total" entre ambos atributos. Así, el estadístico del contraste de la *prueba de asociación de Pearson* es:

$$
\chi^2 = \sum_{i=1}^{r} \sum_{j=1}^{c} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}
$$

donde:

-   ( $O_{ij}$ ) es la frecuencia observada en la celda ( (i, j) ).

-   ( $E_{ij}$ ) es la frecuencia esperada en la celda ( (i, j) ), calculada como ( $E_{ij} = \frac{R_i \cdot C_j}{N}$ ).

-   ( $R_i$ ) es el total de la fila ( i ).

-   ( $C_j$ ) es el total de la columna ( j ).

-   ( $N$ ) es el total general de todas las observaciones.

Los *residuos estandarizados* son una medida de la desviación de las frecuencias observadas respecto a las frecuencias teóricas o esperadas (en caso de independencia perfecta entre atributos) en una tabla de contingencia. Se utilizan para identificar celdas que contribuyen significativamente a la asociación entre las variables. La fórmula para calcular los residuos estandarizados es:

$$
 r_{ij} = \frac{O_{ij} - E_{ij}}{\sqrt{E_{ij} \left(1 - \frac{R_i}{N}\right) \left(1 - \frac{C_j}{N}\right)}}
$$

donde:

-   ( $r_{ij}$ ) es el residuo estandarizado para la celda en la fila ( i ) y la columna ( j ).

El paquete `{stats}` de R (que se carga automáticamente, por defecto, al iniciar R, por lo que no hay que "activarlo"), contiene la función `chisq.test()` que efectúa la prueba de asociación de *Pearson*. El código y resultado de la prueba es:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# INDEPENDENCIA / ASOCIACION
  
  # Test de asociación de Pearson (Ji-Cuadrado)
    
  Prueba_asoc_Pearson <- chisq.test(tab.originales)
  Prueba_asoc_Pearson

```

En nuestro caso, el *p-valor* es menor que 0,05, luego **se rechaza la hipótesis nula de independencia** de los atributos o factores, y admitimos que **existe asociación entre ambos.**

Precisamente, el paquete `{vcd}` ofrece la posibilidad de, para cada frecuencia conjunta, visualizar la diferencia estandarizada entre el valor observado y el que debería darse en el caso de que existiera independencia perfecta entre los dos atributos o factores (*residuo de Pearson*). Para ello, se aplica la función `assoc()`, destinada a construir un gráfico con los residuos de *Pearson* de la tabla de contingencia. Los residuos, si son estadísticamente significativos para una significación de 0,05, se colorean de naranja, y en caso contrario de gris. Para determinar los colores concretos, se ha creado la función `custom_shading()`, que se pasa en el argumento `gp=` para personalizar el color concreto de cada barra del gráfico:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  Residuos_std <- Prueba_asoc_Pearson$stdres

  # Definir una función de sombreado personalizada
  custom_shading <- function(residuals, cutoff = 1.96) {
    # Crear una matriz de colores basada en los residuos estandarizados
    colors <- ifelse(abs(residuals) > cutoff, "orange", "lightgray")
    return(colors)
  }
  
  # Aplicar la función de sombreado en el gráfico mosaico
  
  assoc(tab.originales,
        main = "Asociación: Dimensión Matriz y Valoración Expertos.",
        sub = "Residuos de Pearson Tipificados. Naranja: significativos con sig. = 0,05",
        compress = FALSE,
        gp = gpar(fill = custom_shading(Residuos_std)),
        main_gp = gpar(fontsize = 14),
        sub_gp = gpar(fontsize = 12))
```

Puede observarse cómo en las combinaciones GRANDE/NORMAL, GRANDE/OPTIMA y MEDIA/NORMAL los residuos de Pearson son estadísticamente significativos (lo que, a su vez, indica que las frecuencias correspondientes están muy alejadas de las que debería haber en caso de independencia perfecta), lo que llevaría que la prueba haya rechazado la hipótesis de independencia entre los atributos o factores.

## Análisis de correspondencias.

El *análisis de correspondencias* es una técnica destinada a **representar visualmente una tabla de contingencia, en un gráfico bidimensional**. Puede utilizarse para representar una tabla de dos atributos o factores (análisis de correspondencias simple) o de más de dos (análisis de correspondencias múltiple). Cuando en el gráfico bidimensional se representan más de dos atributos o factores, este análisis de convierte, además, en una **técnica de reducción de la dimensión de la información.** De hecho, se puede afirmar que el análisis de correspondencias es una suerte de análisis de **componentes principales aplicado a variables categóricas**.

De acuerdo a lo anterior, se pueden establecer los siguientes paralelismos:

-   Las componentes en el análisis de componentes principales equivalen a los **ejes o dimensiones** del análisis de correspondencias.

-   La varianza total o *comunalidad* de las variables originales métricas del análisis de componentes principales pasa a ser, en el análisis de correspondencias, la **inercia total**.

-   La varianza que es capaz de asumir cada componente en el análisis de componentes principales ahora se denomina **inercia principal** (del eje o dimensión en cuestión).

-   Las puntuaciones de las componentes para un caso son, ahora, las **coordenadas** de una categoría de uno de los atributos o factores.

-   El papel de las cargas de las variables en las componentes principales lo asumen las **contribuciones** de las categorías o niveles de los factores o atributos en los ejes o dimensiones.

Como principal resultado del análisis se obtendrá un gráfico de dos ejes (dimensiones) en el cuál **se situarán más próximas las categorías de uno y otro factor que mantengan entre sí cierta tendencia a asociarse**.

Existen varios paquetes de R que permiten desarrollar un análisis de correspondencias simple (de dos atributos o factores), como es nuestro ejemplo. Hemos optado, en el ejemplo, por utilizar el paquete `{FactoMineR}`. Este paquete dispone de la función `CA()`, que es la encargada de realizar los cálculos del análisis.

En el siguiente código, el análisis de correspondencias se aplica a la tabla de contingencia "tab.originales", almacenándose la solución en la lista "aceolicas". Luego, se crea la lista "SolucionCA" para almacenar las tres tablas que recogerán los resultados. Después se crea un *data frame*, "EigenCA", con el elemento "eig" de la solución del análisis. Ese elemento es un *data frame* de dos columnas (dimensiones o ejes del análisis), para cada una de las cuales se reúnen tres informaciones: la inercia principal o varianza recogida por la dimensión o eje, el porcentaje que supone respecto a la inercia total puesta en juego, y el porcentaje acumulado. El *data frame* se pasa a formato "tabla" de `kable()` con el nombre "TableEigenCA", que se guarda en la lista "SolucionCA" como su primer elemento.

Después se generan dos tablas con los mismos elementos, para cada uno de los atributos o factores del análisis. La primera corresponde al atributo DIMENSION. Esta tabla cuenta con una fila por cada una de las categorías de DIMENSION, esto es, "GRANDE", "MEDIA" y "REDUCIDA". La segunda corresponde al atributo VALORACION, y cuenta con una fila para sus categorías, "OPTIMA", "NORMAL", "PESIMA". En ambas tablas, las columnas son las siguientes:

1.  **Inercia**: La inercia es una medida de la varianza explicada por cada fila o columna (categoría de alguno de los atributos) en el análisis de correspondencias. Valores más altos indican que la fila o columna contribuye más a la varianza total del análisis (Inercia Total).

2.  **Coordenadas Dim. 1 y Dim. 2**: Coordenadas de las filas o columnas en las dos dimensiones del espacio de correspondencias. Indican la posición de cada fila o columna en el espacio bidimensional.

3.  **cos2 Dim. 1 y Dim. 2 (Coseno Cuadrado)**: El coseno cuadrado de los ángulos entre las filas o columnas y las dimensiones. Mide la calidad de la representación de las filas o columnas en las dimensiones. Valores cercanos a 1 indican que la fila o columna está bien representada en esa dimensión. Valores bajos indican una mala representación.

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# ANALISIS DE CORRESPONDENCIAS SIMPLE

  library (FactoMineR)
  aceolicas <- CA(X = tab.originales, graph = F)

  SolucionCA <- list()
  
  EigenCA <- as.data.frame(t(aceolicas$eig))
  EigenCA$elemento <- c("Inercia Total", "% Inercia Principal", "Acumulada")
  EigenCA <- data.frame(EigenCA, row.names = 3)
  
  
  TableEigenCA <- EigenCA %>%
    kable(format = knitr.table.format,
          caption="Análisis de correspondencias: Dimensión Matrix vs Valoración.",
          col.names = c("Dimensión/Eje 1", "Dimensión/Eje 2"),
          digits = 3,
          align= c("c", "c")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped", "bordered", "condensed",
                  position = "center",
                  font_size = 12) %>%
    row_spec(0, bold= T, align = "c")
  
    SolucionCA[[1]] <-TableEigenCA
  
  # Extraer la información de las filas
    rows_data <- aceolicas$row$coord
    rows_cos2 <- aceolicas$row$cos2

  # Crear un DataFrame para las filas
    rows_df <- data.frame(
    Iner_1000 = aceolicas$row$inertia,
    Dim_1 = rows_data[, 1],
    cos2_1 = rows_cos2[, 1],
    Dim_2 = rows_data[, 2],
    cos2_2 = rows_cos2[, 2]
    )
    rownames(rows_df) <- rownames(rows_data)
    
    TableRows <- rows_df %>%
      kable(format = knitr.table.format,
          caption= (paste0("Análisis de correspondencias: ", colnames(originales)[1], ".")),
          col.names = c("Inercia", "Coordenadas Dim. 1", "Cos2 Dim. 1",
                        "Coordenadas Dim. 2", "Cos2 Dim. 2"),
          digits = 3,
          align= c("c", "c", "c", "c", "c")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped", "bordered", "condensed",
                  position = "center",
                  font_size = 12) %>%
    row_spec(0, bold= T, align = "c")
    
    SolucionCA[[2]] <-TableRows

  # Extraer la información de las columnas
    columns_data <- aceolicas$col$coord
    columns_cos2 <- aceolicas$col$cos2

  # Crear un DataFrame para las columnas
    columns_df <- data.frame(
    Inercia = aceolicas$col$inertia,
    Dim_1 = columns_data[, 1],
    cos2_1 = columns_cos2[, 1],
    Dim_2 = columns_data[, 2],
    cos2_2 = columns_cos2[, 2]
    )
    rownames(columns_df) <- rownames(columns_data)

    TableCols <- columns_df %>%
      kable(format = knitr.table.format,
          caption= (paste0("Análisis de correspondencias: ", colnames(originales)[2], ".")),
          col.names = c("Inercia", "Coordenadas Dim. 1", "Cos2 Dim. 1",
                        "Coordenadas Dim. 2", "Cos2 Dim. 2"),
          digits = 3,
          align= c("c", "c", "c", "c", "c")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped", "bordered", "condensed",
                  position = "center",
                  font_size = 12) %>%
    row_spec(0, bold= T, align = "c")
    
   SolucionCA[[3]] <-TableCols
   
  SolucionCA[[1]]
  SolucionCA[[2]]
  SolucionCA[[3]]
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# ANALISIS DE CORRESPONDENCIAS SIMPLE

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

  library (FactoMineR)
  aceolicas <- CA(X = tab.originales, graph = F)

  SolucionCA <- list()
  
  EigenCA <- as.data.frame(t(aceolicas$eig))
  EigenCA$elemento <- c("Inercia Total", "% Inercia Principal", "Acumulada")
  EigenCA <- data.frame(EigenCA, row.names = 3)
  
if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {        
  TableEigenCA <- EigenCA %>%
    kable(caption="Análisis de correspondencias: Dimensión Matrix vs Valoración.",
          col.names = c("Dimensión/Eje 1", "Dimensión/Eje 2"),
          digits = 3,
          align= c("c", "c")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped", "bordered", "condensed",
                  position = "center",
                  font_size = 12) %>%
    row_spec(0, bold= T, align = "c")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
  TableEigenCA <- EigenCA %>%
    kable(caption="Análisis de correspondencias: Dimensión Matrix vs Valoración.",
          col.names = c("Dimensión/Eje 1", "Dimensión/Eje 2"),
          digits = 3,
          align= c("c", "c"))
}  
    SolucionCA[[1]] <-TableEigenCA
  
  # Extraer la información de las filas
    rows_data <- aceolicas$row$coord
    rows_cos2 <- aceolicas$row$cos2

  # Crear un DataFrame para las filas
    rows_df <- data.frame(
    Iner_1000 = aceolicas$row$inertia,
    Dim_1 = rows_data[, 1],
    cos2_1 = rows_cos2[, 1],
    Dim_2 = rows_data[, 2],
    cos2_2 = rows_cos2[, 2]
    )
    rownames(rows_df) <- rownames(rows_data)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {          
    TableRows <- rows_df %>%
      kable(caption= (paste0("Análisis de correspondencias: ", colnames(originales)[1], ".")),
          col.names = c("Inercia", "Coordenadas Dim. 1", "Cos2 Dim. 1",
                        "Coordenadas Dim. 2", "Cos2 Dim. 2"),
          digits = 3,
          align= c("c", "c", "c", "c", "c")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped", "bordered", "condensed",
                  position = "center",
                  font_size = 12) %>%
    row_spec(0, bold= T, align = "c")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
    TableRows <- rows_df %>%
      kable(caption= (paste0("Análisis de correspondencias: ", colnames(originales)[1], ".")),
          col.names = c("Inercia", "Coordenadas Dim. 1", "Cos2 Dim. 1",
                        "Coordenadas Dim. 2", "Cos2 Dim. 2"),
          digits = 3,
          align= c("c", "c", "c", "c", "c"))
}    
    SolucionCA[[2]] <-TableRows

  # Extraer la información de las columnas
    columns_data <- aceolicas$col$coord
    columns_cos2 <- aceolicas$col$cos2

  # Crear un DataFrame para las columnas
    columns_df <- data.frame(
    Inercia = aceolicas$col$inertia,
    Dim_1 = columns_data[, 1],
    cos2_1 = columns_cos2[, 1],
    Dim_2 = columns_data[, 2],
    cos2_2 = columns_cos2[, 2]
    )
    rownames(columns_df) <- rownames(columns_data)

if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {      
    TableCols <- columns_df %>%
      kable(format = knitr.table.format,
          caption= (paste0("Análisis de correspondencias: ", colnames(originales)[2], ".")),
          col.names = c("Inercia", "Coordenadas Dim. 1", "Cos2 Dim. 1",
                        "Coordenadas Dim. 2", "Cos2 Dim. 2"),
          digits = 3,
          align= c("c", "c", "c", "c", "c")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped", "bordered", "condensed",
                  position = "center",
                  font_size = 12) %>%
    row_spec(0, bold= T, align = "c")
}else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
    TableCols <- columns_df %>%
      kable(caption= (paste0("Análisis de correspondencias: ", colnames(originales)[2], ".")),
          col.names = c("Inercia", "Coordenadas Dim. 1", "Cos2 Dim. 1",
                        "Coordenadas Dim. 2", "Cos2 Dim. 2"),
          digits = 3,
          align= c("c", "c", "c", "c", "c"))  
}  
   SolucionCA[[3]] <-TableCols
   
  SolucionCA[[1]]
  SolucionCA[[2]]
  SolucionCA[[3]]
```

En nuestro ejemplo, la primera tabla nos informa, esencialmente, de que la primera dimensión o eje recoge el 77,5% de la inercia total (varianza o comportamiento) de las categorías de los dos atributos o factores, mientras que la segunda dimensión o eje asume algo menos del 22,5%. Recordemos que esta interpretación es similar a la que se hace cuando se determinan los porcentajes de "comunalidad" que recogen las componentes calculadas, en el análisis de componentes principales. Recordemos también que, en el caso del análisis de correspondencias simple (dos atributos o factores), las dos dimensiones recogen el 100% de la inercia total (puesto que no se "descarta" ninguna dimensión de las calculadas).

Es habitual que los porcentajes de inercia principal asumidos por ambas dimensiones o ejes puedan estar bastante desequilibrados a favor del primer eje; sobre todo cuando hay una fuerte asociación entre los atributos o factores. Algunas características que pueden ayudar a un mayor equilibrio son:

-   Distribución de los Datos: Una distribución equilibrada de las frecuencias en la tabla de contingencia puede ayudar a que la inercia total se distribuya de manera más uniforme.

-   Número de Categorías: Tener un número similar de categorías en ambos atributos puede favorecer una distribución más equitativa de la inercia entre los ejes.

-   Relaciones Simétricas: Que las relaciones entre las categorías de los atributos sean simétricas y no estén dominadas por unas pocas asociaciones fuertes (ninguna valoración domina de un modo extraordinario en alguna dimensión de matriz).

-   Tamaño de la Muestra: Un tamaño de muestra grande.

-   Homogeneidad de las Categorías: Que las categorías dentro de cada atributo sean relativamente homogéneas respecto a sus asociaciones con las categorías del otro atributo (la estructura de valoraciones es similar entre dimensiones).

En cuanto a la segunda tabla, destinada al análisis de las categorías del atributo o factor DIMENSION, podemos concluir lo siguiente: la mayor varianza (inercia) de las tres categorías de dimensión de la compañía matriz de pertenencia corresponde a "GRANDE", seguida de "MEDIA" y "REDUCIDA". En cuanto a la calidad de la representación de estas categorías en las dos dimensiones o ejes; las categorías "GRANDE" y "MEDIA" están representadas principalmente por la dimensión o eje 1; solo la categoría "REDUCIDA" viene mejor representada (levemente) por la dimensión o eje 2.

Por último, la tercera tabla recoge la representación de las categorías del atributo o factor VALORACION: "OPTIMA", "NORMAL" y "PESIMA". La mayor inercia o varianza es la de "NORMAL", seguida de "OPTIMA" y, por último, "PESIMA". En cuanto a la calidad de la representación en las dimensiones o ejes, "NORMAL" y "OPTIMA" vienen representadas, casi exclusivamente, por la dimensión o eje 1; mientras que ocurre lo contrario con la categoría "PESIMA".

El principal output del análisis de correspondencias es el gráfico bidimensional donde se representan las categorías de los atributos o factores. En general, cuanto más cerca se localicen determinada categoría de un factor y determinada categoría del otro, mayor será la relación estadística (asociación) entre ambas categorías, lo que implica a su vez mayor asociación entre los atributos o factores.

Antes de presentar el gráfico bidimensional, vamos a construir un gráfico de barras con los porcentajes de la inercia total que asumen cada una de las dos dimenciones o ejes del gráfico. Esto es importante, ya que si la inercia recogida por la segunda dimensión o eje es muy pequeña, como a veces ocurre, habría que tener en cuenta, sobre todo, la localización de las categorías en la primera dimensión o eje, a la hora de establecer conclusiones en cuanto a la asociación o no entre categorías. También es necesario tener en cuenta la calidad con que cada eje representa a cada categoría de cada variable o factor, medida con el coseno cuadrado (cos2), como ya se ha comentado.

Para crear de un modo sencillo el gráfico de contribuciones de los ejes o dimensiones a la inercia total, puede recurrirse a la función `fviz_screeplot()` del paquete `{factoextra}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Gráfico de contribuciones de las dimensiones  a la Inercia Total

    library(factoextra)

    gcontrib <- fviz_screeplot(aceolicas,
                               addlabels= TRUE,
                               barcolor= "darkblue",
                               barfill= "orange",
                               linecolor= "red") +
                labs(title= "Contribución de los ejes a la Inercia Total.",
                     subtitle = "Dimensión Matriz y Valoración Expertos.") +
                ylab("Porcentaje de Inercia Total") +
                xlab("Eje") +
                theme(text = element_text(size = 12))

gcontrib
```

Puede observarse, como ya se recogió en la primera table de la solución, que la primera dimensión o eje asume el 77,5% de la inercia tital (varianza o comportamiento de las categorías), mientras que la segunda dimensión o eje asume el 22,5% restante.

En cuanto al gráfico bidimensional, puede obtenerse igualmente mediante la función `fviz_ca_biplot()` de `{factoextra}`. Este es el papel que juegan los diferentes argumentos de la función:

-   **`map = "symmetric"`**: Representa tanto las filas como las columnas en el mismo espacio, utilizando la misma escala.

-   **`axes = c(1, 2)`**: Selecciona las dos primeras dimensiones para el gráfico.

-   **`label = "all"`**: Muestra las etiquetas de todas las categorías.

-   **`repel = TRUE`**: Evita la superposición de etiquetas.

-   **`col.col` y `col.row`**: Colores para las columnas y filas, respectivamente.

-   **`labs()`**: Añade títulos y subtítulos al gráfico.

-   **`theme()`**: Ajusta el tamaño del texto en el gráfico.

Es de destacar que el argumento `map=` controla cómo se representan las filas y columnas en el gráfico bidimensional del análisis de correspondencias. Este argumento tiene varias opciones que determinan la escala y la simetría de la representación. Para representar las posibles asociaciones entre las categorías de los dos factores implicados, la opción más apropiada es "symmetric". Esta opción permite visualizar las filas y las columnas en el mismo espacio, facilitando la interpretación de las asociaciones entre las categorías de ambas variables.

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
    gbiplot <- fviz_ca_biplot (aceolicas,
                               axes= c(1,2),
                               label= "all",
                               repel = T,
                               col.col= "orange",
                               col.row= "darkblue",
                               map= "symmetric") +
    labs(title= "Gráfico de dispersión de categorías.",
         subtitle = "Eólicas: Matriz y Valoración Expertos.") +
    theme(text = element_text(size = 12))

    gbiplot
```

En nuestro ejemplo, se aprecia cómo hay una intensa asociación entre la categoría de VALORACIÓN "OPTIMA", y la categoria de DIMENSION (de la empresa matriz correspondiente) "REDUCIDA". También se aprecia cierta asociación entre las categorías de DIMENSION "GRANDE" y VALORACION "NORMAL". Por último la categoría de VALORACION "PESIMA", y la categoría del factor o atributo DIMENSION "MEDIA" están bastante alejados de otras categorías, lo que implica que parece no estar muy asociadas con otras categorías específicas.

Finalmente, para una presentación más compacta de los dos últimos gráficos, puede recurrirse al paquete {patchwork}:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
    gcombinado <- gcontrib / gbiplot
    gcombinado <- gcombinado +
      plot_annotation(title = "DIMENSIÓN MATRIZ vs VALORACIÓN EXPERTOS.",
      subtitle = "Empresas eólicas.",
      caption = "Análisis de Correspondencias Simple.",
      theme = theme(plot.title = element_text(size = 16, face = "bold"),
                    plot.subtitle = element_text(size = 14),
                    plot.caption = element_text(size = 12))
    )

    gcombinado
```

## Modelos logaritmico-lineales aplicados a tablas de contingencia.

Volviendo a la verificación de la existencia de asociación entre factores, hemos de tener en cuenta la posibilidad de que los factores en estudio sean más de dos. En este caso multifactorial, no son aplicable las técnicas exploradas anteriormente, pensadas para el caso bifactorial (simple).

Una posibilidad que se nos ofrece es la aplicación de modelos logarítmico-lineales (log-lineales) a tablas de contingencia multifactoriales, que es lo que se tratará en este apartado.

Los modelos log-lineales aplicados a tablas de contingencia parten de la condición de independencia entre factores. Supongamos el caso simple, con una tabla de contingencia de dos factores o atributos, A y B. Bajo la hipótesis de independencia (absoluta o teórica) entre los factores, tendremos que cada frecuencia absoluta conjunta de la tabla se obtiene como:

$$
n_{ij} = \frac{R_i \cdot C_j}{N} = N \cdot \frac{R_i}{N} \cdot \frac{C_j}{N} 
$$ con:

-   ( $n_{ij}$ ) es la frecuencia conjunta para la categoría o fila ( i ) del atributo o factor A, y la categoría o columna ( j ) del atributo o factor B).
-   ( $R_i$ ) es el total de la fila ( i ) del atributo o factor A.
-   ( $C_j$ ) es el total de la columna ( j ) del atributo o factor B.

Tomando logaritmos:

$$
\ln{n_{ij}} = ln{N} \ + ln{\frac{R_i}{N}} + ln{\frac{C_j}{N}}
$$

Y renombrando los términos:

$$
\ln{n_{ij}} = \lambda + \lambda^A_i + \lambda^B_j
$$

Si no existe independencia entre ambas variables o factores, tendremos:

$$
\ln{n_{ij}} = \lambda + \lambda^A_i + \lambda^B_j + \lambda^{AB}_{ij}
$$

Los términos $\lambda^A_i$ y $\lambda^B_j$ se denominan efectos directos o principales, mientras que el término $\lambda^{AB}_{ij}$ es el efecto conjunto o interacción entre los dos atributos o factores. Bajo la hipótesis de independencia entre los dos factores, ese efecto tomaría valor 0.

Si el modelo planteado solo tiene en la especificación los efectos directos, se dirá que es el **modelo de independencia**. Si se plantean todas las interacciones posibles entre los factores, se hablará del **modelo saturado**. El modelo *saturado* otorga un ajuste perfecto; pero es poco útil a la hora de extraer conclusiones relevantes. Se requiere un modelo que, aunque no ajuste al 100% las frecuencias, **recoja solo los efectos más importantes**.

Si algunos de los efectos más importantes (es decir, significativos en términos estadísticos) son conjuntos (interacciones), podremos concluir que existe asociación entre los factores del modelo (al menos, entre los que existan interacciones importantes, en el caso de más de dos factores). Si no es así, y el modelo que mejor representa la realidad es el de independencia, diremos que los factores son independientes unos de otros, y que no existe asociación (significativa) entre ellos.

Los modelos han de respetar siempre la regla de la jerarquía en su especificación: solo se podrá plantear en el modelo una interacción entre los atributos A y B si se han especificado los efectos directos de A y de B. O se podrá plantear una interacción conjunta entre los factores A, B y C si se han planteado también las interacciones entre A y B, B y C, y A y C.

Los modelos log-lineales se estiman por el método de máxima-verosimilitud.

La estrategia a seguir para estudiar la asociación entre factores será plantear diferentes especificaciones, y comprobar si son aptas para representar bien la realidad (tabla de contingencia). Eso se consigue mediante pruebas que evalúan la magnitud de los residuos, entendidos como la diferencia entre las frecuencias conjuntas realmente observadas, y las frecuencias estimadas por el modelo estimado. Las dos pruebas más comunes son la del *ratio de verosimilitud*, y la de *Pearson*.

SI existen varios modelos que, desde el punto de vista de las pruebas anteriores, son aptos para representarrazonablemente la realidad, se eligirá el mejor de ellos mediante algún criterio específico, como puede ser aquel que minimice el Criterio de Información de Akaike, que penaliza, para una capacidad de explicación semejante, al modelo con una estructura más compleja (más términos).

Los parámetros estimados finalmente, correspondientes a la mejor especificación, informarán si los efectos directos e interacciones o efectos conjuntos entre los distintos factores del modelo final tienen una influencia positiva o negativa sobre el valor de las diferentes frecuencias conjuntas de la tabla de contingencia, lo que informará no solo de si existe asociación entre factores o no; sino también de, en caso de existir, de cómo se materializa tal asociación.

Para ejemplificar la especificación, estimación e interpretación de los modelos log-lineales aplicados a tablas de contingencia, vamos a plantear un caso en el que entran en juego 3 factores o atributos, que caracterizan a un conjunto o muestra de 474 empresas eólicas. Estos factores o atributos son:

-   **DIMENSION:** tamaño del grupo empresarial al que pertenece la empresa en cuestión. Tiene tres niveles: grande, media y reducida.

-   **AUTOFINA:** capacidad de autofinanciación de la empresa a medio y largo plazo. Tiene tres niveles: alta, positiva y negativa.

-   **FJUR**: forma jurídica. Tiene dos posibles categorías: Sociedad anónima o Sociedad limitada.

Los datos se encuentran alojados en la hoja "Datos" del archivo de Microsoft® Excel® "[eolica_contingencia2.xlsx](https://docs.google.com/spreadsheets/d/1f0OQaxyzjKIIXrqhCFL1vIBNcLLGx6R4/edit?usp=sharing&ouid=115375878280465826079&rtpof=true&sd=true)". El código a desarrollar está disponible en el *script* "[loglineal_eolica.R](https://drive.google.com/file/d/1xMmM2nrSHBCMU1EEDePb2BRuv_TbD-7r/view?usp=sharing)". Puede desarrollarse el ejemplo creando para ello un proyecto de RStudio, por ejemplo, el proyecto "loglineal".

Comenzando a ejecutar el código presente en el *script*, la primera parte se dedica, como es habitual, a la gestión de los datos: borrado previo de memoria, importación de los datos alojados en el archivo de Excel y volcado a un *data frame*, corrección del nombre de las filas de este, y selección de las variables categóricas del estudio:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# Analisis de Asociacion y modelos log-lineales de eolicas
# Disculpen por la falta de tildes!

rm(list = ls())

# DATOS

  # Importando datos

    library (readxl)
    eolicas <- read_excel("eolica_contingencia2.xlsx", sheet ="Datos")
    eolicas <- data.frame(eolicas, row.names = 1)
    summary (eolicas)

  # Seleccionando factores/atributos para el analisis

    library(dplyr)
    originales2 <- eolicas %>%
    select(DIMENSION, AUTOFINA, FJUR)
    summary (originales2)
```

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# Analisis de Asociacion y modelos log-lineales de eolicas
# Disculpen por la falta de tildes!

rm(list = ls())

# DATOS

  # Importando datos

    library (readxl)
    eolicas <- read_excel("eolica_contingencia2.xlsx", sheet ="Datos")
    eolicas <- data.frame(eolicas, row.names = 1)
    summary (eolicas)

  # Seleccionando factores/atributos para el analisis

    library(dplyr)
    originales2 <- eolicas %>%
    select(DIMENSION, AUTOFINA, FJUR)
    summary (originales2)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# Analisis de Asociacion y modelos log-lineales de eolicas
# Disculpen por la falta de tildes!

rm(list = ls())

# DATOS

  # Importando datos

    library (readxl)
    eolicas <- read_excel("eolica_contingencia2.xlsx", sheet ="Datos")
    eolicas <- data.frame(eolicas, row.names = 1)
    library(dplyr)
df <- select(eolicas, everything())

# Número de variables por bloque
variables_por_bloque <- 3

# Dividir las variables en bloques
for (i in seq(1, ncol(df), variables_por_bloque)) {
  # Comentario: No mostramos el nombre del bloque
  print(summary(df[, i:min(i + variables_por_bloque - 1, ncol(df))]))
  cat("\n")
}
rm (i)
rm (variables_por_bloque)
rm (df)
```

El *data frame* "originales2" albergará los tres factores o atributos del análisis:

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
  # Seleccionando factores/atributos para el analisis

    library(dplyr)
    originales2 <- eolicas %>%
    select(DIMENSION, AUTOFINA, FJUR)
    summary (originales2)
```

Posteriormente se desarrolla la localización y tratamiento de *missing data*, ya que para realizar el análisis es necesario que todos los casos posean dato en todas las variables. Para tener una idea general, se puede recurrir a la función `vis_miss()` del paquete `{visdat}`, que localizará gráficamente los *missing values* de las diferentes variables, y calculará el porcentaje de casos que supone, con respecto al total de observaciones. En el ejemplo, los casos con missing data se concentran en el atributo o factor AUTOFINA, con 133 casos. El nombre concreto de los casos afectados se visualiza con un filtro construido a partir de la función `filter()` del paquete `{dplyr}`:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Identificando missing values.

    library(visdat)
    vis_miss(originales2)
    originales2 %>% filter(is.na(DIMENSION) |
                           is.na(AUTOFINA)|
                           is.na(FJUR)) %>%
    select(DIMENSION, AUTOFINA, FJUR)  
```

Se decide eliminar los casos que carecen de valor en el atributo AUTOFINA, lo que se realiza mediante otro filtro:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
    originales2 <- originales2 %>%
  filter(! is.na(DIMENSION) &
           ! is.na(AUTOFINA) &
           ! is.na(FJUR)) 
```

El siguiente bloque se ocupa de crear el objeto "table" que recoge la tabla de contingencia formada al cruzar la distribución de las frecuencias o casos entre las diferentes categorías de los distintos factores. La función para convertir el *data frame* en una estructura de almacenamiento de datos especial llamada table (que es la **tabla de contingencia**) es precisamente `table()`. La tabla de contingencia construida, de nombre "tab.originales2", es posteriormente presentada como una tabla diseñada a partir de la función `kable()` del paquete `{knitr}`, y otras funciones incluidas en el paquete `{kableExtra}`:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
# TABLA DE CONTINGENCIA

  tab.originales2 <- table(originales2)

  library(knitr)
  library(kableExtra)
  knitr.table.format = "html"

  tab.originales2 %>%
    kable(format = knitr.table.format,
         caption="Empresas eólicas") %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped", "bordered", "condensed",
                  position = "center",
                  font_size = 12) %>%
    row_spec(0, bold= T, align = "c")

```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# TABLA DE CONTINGENCIA

  tab.originales2 <- table(originales2)

  library(knitr)
  library(kableExtra)
  knitr.table.format = "html"
  
   tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

      if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") { 
  tab.originales2 %>%
    kable(caption="Empresas eólicas") %>%
    kable_styling(full_width = F,
                  bootstrap_options = "striped", "bordered", "condensed",
                  position = "center",
                  font_size = 12) %>%
    row_spec(0, bold= T, align = "c")
      }else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
      tab.originales2 %>%
    kable(caption="Empresas eólicas")
      }
```

Un modo visual de obtener una primera idea de las relaciones que se incluyen en la tabla es construir un gráfico de mosaico, con la función `mosaic()` del paquete `{vcd}`, en la que el área de los diferentes rectángulos sea proporcional a la frecuencia conjunta correspondiente:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Representación gráfica de la tabla con mosaico

    library (vcd)
   mosaic(tab.originales2,
          main = "Eólicas: Dimensión Matriz y Valoración Expertos.",
          shade = T,
          gp = shading_Marimekko(tab.originales2),
          main_gp = gpar(fontsize = 14),
          sub_gp = gpar(fontsize = 12),
          labeling_args = list(gp_labels = gpar(fontsize = 7)))
```

En cuanto a las frecuencias marginales de cada nivel o categoría de los tres atributos o factores, pueden representarse estas mediante gráficos de barras, generados mediante las funciones del paquete `{ggplot2}`, y reunidos en una sola imagen mediante el paquete `{patchwork}`. El código es el siguiente:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Representando frecuencias de categorias en factores

    library (ggplot2)
    library (patchwork)

    g1b <- ggplot(originales2, map= aes(x= DIMENSION,
                                        fill = DIMENSION)) +
    geom_bar() +
    ggtitle("Dimensión del grupo empresarial",
            subtitle = "Empresas eólicas") + 
    ylab("Frecuencias") +
    xlab("Dimensión") 

    g2b <- ggplot(originales2, map= aes(x= AUTOFINA,
                                        fill = AUTOFINA)) +
    geom_bar() +
    ggtitle("Valoración de expertos",
            subtitle = "Empresas eólicas") + 
    ylab("Frecuencias") +
    xlab("Valoración") 

    g3b <- ggplot(originales2, map= aes(x= FJUR,
                                        fill = FJUR)) +
    geom_bar() +
    ggtitle("Forma Jurídica",
            subtitle = "Empresas eólicas") + 
    ylab("Frecuencias") +
    xlab("Forma Jurídica") 

    (g1b / g2b / g3b) + plot_annotation(title = "Frecuencias Marginales.",
                      theme = theme(plot.title = element_text(size = 12)))
```

Los modelos log-lineales se especifican y estiman mediante la función `loglm()` de la librería `{MASS}` (el paquete MASS lo hemos activado junto a la propia función loglm() ya que, de hacerlo con la función `library()`, se crea un conflicto con la función `select()` del paquete `{dplyr}`).

Cuando se estiman los modelos, los diferentes resultados se almacenan en una lista. Entre estos elementos se encuentran los parámetros estimados, que, dentro de la lista de resultados, se almacenan, a su vez, en una estructura que, según la especificación que se elija del modelo, puede llegar a ser compleja. para facilitar el trabajo de extracción de los parámetros y almacenamiento en un data frame, se ha desarrollado una función denominada `extraer_coeficientes()`. Debido a la complejidad del código, y antes de proceder a estimar los modelos, exponemos el código de la función, que recibe como input o argumento el nombre de un modelo log-lineal estimado, y devuelve como output un data frame con los coeficientes o parámetros estimados:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# MODELOS LOG-LINEALES

# Función para extraer coeficientes y sus nombres de un modelo ##########
extraer_coeficientes <- function(modelo) {
  # Extraer los parámetros del modelo
  parametros <- modelo$param
  
  # Crear listas para almacenar nombres y valores de coeficientes
  coef_names <- c()
  coef_values <- c()
  
  # Función para generar nombres de coeficientes
  generate_coef_name <- function(levels) {
    return(paste(levels, collapse = ":"))
  }
  
  # Recorrer los coeficientes y extraer los nombres y valores
  for (term in names(parametros)) {
    if (term == "(Intercept)") {
      coef_names <- c(coef_names, "T. Independiente")
      coef_values <- c(coef_values, parametros[[term]])
    } else if (is.matrix(parametros[[term]])) {
      # Si es una matriz, recorrer filas y columnas
      for (i in 1:nrow(parametros[[term]])) {
        for (j in 1:ncol(parametros[[term]])) {
          coef_names <- c(coef_names,
                          paste(rownames(parametros[[term]])[i],
                                colnames(parametros[[term]])[j],
                                sep = ":"))
          coef_values <- c(coef_values, parametros[[term]][i, j])
        }
      }
    } else if (is.array(parametros[[term]])) {
      # Si es un array de más de dos dimensiones
      dims <- dim(parametros[[term]])
      dimnames_list <- dimnames(parametros[[term]])
      for (i in seq_len(dims[1])) {
        for (j in seq_len(dims[2])) {
          for (k in seq_len(dims[3])) {
            coef_name <- paste(dimnames_list[[1]][i],
                               dimnames_list[[2]][j],
                               dimnames_list[[3]][k],
                               sep = ":")
            coef_names <- c(coef_names, coef_name)
            coef_values <- c(coef_values,
                             parametros[[term]][i, j, k])
          }
        }
      }
    } else {
      levels <- names(parametros[[term]])
      for (level in levels) {
        coef_names <- c(coef_names,
                        generate_coef_name(c(term, level)))
        coef_values <- c(coef_values,
                         parametros[[term]][[level]])
      }
    }
  }
  
  # Verificar la longitud de los vectores antes de crear el data frame
  if (length(coef_names) == length(coef_values)) {
    tabla_coeficientes <- data.frame(Coefficient = coef_names,
                                     Value = coef_values,
                                     stringsAsFactors = FALSE)
    return(tabla_coeficientes)
  } else {
    stop("Error: Las longitudes de coef_names y coef_values no coinciden.")
  }
}
############################################################################   

```

Del mismo modo, se ha creado otra función para facilitar la pesentación de la información más importante que se genera con la estimación del modelo loglineal. Es la función `generar_solucion()`. Esta función recibe como argumento o *intput* el nombre del modelo estimado, y devuelve tres elementos: dos tablas diseñadas con `kable()`, que se almacenan en una lista, y un gráfico de *mosaico* que recoge los **residuos del modelo** (diferencias entre las frecuencias conjuntas reales u observadas de la tabla de contingencia, y sus estimaciones por parte del modelo). La primera de las tablas recoge las pruebas de validez de los modelos del *ratio de verosimilitud* (`deviance`), y de *Pearson*: nombre de la prueba, estadístico del contraste, grados de libertad y *p-valor*. La segunda tabla recoge los coeficientes o parámetros estimados, para lo cual la función llama, a su vez, a la función extraer_coeficientes(), toma el data frame que construya tal función, y la usa como base para diseñar la segunda tabla.

El código de la función es el siguiente:

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
############################################################################
# Función generar tablas y gráfico a partir de modelo log-lineal

generar_solucion <- function(modelo) {
  # Extraer la información del modelo
  summary_modelo <- summary(modelo)
  
  # Crear una tabla con la información relevante de las pruebas de validez
  tabla_informacion <- data.frame(
    Statistic = c("Likelihood Ratio", "Pearson"),
    X2 = c(summary_modelo$tests[1, "X^2"], summary_modelo$tests[2, "X^2"]),
    df = c(summary_modelo$tests[1, "df"], summary_modelo$tests[2, "df"]),
    P_value = c(summary_modelo$tests[1, "P(> X^2)"], summary_modelo$tests[2, "P(> X^2)"]),
    stringsAsFactors = FALSE
  )
  
  # Formatear la tabla usando kable
  independencia_valida_tab <- tabla_informacion %>%
    kable(caption ="Validación del modelo",
          format = "html",
          col.names = c("Prueba", "Estadístico", "Grados Libertad", "P-valor")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = c("striped", "bordered", "condensed"),
                  position = "center",
                  font_size = 12) %>%
    row_spec(0, bold= T, align = "c")
  
  # Extraer los coeficientes del modelo
  independencia_df <- extraer_coeficientes(modelo)
  
  # Formatear la tabla de coeficientes usando kable
  independencia_coef_tab <- independencia_df %>%
    kable(format = "html",
          caption ="Coeficientes del modelo",
          digits = 3) %>%
    kable_styling(full_width = F,
                  bootstrap_options = c("striped", "bordered", "condensed"),
                  position = "center",
                  font_size = 12) %>%
    row_spec(0, bold= T, align = "c")
  
  # Crear el gráfico de mosaico con los residuos y mostrarlo en pantalla directamente
  plot(modelo, panel = mosaic,
       main="Residuos del modelo",
       residuals_type = c("deviance"),
       gp = shading_hcl,
       gp_args = list(interpolate = c(0, 1)),
       main_gp = gpar(fontsize = 14),
       sub_gp = gpar(fontsize = 9),
       labeling_args = list(gp_labels = gpar(fontsize = 7)))
  
  # Guardar las tablas en una lista
  solucion_nombre <- paste0("solucion_", deparse(substitute(modelo)))
  solucion_lista <- list(
    Informacion = independencia_valida_tab,
    Coeficientes = independencia_coef_tab
  )
  
  assign(solucion_nombre, solucion_lista, envir = .GlobalEnv)
}
##########################################################################

```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
############################################################################
# Función generar tablas y gráfico a partir de modelo log-lineal

generar_solucion <- function(modelo) {
  # Extraer la información del modelo
  summary_modelo <- summary(modelo)

tipo_output <- c("html") # pdf, html, docx
knitr::opts_knit$set(rmarkdown.pandoc.to = tipo_output)

  # Crear una tabla con la información relevante de las pruebas de validez
  tabla_informacion <- data.frame(
    Statistic = c("Likelihood Ratio", "Pearson"),
    X2 = c(summary_modelo$tests[1, "X^2"], summary_modelo$tests[2, "X^2"]),
    df = c(summary_modelo$tests[1, "df"], summary_modelo$tests[2, "df"]),
    P_value = c(summary_modelo$tests[1, "P(> X^2)"], summary_modelo$tests[2, "P(> X^2)"]),
    stringsAsFactors = FALSE
  )
  
  # Formatear la tabla usando kable
      if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
    independencia_valida_tab <- tabla_informacion %>%
    kable(caption ="Validación del modelo",
          format = "html",
          col.names = c("Prueba", "Estadístico", "Grados Libertad", "P-valor")) %>%
    kable_styling(full_width = F,
                  bootstrap_options = c("striped", "bordered", "condensed"),
                  position = "center",
                  font_size = 12) %>%
    row_spec(0, bold= T, align = "c")
      }else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
           independencia_valida_tab <- tabla_informacion %>%
             kable(caption ="Validación del modelo",
          format = "html",
          col.names = c("Prueba", "Estadístico", "Grados Libertad", "P-valor")) 
      }  
  # Extraer los coeficientes del modelo
  independencia_df <- extraer_coeficientes(modelo)
  
  # Formatear la tabla de coeficientes usando kable
        if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
  independencia_coef_tab <- independencia_df %>%
    kable(format = "html",
          caption ="Coeficientes del modelo",
          digits = 3) %>%
    kable_styling(full_width = F,
                  bootstrap_options = c("striped", "bordered", "condensed"),
                  position = "center",
                  font_size = 12) %>%
    row_spec(0, bold= T, align = "c")
        }else if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
  independencia_coef_tab <- independencia_df %>%
    kable(format = "html",
          caption ="Coeficientes del modelo",
          digits = 3)
        }
  # Crear el gráfico de mosaico con los residuos y mostrarlo en pantalla directamente
  plot(modelo, panel = mosaic,
       main="Residuos del modelo",
       residuals_type = c("deviance"),
       gp = shading_hcl,
       gp_args = list(interpolate = c(0, 1)),
       main_gp = gpar(fontsize = 14),
       sub_gp = gpar(fontsize = 9),
       labeling_args = list(gp_labels = gpar(fontsize = 7)))
  
  # Guardar las tablas en una lista
  solucion_nombre <- paste0("solucion_", deparse(substitute(modelo)))
  solucion_lista <- list(
    Informacion = independencia_valida_tab,
    Coeficientes = independencia_coef_tab
  )
  
  assign(solucion_nombre, solucion_lista, envir = .GlobalEnv)
}
##########################################################################

```

Con el par de funciones anteriores, es fácil estimar un modelo log-lineal aplicado a tablas de contingencia y recopilar la información más relevante.

Comenzaremos por el **modelo de independencia**, que plantea que solo existen efectos directos en la determinación de las frecuencias conjuntas de la tabla (no-asociación entre factores). La especificación y estimación del modelo es la siguiente:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Modelo Independencia.

    modelo_indep <- MASS::loglm(~ DIMENSION + AUTOFINA + FJUR,
                                data= tab.originales2)

    generar_solucion(modelo_indep)

```

Puede comprobarse que se ha creado la lista "solucion_modelo_indep", que guarda dos elementos (\$Informacion y \$Coeficientes), y se ha generado el gráfico de mosaico de los residuos. Cuanto más intensos son los colores, mayores serán los residuos (en valor absoluto) y, por lo tanto, peor será el ajuste obtenido, lo que se deberá a que se ha considerado (erróneamente) que no hay interacción entre los factores (no asociación o independencia). En cuanto a los tonos de color:

-   **Rectángulos azulados**: Indican que la frecuencia observada es mayor que la frecuencia estimada por el modelo. Es decir, el modelo subestima la frecuencia observada.

-   **Rectángulos rojizos**: Indican que la frecuencia observada es menor que la frecuencia estimada por el modelo. Es decir, el modelo sobreestima la frecuencia observada.

En nuestro caso, existen frecuencias en los que los residuos toman colores bastante intensos, lo que hace pensar en que no se ha producido un buen ajuste. Como este modelo planteaba un escenario en el que no hay asociación entre los atributos; el gráfico parece apoyar la hipótesis de que sí existe asociación, al menos entre algunos de los factores o atributos.

Vamos a interpretar ahora el primer elemento de la lista "solucion_modelo_indep", que es la tabla donde se disponen las pruebas de validez del modelo. Para obtener la tabla ejecutaremos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
    solucion_modelo_indep$Informacion
```

En ambas pruebas (ratio de verosimilitud y Pearson) se obtiene un p-valor de 0. Teniendo en cuenta que la hipótesis nula de ambas pruebas es que existe un buen ajuste; la conclusión es que, según los resultados, **el modelo de independencia no es capaz de representar la realidad con suficiente precisión** (de ahí la magnitud de los residuos). Esto se puede interpretar, a su vez, como que **se rechaza la hipótesis de independencia entre los factores** y se admite que existe asociación entre, al menos, algunos de ellos.

Por último, vamos a mostrar el segundo elemento de la lista "solucion_modelo_indep", que es la tabla donde se disponen los parámetros estimados del modelo, que en este caso corresponden solo a efectos directos o principales. Para obtener la tabla ejecutaremos:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
    solucion_modelo_indep$Coeficientes
```

ssage=FALSE, warning=FALSE}

Puede destacarse el hecho de que, que la capacidad de autofinanciación sea negativa, tiende a provocar una reducción del número de casos en las frecuencias conjuntas implicadas (signo negativo) También el hecho de que la forma jurídica adoptada sea la de *Sociedad Anónima*. En el extremo opuesto, destacan los signos positivos de la forma jurídica de *Sociedad Limitada* y la capacidad de autofinanciación alta, lo que implica que estas categorías tienden a acumular más frecuencias.

Pasamos ahora al **modelo saturado**, en el cuál se especifican todos los efectos directos o principales de los factores, y todas las interacciones posibles entre dichos factores (interacciones dos a dos, y la interacción entre los tres factores de modo simultáneo). Este modelo, en la práctica, no es relevante porque, aunque explica al 100% las frecuencias observadas, no indica cuál de los niveles de los factores o atributos y sus interacciones son los más relevantes (modelo redundante). Para su estimación, simplemente se sustituyen los signos “+” del modelo anterior por los signos “\*”. El modelo se denominará "modelo_sat":

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Modelo Saturado.

    modelo_sat <- MASS::loglm(~ DIMENSION * AUTOFINA * FJUR,
                              data= tab.originales2)

    generar_solucion(modelo_sat)
```

El gráfico de mosaico muestra que, obviamente, todos los residuos son 0, ya que coinciden las frecuencias observadas y las estimadas por el modelo. En cuanto a la tabla de validación:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
    solucion_modelo_sat$Informacion
```

En ambas pruebas el p-valor es 1, dado que no se rechaza la hipótesis de que el modelo representa adecuadamente la realidad (de hecho, la representa perfectamente). Pero, como hemos dicho, desde el punto de vista del análisis estructural el modelo saturado no es muy útil, ya que no distingue entre los efectos e interacciones relevantes y los que son poco importantes. Por último, mostraremos la tabla con los coeficientes estimados, correspondientes tanto a los efectos principales o directos como a las interacciones:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
    solucion_modelo_sat$Coeficientes
```

La estimación del modelo saturado permite aplicar algún algoritmo para la obtención de la especificación de un modelo que, sin llegar a ser el saturado, **asegure una representación valida de la realidad obviando los efectos e interacciones que no sean estadísticamente relevantes**. Por ejemplo, un método es el *step / backward*, que, en función del *Criterio de Información de Akaike* (AIC), irá probando a estimar especificaciones más simples que disminuyan el AIC (lo que implica una mejor especificación). En nuestro caso, se aplicará con el código:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
  # Elección del modelo final.

    modelo_def <- step(modelo_sat, scale = 0,
                       direction = c("backward"),
                       trace = 1, steps = 1000)
```

En el código anterior, se genera el modelo óptimo siguiendo el proceso de eliminar los términos del modelo que más contribuyan a la reducción del valor de AIC, mediante la función `step()`. El modelo así obtenido se guarda como "modelo_def".

En nuestro ejemplo, la especificación del modelo definitivo incluye los tres efectos directos o principales de los factores o atributos, y todas las interacciones entre pares de factores; es decir, se diferencia del modelo saturado en la no inclusión de la interacción entre los tres factores o atributos de modo simultáneo.

Luego, se pasa el modelo a la función `generar_solución()` para obtener los resultados (el gráfico de mosaico y las dos tablas). El gráfico obtenido es:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
    generar_solucion(modelo_def)
```

Los tonos grisáceos y pardos indican que existen leves diferencias entre las frecuencias conjuntas observadas, y las estimadas por el modelo. En cuanto a la tabla de validación:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
    solucion_modelo_def$Informacion
```

Las dos pruebas muestran un *p-valor* superior a 0,05, lo que implica, para ese nivel de significación, el **no rechazo de la hipótesis nula de validez o idoneidad del modelo** para representar la realidad adecuadamente. En cuanto a los coeficientes estimados del modelo:

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
    solucion_modelo_def$Coeficientes
```

En la tabla anterior detaca, dentro de los efectos directos o principales, el coeficiente negativo de la capacidad de autofinanciación negativa y el de forma jurídica de *Sociedad Anónima* como principales categorías que influyen en la disminución de las frecuencias conjuntas implicadas. En el extremo opuesto se encuentra el coeficiente correspondiente a la forma jurídica de *Sociedad Limitada*. En cuanto a las interacciones entre factores, se comprueba que la simultaneidad entre una dimensión de la matriz grande y una capacidad de autofinanciación negativa parece influir en una reducción significativa de las frecuencias conjuntas implicadas. Lo mismo ocurre con la simultaneidad entre una dimensión de la empresa matriz media y una capacidad de autofinanciación alta, y entre una dimensión de la empresa matriz reducida y la forma jurídica de *Sociedad Anónima*. En el extremo opuesto, hay ciertas interacciones que parecen inluir en que las frecuencias conjuntas implicadas aumenten, como por ejemplo una dimensión de la matriz reducida y una capacidad de autofinanciación negativa, o de nuevo una dimensión de la matriz empresarial reducida con una forma jurídica de *Sociedad Limitada*.

## Materiales para realizar las prácticas del capítulo.

En esta sección se muestran los links de acceso a los diferentes materiales (*scripts*, datos...) necesarios para llevar a cabo los contenidos prácticos del capítulo.

**Datos (en formato Microsoft (R) Excel (R)):**

-   eolica_contingencia.xlsx ([obtener aquí](https://docs.google.com/spreadsheets/d/1N5QIRKbm6OKAOaN8DB2YFDptpZ_0pbPc/edit?usp=sharing&ouid=115375878280465826079&rtpof=true&sd=true))

-   eolica_contingencia2.xlsx ([obtener aquí](https://docs.google.com/spreadsheets/d/1f0OQaxyzjKIIXrqhCFL1vIBNcLLGx6R4/edit?usp=sharing&ouid=115375878280465826079&rtpof=true&sd=true))

-   **Scripts:**

-   correspondencias_eolica.R [(](https://drive.google.com/file/d/1p4p17OudRprwkpF7wvvF1gwHZG82uv89/view?usp=sharing)[obtener aquí](https://drive.google.com/file/d/1u8gJa3jQH2cwuQ8KsISQ9esHKrFAO_au/view?usp=sharing))

-   loglineal_eolica.R ([obtener aquí](https://drive.google.com/file/d/1xMmM2nrSHBCMU1EEDePb2BRuv_TbD-7r/view?usp=sharing))

<!--chapter:end:10-Analisis_datos_cualitativos.Rmd-->

`r if (knitr:::is_html_output()) '
# Bibliografía {-}
'`

<!--chapter:end:11-references.Rmd-->

